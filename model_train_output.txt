You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/daredevil/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1/5: 100%|██████████| 439/439 [2:30:17<00:00, 20.54s/it]  
Epoch 2/5: 100%|██████████| 439/439 [3:25:53<00:00, 28.14s/it]  
Epoch 3/5: 100%|██████████| 439/439 [3:12:39<00:00, 26.33s/it]  
Epoch 4/5: 100%|██████████| 439/439 [3:22:07<00:00, 27.63s/it]  
Epoch 5/5: 100%|██████████| 439/439 [3:19:21<00:00, 27.25s/it]  
Evaluating on Test Dataset: 100%|██████████| 110/110 [16:06<00:00,  8.79s/it]
