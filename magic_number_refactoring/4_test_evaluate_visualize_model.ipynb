{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Model\n",
    "2. Test Model\n",
    "3. Print Raw Results i.e generated refactorings\n",
    "4. CodeBLEU Evaluation\n",
    "5. ROGUE1, ROGUE2 and ROGUE-LCS Evaluation\n",
    "6. METEOR Evaluation\n",
    "7. Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model - used for training and testing in different sessions\n",
    "\n",
    "# Run prep cell first\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "model = T5ForConditionalGeneration.from_pretrained('magic_smell_model_s_3lines_700_e40_b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "model.eval()\n",
    "all_references = []  # List to store reference sequences\n",
    "all_predictions = []  # List to store predicted sequences\n",
    "all_prediction_ids = []\n",
    "all_prediction_ids_labelled = []\n",
    "all_predictions_decoded = []\n",
    "all_predictions_decoded_labelled = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc='Evaluating on Test Dataset'):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        predicted_ids = model.generate(**inputs, max_length=512)\n",
    "        predicted_code = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "\n",
    "        # Append to reference and prediction lists\n",
    "        all_references.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted_code)\n",
    "\n",
    "        all_prediction_ids.extend(predicted_ids)\n",
    "        all_prediction_ids_labelled.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "        tokenized_predicted_code = [tokenizer.encode_plus(code, return_tensors='pt', padding='max_length', truncation=True, max_length=512) for code in predicted_code]\n",
    "        all_predictions_decoded.extend(tokenized_predicted_code)\n",
    "        labels_predicted = torch.stack([item['input_ids'].squeeze() for item in tokenized_predicted_code])\n",
    "        # all_predictions_decoded_labelled.extend(labels_predicted.cpu.numpy())\n",
    "        all_predictions_decoded_labelled.extend(labels_predicted.numpy())\n",
    "\n",
    "\n",
    "# Save the results to a text file\n",
    "with open('test_results.txt', 'w') as file:\n",
    "    for reference, prediction in zip(all_references, all_predictions):\n",
    "        file.write(f\"Reference: {reference}\\n\")\n",
    "        file.write(f\"Prediction: {prediction}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeBLEU Evaluation\n",
    "\n",
    "# !pip3 install sacrebleu\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "# Check if the lists are not empty\n",
    "if all_predictions and refactored_codes:\n",
    "    # Convert NumPy arrays to Python lists of strings\n",
    "    references = [str(ref) for ref in refactored_codes]\n",
    "    predictions = [str(pred) for pred in all_predictions]\n",
    "\n",
    "    # Calculate CodeBLEU\n",
    "    codebleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    print(f\"CodeBLEU: {codebleu.score}\")\n",
    "    print(refactored_codes)\n",
    "    print(all_predictions)\n",
    "else:\n",
    "    print(\"Error: Empty prediction or reference list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROGUE1, ROGUE2 and ROGUE-LCS Evaluation\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize lists to store individual ROUGE scores\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "# Iterate over refactored_codes and all_predictions\n",
    "for ref_code, pred_code in zip(refactored_codes, all_predictions):\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(ref_code, pred_code)\n",
    "    \n",
    "    # Append individual ROUGE scores\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Calculate mean ROUGE scores\n",
    "mean_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "mean_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "mean_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "# Print mean ROUGE scores\n",
    "print(\"Mean ROUGE-1:\", mean_rouge1)\n",
    "print(\"Mean ROUGE-2:\", mean_rouge2)\n",
    "print(\"Mean ROUGE-L:\", mean_rougeL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METEOR Evaluation\n",
    "\n",
    "import nltk\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Check if the lists are not empty\n",
    "if all_predictions and refactored_codes:\n",
    "    # Convert NumPy arrays to strings\n",
    "    hypothesis_strings = str(str(pred) for pred in all_predictions)\n",
    "\n",
    "    # Preprocess references by converting to strings\n",
    "    references_strings = []\n",
    "    for ref in refactored_codes:\n",
    "        # Convert each tokenized reference to a single string\n",
    "        ref_string = ' '.join([str(token) for token in ref])\n",
    "        references_strings.append(ref_string)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_avg_score = meteor_score.meteor_score(references_strings, hypothesis_strings)\n",
    "    print(f\"METEOR: {meteor_avg_score}\")\n",
    "else:\n",
    "    print(\"Error: Empty prediction or reference list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define metrics\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'METEOR']\n",
    "\n",
    "final_scores = [codebleu.score,  mean_rouge1,  mean_rouge2, mean_rougeL, meteor_avg_score]\n",
    "\n",
    "# Plotting final scores\n",
    "plt.bar(metrics, final_scores)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Final Evaluation Metrics')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
