{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparation for Training (Part 1/2)\n",
    "    --> Performing Data Split    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for Training\n",
    "\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your generated dataset\n",
    "# Replace 'your_dataset_path' with the actual path to your dataset\n",
    "dataset_path = 'data/analyzed_dataset_3_lines/analyzed_dataset_700.jsonl'\n",
    "\n",
    "# Load data from the JSONL file\n",
    "with open(dataset_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Extract input and target values\n",
    "magic_number_smells = [item['magic_number_smell'] for item in data]\n",
    "refactored_codes = [item['refactored_code'] for item in data]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_magic_number_smells, test_magic_number_smells, train_refactored_codes, test_refactored_codes = train_test_split(\n",
    "    magic_number_smells, refactored_codes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create dictionaries for training and testing datasets\n",
    "train_dataset = [{'magic_number_smell': magic_number_smell, 'refactored_code': refactored_code} for magic_number_smell, refactored_code in zip(train_magic_number_smells, train_refactored_codes)]\n",
    "test_dataset = [{'magic_number_smell': magic_number_smell, 'refactored_code': refactored_code} for magic_number_smell, refactored_code in zip(test_magic_number_smells, test_refactored_codes)]\n",
    "\n",
    "# Save the datasets to JSONL files\n",
    "train_file_path = 'data/train_dataset.jsonl'\n",
    "test_file_path = 'data/test_dataset.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preparation for Training (Part 2/2)\n",
    "    --> Initializing Tokenizer - CodeT5Tokenizer\n",
    "    --> Initializing Optimizer - AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "\n",
    "with open(train_file_path, 'w') as f:\n",
    "    for item in train_dataset:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open(test_file_path, 'w') as f:\n",
    "    for item in test_dataset:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        magic_number_smell = item['magic_number_smell']\n",
    "        refactored_code = item['refactored_code']\n",
    "\n",
    "        # Tokenize and convert to PyTorch tensors\n",
    "        inputs = self.tokenizer.encode_plus(magic_number_smell, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        targets = self.tokenizer.encode_plus(refactored_code, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze(),\n",
    "        }\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CodeDataset(train_dataset, tokenizer)\n",
    "test_dataset = CodeDataset(test_dataset, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loading onto processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print Expected Refactored Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Expected Refactored Code\n",
    "print(refactored_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "last_3_test_losses = []  # Track last 5 test losses for early stopping\n",
    "max_overfit_epochs = 3  # Maximum consecutive epochs for which test loss can increase before stopping\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40\n",
    "stop_training = False  # Flag to indicate if training should stop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs} (Training)'):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_losses.append(loss.item())\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs} (Testing)'):\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            epoch_test_losses.append(loss.item())\n",
    "\n",
    "    # Calculate average testing loss for the epoch\n",
    "    test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Print and/or log the training and testing losses for monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint_path = f'magic_smell_model_s_3lines_700_e40_b4_epoch_{epoch + 1}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Early stopping condition for same test losses\n",
    "    if len(last_3_test_losses) == 3:\n",
    "        if all(loss == last_3_test_losses[0] for loss in last_3_test_losses):\n",
    "            print(\"Early stopping: Test losses remained the same for 3 epochs.\")\n",
    "            stop_training = True\n",
    "            break\n",
    "        else:\n",
    "            last_3_test_losses.pop(0)\n",
    "    last_3_test_losses.append(test_loss)\n",
    "    \n",
    "    # Early stopping condition for overfitting\n",
    "    if epoch > 0 and test_loss > test_losses[-2]:\n",
    "        overfit_epochs += 1\n",
    "        if overfit_epochs >= max_overfit_epochs:\n",
    "            print(f\"Early stopping: Test loss increased continuously for {max_overfit_epochs} epochs.\")\n",
    "            stop_training = True\n",
    "            break\n",
    "    else:\n",
    "        overfit_epochs = 0\n",
    "\n",
    "\n",
    "    if stop_training:\n",
    "        break\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('magic_smell_model_s_3lines_700_e40_b4')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
