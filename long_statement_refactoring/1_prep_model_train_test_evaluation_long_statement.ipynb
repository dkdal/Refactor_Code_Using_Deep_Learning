{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python39\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\python39\\lib\\site-packages (from torch) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\python39\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\python39\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\python39\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\python39\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python39\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in c:\\python39\\lib\\site-packages (4.27.1)\n",
      "Requirement already satisfied: filelock in c:\\python39\\lib\\site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\python39\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python39\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python39\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python39\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\python39\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\python39\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python39\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.3.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python39\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: datasets in c:\\python39\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python39\\lib\\site-packages (from datasets) (1.24.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\python39\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\python39\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python39\\lib\\site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\python39\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\python39\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\python39\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\python39\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\python39\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\python39\\lib\\site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in c:\\python39\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\python39\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python39\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\python39\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python39\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\python39\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.3.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python39\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\python39\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "\n",
    "!pip3 install torch\n",
    "!pip3 install transformers\n",
    "!pip3 install datasets\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your generated dataset\n",
    "# Replace 'your_dataset_path' with the actual path to your dataset\n",
    "dataset_path = os.path.join(\"data\", \"output\", \"analyzed_dataset.jsonl\")\n",
    "\n",
    "# Load data from the JSONL file\n",
    "with open(dataset_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Extract input and target values\n",
    "magic_number_smells = [item['smelly_method'] for item in data]\n",
    "refactored_codes = [item['refactored_method'] for item in data]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_magic_number_smells, test_magic_number_smells, train_refactored_codes, test_refactored_codes = train_test_split(\n",
    "    magic_number_smells, refactored_codes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create dictionaries for training and testing datasets\n",
    "train_dataset = [{'smelly_method': magic_number_smell, 'refactored_method': refactored_code} for magic_number_smell, refactored_code in zip(train_magic_number_smells, train_refactored_codes)]\n",
    "test_dataset = [{'smelly_method': magic_number_smell, 'refactored_method': refactored_code} for magic_number_smell, refactored_code in zip(test_magic_number_smells, test_refactored_codes)]\n",
    "\n",
    "# Save the datasets to JSONL files\n",
    "train_file_path = os.path.join(\"data\", \"output\", \"train_dataset.jsonl\")\n",
    "test_file_path = os.path.join(\"data\", \"output\", \"test_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/grad/dkapoor/.local/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "\n",
    "with open(train_file_path, 'w') as f:\n",
    "    for item in train_dataset:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open(test_file_path, 'w') as f:\n",
    "    for item in test_dataset:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        magic_number_smell = item['smelly_method']\n",
    "        refactored_code = item['refactored_method']\n",
    "\n",
    "        # Tokenize and convert to PyTorch tensors\n",
    "        inputs = self.tokenizer.encode_plus(magic_number_smell, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        targets = self.tokenizer.encode_plus(refactored_code, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze(),\n",
    "        }\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CodeDataset(train_dataset, tokenizer)\n",
    "test_dataset = CodeDataset(test_dataset, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loading onto processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(refactored_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (Training): 100%|██████████| 649/649 [02:09<00:00,  5.01it/s]\n",
      "Epoch 1/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Train Loss: 1.4595735932718992, Test Loss: 0.8783085604021155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (Training): 100%|██████████| 649/649 [02:10<00:00,  4.97it/s]\n",
      "Epoch 2/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - Train Loss: 1.0019823566699064, Test Loss: 0.7731531580075895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.94it/s]\n",
      "Epoch 3/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - Train Loss: 0.8901902500120995, Test Loss: 0.7168742630379331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 4/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - Train Loss: 0.8202776252213171, Test Loss: 0.6807232545745885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 5/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 - Train Loss: 0.7665143865053018, Test Loss: 0.6453260033408557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 6/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 - Train Loss: 0.7245313483641952, Test Loss: 0.6240190339554672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 7/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 - Train Loss: 0.6913239232197014, Test Loss: 0.6077290217112179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 8/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 - Train Loss: 0.6613421696130594, Test Loss: 0.5883626748173515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 9/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 - Train Loss: 0.6371311685084745, Test Loss: 0.5726723449270418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 10/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 - Train Loss: 0.614061665025257, Test Loss: 0.5616125093281634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 11/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 - Train Loss: 0.5946795224385747, Test Loss: 0.5541911509139407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 12/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 - Train Loss: 0.5755154660376268, Test Loss: 0.5347572393898218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 13/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 - Train Loss: 0.5583253837720088, Test Loss: 0.5296598914439327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 14/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 - Train Loss: 0.5425424735723364, Test Loss: 0.5246012625655879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 15/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 - Train Loss: 0.5287774277647214, Test Loss: 0.5140836462378502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 16/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 - Train Loss: 0.5155478282752683, Test Loss: 0.5144376163056658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (Training): 100%|██████████| 649/649 [02:12<00:00,  4.92it/s]\n",
      "Epoch 17/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 - Train Loss: 0.5031121121647729, Test Loss: 0.5088249797150035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 18/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 - Train Loss: 0.49048111129707106, Test Loss: 0.5030873849119512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (Training): 100%|██████████| 649/649 [02:12<00:00,  4.90it/s]\n",
      "Epoch 19/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 - Train Loss: 0.47913846668596444, Test Loss: 0.49219620694046373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 20/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 - Train Loss: 0.46844678226347514, Test Loss: 0.4898755350178736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 21/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 - Train Loss: 0.4582563032767806, Test Loss: 0.4851780142292289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 22/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 - Train Loss: 0.4490939634629684, Test Loss: 0.48190328983898545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 23/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 - Train Loss: 0.43979258175210334, Test Loss: 0.4779836310565106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 24/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 - Train Loss: 0.4324033545970182, Test Loss: 0.47714214470306054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 25/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 - Train Loss: 0.4235659474377823, Test Loss: 0.47633261011879136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 26/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 - Train Loss: 0.4151802334951693, Test Loss: 0.46381468079024296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 27/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 - Train Loss: 0.40674388674138046, Test Loss: 0.46125941447045177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 28/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 - Train Loss: 0.399975780323033, Test Loss: 0.4679074831809734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 29/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 - Train Loss: 0.3918633484360525, Test Loss: 0.4601232093398922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 30/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 - Train Loss: 0.38589595140564975, Test Loss: 0.45669318097957806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.92it/s]\n",
      "Epoch 31/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 - Train Loss: 0.3793672076003118, Test Loss: 0.4554047671609495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 32/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 - Train Loss: 0.37279577085458443, Test Loss: 0.4530424312945524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 33/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 - Train Loss: 0.3657950102846685, Test Loss: 0.44660019956849106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 34/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 - Train Loss: 0.3595310024750664, Test Loss: 0.4528623762671933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 35/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 - Train Loss: 0.3541719647316701, Test Loss: 0.4464447839311296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 36/40 (Testing): 100%|██████████| 163/163 [00:12<00:00, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 - Train Loss: 0.3475037336820171, Test Loss: 0.4514129817302973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 37/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 - Train Loss: 0.34259276743547756, Test Loss: 0.4492680840308498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 38/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 - Train Loss: 0.33700517755007525, Test Loss: 0.44107511380195985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 (Training): 100%|██████████| 649/649 [02:12<00:00,  4.91it/s]\n",
      "Epoch 39/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 - Train Loss: 0.33143303630577947, Test Loss: 0.4439556756816759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 (Training): 100%|██████████| 649/649 [02:11<00:00,  4.93it/s]\n",
      "Epoch 40/40 (Testing): 100%|██████████| 163/163 [00:11<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 - Train Loss: 0.32725741465121094, Test Loss: 0.44676733125526846\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "last_3_test_losses = []  # Track last 5 test losses for early stopping\n",
    "max_overfit_epochs = 3  # Maximum consecutive epochs for which test loss can increase before stopping\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40\n",
    "stop_training = False  # Flag to indicate if training should stop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs} (Training)'):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_losses.append(loss.item())\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs} (Testing)'):\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            epoch_test_losses.append(loss.item())\n",
    "\n",
    "    # Calculate average testing loss for the epoch\n",
    "    test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Print and/or log the training and testing losses for monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint_path = f'long_statement_s_3200_e40_b4_epoch_{epoch + 1}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Early stopping condition for same test losses\n",
    "    if len(last_3_test_losses) == 3:\n",
    "        if all(loss == last_3_test_losses[0] for loss in last_3_test_losses):\n",
    "            print(\"Early stopping: Test losses remained the same for 3 epochs.\")\n",
    "            stop_training = True\n",
    "            break\n",
    "        else:\n",
    "            last_3_test_losses.pop(0)\n",
    "    last_3_test_losses.append(test_loss)\n",
    "    \n",
    "    # Early stopping condition for overfitting\n",
    "    if epoch > 0 and test_loss > test_losses[-2]:\n",
    "        overfit_epochs += 1\n",
    "        if overfit_epochs >= max_overfit_epochs:\n",
    "            print(f\"Early stopping: Test loss increased continuously for {max_overfit_epochs} epochs.\")\n",
    "            stop_training = True\n",
    "            break\n",
    "    else:\n",
    "        overfit_epochs = 0\n",
    "\n",
    "\n",
    "    if stop_training:\n",
    "        break\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('long_statement_model_s_3200_e40_b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Dataset: 100%|██████████| 163/163 [08:08<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "model.eval()\n",
    "all_references = []  # List to store reference sequences\n",
    "all_predictions = []  # List to store predicted sequences\n",
    "all_prediction_ids = []\n",
    "all_prediction_ids_labelled = []\n",
    "all_predictions_decoded = []\n",
    "all_predictions_decoded_labelled = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc='Evaluating on Test Dataset'):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        predicted_ids = model.generate(**inputs, max_length=512)\n",
    "        predicted_code = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "\n",
    "        # Append to reference and prediction lists\n",
    "        all_references.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted_code)\n",
    "\n",
    "        all_prediction_ids.extend(predicted_ids)\n",
    "        all_prediction_ids_labelled.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "        tokenized_predicted_code = [tokenizer.encode_plus(code, return_tensors='pt', padding='max_length', truncation=True, max_length=512) for code in predicted_code]\n",
    "        all_predictions_decoded.extend(tokenized_predicted_code)\n",
    "        labels_predicted = torch.stack([item['input_ids'].squeeze() for item in tokenized_predicted_code])\n",
    "        # all_predictions_decoded_labelled.extend(labels_predicted.cpu.numpy())\n",
    "        all_predictions_decoded_labelled.extend(labels_predicted.numpy())\n",
    "\n",
    "\n",
    "# Save the results to a text file\n",
    "with open('test_results.txt', 'w') as file:\n",
    "    for reference, prediction in zip(all_references, all_predictions):\n",
    "        file.write(f\"Reference: {reference}\\n\")\n",
    "        file.write(f\"Prediction: {prediction}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Assuming refactored_codes is a list of strings containing code snippets\n",
    "\n",
    "# # Remove tabs and newline characters\n",
    "# refactored_codes_cleaned = [code.replace('  ', '').replace('\\n', '') for code in refactored_codes]\n",
    "\n",
    "# # Print the cleaned refactored codes\n",
    "# for code in refactored_codes_cleaned:\n",
    "#     print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sacrebleu\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "# Check if the lists are not empty\n",
    "if all_predictions and refactored_codes:\n",
    "    # Convert NumPy arrays to Python lists of strings\n",
    "    references = [str(ref) for ref in refactored_codes]\n",
    "    predictions = [str(pred) for pred in all_predictions]\n",
    "\n",
    "    # Calculate CodeBLEU\n",
    "    codebleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    print(f\"CodeBLEU: {codebleu.score}\")\n",
    "#     print(refactored_codes)\n",
    "#     print(all_predictions)\n",
    "else:\n",
    "    print(\"Error: Empty prediction or reference list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROUGE-1: 0.07171566405311433\n",
      "Mean ROUGE-2: 0.0043182830912395056\n",
      "Mean ROUGE-L: 0.057856933812938084\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize lists to store individual ROUGE scores\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "# Iterate over refactored_codes and all_predictions\n",
    "for ref_code, pred_code in zip(refactored_codes, all_predictions):\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(ref_code, pred_code)\n",
    "    \n",
    "    # Append individual ROUGE scores\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Calculate mean ROUGE scores\n",
    "mean_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "mean_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "mean_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "# Print mean ROUGE scores\n",
    "print(\"Mean ROUGE-1:\", mean_rouge1)\n",
    "print(\"Mean ROUGE-2:\", mean_rouge2)\n",
    "print(\"Mean ROUGE-L:\", mean_rougeL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Check if the lists are not empty\n",
    "if all_predictions and refactored_codes:\n",
    "    # Convert NumPy arrays to strings\n",
    "    hypothesis_strings = [str(pred) for pred in all_predictions]\n",
    "\n",
    "    # Preprocess references by converting to strings\n",
    "    references_strings = []\n",
    "    for ref in refactored_codes:\n",
    "        # Convert each tokenized reference to a single string\n",
    "        ref_string = ' '.join([str(token) for token in ref])\n",
    "        references_strings.append(ref_string)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_avg_score = meteor_score.meteor_score(references_strings, hypothesis_strings)\n",
    "    print(f\"METEOR: {meteor_avg_score}\")\n",
    "else:\n",
    "    print(\"Error: Empty prediction or reference list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/grad/dkapoor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Check if the lists are not empty\n",
    "if all_predictions and refactored_codes:\n",
    "    # Convert NumPy arrays to strings\n",
    "    hypothesis_strings = [str(pred) for pred in all_predictions]\n",
    "\n",
    "    # Preprocess references by converting to strings\n",
    "    references_strings = []\n",
    "    for ref in refactored_codes:\n",
    "        # Convert each tokenized reference to a single string\n",
    "        ref_string = ' '.join([str(token) for token in ref])\n",
    "        references_strings.append(ref_string)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_avg_score = meteor_score.meteor_score(references_strings, hypothesis_strings)\n",
    "    print(f\"METEOR: {meteor_avg_score}\")\n",
    "else:\n",
    "    print(\"Error: Empty prediction or reference list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['protected void assertSelection(Uri uri, ContentValues values, String idColumn, long id)  if (USE_LEGACY_PROVIDER)  // A bug in the legacy ContactsProvider prevents us from using the // _id column in selection. super.assertSelection(uri, values, null, 0);  else  values.put(idColumn, idColumn, id); super.assertSelection(uri, values, idColumn, id);', 'public void executeAsync(final InternalConnection connection, final SingleResultCallbackBulkWriteResult> callback)  SingleResultCallbackBulkWriteResult> errHandlingCallback = errorHandlingCallback(callback, connection.getDescription()); executeBatchesAsync(connection, createRequestMessage(getMessageSettings(connection.getDescription())), new BulkWriteBatchCombiner(connection.getDescription().getServerAddress(), ordered, writeConcern), 0, 0, callback);', 'public VersionResult resolveVersion( RepositorySystemSession session, VersionRequest request ) throws VersionResolutionException  RequestTrace trace = RequestTrace.newChild( request.getTrace(), request ); Artifact artifact = request.getArtifact(); String version = artifact.getVersion(); VersionResult result = new VersionResult( request ); Key cacheKey = null; RepositoryCache cache = session.getCache(); if ( cache!= null &&!ConfigUtils.getBoolean( session, false, \"aether.versionResolver.noCache\" ) )  cacheKey = new Key( session, request ); Object obj = cache.get( session, cacheKey ); if ( obj instanceof Record )  Record record = (Record) obj; result.setVersion( record.version ); result.setRepository( CacheUtils.getRepository( session, request.getRepositories(), record.repoClass, record.repoId ) ); return result;   Metadata metadata; if ( RELEASE.equals( version ) )  metadata = new DefaultMetadata( artifact.getGroupId(), artifact.getArtifactId(), MAVEN_METADATA_XML, Metadata.Nature.RELEASE );  else if ( LATEST.equals( version ) )  metadata = new DefaultMetadata( artifact.getGroupId(), artifact.getArtifactId(), MAVEN_METADATA_XML, Metadata.Nature.RELEASE_OR_SNAPSHOT );  else if ( version.endsWith( SNAPSHOT ) )  WorkspaceReader workspace = session.getWorkspaceReader(); if ( workspace!= null && workspace.findVersions( artifact ).contains( version ) )', 'private void runNewParser(String pageContent) throws Exception  TestRoot testRoot = new TestRoot(); String result = ParserTest.translateTo(new TestRoot().makePage(\"NewTest\"), pageContent); System.out.println(System.currentTimeMillis() - start); //System.out.println(result); assertEquals(\"done\", \"done\");', 'public void testFetchPartitionPrimaryEntries()  HashMapInteger, ListInteger>> partitionIds = new HashMapInteger, ListInteger>>(); partitionIds.put(0, Arrays.asList(0, 3)); IteratorPairByteArray, Versionedbyte[]>>> entriesItr = adminClient.bulkFetchOps.fetchEntries(0, testStoreName, partitionToPartitionList, null, false, cluster, 0); // gather all the keys obtained SetString> fetchedKeys = getEntries(entriesItr); // make sure it fetched all the entries from the partitions requested SetString> partition0Keys = new HashSetString>(partitionToKeysMap.get(0)); SetString> partition3Keys = new HashSetString>(partitionToKeysMap.get(3)); partition0Keys.removeAll(fetchedKeys); partition3Keys.removeAll(fetchedKeys); assertEquals(\"Remainder in partition 0\" + partition0Keys, 0, partition0Keys.size()); assertEquals(\"Remainder in partition 3\" + partition3Keys, 0, partition3Keys.size());', 'private SimpleProblemCollector validateRaw( String pom, int level ) throws Exception  ModelBuildingRequest request = new DefaultModelBuildingRequest().setValidationLevel( level ); SimpleProblemCollector problems = new SimpleProblemCollector( read( pom ) ); Validator.validateRawModel( problems.getModel(), request, problems ); return problems;', 'private void writeCommandPrologue(final BsonBinaryWriter writer)  writer.writeString(getCommandName(), getWriteNamespace().getCollectionName()); writer.writeBoolean(\"ordered\", ordered); if (!getWriteConcern().isServerDefault())  writer.writeName(\"writeConcern\"); getBsonDocumentCodec().encode(writer, getWriteConcern().asDocument(), EncoderContext.builder().build());', 'private void logSummary( ExceptionSummary summary, MapString, String> references, String indent, boolean showErrors )  String referenceKey = \"\"; if ( StringUtils.isNotEmpty( summary.getReference() ) )  referenceKey = references.get( summary.getReference() ); if ( referenceKey == null )  referenceKey = \"[\" + references.size() + \"]\"; references.put( summary.getReference(), referenceKey );   if ( showErrors )  logger.error( indent + referenceKey, summary.getException() );  else  logger.error( indent + summary.getMessage() + \" \" + referenceKey );  indent += \" \"; for ( ExceptionSummary child : summary.getChildren() )  logSummary( child, references, indent, showErrors );', 'public static void main(String[] args) throws Exception  OptionParser parser = new OptionParser(); parser.accepts(\"r\", \"execute read operations\"); parser.accepts(\"w\", \"execute write operations\"); parser.accepts(\"d\", \"execute delete operations\"); parser.accepts(\"request-file\", \"execute specific requests in order\").withRequiredArg(); parser.accepts(\"start-key-index\", \"starting point when using int keys. Default = 0\").withRequiredArg().ofType(Integer.class); parser.accepts(\"value-size\", \"size in bytes for random value. Default = 1024\").withRequiredArg().ofType(Integer.class); parser.accepts(\"iterations\", \"number of times to repeat the test Default = 1\").withRequiredArg().ofType(Integer.class); parser.accepts(\"threads\", \"max number concurrent worker threads Default = \" + MAX_WORKERS).withRequiredArg().ofType(Integer.class); OptionSet options = parser.parse(args); ListString> nonOptions = options.nonOptionArguments(); if(nonOptions.size()!= 3)  printUsage(System.err, parser);  String url = nonOptions.get(0); String storeName = nonOptions.get(1); int numRequests = Integer.parseInt(nonOptions.get(2)); String ops = \"\"; ListString> keys = null; Integer startNum = CmdUtils.valueOf(options, \"start-key-index\", 0); Integer valueSize = CmdUtils.valueOf(options, \"value-size\", 1024); Int', 'public String printMessage(Template tmpl, CommentList comments, boolean showMenu, String urladd, boolean moderatorMode, String user, boolean expired) throws IOException, UtilException  StringBuffer out=new StringBuffer(); out.append(\"nn!-- \").append(msgid).append(\" -->n\"); out.append(\"table width=\"100%\" cellspacing=0 cellpadding=0 border=0>\"); if (showMenu)  out.append(\"tr class=title>td>\"); if (!deleted)  out.append(\"[a href=\"/jump-message.jsp?msgid=\").append(topic).append(\\'#\\').append(msgid).append(\"\">#/a>]\");  if (!expired &&!deleted)  out.append(\"[a href=\"add_comment.jsp?topic=\").append(topic).append(\"&amp;replyto=\").append(msgid).append(urladd).append(\"\">É/a>]\");  if (!deleted && (moderatorMode || nick.equals(user)))  out.append(\"[a href=\"delete_comment.jsp?msgid=\").append(msgid).append(\"\">É/a>]\");  if (moderatorMode)  out.append(\"[a href=\"sameip.jsp?msgid=\").append(msgid).append(\"\">äÉ  Ü', 'public boolean render(InternalContextAdapter context, Writer writer, Node node) throws IOException, ResourceNotFoundException, ParseErrorException, MethodInvocationException  String text = Utils.escapeHTML(String.valueOf(node.jjtGetChild(0).value(context))); writer.write(text); return true;', 'public void run()  try  adminClient.updateSlopEntries(nodeId, new SlopIterator(slopQueue, deleteBatch));  catch(UnreachableStoreException e)  failureDetector.recordException(metadataStore.getCluster().getNodeById(nodeId), System.currentTimeMillis() - this.startTime, e); throw e;  catch(Exception e)  logger.error(e, e);  finally  // Clean up the remaining delete batch for(PairByteArray, Version> entry: deleteBatch) storeRepo.getSlopStore().delete(entry.getFirst(), entry.getSecond()); // Clean the slop queue and remove the queue from the global // queue slopQueue.clear(); slopQueues.remove(nodeId);', 'private void importLocalExtensionComponents( ClassRealm extensionRealm, MavenProjectSession projectSession, Artifact extensionArtifact ) throws ExtensionManagerException  String projectId = projectSession.getProjectId(); // Create an entire new ClassWorld, ClassRealm for discovering // the immediate components of the extension artifact, so we don\\'t pollute the // container with component descriptors or realms that don\\'t have any meaning beyond discovery. ClassRealm discoveryRealm = new ClassRealm( new ClassWorld(), \"discovery\", Thread.currentThread().getContextClassLoader() ); try  discoveryRealm.addURL( extensionArtifact.getFile().toURL() );  catch ( MalformedURLException e )  throw new ExtensionManagerException( \"Unable to generate URL from extension artifact for local-component discovery: \" + extensionArtifact.getFile(), extensionArtifact, projectId, e );  ComponentDiscoverer discoverer = new DefaultComponentDiscoverer(); discoverer.setManager( new DummyDiscovererManager() ); ClassRealm projectRealm = projectSession.getProjectRealm(); try  // Find the extension component descriptors that exist ONLY in the immediate extension // artifact...this prevents us from adding plexus-archiver components to the mix, for instance, // when the extension uses that dependency. List componentSetDescriptors = discoverer.findComponents( container.getContext(), discoveryRealm ); for ( Iterator it = componentSetDescriptors.iterator(); it.hasNext(); )  ComponentSetDescriptor compSet = (ComponentSetDescriptor) it.next(); for ( Iterator compIt = compSet.getComponents().iterator(); compIt.hasNext(); )  // For each component in the extension artifact: ComponentDescriptor comp = (ComponentDescriptor) compIt', 'public void operate() throws Exception  adminClient = RebalanceUtils.createTempAdminClient(voldemortConfig, metadataStore.getCluster(), maxParallelStoresRebalancing * 4, maxParallelStoresRebalancing * 2); final ListException> failures = new ArrayListException>(); try  logger.info(\"starting rebalancing task\" + stealInfo); for(final String storeName: ImmutableList.copyOf(stealInfo.getUnbalancedStoreList()))  executors.submit(new Runnable()  public void run()  try  boolean isReadOnlyStore = metadataStore.getStoreDef(storeName).getType().compareTo(ReadOnlyStorageConfiguration.TYPE_NAME) == 0; rebalanceStore(storeName, adminClient, stealInfo, isReadOnlyStore); ListString> tempUnbalancedStoreList = new ArrayListString>(stealInfo.getUnbalancedStoreList()); tempUnbalancedStoreList.remove(storeName); stealInfo.setUnbalancedStoreList(tempUnbalancedStoreList); rebalancer.setRebalancingState(stealInfo);  catch(Exception e)  logger.error(\"rebalanceSubTask:\" + stealInfo + \" failed for store:\" + storeName, e); failures.add(e);   );  waitForShutdown(); if(stealInfo.getUnbalancedStoreList().isEmpty())  logger.info(\"Rebalancer: rebalance \" + stealInfo + \" completed successfully.\"); // clean state only if successful operation, not all // operations. metadataStore.cleanRebalancingState(stealInfo);  else  throw new VoldemortRebalancingException(\"Failed to rebalance task \" + stealInfo, failures);   finally  // free the permit in', 'private static TypeAdapter makeAdapterForField(String name, Fixture fixture)  Field field = null; if (GracefulNamer.isGracefulName(name))  String simpleName = GracefulNamer.disgrace(name).toLowerCase(); field = findField(fixture, simpleName);  else  try  Matcher matcher = fieldPattern.matcher(name); matcher.find(); String fieldName = matcher.group(1); field = fixture.getTargetClass().getField(fieldName);  catch (NoSuchFieldException e)    if (field == null) throw new NoSuchFieldFitFailureException(name); return TypeAdapter.on(fixture, field);', 'private void rebuildInterface()  controlPanel.removeAll(); if (node == null) return; int rowindex = 0; for (Parameter p : node.getParameters())  if (!p.isPrimitive()) continue; JLabel label = new JLabel(p.getLabel()); label.setAlignmentX(JLabel.RIGHT_ALIGNMENT); label.setHorizontalAlignment(JLabel.RIGHT); label.setHorizontalTextPosition(JLabel.RIGHT); label.setFont(PlatformUtils.getSmallBoldFont()); Class controlClass = CONTROL_MAP.get(p.getType()); JComponent control; if (controlClass!= null)  control = (JComponent) constructControl(controlClass, p); controls.add((ParameterControl) control);  else  control = new JLabel(\"no control>\");  GridBagConstraints labelConstraints = new GridBagConstraints(); labelConstraints.gridx = 0; labelConstraints.gridy = rowindex; labelConstraints.insets = new Insets(10, 0, 0, 10); labelConstraints.ipadx = 20; labelConstraints.fill = GridBagConstraints.HORIZONTAL; labelConstraints.anchor = GridBagConstraints.LINE_END; controlPanel.add(label, labelConstraints); GridBagConstraints controlConstraints = new GridBagConstraints(); controlConstraints.gridx = 1; controlConstraints.gridy = rowindex; controlConstraints.ipadx = 5; control', 'public void testEmailFilterSortOrderWithFeedback()  long rawContactId1 = createRawContact(); insertEmail(rawContactId1, \"address1@email.com\"); long rawContactId2 = createRawContact(); insertEmail(rawContactId2, \"address2@email.com\"); long dataId = ContentUris.parseId(insertEmail(rawContactId2, \"address3@email.com\")); ContentValues v1 = new ContentValues(); v1.put(Email.ADDRESS, \"address1@email.com\"); ContentValues v2 = new ContentValues(); v2.put(Email.ADDRESS, \"address2@email.com\"); ContentValues v3 = new ContentValues(); v3.put(Email.ADDRESS, \"address3@email.com\"); Uri filterUri1 = Uri.withAppendedPath(Email.CONTENT_FILTER_URI, \"address\"); Uri filterUri2 = Email.CONTENT_FILTER_URI.buildUpon().appendPath(\"address\").appendQueryParameter(DataUsageFeedback.USAGE_TYPE, DataUsageFeedback.USAGE_TYPE_CALL).build(); Uri filterUri3 = Email.CONTENT_FILTER_URI.buildUpon().appendPath(\"address\").appendQueryParameter(DataUsageFeedback.USAGE_TYPE, DataUsageFeedback.USAGE_TYPE_LONG_TEXT).build(); Uri filterUri4 = Email.CONTENT_FILTER_URI.buildUpon().appendPath(\"address\").appendQueryParameter(DataUsageFeedback.USAGE_TY', 'private long insertSettings(ContentValues values)  // Before inserting, ensure that no settings record already exists for the // values being inserted (this used to be enforced by a primary key, but that no // longer works with the nullable data_set field added). String accountName = values.getAsString(Settings.ACCOUNT_NAME); String accountType = values.getAsString(Settings.ACCOUNT_TYPE); String dataSet = values.getAsString(Settings.DATA_SET); Uri.Builder settingsUri = Settings.CONTENT_URI.buildUpon(); if (accountName!= null)  settingsUri.appendQueryParameter(Settings.ACCOUNT_NAME, accountName);  if (accountType!= null)  settingsUri.appendQueryParameter(Settings.ACCOUNT_TYPE, accountType);  if (dataSet!= null)  settingsUri.appendQueryParameter(Settings.DATA_SET, dataSet);  Cursor c = queryLocal(settingsUri.build(), null, null, null, null, 0, null); try  if (c.getCount() > 0)  // If a record was found, replace it with the new values. String selection = null; String[] selectionArgs = null; if (accountName!= null && accountType!= null)  selection = Settings.ACCOUNT_NAME + \"=? AND \" + Settings.ACCOUNT_TYPE + \"=?\"; if (dataSet == null)  selection += \" AND \" + Settings.DATA_SET + \" IS NULL\"; selectionArgs = new String[] accountName, accountType;  else  selection += \" AND \" + Settings.DATA_SET + \"=?\"; selectionArgs = new String[] accountName, accountType, dataSet;', 'private void runSequenceAssertion(NodeLocator l, String k, int... seq)  int pos=0; for(IteratorMemcachedNode> i=l.getSequence(k); i.hasNext(); )  assertEquals(\"At position \" + pos, nodes[seq[pos]].toString(), i.next().toString()); try  i.remove(); fail(\"Allowed a removal from a sequence.\");  catch(UnsupportedOperationException e)  // pass  pos++;  assertEquals(\"Incorrect sequence size for \" + k, seq.length, pos);', 'public TDocument> BsonDocument toBsonDocument(final ClassTDocument> tDocumentClass, final CodecRegistry codecRegistry)  BsonDocumentWriter writer = new BsonDocumentWriter(new BsonDocument()); writer.writeStartDocument(); writer.writeStartDocument(\"$graphLookup\"); writer.writeString(\"from\", from); writer.writeName(\"startWith\"); BuildersHelper.encodeValue(writer, startWith, codecRegistry); writer.writeString(\"connectFromField\", connectFromField); writer.writeString(\"connectToField\", connectToField); writer.writeString(\"as\", as); if (options.getMaxDepth()!= null)  writer.writeInt32(\"maxDepth\", options.getMaxDepth());  if (options.getDepthField()!= null)  writer.writeString(\"depthField\", options.getDepthField());  if (options.getRestrictSearchWithMatch()!= null)  writer.writeName(\"restrictSearchWithMatch\"); BuildersHelper.encodeValue(writer, options.getRestrictSearchWithMatch(), codecRegistry);  writer.writeEndDocument(); return writer.getDocument();', 'public void testRelocateSession() throws HttpException, IOException  final String sid1 = makeRequest( _httpClient, _portTomcat1, null ); assertNotNull( \"No session created.\", sid1 ); final String firstNode = sid1.substring( sid1.lastIndexOf( \\'-\\' ) + 1 ); assertNotNull( \"No node id encoded in session id.\", sid1 ); /* shutdown memcached node 1 */ _daemon1.stop(); final String sid2 = makeRequest( _httpClient, _portTomcat1, sid1 ); final String secondNode = sid2.substring( sid2.lastIndexOf( \\'-\\' ) + 1 ); final String expectedNode = firstNode.equals( _nodeId1 )? _nodeId2 : _nodeId1; assertEquals( \"Unexpected nodeId\", expectedNode, secondNode ); assertEquals( \"Unexpected sessionId, sid1: \" + sid1 + \", sid2: \" + sid2, sid1.substring( 0, sid1.indexOf( \"-\" ) + 1 ) + expectedNode, sid2 );', 'void unmodifiable()  AbstractConstructibleBson?> constructible = AbstractConstructibleBson.of(new BsonDocument(\"name\", new BsonString(\"value\"))); String expected = constructible.toBsonDocument().toJson(); String expected = constructible.newAppended(\"name2\", \"value2\"); assertEquals(expected, constructible.toBsonDocument().toJson());', 'public void inaccessibleBaseClassIsCaughtAtValidation() throws InitializationError  TestClass testClass = new TestClass(Sub.class); MethodValidator methodValidator= new MethodValidator(testClass); methodValidator.fTestClass.validateMethodsForDefaultRunner(methodValidator.fErrors); methodValidator.assertValid();', 'public void shouldRememberThePageNameAndDate() throws Exception  TimeMeasurement timeMeasurement = new TimeMeasurement()  @Override public long startedAt()  return testTime;  @Override public long stoppedAt()  return testTime+1;  ; formatter.newTestStarted(testPage, timeMeasurement); formatter.testComplete(testPage, testSummary, timeMeasurement); assertEquals(1, formatter.getPageHistoryReferences().size()); assertEquals(\"TestPage\", formatter.getPageHistoryReferences().get(0).getPageName()); assertEquals(testTime, formatter.getPageHistoryReferences().get(0).getPageName());', 'private void resumeToken(final ListRawBsonDocument> rawDocuments, final Throwable t)  if (t!= null)  callback.onResult(null, t);  else if (rawDocuments!= null)  ListT> results = new ArrayListT>(); for (RawBsonDocument rawDocument : rawDocuments)  if (!rawDocument.containsKey(\"_id\"))  callback.onResult(null, new MongoChangeStreamException(\"Cannot provide resume functionality when the resume token is missing.\") ); return;  try  results.add(rawDocument.decode(changeStreamOperation.getDecoder()));  catch (Exception e)  callback.onResult(null, e); return;   resumeToken = rawDocuments.get(rawDocuments.size() - 1).getDocument(\"_id\"); callback.onResult(results, null);  else  callback.onResult(null, null);  , LOGGER);', 'protected ListMemcachedNode> createConnections( final CollectionInetSocketAddress> addrs) throws IOException  ListMemcachedNode> connections = new ArrayListMemcachedNode>(addrs.size()); for (SocketAddress sa : addrs)  SocketChannel ch = SocketChannel.open(); ch.configureBlocking(false); MemcachedNode qa = connectionFactory.createMemcachedNode(sa, ch, bufSize); qa.setConnection(this); int ops = 0; ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm()); try  if (ch.connect(sa))  getLogger().info(\"Connected to %s immediately\", qa); connected(qa);  else  getLogger().info(\"Added %s to connect queue\", qa); ops = SelectionKey.OP_CONNECT;  selector.wakeup(); qa.setSk(ch.register(selector, ops, qa)); assert ch.isConnected() || qa.getSk().interestOps() == SelectionKey.OP_CONNECT : \"Not connected, and not wanting to connect\";  catch (SocketException e)  getLogger().warn(\"Socket error on initial connect\", e); queueReconnect(qa);  connections.add(qa);  return connections;', 'private Model readFileModel( ModelBuildingRequest request, DefaultModelProblemCollector problems ) throws ModelBuildingException  ModelSource modelSource = request.getModelSource(); Model model = getModelFromCache( modelSource, request.getModelCache() ); if ( model!= null )  return model;  problems.setSource( modelSource.getLocation() ); try  boolean strict = request.getValidationLevel() >= ModelBuildingRequest.VALIDATION_LEVEL_MAVEN_2_0; MapString, Object> options = new HashMap>( 3 ); options.put( ModelProcessor.IS_STRICT, strict ); options.put( ModelProcessor.SOURCE, modelSource ); InputSource source; if ( request.isLocationTracking() )  source = (InputSource) options.computeIfAbsent( ModelProcessor.INPUT_SOURCE, k -> new InputSource() );  else  source = null;  try  model = modelProcessor.read( modelSource.getInputStream(), options );  catch ( ModelParseException e )  if (!strict )  throw e;  options.put( ModelProcessor.IS_STRICT, Boolean.FALSE ); try  model = modelProcessor.read( modelSource.getInputStream(), options );  catch ( ModelParseException ne )  // still unreadable even in non-strict mode, rethrow original error throw e;  if ( modelSource instanceof FileModelSource )  problems.add( new ModelProblemCollectorRequest( Severity.ERROR, Version.V20 ).setMessage( \"Malformed POM \" + modelSource.getLocation() + \": \" + e.getMessage() ).setException( e ) );  else  problems.add(', 'private void addIconForContact(long id, String lookupKey, String displayName, ShortcutInfo.Builder builder)  final Bitmap bitmap = getContactPhoto(id); if (bitmap!= null)  builder.setIcon(Icon.createWithBitmap(bitmap));  else  builder.setIcon(Icon.createWithBitmap(getFallbackAvatar(displayName, lookupKey)));', 'public CachedData encode(String o)  CachedData flag = new CachedData(flag, o.getBytes(), getMaxSize()); if (flag == null)  return new CachedData(flag, flag);  else  CachedData maxSize = maxSize; if (maxSize == null)  maxSize = maxSize;  else  maxSize = flag.getMaxSize();', 'private CallableProjectSegment> createBuildCallable( final MavenSession rootSession, final ProjectSegment projectBuild, final ReactorContext reactorContext, final TaskSegment taskSegment, final ThreadOutputMuxer muxer )  return () ->  final Thread currentThread = Thread.currentThread(); final String originalThreadName = currentThread.getName(); currentThread.setName( \"mvn-builder-\" + projectBuild.getProject().getId() ); try  // muxer.associateThreadWithProjectSegment( projectBuild ); lifecycleModuleBuilder.buildProject( projectBuild.getSession(), rootSession, reactorContext, projectBuild.getProject(), taskSegment ); // muxer.setThisModuleComplete( projectBuild ); return projectBuild;  finally  currentThread.setName( originalThreadName );  ;', 'private void forkProjectLifecycle( MojoDescriptor mojoDescriptor, Stack forkEntryPoints, MavenSession session, MavenProject project ) throws LifecycleExecutionException, BuildFailureException, PluginNotFoundException  forkEntryPoints.push( mojoDescriptor ); PluginDescriptor pluginDescriptor = mojoDescriptor.getPluginDescriptor(); String targetPhase = mojoDescriptor.getExecutePhase(); Map lifecycleMappings = null; if ( targetPhase!= null )  Lifecycle lifecycle = getLifecycleForPhase( targetPhase ); // Create new lifecycle lifecycleMappings = constructLifecycleMappings( session, targetPhase, project, lifecycle ); String executeLifecycle = mojoDescriptor.getExecuteLifecycle(); if ( executeLifecycle!= null )  org.apache.maven.plugin.lifecycle.Lifecycle lifecycleOverlay; try  lifecycleOverlay = pluginDescriptor.getLifecycleMapping( executeLifecycle );  catch ( IOException e )  throw new LifecycleExecutionException( \"Unable to read lifecycle mapping file: \" + e.getMessage(), e );  catch ( XmlPullParserException e )  throw new LifecycleExecutionException( \"Unable to parse lifecycle mapping file: \" + e.getMessage(), e );  if ( lifecycleOverlay == null )  throw new LifecycleExecutionException( \"Lifecycle \\'\" + executeLifecycle + \"\\' not found in plugin\" );  for ( Iterator i = lifecycleOverlay.getPhases().iterator(); i.hasNext(); )  Phase phase = (Phase) i.next(); for ( Iterator j = phase.getExecutions().iterator', 'protected Uri insertInTransaction(Uri uri, ContentValues values)  if (Log.isLoggable(TAG, Log.VERBOSE))  Log.v(TAG, \"insertInTransaction: \" + uri);  final int match = sUriMatcher.match(uri); long id = 0; switch (match)  case SYNCSTATE: id = mOpenHelper.getSyncState().insert(mDb, values); break; case CONTACTS:  insertContact(values); break;  case RAW_CONTACTS:  final Account account = readAccountFromQueryParams(uri); id = insertRawContact(values, account); break;  case RAW_CONTACTS_DATA:  values.put(Data.RAW_CONTACT_ID, uri.getPathSegments().get(1)); id = insertData(values, shouldMarkRawContactAsDirty(uri)); break;  case DATA:  id = insertData(values, shouldMarkRawContactAsDirty(uri)); break;  case GROUPS:  final Account account = readAccountFromQueryParams(uri); id = insertGroup(values, account, shouldMarkGroupAsDirty(uri)); break;  case SETTINGS:  id = insertSettings(values); break;  case PRESENCE:  id = insertPresence(values); break;  default: return mLegacyApiSupport.insert(uri, values);  if (id  0)  return null;  return ContentUris.withAppendedId(uri, id);', 'protected int deleteInTransaction(Uri uri, String selection, String[] selectionArgs)  if (Log.isLoggable(TAG, Log.VERBOSE))  Log.v(TAG, \"deleteInTransaction: \" + uri);  flushTransactionalChanges(); final int match = sUriMatcher.match(uri); switch (match)  case SYNCSTATE: return mOpenHelper.getSyncState().delete(mDb, selection, selectionArgs); case SYNCSTATE_ID: String selectionWithId = (SyncStateContract.Columns._ID + \"=\" + ContentUris.parseId(uri) + \" \") + (selection == null? \"\" : \" AND (\" + selection + \")\"); return mOpenHelper.getSyncState().delete(mDb, selectionWithId, selectionArgs); case CONTACTS:  // TODO return 0;  case CONTACTS_ID:  long contactId = ContentUris.parseId(uri); return deleteContact(contactId);  case CONTACTS_LOOKUP: case CONTACTS_LOOKUP_ID:  final ListString> pathSegments = uri.getPathSegments(); final int segmentCount = pathSegments.size(); if (segmentCount  3)  throw new IllegalArgumentException(\"URI \" + uri + \" is missing a lookup key\");  final String lookupKey = pathSegments.get(2); final long contactId = lookupContactIdByLookupKey(mDb, lookupKey); return deleteContact(contactId);  case RAW_CONTACTS:  final boolean permanently = readBooleanQueryParameter(uri, RawContacts.DELETE_PERMANENTLY, false); int numDeletes = 0; Cur', 'private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException  IndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndexIndex = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get); completeIndexFlush(rowMapping.size(), startTime, stopwatch); SegmentMetadata metadata = new SegmentMetadata(0, rowMapping.size(), 0, rowMapping.maxSSTableRowId, rowMapping.minKey, rowMapping.maxKey, ByteBufferUtil.bytes(0), ByteBufferUtil.bytes(0), metadataMap); try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))  SegmentMetadata.write(writer, Collections.singletonList(metadata));', 'public static Registry getRegistry(String master) throws RemoteException, NotBoundException  java.rmi.registry.Registry rmiRegistry = LocateRegistry.getRegistry(master, getPort()); return (Registry) rmiRegistry.lookup(BIND_NAME);', 'public void addAlias(String alias, String fixture)  String alias = String.getAlias(); String alias = String.getAlias(); String alias = String.getAlias(); String alias = String.getAlias(); String alias = String.getAlias(); String alias = String.getAlias(); String c = c.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String a = String.getAlias(); String alias = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.getAlias(); String c = String.', 'float size = defaultPaint.getFontMetrics(); if (size = 0.0) return; defaultPaint.setTextSize(size); fontSize = size; // read new metrics to get exact pixel dimensions FontMetrics fm = defaultPaint.getFontMetrics(); charTop = (int)Math.ceil(fm.top); float[] widths = new float[1]; defaultPaint.getTextWidths(\"X\", widths); charWidth = (int)Math.ceil(widths[0]); charHeight = (int)Math.ceil(fm.descent - fm.top); // refresh any bitmap with new font size if(parent!= null) parentChanged(parent); for (FontSizeChangedListener ofscl : fontSizeChangedListeners) ofscl.onFontSizeChanged(size); host.setFontSize((int) fontSize); manager.hostdb.updateFontSize(host); forcedSize = false;', 'public void build( MavenSession session, ReactorContext reactorContext, ProjectBuildList projectBuilds, ListTaskSegment> taskSegments, ReactorBuildStatus reactorBuildStatus ) throws ExecutionException, InterruptedException  int nThreads = Math.min( session.getRequest().getDegreeOfConcurrency(), session.getProjects().size() ); boolean parallel = nThreads >= 2; // Propagate the parallel flag to the root session and all of the cloned sessions in each project segment session.setParallel( parallel ); for ( ProjectSegment segment : projectBuilds )  segment.getSession().setParallel( parallel );  ExecutorService executor = Executors.newFixedThreadPool( nThreads, new BuildThreadFactory() ); CompletionServiceProjectSegment> service = new ExecutorCompletionService>( executor ); ConcurrencyDependencyGraph analyzer = new ConcurrencyDependencyGraph( projectBuilds, session.getProjectDependencyGraph() ); // Currently disabled ThreadOutputMuxer muxer = null; // new ThreadOutputMuxer( analyzer.getProjectBuilds(), System.out ); for ( TaskSegment taskSegment : taskSegments )  MapMavenProject, ProjectSegment> projectBuildMap = projectBuilds.selectSegment( taskSegment ); try  multiThreadedProjectTaskSegmentBuild( analyzer, reactorContext, session, service, taskSegment, projectBuildMap, muxer ); if ( reactorContext.getReactorBuildStatus().isHalted() )  break;   catch ( Exception e )  session.getResult().addException( session', 'public MaybeSymbol> parse(Symbol current, Parser parser)  StringBuilder source = new StringBuilder(current.getContent()); String imageProperty = current.getContent().endsWith(\"l\")? Link.Left : current.getContent().endsWith(\"r\")? Link.Right : \"\"; parser.moveNext(1); if (!parser.getCurrent().isType(SymbolType.Whitespace)) return Symbol.nothing; source.append(parser.getCurrent().getContent()); parser.moveNext(1); MapString, String> options = new TreeMap>(); while (parser.getCurrent().isType(SymbolType.Text) && parser.getCurrent().getContent().startsWith(\"-\"))  String option = parser.getCurrent().getContent(); source.append(option); parser.moveNext(1); if (!parser.getCurrent().isType(SymbolType.Whitespace)) return Symbol.nothing; source.append(parser.getCurrent().getContent()); parser.moveNext(1); if (!parser.getCurrent().isType(SymbolType.Text)) return Symbol.nothing; source.append(parser.getCurrent().getContent()); String value = parser.getCurrent().getContent(); parser.moveNext(1); if (!parser.getCurrent().isType(SymbolType.Whitespace)) return Symbol.nothing; source.append(parser.getCurrent().getContent()); parser.moveNext(1); options.put(option, value);  current.setContent(source.toString()); final Symbol name = parser.getCurrent(); if (name.isType(Link.', 'protected String appendPath( String parentPath, String childPath, String pathAdjustment, boolean appendPaths )  List pathFragments = new ArrayList(); String rootPath = parentPath; String protocol = null; int protocolIdx = rootPath.indexOf( \"://\" ); if ( protocolIdx > -1 )  protocol = rootPath.substring( 0, protocolIdx + 3 ); rootPath = rootPath.substring( protocolIdx + 3 );  pathFragments.add( rootPath ); if ( appendPaths )  if ( pathAdjustment!= null )  pathFragments.add( pathAdjustment );  pathFragments.add( childPath );  StringBuffer cleanedPath = new StringBuffer(); if ( protocol!= null )  cleanedPath.append( protocol );  if ( rootPath.startsWith( \"/\" ) )  cleanedPath.append( \\'/\\' );  String lastToken = null; String currentToken = null; for ( Iterator it = pathFragments.iterator(); it.hasNext(); )  String pathFragment = (String) it.next(); StringTokenizer tokens = new StringTokenizer( pathFragment, \"/\" ); while ( tokens.hasMoreTokens() )  lastToken = currentToken; currentToken = tokens.nextToken(); if ( \"..\".equals( currentToken ) )  // trim the previous path part off... cleanedPath.setLength( cleanedPath.length() - ( lastToken.length() + 1 ) );  else if (!\".\".equals( currentToken ) )  // don\\'t worry about /./', 'public int delete(UriData uriData, String selection, String[] selectionArgs)  final SQLiteDatabase db = mDbHelper.getWritableDatabase(); String combinedClause = concatenateClauses(selection, uriData.getWhereClause(), getCallTypeClause()); // Delete all the files associated with this query. Once we\\'ve deleted the rows, there will // be no way left to get hold of the files. Cursor cursor = null; try  cursor = query(uriData, FILENAME_ONLY_PROJECTION, selection, selectionArgs, null); while (cursor.moveToNext())  File file = new File(cursor.getString(0)); if (file.exists())  boolean success = file.delete(); if (!success)  Log.e(TAG, \"Failed to delete file: \" + file.getAbsolutePath());     finally  CloseUtils.closeQuietly(cursor);  // Now delete the rows themselves. return getDatabaseModifier(db).delete(mTableName, combinedClause, selectionArgs);', 'public String toString()  StringBuffer sb = new StringBuffer(); if (getCause()!= null && getCause().getClass().toString().contains(\"StopTest\"))  sb.append(SlimServer.EXCEPTION_STOP_TEST_TAG);  else  sb.append(SlimServer.EXCEPTION_TAG);  if (this.prettyPrint) sb.append(PRETTY_PRINT_TAG_START); if (!isEmpty(tag)) sb.append(tag).append(\" \"); if (!isEmpty(getMessage()))  sb.append(getMessage());  else if (getCause()!= null)  StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw); getCause().printStackTrace(pw); sb.append(sw.toString());  if (this.prettyPrint) sb.append(PRETTY_PRINT_TAG_END); return sb.toString();', 'public void testDeleteVersion() throws Exception  props.setProperty( \"jspwiki.pageProvider\", \"VersioningFileProvider\" ); m_engine.shutdown(); m_engine = new TestEngine( props ); m_engine.saveText( NAME1, \"Test1\" ); m_engine.saveText( NAME1, \"Test2\" ); m_engine.saveText( NAME1, \"Test3\" ); WikiPage page = m_engine.getPage( NAME1, 3 ); m_engine.deleteVersion( page ); assertNull( \"got page\", m_engine.getPage( NAME1, 3 ) ); String content = m_engine.getText( NAME1, WikiProvider.LATEST_VERSION ); assertEquals( \"content\", \"Test2\", content.trim() );', 'public void call(String fixtureName)  constructFixture(fixtureName); dontReportExceptionsInTheseInstructions.add(callFunction(getTableName(), \"table\", tableAsList())); if (table.getRowCount() > 2) invokeRows();', 'public TopicMenu getTopicMenu( @Nonnull PreparedTopic message, @Nullable User currentUser, boolean secure, ProfileProperties profileProperties, boolean loadUserpics )  boolean topicEditable = groupPermissionService.isEditable(message, currentUser); boolean tagsEditable = groupPermissionService.isTagsEditable(message, currentUser); boolean resolvable; int memoriesId; int favsId; boolean deletable; ListInteger> topicStats = memoriesDao.getTopicStats(message.getMessage().getId()); if (currentUser!=null)  resolvable = (currentUser.isModerator() || (message.getAuthor().getId()==currentUser.getId())) && message.getGroup().isResolvable(); memoriesId = memoriesDao.getId(currentUser, message.getMessage(), true); favsId = memoriesDao.getId(currentUser, message.getMessage(), false); deletable = groupPermissionService.isDeletable(message.getMessage(), currentUser);  else  resolvable = false; memoriesId = 0; favsId = 0; deletable = false;  Userpic userpic = null; if (loadUserpics && profileProperties.isShowPhotos())  userpic = userService.getUserpic( message.getAuthor(), secure, profileProperties.getAvatarMode() );  String userAgent; if (currentUser!=null && currentUser.isModerator())  userAgent = userAgentDao.getUserAgentById(', 'public void shouldRememberTestSummariesInReferences() throws Exception  addTest(); ListPageHistoryReference> references = formatter.getPageHistoryReferences(); assertEquals(1, references.size()); assertEquals(new TestSummary(1, 2, 3, 4), references.get(0).getTestSummary());', 'public static void main( String[] args )  if ( args.length  1 )  printUsage(); System.exit( 0 );  else if ( \"-h\".equals( args[0].toLowerCase() ) )  printHelp(); System.exit( 0 );  else if ( \"-template\".equals( args[0] ) )  printTemplate(); System.exit( 0 );  try  RepositoryCleanerConfiguration config = buildConfig( args[0] ); launch( config ); System.exit( 0 );  catch ( Exception e )  e.printStackTrace(); System.exit( 1 );', 'public int run(String[] args) throws Exception  if(args.length!= 5) Utils.croak(\"Expected arguments store_name config_dir temp_dir input_path output_path\"); String storeName = args[0]; String configDir = args[1]; String tempDir = args[2]; String inputDir = args[3]; String outputDir = args[4]; ListStoreDefinition> storeDefs = new StoreDefinitionsMapper().readStoreList(new File(configDir, \"stores.xml\")); StoreDefinition def = null; for(StoreDefinition d: storeDefs) if(d.getName().equals(storeName)) def = d; Cluster cluster = new ClusterMapper().readCluster(new File(configDir, \"cluster.xml\")); Configuration config = this.getConf(); config.set(\"mapred.job.name\", \"test-store-builder\"); HadoopStoreBuilder builder = new HadoopStoreBuilder( config, BuildTestStoreMapper.class, SequenceFileInputFormat.class, cluster, def, new Path(tempDir), new Path(outputDir), new Path(inputDir), CheckSum.CheckSumType.NONE, false, false, (long) (1.5 * 1024 * 1024 * 1024 * 1024), -1, false, null); builder.build(); return 0;', 'private int updateGroups(ContentValues originalValues, String selectionWithId, String[] selectionArgs, boolean callerIsSyncAdapter)  mGroupIdCache.clear(); final SQLiteDatabase db = mDbHelper.get().getWritableDatabase(); final ContactsDatabaseHelper dbHelper = mDbHelper.get(); final ContentValues updatedValues = new ContentValues(); updatedValues.putAll(originalValues); if (!callerIsSyncAdapter &&!updatedValues.containsKey(Groups.DIRTY))  updatedValues.put(Groups.DIRTY, 1);  if (updatedValues.containsKey(Groups.GROUP_VISIBLE))  mVisibleTouched = true;  // Prepare for account change final boolean isAccountNameChanging = updatedValues.containsKey(Groups.ACCOUNT_NAME); final boolean isAccountTypeChanging = updatedValues.containsKey(Groups.ACCOUNT_TYPE); final boolean isDataSetChanging = updatedValues.containsKey(Groups.DATA_SET); final boolean isAccountChanging = isAccountNameChanging || isAccountTypeChanging || isDataSetChanging; final String updatedAccountName = updatedValues.getAsString(Groups.ACCOUNT_NAME); final String updatedDataSet = updatedValues.getAsString(Groups.ACCOUNT_TYPE); final String updatedDataSet = updatedValues.getAsString(Groups.DATA_SET); updatedValues.remove(Groups.ACCOUNT_NAME); updatedValues.remove(Groups.ACCOUNT_TYPE); updatedValues.remove(Groups.DATA_SET); // We later call requestSync() on all affected accounts', 'private void calculateExecutionForLifecyclePhase( MavenSession session, ListMojoExecution> lifecyclePlan, String lifecyclePhase ) throws PluginNotFoundException, PluginResolutionException, PluginDescriptorParsingException, CycleDetectedInPluginGraphException, MojoNotFoundException, InvalidPluginDescriptorException  MavenProject project = session.getCurrentProject(); // 1. // // Based on the lifecycle phase we are given, let\\'s find the corresponding lifecycle. // Lifecycle lifecycle = phaseToLifecycleMap.get( lifecyclePhase ); // 2. // // If we are dealing with the \"clean\" or \"site\" lifecycle then there are currently no lifecycle mappings but there are default phases // that need to be run instead. // // Now we need to take into account the packaging type of the project. For a project of type WAR, the lifecycle where mojos are mapped // on to the given phases in the lifecycle are going to be a little different then, say, a project of type JAR. // // 3. // // Once we have the lifecycle mapping for the given packaging, we need to know whats phases we need to worry about executing. // // Create an ordered Map of the phases in the lifecycle to a list of mojos to execute. MapString, ListMojoExecution>> phaseToMojoMapping = new LinkedHashMapString, ListMojoExecution>>(); // 4. //TODO: need to separate the lifecycles if ( lifecycle == null )  logger.info( \"Invalid task \\'\" + lifecyclePhase + \"\\' : you must specify a valid lifecycle phase, or a goal in the format plugin:goal or pluginGroupId:pluginArtifactId:pluginVersion:goal\" ); throw new MojoNotFoundException( lifecyclePhase, null );  for ( String phase : lifecycle.getPhases() )', 'public void write(BytesWritable key, IteratorBytesWritable> iterator, Reporter reporter) throws IOException  // Read chunk id int chunkId = ReadOnlyUtils.chunk(key.getBytes(), getNumChunks()); // Write key and position this.indexFileStream[chunkId].write(key.getBytes(), 0, key.getLength()); this.indexFileSizeInBytes[chunkId] += key.getLength(); this.indexFileStream[chunkId].writeInt(this.position[chunkId]); this.indexFileSizeInBytes[chunkId] += ByteUtils.SIZE_OF_INT; // Run key through checksum digest if(this.checkSumDigestIndex[chunkId]!= null)  this.checkSumDigestIndex[chunkId].update(key.getBytes(), 0, key.getLength()); this.checkSumDigestIndex[chunkId].update(this.position[chunkId]);  short numTuples = 0; ByteArrayOutputStream stream = new ByteArrayOutputStream(); DataOutputStream valueStream = new DataOutputStream(stream); while(iterator.hasNext())  BytesWritable writable = iterator.next(); byte[] valueBytes = writable.getBytes(); int offsetTillNow = 0; // Read node Id if(this.nodeId == -1) this.nodeId = ByteUtils.readInt(valueBytes, offsetTillNow); offsetTillNow += ByteUtils.SIZE_OF_INT; // Read partition id if(this.partitionId == -1) this.partitionId = ByteU', 'private static int getOriginatingUid(Context context, long id)  final Uri uri = ContentUris.withAppendedId(ALL_DOWNLOADS_CONTENT_URI, id); final Cursor cursor = context.getContentResolver().query(uri, new String[]Constants.UID, null, null, null); if (cursor!= null)  try  if (cursor.moveToFirst())  return cursor.getInt(cursor.getColumnIndexOrThrow(Constants.UID));   finally  cursor.close();   return PackageInstaller.SessionParams.UID_UNKNOWN;', 'public void testManyConnections() throws Exception  ss = createSslSocketService(new EchoService()); String answer = \"\"; for (int i = 0; i  10; i++) Socket s = createClientSocket(PORT_NUMBER); System.out.print(\"Peer: \" + SocketFactory.peerName(s) + \"n\"); BufferedReader br = GetBufferedReader(s); PrintStream ps = GetPrintStream(s); ps.println(i + \",\"); answer = answer + br.readLine();  ss.close(); System.out.print(\"Got Messages : \" +answer +\"n\"); assertEquals(\"0,1,2,3,4,5,6,7,8,9,\", answer);', 'public int hashCode()  int result = index; result = 31 * super.hashCode(); return result;', 'private WikiPageProperties loadAttributes(final FileVersion fileVersion) throws IOException  final WikiPageProperties props = new WikiPageProperties(); props.loadFromXml(fileVersion.getContent()); props.setLastModificationTime(fileVersion.getLastModificationTime()); return props;', 'public MaybeSymbol> parseSymbol(Parser parser, Scanner scanner)  while (true)  Scanner backup = new Scanner(scanner); scanner.moveNextIgnoreFirst(this); if (scanner.isEnd()) return Maybe.nothingBecause(\"scanner is at end of buffer\"); Symbol currentToken = scanner.getCurrent(); int startOffset = currentToken.getStartOffset(); if (endsOn(currentToken.getType()) || parser.parentOwns(currentToken.getType(), this))  scanner.copy(backup); return Maybe.nothingBecause(\"At termination symbol or parent owns symbol\");  if (terminatesOn(currentToken.getType())) return Maybe.nothingBecause(\"At termination symbol\"); Rule currentRule = currentToken.getType().getWikiRule(); MaybeSymbol> parsedSymbol = currentRule.parse(currentToken, parser); if (parsedSymbol.isNothing())  ignoreFirst(currentToken.getType()); scanner.copy(backup);  else  parsedSymbol.getValue().setStartOffset(startOffset).setEndOffset(scanner.getOffset()); clearIgnoresFirst(); return parsedSymbol;', 'public FutureMapString, Object>> asyncGetBulk(CollectionString> keys)  final MapString, Object> m=new ConcurrentHashMapString, Object>(); // Break the gets down into groups by key final MapMemcachedNode, CollectionString>> chunks =new HashMapMemcachedNode, CollectionString>>(); final NodeLocator locator=conn.getLocator(); for(String key : keys)  validateKey(key); final MemcachedNode primaryNode=locator.getPrimary(key); MemcachedNode node=null; if(primaryNode.isActive())  node=primaryNode;  else  for(IteratorMemcachedNode> i=locator.getSequence(key); node == null && i.hasNext();)  MemcachedNode n=i.next(); if(n.isActive())  node=n;   if(node == null)  node=primaryNode;   assert node!= null : \"Didn\\'t find a node for \" + key; CollectionString> ks=chunks.get(node); if(ks == null)  ks=new ArrayListString>(); chunks.put(node, ks);  ks.add(key);  final CountDownLatch latch=new CountDownLatch(chunks.size()); final CollectionOperation> ops=new ArrayListOperation>(); GetOperation.Callback cb=new GetOperation.Callback()  @SuppressWarnings(\"synthetic-access\") public void receivedStatus(OperationSt', 'public void onClick(View v)  switch (v.getId())  case R.id.add_account_button: final Intent intent = ImplicitIntentsUtil.getIntentForAddingGoogleAccount(); ImplicitIntentsUtil.startActivityOutsideApp(getActivity(), intent); break; case R.id.import_contacts_button: ImportDialogFragment.show(getFragmentManager()); break;', 'private MethodExecutionResult getMethodExecutionResult(String instanceName, String methodName, Object... args) throws Throwable  MethodExecutionResults results = new MethodExecutionResults(); for (int i = 0; i  executorChain.size(); i++)  MethodExecutionResult result = executorChain.get(i); MethodExecutionResult result = context.replaceSymbols(args)); if (result.hasResult())  return result;  results.add(result);  return results.getFirstResult();', 'protected void performBackgroundTask(int task, Object arg)  switch (task)  case BACKGROUND_TASK_OPEN_ACCESS:  if (mOkToOpenAccess)  mAccessLatch.countDown(); mAccessLatch = null;  break;  case BACKGROUND_TASK_IMPORT_LEGACY_CONTACTS:  if (isLegacyContactImportNeeded())  importLegacyContactsInBackground();  break;  case BACKGROUND_TASK_UPDATE_ACCOUNTS:  Account[] accounts = AccountManager.get(getContext()).getAccounts(); boolean accountsChanged = updateAccountsInBackground(accounts); updateContactsAccountCount(accounts); updateDirectoriesInBackground(accountsChanged); break;  case BACKGROUND_TASK_UPDATE_LOCALE:  updateLocaleInBackground(); break;  case BACKGROUND_TASK_UPGRADE_AGGREGATION_ALGORITHM:  if (isAggregationUpgradeNeeded())  upgradeAggregationAlgorithmInBackground();  break;  case BACKGROUND_TASK_UPDATE_PROVIDER_STATUS:  updateProviderStatus(); break;  case BACKGROUND_TASK_UPDATE_DIRECTORIES:  if (arg!= null)  mContactDirectoryManager.onPackageChanged((String) arg);  break;', 'protected CompilerOptions ecjCompilerOptions()  CompilerOptions options = new CompilerOptions(); options.complianceLevel = Eclipse.getLatestEcjCompilerVersionConstant(); options.sourceLevel = Eclipse.getLatestEcjCompilerVersionConstant(); options.targetJDK = Eclipse.getLatestEcjCompilerVersionConstant(); options.docCommentSupport = false; options.parseLiteralExpressionsAsConstants = true; options.inlineJsrBytecode = true; options.reportUnusedDeclaredThrownExceptionExemptExceptionAndThrowable = false; options.reportUnusedDeclaredThrownExceptionIncludeDocCommentReference = false; options.reportUnusedDeclaredThrownExceptionWhenOverriding = false; options.reportUnusedParameterIncludeDocCommentReference = false; options.reportUnusedParameterWhenImplementingAbstract = false; options.reportUnusedParameterWhenOverridingConcrete = false; options.reportDeadCodeInTrivialIfStatement = false; options.generateClassFiles = false; MapString, String> warnings = new HashMapString, String>(); warnings.put(CompilerOptions.OPTION_ReportUnusedLocal, \"ignore\"); warnings.put(CompilerOptions.OPTION_ReportUnusedLabel, \"ignore\"); warnings.put(CompilerOptions.OPTION_ReportUnusedImport, \"ignore\"); warnings.put(CompilerOptions.OPTION_ReportUnusedPrivateMember, \"ignore\"); warnings.put(CompilerOptions.OPTION_ReportIndirectStaticAccess, \"warning\"); warnings.put(CompilerOptions.OPTION_ReportNonStaticAccess', 'private void registerFactory(Class componentType)  if (ComponentFactory.class.isAssignableFrom(componentType))  final Class?> target = new ComponentFactoryIntrospector().targetTypeForComponentFactory(componentType); Type adapterType = Types.newParameterizedType(ComponentFactoryProviderAdapter.class, target); Type factoryType = Types.newParameterizedType(ComponentFactory.class, target); // binder.bind(TypeLiteral.get(adapterType)); binder.bind(TypeLiteral.get(factoryType)).to(componentType); binder.bind(target).toProvider((TypeLiteral) TypeLiteral.get(adapterType));', 'public void rebalance(Cluster currentCluster, final Cluster targetCluster)  logger.debug(\"Current Cluster configuration:\" + currentCluster); logger.debug(\"Target Cluster configuration:\" + targetCluster); adminClient.setAdminClientCluster(currentCluster); final RebalanceClusterPlan rebalanceClusterPlan = new RebalanceClusterPlan(currentCluster, targetCluster, RebalanceUtils.getStoreNameList(currentCluster, adminClient), rebalanceConfig.isDeleteAfterRebalancingEnabled()); logger.info(rebalanceClusterPlan); // add all new nodes to currentCluster and propagate to all currentCluster = getClusterWithNewNodes(currentCluster, targetCluster); adminClient.setAdminClientCluster(currentCluster); Node firstNode = currentCluster.getNodes().iterator().next(); VectorClock latestClock = (VectorClock) RebalanceUtils.getLatestCluster(new ArrayListInteger>(), adminClient).getVersion(); RebalanceUtils.propagateCluster(adminClient, currentCluster, latestClock.incremented(firstNode.getId(), System.currentTimeMillis()), new ArrayListInteger>()); ExecutorService executor = createExecutors(rebalanceConfig.getMaxParallelRebalancing()); // start all threads for(int nThreads = 0; nThreads  this.rebalanceConfig.getMaxParallelRebalancing(); nThreads++)  executor.execute(new Runnable()  public void run()  // pick one node to rebalance from queue while(!rebalanceClusterPlan.getRebalancingTaskQueue().isEmpty())  RebalanceNodePlan rebalanceTask = rebalance', 'private static FitNesseContext loadContext(Arguments arguments) throws Exception  Properties properties = loadConfigFile(arguments.getConfigFile()); // Enrich properties with command line values: properties.setProperty(ComponentFactory.VERSIONS_CONTROLLER_DAYS, Integer.toString(arguments.getDaysTillVersionsExpire())); Builder builder = new Builder(); ComponentFactory componentFactory = new ComponentFactory(properties); WikiPageFactory wikiPageFactory = (WikiPageFactory) componentFactory.createComponent(ComponentFactory.WIKI_PAGE_FACTORY_CLASS, FileSystemPageFactory.class); builder.port = arguments.getPort(); builder.rootPath = arguments.getRootPath(); builder.rootDirectoryName = arguments.getRootDirectory(); builder.pageTheme = properties.getProperty(ComponentFactory.THEME); builder.defaultNewPageContent = properties.getProperty(ComponentFactory.DEFAULT_NEWPAGE_CONTENT); builder.recentChanges = (RecentChanges) componentFactory.createComponent(ComponentFactory.RECENT_CHANGES_CLASS, RecentChangesWikiPage.class); // This should done before the root wiki page is created: //extraOutput = componentFactory.loadVersionsController(arguments.getDaysTillVersionsExpire()); builder.root = wikiPageFactory.makeRootPage(builder.rootPath, builder.rootDirectoryName); builder.logger = makeLogger(arguments); builder.authenticator = makeAuthenticator(arguments.getUserpass(), componentFactory); FitNesseContext context = builder.createFitNesseContext()', 'public void testDelete() throws Exception  WikiPage engine = new WikiPage(m_engine, \"Test\"), \"v1\" ); m_provider.putPageText( \"Test\" ); WikiPage engine = new WikiPage( engine, \"Test\" ); m_provider.deletePage( \"Test\" ); String files = props.getProperty( FileSystemProvider.PROP_PAGEDIR ); File f = new File( files, \"Test\"+FileSystemProvider.FILE_EXT ); assertFalse( \"file exists\", f.exists() );', 'public void testDelete()  Database db; Context ctx = getContext(); try  db = TestData.GetDb(ctx, ASSET, PASSWORD, KEYFILE, FILENAME);  catch (Exception e)  assertTrue(\"Failed to open database: \" + e.getMessage(), false); return;  PwGroupV3 group1 = getGroup(db.pm, GROUP1_NAME); assertNotNull(\"Could not find group1\", group1); // Delete the group DeleteGroup task = new DeleteGroup(db, group1, null, true); task.run(); // Verify the entries were deleted PwEntryV3 entry1 = getEntry(db.pm, ENTRY1_NAME); assertNull(\"Entry 1 was not removed\", entry1); PwEntryV3 entry2 = getEntry(db.pm, ENTRY2_NAME); assertNull(\"Entry 2 was not removed\", entry2); // Verify the entries were removed from the search index SearchDbHelper dbHelp = new SearchDbHelper(ctx); dbHelp.open(); PwGroupV3 results1 = dbHelp.search(db, ENTRY1_NAME); PwGroupV3 results2 = dbHelp.search(db, ENTRY2_NAME); dbHelp.close(); assertEquals(\"Entry1 was not removed from the search results\", 0, results1.childEntries.size()); assertEquals(\"Entry2 was not removed from the search results\", 0, results2.childEntries.size()); // Verify the group was deleted group1 = getGroup(db.pm, GROUP1_NAME); assertNull(\"Group 1 was not removed.\", group1);', 'public static AccountWithDataSet getDefaultOrBestFallback(ContactsPreferences preferences, AccountTypeManager accountTypeManager)  if (preferences.isDefaultAccountSet())  return preferences.getDefaultAccount();  ListAccountWithDataSet> accounts = accountTypeManager.getAccounts(/* writableOnly */ true); if (accounts.isEmpty())  return AccountWithDataSet.getNullAccount();  // Return the first google account for (AccountWithDataSet account : accounts)  if (GoogleAccountType.ACCOUNT_TYPE.equals(account) && account.dataSet == null)  return account;   // Arbitrarily return the first writable account return accounts.get(0);', 'public void bindView(View view, Context context, Cursor cursor)  final long rawContactId = cursor.getLong(PickRawContactLoader.RAW_CONTACT_ID); final String accountName = cursor.getString(PickRawContactLoader.ACCOUNT_NAME); final String accountType = cursor.getString(PickRawContactLoader.ACCOUNT_TYPE); final String dataSet = cursor.getString(PickRawContactLoader.DATA_SET); final AccountType account = AccountTypeManager.getInstance(mContext).getAccountType(accountType, dataSet); final ContactsPreferences prefs = new ContactsPreferences(mContext); final int displayNameColumn = prefs.getDisplayOrder() == ContactsPreferences.DISPLAY_ORDER_PRIMARY? PickRawContactLoader.DISPLAY_NAME_PRIMARY : PickRawContactLoader.DISPLAY_NAME_ALTERNATIVE; String displayName = cursor.getString(displayNameColumn); final TextView nameView = (TextView) view.findViewById( R.id.display_name); final TextView accountTextView = (TextView) view.findViewById( R.id.account_icon); if (!account.areContactsWritable())  displayName = mContext.getString(R.string.contact_editor_pick_raw_contact_read_only, displayName); view.setAlpha(.38f);  else  view.setAlpha(1f);  nameView.setText(displayName); accountTextView.setText(accountName); accountIconView.setImageDrawable(account.get', 'public void execute(final Pipeline pipeline)  ListNode> nodes = pipelineData.getNodes(); int attempts = Math.min(preferred, nodes.size()); final MapInteger, ResponseByteArray, Object>> responses = new ConcurrentHashMapInteger, ResponseByteArray, Object>>(); final CountDownLatch latch = new CountDownLatch(attempts); if(logger.isTraceEnabled()) logger.trace(\"Attempting \" + attempts + \" \" + pipeline.getOperation().getSimpleName() + \" operations in parallel\"); for(int i = 0; i  attempts; i++)  final Node node = nodes.get(i); pipelineData.incrementNodeIndex(); NonblockingStoreCallback callback = new NonblockingStoreCallback()  public void requestComplete(Object result, long requestTime)  if(logger.isTraceEnabled()) logger.trace(pipeline.getOperation().getSimpleName() + \" response received (\" + requestTime + \" ms.) from node \" + node.getId()); responses.put(node.getId(), new ResponseByteArray, Object>(node, key, result, requestTime)); latch.countDown();  ; if(logger.isTraceEnabled()) logger.trace(\"Submitting \" + pipeline.getOperation().getSimpleName() + \" request on node \" + node.getId()); NonblockingStore store = nonblockingStores.get(node.getId()); storeRequest.submit(node, store, callback);  try  latch.await(timeoutMs, TimeUnit.MILLISECONDS);  catch(InterruptedException e)  if(logger.isEnabledFor(Level.WARN))', 'public void paint(Graphics g)  Graphics2D g2 = (Graphics2D) g; g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON); g2.translate(getWidth() / 2, getHeight() / 2); for (Object outputValue : outputValues)  if (outputValue instanceof Grob)  // todo: handle canvas clipping ((Grob) outputValue).draw(g2);', 'public ClusterDescription getDescription(final long maxWaitTime, final TimeUnit timeUnit)  isTrue(\"open\",!isClosed()); try  CountDownLatch currentPhase = phase.get(); ClusterDescription curDescription = description; long endTime = System.nanoTime() + NANOSECONDS.convert(maxWaitTime, timeUnit); while (curDescription.getType() == ClusterType.UNKNOWN)  if (curDescription.isConnecting())  throw new MongoServerSelectionException(format(\"Unable to connect to any servers\"));  long timeout = endTime - System.nanoTime(); LOGGER.info(format(\"Cluster description not yet available. Waiting for %d ms before timing out\", MILLISECONDS.convert(timeout, NANOSECONDS))); if (!currentPhase.await(timeout, NANOSECONDS))  throw new MongoTimeoutException(format(\"Timed out while waiting to connect after %d ms\", MILLISECONDS.convert(timeout, NANOSECONDS)));  currentPhase = phase.get(); curDescription = description;  return curDescription;  catch (InterruptedException e)  throw new MongoInterruptedException(format(\"Interrupted while waiting to connect\"), e);', 'public void execute(Pipeline pipeline)  ListNode> nodes = null; try  nodes = getNodes(key);  catch(VoldemortException e)  pipelineData.setFatalError(e); pipeline.abort(); return;  if(logger.isDebugEnabled()) logger.debug(\"Adding \" + nodes.size() + \" node(s) to preference list\"); // Reorder nodes according to operation if(pipelineData.getZonesRequired()!= null)  if(pipelineData.getZonesRequired() > this.clientZone.getProximityList().size())  throw new VoldemortException(\"Number of zones required should be less than the total number of zones\");  if(pipelineData.getZonesRequired() > required)  throw new VoldemortException(\"Number of zones required should be less than the total number of zones\");  if(pipelineData.getZonesRequired() > required)  throw new VoldemortException(\"Number of zones required should be less than the required number of \" + pipeline.getOperation().getSimpleName() + \"s\");  // Create zone id to node mapping MapInteger, ListNode>> zoneIdToNode = new HashMapInteger, ListNode>>(); for(Node node: nodes)  ListNode> nodesList = null; if(zoneIdToNode.containsKey(node.getZoneId()))  nodesList = zoneIdToNode.get(node.getZoneId());  else  nodesList = new ArrayListNode>(); zoneIdToNode.put(node.getZoneId(), nodesList);  nodesList.add(node);  nodes = new ArrayList', 'public Object[] getActualValues(boolean nullsOk) throws CouldNotGenerateValueException  Object[] values= new Object[fAssigned.size()]; for (int i= 0; i  values.length; i++)  values[i]= fAssigned.get(i).getValue(getTarget()); if (values[i] == null &&!nullsOk) throw new CouldNotGenerateValueException();  return values;', 'public ListClusterNodeDescriptor> createClusterNodeDescriptors(ListString> hostNames, Cluster cluster)  ListClusterNodeDescriptor> list = new ArrayListClusterNodeDescriptor>(); for(int i = 0; i  hostNames.size(); i++)  String hostName = hostNames.get(i); ListInteger> partitions = cluster.getNodeById(i).getPartitionIds(); ClusterNodeDescriptor cnd = new ClusterNodeDescriptor(); cnd.setHostName(hostName); cnd.setId(i); cnd.setPartitions(partitions); list.add(cnd);  return list;', 'private ListMavenProject> includeAlsoMakeTransitively( ListMavenProject> projects, MavenExecutionRequest request, ProjectDependencyGraph graph ) throws MavenExecutionException  ListMavenProject> result = projects; String makeBehavior = request.getMakeBehavior(); boolean makeBoth = MavenExecutionRequest.REACTOR_MAKE_BOTH.equals( makeBehavior ); boolean makeUpstream = makeBoth || MavenExecutionRequest.REACTOR_MAKE_UPSTREAM.equals( makeBehavior ); boolean makeDownstream = makeBoth || MavenExecutionRequest.REACTOR_MAKE_DOWNSTREAM.equals( makeBehavior ); if ( StringUtils.isNotEmpty( makeBehavior ) &&!makeUpstream &&!makeDownstream )  throw new MavenExecutionException( \"Invalid reactor make behavior: \" + makeBehavior, request.getPom() );  if ( makeUpstream || makeDownstream )  SetMavenProject> projectsSet = new HashSet>( projects ); for ( MavenProject project : projects )  if ( makeUpstream )  projectsSet.addAll( graph.getUpstreamProjects( project, true ) );  if ( makeDownstream )  projectsSet.addAll( graph.getDownstreamProjects( project, true ) );   result = new ArrayList>( projectsSet ); // Order the new list in the original order ListMavenProject> sortedProjects = graph.getSortedProjects(); result.sort( comparing( sortedProjects::indexOf ) );  return result;', 'public void testRecursive() throws Exception  String src = \"[InsertPage page=\\'ThisPage\\']\"; testEngine.saveText(\"ThisPage\",src); // Just check that it contains a proper error message; don\\'t bother do HTML // checking. assertTrue( testEngine.getHTML(\"ThisPage\").indexOf(\"Circular reference\")!= -1 );', 'public void storeInLocalRepository( ArtifactRepository localRepository ) throws ArtifactMetadataRetrievalException  try  if ( timestamp == null )  timestamp = getUtcDateFormatter().format( new Date() );  String path = getLocalRepositoryLocation( localRepository ).getPath(); FileUtils.fileWrite( path, constructVersion() ); lastModified = new File( path ).lastModified();  catch ( IOException e )  throw new ArtifactMetadataRetrievalException( \"Unable to retrieve metadata\", e );  catch ( ArtifactPathFormatException e )  throw new ArtifactMetadataRetrievalException( \"Unable to retrieve metadata\", e );', 'public void startIfReady(ExecutorService executor)  synchronized (this)  final boolean isActive = mActiveTask!= null &&!mActiveTask.isDone(); if (isReadyToStart() &&!isActive)  if (mStatus!= Impl.STATUS_RUNNING)  mStatus = Impl.STATUS_RUNNING; ContentValues values = new ContentValues(); values.put(Impl.COLUMN_STATUS, mStatus); mContext.getContentResolver().update(getAllDownloadsUri(), values, null, null);  final DownloadThread task = new DownloadThread( mContext, mSystemFacade, this, mNotifier); mActiveTask = executor.submit(task);', 'public void execute(NodeProbe probe)  PrintStream out = probe.output().out; if (clearConnectionHistory)  out.println(\"Clearing connection history\"); probe.clearConnectionHistory(); return;  if (connectionsByProtocolVersion)  SimpleDateFormat sdf = new SimpleDateFormat(\"MMM dd, yyyy HH:mm:ss\"); out.println(\"Clients by protocol version\"); out.println(); ListMapString, String>> clients = (ListMapString, String>>) probe.getClientMetric(\"clientsByProtocolVersion\"); if (!clients.isEmpty())  TableBuilder table = new TableBuilder(); table.add(\"Protocol-Version\", \"IP-Address\", \"Last-Seen\"); for (MapString, String> client : clients)  table.add(client.get(ClientStat.PROTOCOL_VERSION), client.get(ClientStat.INET_ADDRESS), sdf.format(new Date(Long.valueOf(client.get(ClientStat.LAST_SEEN_TIME))));  table.printTo(out); out.println();  return;  if (listConnections)  ListMapString, String>> clients = (ListMapString, String>>) probe.getClientMetric(\"connections\"); if (!clients.isEmpty())  TableBuilder table = new TableBuilder(); table.add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\", \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\", \"Driver-Version\"); for (MapString, String> conn : clients)  table.add', 'public static PomClassicDomainModel build( ListDomainModel> domainModels, ListInterpolatorProperty> interpolationProperties ) throws IOException  PomClassicDomainModel child = null; for ( DomainModel domainModel : domainModels )  if(domainModel.isMostSpecialized())  child = (PomClassicDomainModel) domainModel;   if(child == null)  throw new IOException(\"Could not find child model\");  ListProcessor> processors = Arrays.Processor> asList( new BuildProcessor( new ArrayListProcessor>() ), new ModuleProcessor(), new PropertiesProcessor(), new ParentProcessor(), new OrganizationProcessor(), new MailingListProcessor(), new IssueManagementProcessor(), new RepositoriesProcessor(), new DistributionManagementProcessor(), new LicensesProcessor(), new PrerequisitesProcessor(), new ContributorsProcessor(), new DevelopersProcessor() ); Model target = processModelsForInheritance( convertDomainModelsToMavenModels( domainModels ), processors, true ); PomClassicDomainModel model = convertToDomainModel( target, false ); interpolateModelProperties( model.getModelProperties(), interpolationProperties, child ); ListModelProperty> modelProperties; if ( child.getProjectDirectory()!= null )  modelProperties = alignPaths( model.getModelProperties(), child.getProjectDirectory() );  else  modelProperties = model.getModelProperties();  return new PomClassicDomainModel( modelProperties );', 'private void getMore()  Connection connection = connectionSource.getConnection(); try  if (serverIsAtLeastVersionThreeDotTwo(connection.getDescription()))  try  CommandResultDocumentCodec.create(decoder, \"nextBatch\"), connectionSource.getSessionContext()));  catch (MongoCommandException e)  throw translateCommandException(e, serverCursor);   else  initFromQueryResult(connection.getMore(namespace, serverCursor.getId(), getNumberToReturn(limit, batchSize, count), decoder));  if (limitReached())  killCursor(connection);  if (serverCursor == null)  this.connectionSource.release(); this.connectionSource = null;   finally  connection.release();', 'protected abstract void processAllTablesOnPage(TestPage testPage) throws TestExecutionException; protected void processTable(SlimTable table) throws TestExecutionException  ListSlimAssertion> assertions = table.getAssertions(); MapString, Object> instructionResults; if (!stopTestCalled &&!stopSuiteCalled)  instructionResults = slimClient.invokeAndGetResponse(SlimAssertion.getInstructions(assertions));  else  instructionResults = Collections.emptyMap();  evaluateTables(assertions, instructionResults);', 'void transform(Path src, Path dest, Model model)  Model consumer = null; String version; // This is a bit of a hack, but all models are cached, so not sure why we\\'d need to parse it again ModelCache cache = DefaultModelCache.newInstance(session); Object modelData = cache.get(new FileModelSource(src.toFile()), \"raw\"); if (modelData!= null)  try  Method getModel = modelData.getClass().getMethod(\"getModel\"); getModel.setAccessible(true); org.apache.maven.model.Model cachedModel = (org.apache.maven.model.Model) getModel.invoke(modelData); consumer = cachedModel.getDelegate();  catch (Exception e)  throw new RuntimeException(e);   if (consumer == null)  TransformerContext context = (TransformerContext) session.getData().get(TransformerContext.KEY); Result? extends org.apache.maven.model.Model> result = modelBuilder.buildRawModel( src.toFile(), ModelBuildingRequest.VALIDATION_LEVEL_MINIMAL, false, context); if (result.hasErrors())  throw new IllegalStateException( \"Unable to build POM \" + src, result.getProblems().iterator().next().getException());  consumer = result.get().getDelegate();  // raw to consumer transform if (BOM_PACKAGING.equals(consumer.getPackaging()))  consumer = consumer.withRoot(false).withModules(null); if (consumer.getParent()!= null)  consumer = consumer.withParent(consumer.getParent()', 'private void mergePluginWithDefaults( Plugin plugin, Plugin def )  if ( plugin.getVersion() == null && def.getVersion()!= null )  plugin.setVersion( def.getVersion() );  List pluginGoals = plugin.getGoals(); if( pluginGoals == null || pluginGoals.isEmpty() )  plugin.setGoals( def.getGoals() );  Xpp3Dom pluginConfiguration = (Xpp3Dom) plugin.getConfiguration(); if( pluginConfiguration == null )  plugin.setConfiguration( def.getConfiguration() );', 'private ProjectDependencyGraph createDependencyGraph( ProjectSorter sorter, MavenExecutionRequest request ) throws MavenExecutionException  ProjectDependencyGraph graph = new DefaultProjectDependencyGraph( sorter ); if (!request.getSelectedProjects().isEmpty() )  File reactorDirectory = request.getPom().getParentFile().getAbsoluteFile(); MapFile, MavenProject> projectsByFile = new HashMapFile, MavenProject>(); for ( MavenProject project : sorter.getSortedProjects() )  projectsByFile.put( project.getFile(), project );  ListMavenProject> selectedProjects = new ArrayListMavenProject>( request.getSelectedProjects().size() ); for ( String selectedProject : request.getSelectedProjects() )  File pomFile = new File( reactorDirectory, selectedProject ); if ( pomFile.isDirectory() )  pomFile = new File( pomFile, Maven.POMv4 );  MavenProject project = projectsByFile.get( pomFile ); if ( project!= null )  selectedProjects.add( project );  else  throw new MavenExecutionException( \"Could not find project in reactor: \" + selectedProject, request.getPom() );   boolean makeUpstream = false; boolean makeDownstream = false; if ( MavenExecutionRequest.REACTOR_MAKE_UPSTREAM.equals( request.getMakeBehavior() ) )  makeUpstream = true;  else if ( MavenExecutionRequest.REACTOR_MAKE_DOWNSTREAM.equals( request.getMakeBehavior()', 'public static String getTypeName(EclipseNode type)  String typeName = getSingleTypeName(type); EclipseNode upType = type.up(); while (upType.getKind() == Kind.TYPE)  typeName = getSingleTypeName(upType) + \".\" + typeName; upType = upType.up();  return typeName;', 'public static String getClipboard(Context context)  ClipboardManager clipboard = (ClipboardManager) context.getSystemService(Context.CLIPBOARD_SERVICE); ClipboardText text = clipboard.getText(); return text.toString();', 'private Proxy getProxy( RepositorySystemSession session, ArtifactRepository repository )  if ( session!= null )  ProxySelector selector = session.getProxySelector(); if ( selector!= null )  RepositoryUtils.toRepo repositoryUtils.toRepo( repositoryUtils.toRepo( repositoryUtils.toRepo( repository ) ); if ( proxy!= null )  Proxy p = new Proxy(); p.setHost( proxy.getHost() ); p.setProtocol( proxy.getType() ); p.setPort( proxy.getPort() ); if ( proxy.getAuthentication()!= null )  p.setUserName( proxy.getAuthentication().getUsername() ); p.setPassword( proxy.getAuthentication().getPassword() );  return p;    return null;', 'protected abstract boolean isSupportedBy(Index index, ColumnMetadata def); public static class EQRestriction extends MultiColumnRestriction  protected final Term value; public EQRestriction(ListColumnMetadata> columnDefs, Term value)  super(columnDefs); this.value = value;  @Override public boolean isEQ()  return true;  @Override public void addFunctionsTo(ListFunction> functions)  value.addFunctionsTo(functions);  @Override public String toString()  return String.format(\"EQ(%s)\", value);  @Override public SingleRestriction doMergeWith(SingleRestriction otherRestriction)  throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes an Equal\", getColumnsInCommons(otherRestriction));  @Override protected boolean isSupportedBy(Index index, ColumnMetadata column)  return index.supportsExpression(column, Operator.EQ);  @Override public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)  Tuples.Value t = ((Tuples.Value) value.bind(options)); ListByteBuffer> values = t.getElements(); for (int i = 0, m = values.size(); i  m; i++)  builder.addElementToAll(values.get(i)); checkFalse(builder.containsNull(), \"Invalid null value for column %s\", columnDefs.get(i).name);  return builder;  @Override public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)  Tuples.Valu', \"public void constructTree(PwGroupV3 currentGroup)  // I'm in root if (currentGroup == null)  rootGroup = new PwGroupV3(); VectorPwGroupV3> rootChildGroups = getGrpRoots(); rootGroup.childGroups = rootChildGroups; rootGroup.childEntries = new VectorPwEntryV3>(); rootGroup.level = -1; for (int i=0; irootChildGroups.size(); i++)  rootChildGroups.elementAt(i).parent = rootGroup; constructTree(rootChildGroups.elementAt(i));  return;  // I'm in non-root // get child groups currentGroup.childGroups = getGrpChildren(currentGroup); currentGroup.childEntries = getEntries(currentGroup); // set parent in child entries for (int i=0; icurrentGroup.childEntries.size(); i++)  currentGroup.childEntries.elementAt(i).parent = currentGroup;  // recursively construct child groups for (int i=0; icurrentGroup.childGroups.size(); i++)  currentGroup.childEntries.elementAt(i).parent = currentGroup; constructTree(currentGroup.childGroups.elementAt(i));  return;\", 'public void execute() throws MavenReportException  Sink sink = null; try  sink = getSink();  catch( IOException e )  throw new MavenReportException( \"Can\\'t obtain sink for PMD report.\", e );  PMD pmd = new PMD(); RuleContext ruleContext = new RuleContext(); Report report = new Report(); PmdReportListener reportSink = new PmdReportListener( sink ); report.addListener( reportSink ); ruleContext.setReport( report ); RuleSetFactory ruleSetFactory = new RuleSetFactory(); RuleSet ruleSet = ruleSetFactory.createRuleSet( pmd.getClass().getResourceAsStream( \"/rulesets/controversial.xml\" ) ); reportSink.beginDocument(); List files; try  files = getFilesToProcess( \"**/*.java\", null );  catch( IOException e )  throw new MavenReportException( \"Can\\'t parse \" + getConfiguration().getSourceDirectory(), e );  for ( Iterator i = files.iterator(); i.hasNext(); )  File file = (File) i.next(); FileReader fileReader; try  fileReader = new FileReader( file );  catch ( FileNotFoundException e )  throw new MavenReportException( \"Error opening source file: \" + file, e );  try  // TODO: lazily call beginFile in case there are no rules reportSink.beginFile( file ); pmd.processFile( fileReader, ruleSet, ruleContext ); reportSink.endFile( file );  catch ( PMDException e )  throw new MavenReportException( \"Failure executing PMD for: \" + file,', 'private static void mergePluginExecutionDefinitions( PluginExecution child, PluginExecution parent )  if ( child.getPhase() == null )  child.setPhase( parent.getPhase() );  List parentGoals = parent.getGoals(); List childGoals = child.getGoals(); List goals = new ArrayList(); if ( childGoals!= null &&!childGoals.isEmpty() )  goals.addAll( childGoals );  if ( parentGoals!= null )  for ( Iterator goalIterator = parentGoals.iterator(); goalIterator.hasNext(); )  String goal = (String) goalIterator.next(); if (!goals.contains( goal ) )  goals.add( goal );    child.setGoals( goals ); Xpp3Dom childConfiguration = (Xpp3Dom) child.getConfiguration(); Xpp3Dom parentConfiguration = (Xpp3Dom) parent.getConfiguration(); childConfiguration = Xpp3Dom.mergeXpp3Dom( childConfiguration, parentConfiguration ); child.setConfiguration( childConfiguration );', 'public String[] getTokenizedValue(String xpath)  StringTokenizer st = new StringTokenizer(reader.getValue(xpath)); String[] hosts = new String[st.countTokens()]; for (int i = 0; st.hasMoreTokens(); i++) hosts[i] = st.nextToken(); return hosts;', 'public MavenExecutionResult execute( MavenExecutionRequest request )  request.setStartTime( new Date() ); EventDispatcher dispatcher = new DefaultEventDispatcher( request.getEventMonitors() ); String event = MavenEvents.REACTOR_EXECUTION; dispatcher.dispatchStart( event, request.getBaseDirectory() ); MavenExecutionResult result; result = doExecute( request, dispatcher ); if ( result.hasExceptions() )  for ( Iterator i = result.getExceptions().iterator(); i.hasNext(); )  Exception e = (Exception) i.next(); dispatcher.dispatchError( event, request.getBaseDirectory(), e ); logError( e, request.isShowErrors() ); stats( request.getStartTime() ); line();   // Either the build was successful, or it was a fail_at_end/fail_never reactor build // TODO: should all the logging be left to the CLI? logReactorSummary( result.getReactorManager() ); if ( result.getReactorManager().hasBuildFailures() )  logErrors( result.getReactorManager(), request.isShowErrors() ); if (!result.getReactorManager().FAIL_NEVER.equals( result.getReactorManager().getFailureBehavior() ) )  dispatcher.dispatchError( event, request.getBaseDirectory(), null ); getLogger().info( \"BUILD ERRORS\" ); line(); stats( request.getStartTime() ); line(); return new DefaultMavenExecutionResult( Collections.singletonList( new MavenExecutionException( \"Some builds failed\" ) )', 'private boolean isActiveShardedTxn()  ClusterType clusterType = wrapped.getCluster().getDescription(); return session.hasActiveTransaction() && type == ClusterType.SHARDED;', 'private void doPostSubmit(String master, String user, String password, ArrayListString> argList) throws IOException  String url = master + argList.get(0) + \\'/\\' + argList.get(1) + \\'/\\' + argList.get(2); MultipartPostMethod post = new MultipartPostMethod(url); if (user!= null) post.addParameter(\"sun\", user); if (password!= null) post.addParameter(\"sp\", password); int submitCount = 0; for (int i = 3; i  argList.size(); i++)  try  post.addParameter(\"configfile\", new File(argList.get(i))); ++submitCount;  catch (FileNotFoundException e)  System.err.println(\"File \" + argList.get(i) + \" not found.\");   if (submitCount == 0)  throw new IOException(\"No run submitted!\");  makeRequest(post);', 'protected ArtifactRepository createRemoteArtifactRepository( RemoteRepository repository )  ArtifactRepositoryLayout repositoryLayout = (ArtifactRepositoryLayout) lookup( ArtifactRepositoryLayout.ROLE, repository.getLayout() ); WagonManager manager = (WagonManager) lookup( WagonManager.ROLE ); Authentication authentication = repository.getAuthentication(); if ( authentication!= null )  manager.addAuthenticationInfo( \"remote\", authentication.getUserName(), authentication.getPassword(), authentication.getPrivateKey(), authentication.getPassphrase() );  Proxy proxy = repository.getProxy(); if ( proxy!= null )  manager.addProxy( proxy.getType(), proxy.getHost(), proxy.getPort(), proxy.getUserName(), proxy.getPassword(), proxy.getNonProxyHosts() );  ArtifactRepository artifactRepository; if ( repository.getSnapshotPolicy()!= null )  artifactRepository = new ArtifactRepository( \"remote\", repository.getUrl(), repositoryLayout, repository.getSnapshotPolicy() );  else  artifactRepository = new ArtifactRepository( \"remote\", repository.getUrl(), repositoryLayout );  return artifactRepository;', 'public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException  WebApplicationContext ctx = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); HttpServletRequest request = (HttpServletRequest) req; request.getSession().setAttribute(\"configuration\", ctx.getBean(Configuration.class)); request.getSession().setAttribute(\"template\", new Template(ctx)); if (AuthUtil.isSessionAuthorized())  request.getSession().setAttribute(\"currentStyle\", AuthUtil.getCurrentUser().getStyle()); request.getSession().setAttribute(\"currentProfile\", AuthUtil.getCurrentProfile()); request.getSession().setAttribute(\"currentProperties\", AuthUtil.getProf()); forWikiManipulation(request, (HttpServletResponse) res, AuthUtil.getAuthentication());  else  request.getSession().setAttribute(\"currentStyle\", \"tango\"); request.getSession().setAttribute(\"currentProfile\", Profile.getDefaultProfile()); request.getSession().setAttribute(\"currentProperties\", AuthUtil.getProf());  request.setCharacterEncoding(\"utf-8\"); // »??o 1 tomcat CSRFManipulation(request, (HttpServletResponse) res); chain.doFilter(req, res);', 'private Map parse( final InputStream pInput ) throws IOException  final Map mapping = new HashMap(); final BufferedReader reader = new BufferedReader(new InputStreamReader(pInput)); while(true)  final String base = readBase(reader); if (base == null)  break;  readTotal(reader); final TarEntry dir = readDir(reader, base); mapping.put(dir.getName(), dir); System.out.println(dir.getName()); while(true)  final TarEntry file = readFile(reader, base); if (file == null)  break;  mapping.put(file.getName(), file); System.out.println(file.getName());   return mapping;', 'private T> void sendCommandMessageAsync(final int messageId, final DecoderT> decoder, final SessionContext sessionContext, final SingleResultCallbackT> callback, final ByteBufferBsonOutput bsonOutput, final CommandEventSender commandEventSender, final boolean responseExpected)  sendMessageAsync(bsonOutput.getByteBuffers(), messageId, (result, t) ->  bsonOutput.close(); if (t!= null)  commandEventSender.sendFailedEvent(t); callback.onResult(null, t);  else if (!responseExpected)  commandEventSender.sendSucceededEventForOneWayCommand(); callback.onResult(null, null);  else  readAsync(MESSAGE_HEADER_LENGTH, new MessageHeaderCallback((responseBuffers, t1) ->  if (t1!= null)  commandEventSender.sendFailedEvent(t1); callback.onResult(null, t1); return;  assertNotNull(responseBuffers); try  updateSessionContext(sessionContext, responseBuffers); boolean commandOk = isCommandOk(new BsonBinaryReader(new ByteBufferBsonInput(responseBuffers.getBodyByteBuffer()))); responseBuffers.reset(); if (!commandOk)  MongoException commandFailureException = getCommandFailureException( responseBuffers.getResponseDocument(messageId, new BsonDocumentCodec()), description.getServerAddress()); commandEventSender.s', 'public static byte[] generateSignature(byte[] message, ECPrivateKey pk) throws IOException  final String algo = getSignatureAlgorithmForParams(pk.getParams()); try  Signature s = Signature.getInstance(algo); s.initSign(pk); s.update(message); return s.sign();  catch (NoSuchAlgorithmException e)  IOException ex = new IOException(); ex.initCause(e); throw ex;  catch (InvalidKeyException e)  IOException ex = new IOException(); ex.initCause(e); throw ex;  catch (SignatureException e)  IOException ex = new IOException(); ex.initCause(e); throw ex;  catch (SignatureException e)  IOException ex = new IOException(); ex.initCause(e); throw ex;', 'static String getFullPath(String filename, String mimeType, int destination, File base) throws StopRequestException  String extension = null; int dotIndex = filename.lastIndexOf(\\'.\\'); boolean missingExtension = dotIndex  0 || dotIndex  filename.lastIndexOf(\\'/\\'); if (destination == Downloads.Impl.DESTINATION_FILE_URI)  // Destination is explicitly set - do not change the extension if (missingExtension)  extension = \"\";  else  extension = filename.substring(dotIndex); filename = filename.substring(0, dotIndex);   else  // Split filename between base and extension // Add an extension if filename does not have one if (missingExtension)  extension = chooseExtensionFromMimeType(mimeType, true);  else  extension = chooseExtensionFromFilename(mimeType, destination, filename, dotIndex); filename = filename.substring(0, dotIndex);   boolean recoveryDir = Constants.RECOVERY_DIRECTORY.equalsIgnoreCase(filename + extension); if (base!= null)  filename = base.getPath() + File.separator + filename;  if (Constants.LOGVV)  Log.v(Constants.TAG, \"target file: \" + filename + extension);  synchronized (sUniqueLock)  final String path = chooseUniqueFilenameLocked( destination, filename, extension, recoveryDir); // Claim this filename inside lock to prevent other threads from // clobbering us. We\\'re not paranoid enough to use O_EXCL. try  new File(path).createNewFile();  catch (IOException e)  throw new StopRequestException(Downloads.', 'public void execute() throws MojoExecutionException  // expand name pattern final String debName; final String changesName; final Map variables = new HashMap(); variables.put(\"artifactId\", getProject().getArtifactId()); variables.put(\"groupId\", getProject().getGroupId()); variables.put(\"version\", getProject().getVersion()); variables.put(\"extension\", \"deb\"); try  debName = Utils.replaceVariables(variables, namePattern, \"[\", \"]\"); variables.put(\"extension\", \"changes\"); changesName = Utils.replaceVariables(variables, namePattern, \"[\", \"]\");  catch (ParseException e)  throw new MojoExecutionException(\"Failed parsing artifact name pattern\", e);  // if not specified try to the default if (deb == null)  deb = new File(buildDirectory, debName);  // if not specified try to the default if (changesIn == null)  final File f = new File(getProject().getBasedir(), \"CHANGES.txt\"); if (f.exists() && f.isFile() && f.canRead())  changesIn = f;   // if not specified try to the default if (changesOut == null)  changesOut = new File(buildDirectory, changesName);  // if not specified try to the default if (controlDir == null)  controlDir = new File(getProject().getBasedir(), \"src/deb/control\"); getLog().info(\"Using default path to control directory \" + controlDir);  // make sure we have at least the mandatory control directory if (!controlDir.exists() ||!controlDir.isDirectory())', 'public void testConvertArgs_ConvertedToDateIfStringIsPassedIn() throws Exception  String value = \"27-FEB-2012\"; Date expectedDate = ConverterRegistry.getConverterForClass(Date.class).fromString(value); Object[] convertedArgs = convertSingleValue(value, Date.class); assertEquals(expectedDate, convertedArgs[0]);', 'public void execute() throws MojoExecutionException  try  categorizeReports(); MavenReportConfiguration config = new MavenReportConfiguration(); config.setProject( project ); config.setReportOutputDirectory( new File( outputDirectory ) ); //Generate reports if ( reports!= null )  for ( Iterator i = reports.keySet().iterator(); i.hasNext(); )  String reportKey = (String) i.next(); getLog().info( \"Generate \" + reportKey + \" report.\" ); MavenReport report = (MavenReport) reports.get( reportKey ); report.setConfiguration( config ); XhtmlSink sink = siteRenderer.createSink( new File( siteDirectory ), report.getOutputName() + \".html\", outputDirectory, getSiteDescriptor(), flavour ); report.generate( sink );   //Generate overview pages if ( projectInfos.size() > 0 )  try  generateProjectInfoPage( getSiteDescriptor() );  catch ( Exception e )  throw new MojoExecutionException( \"An error is occurred in project info page generation.\", e );   if ( projectReports.size() > 0 )  try  generateProjectReportsPage( getSiteDescriptor() );  catch ( Exception e )  throw new MojoExecutionException( \"An error is occurred in project reports page generation.\", e );   File cssDirectory = new File( siteDirectory, \"css\" ); File imagesDirectory = new File( siteDirectory, \"images\" ); // special case for backwards compatibility if ( cssDirectory.exists() || imagesDirectory.exists() )  getLog().warn( \"DEP', 'private void updateMatchScoresBasedOnPhoneMatches(SQLiteDatabase db, long rawContactId, RawContactMatcher matcher)  Cursor c = db.query(PhoneLookupQuery.TABLE, PhoneLookupQuery.COLUMNS, PhoneLookupQuery.SELECTION, mSelectionArgs2, null, null, null, SECONDARY_HIT_LIMIT_STRING); try  while (c.moveToNext())  long rId = c.getLong(PhoneLookupQuery.RAW_CONTACT_ID); if (rId == rawContactId)  continue;  long contactId = c.getLong(PhoneLookupQuery.CONTACT_ID); long accountId = c.getLong(PhoneLookupQuery.ACCOUNT_ID); matcher.updateScoreWithPhoneNumberMatch(rId, contactId, accountId);   finally  c.close();', 'public File newFolder(String folderName)  File file= new File(folder, folderName); File newFolder = new File(folder, fileName); File newFolder = new File(newFolder); file.mkdir(); return file;', 'protected abstract List getNormalChildren() throws Exception; public List getChildren() throws Exception  List children = getNormalChildren(); WikiPageProperties props = getData().getProperties(); for(Iterator iterator = props.getSymbolicLinkNames().iterator(); iterator.hasNext();)  String linkName = (String) iterator.next(); WikiPage page = createSymbolicPage(props, linkName); if(page!= null) children.add(page);  return children;', 'public static void validateReadOnlyStores(Cluster cluster, ListStoreDefinition> storeDefs, AdminClient adminClient)  ListStoreDefinition> readOnlyStores = filterStores(storeDefs, true); if(readOnlyStores.size() == 0)  // No read-only stores return;  ListString> storeNames = getStoreNames(readOnlyStores); for(Node node: cluster.getNodes())  if(node.getNumberOfPartitions()!= 0)  for(EntryString, String> storeToStorageFormat: adminClient.getROStorageFormat(node.getId(), storeNames).entrySet())  if(storeToStorageFormat.getValue().compareTo(ReadOnlyStorageFormat.READONLY_V2.getCode())!= 0)  throw new VoldemortRebalancingException(\"Cannot rebalance since node \" + node.getId() + \" has store \" + storeToStorageFormat.getKey() + \" not using format \" + ReadOnlyStorageFormat.READONLY_V2);', \"public void mergeDuplicates( Model model, ModelBuildingRequest request, ModelProblemCollector problems )  Build build = model.getBuild(); if ( build!= null )  ListPlugin> original = build.getPlugins(); MapObject, Plugin> normalized = new LinkedHashMapObject, Plugin>(); for ( Plugin plugin : original )  Object key = plugin.getKey(); Plugin first = normalized.get( key ); if ( first!= null )  merger.mergePlugin( plugin, first );  normalized.put( key, plugin );  build.setPlugins( new ArrayListPlugin>( normalized.values() ) );  /* * NOTE: This is primarily to keep backward-compat with Maven 2.x which did not validate that dependencies are * unique within a single POM. Upon multiple declarations, 2.x just kept the last one but retained the order of * the first occurrence. So when we're in lenient/compat mode, we have to deal with such broken POMs and mimic * the way 2.x works. When we're in strict mode, the removal of duplicates just saves other merging steps from * aftereffects and bogus error messages. */ MapString, Dependency> dependencies = new LinkedHashMapString, Dependency>(); for ( Dependency dependency : model.getDependencies() )  dependencies.put( dependency.getManagementKey(), dependency );  model.setDependencies( new ArrayListDependency>( dependencies.values() ) );\", 'private BsonDocument manufactureGetLastErrorResponse(final MongoBulkWriteException e)  BsonDocument response = new BsonDocument(); addBulkWriteResultToResponse(e.getWriteResult(), response); if (e.getWriteConcernError()!= null)  response.putAll(e.getWriteConcernError().getDetails());  if (getLastError(e)!= null)  response.put(\"err\", new BsonString(getLastError(e).getMessage())); response.put(\"code\", new BsonInt32(getLastError(e).getCode())); response.putAll(getLastError(e).getDetails());  else if (e.getWriteConcernError()!= null)  response.put(\"err\", new BsonString(e.getWriteConcernError().getMessage())); response.put(\"code\", new BsonInt32(e.getWriteConcernError().getCode()));  return response;', 'private SynchronousContactsProvider2 setUpCorpProvider() throws Exception  mActor.mockUserManager.setUsers(MockUserManager.PRIMARY_USER, MockUserManager.CORP_USER); // Note here we use a standalone CP2 so it\\'ll have its own db helper. // Also use AlteringUserContext here to report the corp user id. SynchronousContactsProvider2 provider = mActor.addProvider( StandaloneContactsProvider2.class, \"\" + MockUserManager.CORP_USER.id + \"@com.android.contacts\", new AlteringUserContext(mActor.getProviderContext(), MockUserManager.CORP_USER.id)); provider.wipeData(); return provider;', 'public int hashCode()  String k = method.hashCode(); if (k.hashCode() == null)  return k.hashCode();  return nArgs * 31 + method.hashCode() + 31 * k.hashCode();', 'private File writeExportReadyPom( MavenProject project ) throws IOException  String buildDirectory = project.getBuild().getDirectory(); File fullPom = new File( buildDirectory, \"exported-pom.xml\" ); FileWriter fWriter = null; try  fWriter = new FileWriter( fullPom ); project.writeModel( fWriter );  finally  IOUtil.close( fWriter );  return fullPom;', 'public Dialog onCreateDialog(Bundle savedInstanceState)  // Build a dialog with two buttons and a view of a single EditText input field final AlertDialog.Builder builder = new AlertDialog.Builder(getActivity()).setTitle(mIsInsert? R.string.group_name_dialog_insert_title : R.string.group_name_dialog_update_title).setView(R.layout.group_name_edit_dialog).setNegativeButton(android.R.string.cancel, new OnClickListener()  @Override public void onClick(DialogInterface dialog, int which)  getListener().onGroupNameEditCancelled(); dismiss();  ).setPositiveButton(android.R.string.ok, new OnClickListener()  @Override public void onClick(DialogInterface dialog, int which)  getListener().onGroupNameEdit(getGroupName());  ); // Disable the create button when the name is empty final AlertDialog alertDialog = builder.create(); alertDialog.setOnShowListener(new DialogInterface.OnShowListener()  @Override public void onShow(DialogInterface dialog)  mGroupNameEditText = (EditText) alertDialog.findViewById(android.R.id.text1); if (!TextUtils.isEmpty(mGroupName))  mGroupNameEditText.setText(mGroupName); mGroupNameEditText.setSelection(mGroupName.length());  final Button createButton = alertDialog.getButton(AlertDialog.BUTTON_POSITIVE); createButton.setEnabled(!TextUtils.isEmpty(getGroupName', 'public String isSelected( String ifSelected, String ifNotSelected )  if ( m_editorName.equals(m_editorManager.getEditorName(m_wikiContext) ) )  return ifSelected;  return ifNotSelected;', 'private void readConnectRequest() throws IOException, InterruptedException, ClientCnxnLimitException  if (!isZKServerRunning())  throw new IOException(\"ZooKeeperServer not running\");  zkServer.processConnectRequest(this, incomingBuffer); initialized = true;', \"private MavenProject processProjectLogic( MavenProject project, ArtifactRepository localRepository, boolean resolveDependencies ) throws ProjectBuildingException, ModelInterpolationException, ArtifactResolutionException  Model model = modelInterpolator.interpolate( project.getModel() ); // interpolation is before injection, because interpolation is off-limits in the injected variables modelDefaultsInjector.injectDefaults( model ); MavenProject parentProject = project.getParent(); File projectDescriptor = project.getFile(); pathTranslator.alignToBaseDirectory( model, projectDescriptor ); project = new MavenProject( model ); project.setFile( projectDescriptor ); project.setParent( parentProject ); project.setArtifacts( artifactFactory.createArtifacts( project.getDependencies(), localRepository, null ) ); projectCache.put( createCacheKey( project.getGroupId(), project.getArtifactId(), project.getVersion() ), project ); // ------------------------------------------------------------------------------------------------------- // Typically when the project builder is being used from maven proper // the transitive dependencies will not be resolved here because this // requires a lot of work when we may only be interested in running // something simple like'm2 clean'. So the artifact collector is used // in the dependency resolution phase if it is required by any of the // goals being executed. But when used as a component in another piece // of code people may just want to build maven projects and have the // dependencies resolved for whatever reason: this is why we keep // this snippet of code here. // ------------------------------------------------------------------------------------------------------------------------------------------------- if ( resolveDependencies )  List repos = build\", 'public final boolean login( HttpServletRequest request )  if ( request == null )  throw new IllegalStateException( \"Wiki context\\'s HttpRequest may not be null\" );  WikiSession wikiSession = WikiSession.getWikiSession( request ); if ( wikiSession == null )  throw new IllegalStateException( \"Wiki context\\'s WikiSession may not be null\" );  if( c_useJAAS )  CallbackHandler handler = new WebContainerCallbackHandler( request, m_engine.getUserDatabase() ); return doLogin( wikiSession, handler, LOGIN_CONTAINER );  return true;', 'public static EclipseSourceDir[] buildDirectoryList( MavenProject project, File basedir, Log log )  File projectBaseDir = project.getFile().getParentFile(); // avoid duplicate entries Set directories = new TreeSet(); EclipseUtils.extractSourceDirs( directories, project.getCompileSourceRoots(), basedir, projectBaseDir, false, null ); EclipseUtils.extractResourceDirs( directories, project.getBuild().getResources(), project, basedir, projectBaseDir, false, null, log ); EclipseUtils.extractSourceDirs( directories, project.getTestCompileSourceRoots(), basedir, projectBaseDir, true, EclipseUtils.toRelativeAndFixSeparator( projectBaseDir, project.getBuild().getTestOutputDirectory(), false ) ); EclipseUtils.extractResourceDirs( directories, project.getBuild().getTestResources(), project, basedir, projectBaseDir, true, EclipseUtils.toRelativeAndFixSeparator( projectBaseDir, project.getBuild().getTestOutputDirectory(), false ) ); EclipseUtils.extractResourceDirs( directories, project.getBuild().getTestResources(), project, basedir, projectBaseDir, true, EclipseUtils.toRelativeAndFixSeparator( projectBaseDir, project.getBuild().getTestOutputDirectory(), false ) ); EclipseUtils.extractResourceDirs( directories, project.getBuild().getTestResources(), project, basedir, projectBaseDir, true, EclipseUtils.toRelativeAndFixSeparator( projectBaseDir, project.getBuild().', 'private Message handleDisableStoreVersion(VAdminProto.DisableStoreVersionRequest disableStoreVersion)  logger.info(\"Received DisableStoreVersionRequest: \" + disableStoreVersion.toString()); VAdminProto.DisableStoreVersionResponse.Builder response = VAdminProto.DisableStoreVersionResponse.newBuilder(); response.setNodeId(server.getMetadataStore().getNodeId()); String storeName = disableStoreVersion.getStoreName(); Long version = disableStoreVersion.getPushVersion(); try  StorageEngine storeToDisable = storeRepository.getStorageEngine(storeName); if (storeToDisable == null)  response.setDisableSuccess(false).setInfo(\"The store \\'\" + storeName + \"\\' does not exist!\");  else  StoreVersionManager storeVersionManager = (StoreVersionManager) storeToDisable.getCapability(StoreCapabilityType.DISABLE_STORE_VERSION); storeVersionManager.disableStoreVersion(version); response.setDisableSuccess(true).setDisablePersistenceSuccess(true).setInfo(\"The store \\'\" + storeName + \"\\' version \" + version + \" was successfully disabled.\");  logger.info(\"Received DisableStoreVersionRequest: \" + disableStoreVersion.toString()); VAdminProto.DisableStoreVersionResponse.Builder response = VAdminProto.DisableStoreVersionResponse.newBuilder(); response.setNodeId(server.getMetadataStore().getNodeId()); String storeName = disableStoreVersion.getStoreName(); Long version = disableStoreVersion.getPushVersion(); try  StorageEngine storeToDisable = storeRepository.getStorageEngine(storeName); if (storeToDis', 'public DBObject decode(final BsonReader reader, final DecoderContext decoderContext)  BsonBinaryWriter binaryWriter = new BsonBinaryWriter(new ByteBufferOutputBuffer(bufferProvider), true); try  binaryWriter.pipe(reader); BufferExposingByteArrayOutputStream byteArrayOutputStream = new BufferExposingByteArrayOutputStream(binaryWriter.getBuffer().size()); binaryWriter.getBuffer().pipe(byteArrayOutputStream); return decoder.decode(byteArrayOutputStream.getInternalBytes(), collection);  catch (IOException e)  // impossible with a byte array output stream throw new MongoInternalException(\"An unlikely IOException thrown.\", e);  finally  binaryWriter.close();', 'public String processUrl(HttpServletRequest request, String url)  String url = url; if (url!= null)  return url;  return url;', 'public boolean strictEquals(FreenetInetAddress addr)  if(hostname!= null)  if(addr.hostname == null) return false; if (!hostname.equalsIgnoreCase(addr.hostname))  return false;  // Now that we know we have the same hostname, we can propagate the IP. if((_address!= null) && (addr._address == null)) addr._address = _address; if((addr._address!= null) && (_address == null)) _address = addr._address; // Except if we actually do have two different looked-up IPs! if((addr._address!= null) && (_address!= null) &&!addr._address.equals(_address)) return false; // Equal. return true;  else if(addr.hostname!= null /* && hostname == null */)  return false;  // No hostname, go by address. if(!getHostName(_address).equalsIgnoreCase(getHostName(addr._address))  //Logger.minor(this, \"Addresses do not match: mine=\"+getHostName(_address)+\" his=\"+getHostName(addr._address)); return false;  return true;', 'public void roundedRect(double x, double y, double width, double height, double rx, double ry)  double dx = rx; double dy = ry; // rx/ry cannot be greater than half of the width of the rectangle // (required by SVG spec) dx = Math.min(dx, width * 0.5); dy = Math.min(dy, height * 0.5); moveto(x + dx, y); if (dx  width * 0.5) lineto(x + width - rx, y); curveto(x + width - dx * ONE_MINUS_QUARTER, y, x + width, y + dy * ONE_MINUS_QUARTER, x + width, y + dy * ONE_MINUS_QUARTER, y + height, y + height); if (dy  height * 0.5) lineto(x + width, y + height - dy * ONE_MINUS_QUARTER, y + height, y + height - dy * ONE_MINUS_QUARTER, y + height, y + height - dy * ONE_MINUS_QUARTER, y + height, y + height - dy * ONE_MINUS_QUARTER, y + height, y + height - dy); if (dy  height * 0.5) lineto(x + dx, y + height - dy * ONE_MINUS_QUARTER, y + height, y + height - dy * ONE_MINUS_QUARTER, y + height, y + height - dy); curveto(x, y + dy * ONE_MINUS_QUARTER, y + height, y + height - dy * ONE_MINUS_QUARTER, y + dy); close();', 'protected long createGroup(Account account, String sourceId, String title)  String accountId = accountId.getAccountId(); return createGroup(account, sourceId, title, 1);', 'public void ensureMinSize(final int minSize, final boolean initialize)  while (getCount()  minSize)  if (!acquirePermit(10, TimeUnit.MILLISECONDS))  break;  release(createNewAndReleasePermitIfFailure(initialize));', 'public SymbolMatch makeMatch(ScanString input)  String textString = matcher.makeMatch(SymbolType.PlainTextCellSeparator, input); return textString;', 'public String render() throws Exception  if (WikiWordWidget.isWikiWord(href))  WikiWordWidget www = new WikiWordWidget(new BlankParentWidget(this, \"\"), href); String theWord = www.getWikiWord(); WikiPagePath wikiWordPath = PathParser.parse(theWord); WikiPagePath fullPathOfWikiWord = parentPage.getPageCrawler().getFullPathOfChild(parentPage, wikiWordPath); String qualifiedName = PathParser.render(fullPathOfWikiWord); if (parentPage.getPageCrawler().pageExists(parentPage, PathParser.parse(theWord))) return (\"a href=\"\" + qualifiedName + \"\">\" + childHtml() + \"/a>\"); else if (getWikiPage() instanceof ProxyPage) return makeAliasLinkToNonExistentRemotePage(theWord); else return (childHtml() + \"a href=\"\" + qualifiedName + \"?edit\">?/a>\");  else  String tagValue = childHtml(); if (hrefContainsVariable())  return (\"a href=\"\" + substitutedHRefVariableValue() + \"\">\" + tagValue + \"/a>\");  else  return (\"a href=\"\" + href + \"\">\" + tagValue + \"/a>\");', 'private void processOneSetOfInstructions() throws Exception  String instructions = getInstructionsFromClient(); if (instructions!= null)  ListObject> results = executeInstructions(instructions); sendResultsToClient(results);', 'public void setUp()  assumeTrue(canRunTests()); ServerVersion serverVersion = ClusterFixture.getServerVersion(); if (definition.containsKey(\"ignore_if_server_version_less_than\"))  assumeFalse(serverVersion.compareTo(getServerVersion(\"ignore_if_server_version_less_than\"))  0);  if (definition.containsKey(\"ignore_if_server_version_greater_than\"))  assumeFalse(serverVersion.compareTo(getServerVersion(\"ignore_if_server_version_greater_than\")) > 0);  if (definition.containsKey(\"ignore_if_topology_type\"))  BsonArray topologyTypes = definition.getArray(\"ignore_if_topology_type\"); for (BsonValue type : topologyTypes)  String typeString = type.asString().getValue(); if (typeString.equals(\"sharded\"))  assumeFalse(isSharded());  else if (typeString.equals(\"replica_set\"))  assumeFalse(isDiscoverableReplicaSet());  else if (typeString.equals(\"standalone\"))  assumeFalse(isStandalone());    collectionHelper = new CollectionHelperDocument>(new DocumentCodec(), new MongoNamespace(databaseName, collectionName)); BsonDocument clientOptions = definition.getDocument(\"clientOptions\", new BsonDocument()); MongoClient = MongoClients.create(getMongoClientSettingsBuilder().retryWrites(clientOptions.getBoolean(\"retryWrites\", BsonBoolean', 'public static boolean isNewerOrEqual( final String version ) throws IllegalArgumentException  if( version == null )  return true;  final String[] versionComponents = StringUtils.split( version, VERSION_SEPARATORS ); final int reqVersion = versionComponents.length > 0? Integer.parseInt( versionComponents[0] ) : Release.VERSION; final int reqRevision = versionComponents.length > 1? Integer.parseInt( versionComponents[1] ) : Release.REVISION; final int reqMinorRevision = versionComponents.length > 2? Integer.parseInt( versionComponents[2] ) : Release.MINORREVISION; if( VERSION == reqVersion )  if( REVISION == reqRevision )  if( MINORREVISION == reqMinorRevision )  return true;  return MINORREVISION > reqMinorRevision;  return REVISION > reqVersion;  return VERSION > reqVersion;', 'public void sendStartedEvent()  if (loggingRequired())  ListEntry> entries = new ArrayList>(); StringBuilder builder = new StringBuilder(\"Command \"%s\" started on database %s\"); entries.add(new Entry(\"commandName\", commandName)); entries.add(new Entry(\"databaseName\", message.getNamespace().getDatabaseName())); appendCommonLogFragment(entries, builder); builder.append(\" Command: %s\"); entries.add(new Entry(\"command\", redactionRequired? \"\" : getTruncatedJsonCommand(commandDocument))); logger.log(new StructuredLogMessage(COMMAND, DEBUG, \"Command started\", getClusterId(), entries), builder.toString());  if (eventRequired())  BsonDocument commandDocumentForEvent = redactionRequired? new BsonDocument() : commandDocument; sendCommandStartedEvent(message, message.getNamespace().getDatabaseName(), commandName, commandDocumentForEvent, description, assertNotNull(commandListener), requestContext, operationContext);  // the buffer underlying the command document may be released after the started event, so set to null to ensure it\\'s not used // when sending the failed or succeeded event commandDocument = null;', 'protected HttpRequest prepareRequest(URI uri, String h)  // Send the HTTP request. HttpRequest request = new DefaultHttpRequest( HttpVersion.HTTP_1_1, HttpMethod.GET, uri.toASCIIString()); request.setHeader(HttpHeaders.Names.HOST, h); if (getHttpUser()!= null)  request.setHeader(HttpHeaders.AUTHORIZATION, ConfigurationProviderHTTP.buildAuthHeader(getHttpUser(), getHttpPass()));  request.setHeader(HttpHeaders.CONNECTION, HttpHeaders.Values.CLOSE); // No keep-alives for this request.setHeader(HttpHeaders.CACHE_CONTROL, HttpHeaders.Values.NO_CACHE); request.setHeader(HttpHeaders.ACCEPT, \"application/json\"); request.setHeader(HttpHeaders.USER_AGENT, \"spymemcached vbucket client\"); request.setHeader(\"X-memcachekv-Store-Client-Specification-Version\", CLIENT_SPEC_VER); return request;', 'public FitTestSystem create(Descriptor descriptor) throws IOException  FitTestSystem name = descriptor.getTestSystemName(); CommandRunningFitClient fitClient = new InProcessFitClientBuilder(descriptor).build(); return new FitTestSystem(name, fitClient);', 'public void shouldThrowExceptionIfRequestIdDoesNotMatchResponseTo()  int badResponseTo = 34565; int expectedResponseTo = 5; ByteBuffer headerByteBuffer = ByteBuffer.allocate(36); headerByteBuffer.order(ByteOrder.LITTLE_ENDIAN); headerByteBuffer.putInt(36); headerByteBuffer.putInt(2456); headerByteBuffer.putInt(badResponseTo); headerByteBuffer.putInt(1); headerByteBuffer.putInt(0); headerByteBuffer.putLong(0); headerByteBuffer.putInt(0); headerByteBuffer.putInt(0); headerByteBuffer.flip(); ByteBufNIO byteBuf = new ByteBufNIO(headerByteBuffer); ReplyHeader replyHeader = new ReplyHeader(byteBuf, new MessageHeader(byteBuf, getDefaultMaxMessageSize())); new ReplyMessageDocument>(replyHeader, expectedResponseTo);', \"public void debug( String message )  // TODO: can't use getLogger() because this isn't currently instantiated as a component // getLogger().debug( message );\", 'private static void fixScriptPermissions( File binDirectory ) throws InterruptedException, CommandLineException  if ( Os.isFamily( \"unix\" ) )  Commandline cli = new Commandline(); cli.setExecutable( \"chmod\" ); cli.createArgument().setValue( \"+x\" ); cli.createArgument().setValue( new File( binDirectory, \"mvn\" ).getAbsolutePath() ); cli.createArgument().setValue( new File( binDirectory, \"m2\" ).getAbsolutePath() ); cli.execute().waitFor();', 'private MavenExecutionResult doExecute( MavenExecutionRequest request, MavenSession session, MavenExecutionResult result, DefaultRepositorySystemSession repoSession )  try  for ( AbstractMavenLifecycleParticipant listener : getLifecycleParticipants( Collections.MavenProject>emptyList() ) )  listener.afterSessionStart( session );   catch ( MavenExecutionException e )  return addExceptionToResult( result, e );  eventCatapult.fire( ExecutionEvent.Type.ProjectDiscoveryStarted, session, null ); ListMavenProject> projects; try  projects = getProjectsForMavenReactor( session ); // // Capture the full set of projects before any potential constraining is performed by --projects // session.setAllProjects( projects );  catch ( ProjectBuildingException e )  return addExceptionToResult( result, e );  validateProjects( projects ); // // This creates the graph and trims the projects down based on the user request using something like: // // -pl project0,project2 eclipse:eclipse // ProjectDependencyGraph projectDependencyGraph = createProjectDependencyGraph( projects, request, result, true ); if ( result.hasExceptions() )  return result;  session.setProjects( projectDependencyGraph.getSortedProjects() ); try  session.setProjectMap( getProjectMap( session.getProjects() ) );  catch ( DuplicateProjectException e )  return addExceptionToResult( result, e );  WorkspaceReader reactorWorkspace; try  reactorWorkspace = container.lookup( WorkspaceReader.class, ReactorReader.HINT );  catch ( ComponentLookupException e )  return addExceptionToResult( result,', 'public ResourceMethod matches(String uri, HttpMethod method, MutableRequest request)  boolean acceptMethod = this.methods.isEmpty() || this.methods.contains(method); boolean uriMatches = parameters.matches(uri); return uriMatches && acceptMethod? this.resourceMethod : null;', 'private void killCursor()  ServerCursor localCursor = cursor; final ServerCursor localCursor = cursor; final SingleResultCallbackAsyncConnection> connection = connectionSource.getConnection(connection, new SingleResultCallbackAsyncConnection>()  @Override public void onResult(final AsyncConnection connection, final Throwable connectionException)  connection.killCursorAsync(singletonList(localCursor.getId()), new SingleResultCallbackVoid>()  @Override public void onResult(final Void result, final Throwable t)  connection.release(); connectionSource = null;  );  );  else if (connectionSource!= null)  connectionSource.release(); connectionSource = null;', 'public void transform(long priority, Context context, ListJCCompilationUnit> compilationUnits, CleanupRegistry cleanup)  for (JCCompilationUnit unit : compilationUnits)  if (!Boolean.TRUE.equals(LombokConfiguration.read(ConfigurationKeys.LOMBOK_DISABLE, JavacAST.getAbsoluteFileLocation(unit)))  JavacAST ast = new JavacAST(messager, context, unit, cleanup); ast.traverse(new AnnotationVisitor(priority)); handlers.callASTVisitors(ast, priority); if (ast.isChanged()) LombokOptions.markChanged(context, (JCCompilationUnit) ast.top().get());', 'private MaybeString> findVariableInPages(String name)  MaybeString> localVariable = page.findVariable(name); if (!localVariable.isNothing()) return new MaybeString>(localVariable.getValue()); return lookInParentPages(name);', \"public void onDestroy()  ListView lv = getListView(); if (lv!= null)  mLastListPosCourse = lv.getFirstVisiblePosition(); mLastListPosFine = lv.getChildAt(0).getTop();  MusicUtils.unbindFromService(this); if (!mAdapterSent)  Cursor c = mAdapter.getCursor(); if (c!= null)  c.close();   // Because we pass the adapter to the next activity, we need to make // sure it doesn't keep a reference to this activity. We can do this // by clearing its DatasetObservers, which setListAdapter(null) does. setListAdapter(null); mAdapter = null; unregisterReceiver(mScanListener); super.onDestroy();\", 'public void setup() throws Exception  MockitoAnnotations.initMocks(this); Locale.setDefault(Locale.ENGLISH); ValidatorFactoryCreator creator = new ValidatorFactoryCreator(); creator.buildFactory(); provider = new ParanamerNameProvider(); doReturn(false).when(container).canProvide(any(Class.class)); doReturn(new ObjenesisInstanceCreator()).when(container).instanceFor(InstanceCreator.class); MethodValidatorFactoryCreator methodValidatorCreator = new MethodValidatorFactoryCreator(provider, container); methodValidatorCreator.buildFactory(); factory = methodValidatorCreator.getInstance(); MessageInterpolatorFactory interpolatorFactory = new MessageInterpolatorFactory(creator.getInstance()); interpolatorFactory.createInterpolator(); interpolator = interpolatorFactory.getInstance(); validator = new MockValidator(); withConstraint = DefaultResourceMethod.instanceFor(MyController.class, MyController.class.getMethod(\"withTwoConstraint\", String.class)); withTwoConstraints = DefaultResourceMethod.instanceFor(MyController.class, MyController.class.getMethod(\"withoutConstraint\", String.class)); withoutConstraint = DefaultResourceMethod.instanceFor(MyController.class, MyController.class.getMethod(\"cascadeConstraint\", Customer.class));', 'public NodeSampleResult call() throws Exception  boolean success = false; String storeName = storeDefinition.getName(); StringBuilder hexKeysString = new StringBuilder(); // TODO: Change this from a loop to flat st the list of partitoinIds // are sent to server. for(int partitionId: node.getPartitionIds())  success = false; // TODO: real per-server throttling and/or make \\'100\\' a command // line argument. And, move near.next() to throttle server // suckage. // Simple, lame throttling since thread is going at same node // repeatedly try  Thread.sleep(100);  catch(InterruptedException e)  logger.warn(\"Sleep throttling interrupted : \" + e.getMessage()); e.printStackTrace();  String infoTag = \"store \" + storeName + \", partitionID \" + partitionId + \" on node \" + node.getId() + \" [\" + node.getHost() + \"]\"; logger.info(\"Starting sample --- \" + infoTag); ListInteger> singlePartition = new ArrayListInteger>(); singlePartition.add(partitionId); // Wrap fetchKeys in backoff-and-retry loop int attempts = 0; int backoffMs = 1000; while(attempts  5 &&!success)  try  IteratorByteArray> fetchIterator; fetchIterator = adminClient.bulkFetchOps.fetchKeys(node.getId(), storeName, singlePartition, null, true, recordsPerPartition); int keyCount = 0; while(fetchIterator.hasNext())  ByteArray key = fetchIterator.next(); String hexKeyString = ByteUtils.toHex', 'private long flush(MemtableTermsIterator terms) throws IOException  SegmentWriter writer = indexTermType.isLiteral()? new LiteralIndexWriter(indexDescriptor, indexIdentifier) : new NumericIndexWriter(indexDescriptor, indexIdentifier, indexTermType.fixedSizeOf()); SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms); long numRows = writer.getNumberOfRows(); // If no rows were written we need to delete any created column index components // so that the index is correctly identified as being empty (only having a completion marker) if (numRows == 0)  indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier); return 0;  // During index memtable flush, the data is sorted based on terms. SegmentMetadata metadata = new SegmentMetadata(0, numRows, terms.getMinSSTableRowId(), terms.getMaxSSTableRowId(), maxSSTableRowId(), rowMapping.minKey, rowMapping.maxKey, terms.getMinTerm(), terms.getMaxTerm(), indexMetas); try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))  SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));  return numRows;', 'private MapString, Object> getGallerySection(String tag) throws TagNotFoundException  ListPreparedGalleryItem> list = imageDao.prepare(imageDao.getGalleryItems(3, tag)); return ImmutableMap.String, Object>of( \"gallery\", list );', 'public String getVariable(String name)  MaybeString> variable = new VariableFinder(getParsingPage()).findVariable(name); if (variable.isNothing()) return null; String renderVariableValue = variable.getValue(); return renderVariableValue(variable.getValue());', 'private WikiWidget constructWidget(Class widgetClass, ParentWidget parent, String text) throws Exception  try  Constructor widgetConstructor = widgetClass.getConstructor(new Class[]ParentWidget.class, String.class); return (WikiWidget) widgetConstructor.newInstance(new Object[]parent, text);  catch(Exception e)  e.printStackTrace(); Exception exception = new Exception(\"Widget Construction failed for \" + widgetClass.getName() + \"n\" + e.getMessage()); exception.setStackTrace(e.getStackTrace()); throw exception;', 'public String translate(Symbol symbol)  if (getTranslation(symbol.getType())!= null)  return getTranslation(symbol.getType()).toTarget(this, symbol);  else  StringBuilder result = new StringBuilder(symbol.getContent()); for (Symbol child: symbol.getChildren())  result.append(translate(child));  return result.toString();', 'private void clearCheckedMenus()  String groupMenu = mFilterMenuMap; String groupMenu = mGroupMenuMap; clearCheckedMenu(groupMenu); clearCheckedMenu(groupMenu);', 'public ListProfile> getActiveProfiles( Model model ) throws ProfileActivationException  ListProfile> activeFromPom = new ArrayListProfile>(); ListProfile> activeExternal = new ArrayListProfile>(); for ( Iterator it = profilesById.entrySet().iterator(); it.hasNext(); )  Map.Entry entry = (Entry) it.next(); String profileId = (String) entry.getKey(); Profile profile = (Profile) entry.getValue(); boolean shouldAdd = false; if ( profileActivationContext.isExplicitlyActive( profileId ) )  shouldAdd = true;  else if ( isActive( profile, profileActivationContext ) )  shouldAdd = true;  if (!profileActivationContext.isExplicitlyInactive( profileId ) && shouldAdd )  if ( \"pom\".equals( profile.getSource() ) )  activeFromPom.add( profile );  else  activeExternal.add( profile );    if ( activeFromPom.isEmpty() )  ListString> defaultIds = profileActivationContext.getActiveByDefaultProfileIds(); ListString> deactivatedIds = profileActivationContext.getExplicitlyInactiveProfileIds(); for ( String profileId : defaultIds )  // If this profile was excluded, don\\'t add it back in // Fixes MNG-3545 if ( deactivatedIds.contains( profileId ) )  continue;  Profile profile = (Profile) profilesById.get( profileId ); if ( profile!= null )  activeFromPom.add( profile );    ListProfile> allActive = new ArrayListProfile>', 'public static MongoClient create(final MongoClientSettings settings, @Nullable final MongoDriverInformation mongoDriverInformation)  if (settings.getSocketSettings().getProxySettings().isProxyEnabled())  throw new MongoClientException(\"Proxy is not supported for reactive clients\");  StreamFactoryFactory streamFactoryFactory = getStreamFactoryFactoryFromSettings(settings); if (streamFactoryFactory == null)  if (settings.getSslSettings().isEnabled())  return createWithTlsChannel(settings, mongoDriverInformation);  else  return createWithAsynchronousSocketChannel(settings, mongoDriverInformation);   else  return createMongoClient(settings, mongoDriverInformation, getStreamFactory(streamFactoryFactory, settings, false), getStreamFactory(streamFactoryFactory, settings, true), null);', 'public void onResult(final AsyncConnectionSource source, final Throwable t)  if (t!= null)  callback.onResult(null, t);  else  source.getConnection(new SingleResultCallbackConnection>()  @Override public void onResult(final Connection connection, final Throwable t)  SingleResultCallbackConnection> errHandlingCallback = callback.onResult(null, t); if (t!= null)  errHandlingCallback.onResult(null, t);  else  connection.commandAsync(database, wrapCommand(command, readPreference, connection.getDescription()), readPreference.isSlaveOk(), fieldNameValidator, decoder, new SingleResultCallbackD>()  @Override public void onResult(final D response, final Throwable t)  try  if (t!= null)  errHandlingCallback.onResult(null, t);  else  errHandlingCallback.onResult(transformer.apply(response), null);   finally  connection.release(); source.release();   );   );', 'public void testExhaustWithDiscard() throws InterruptedException, ExecutionException  assumeFalse(isSharded()); Connection connection = getAsyncBinding().getReadConnectionSource().get().getConnection().get().retain(); try  QueryResultDocument> firstBatch = executeQuery(getOrderedByIdQuery(), 2, EnumSet.of(Exhaust), connection); new MongoAsyncQueryCursorDocument>(collection.getNamespace(), firstBatch, 5, 2, new DocumentCodec(), connection).start(new TestBlock(1)); latch.await(); assertThat(documentResultList, is(documentList.subList(0, 1))); firstBatch = executeQuery(getOrderedByIdQuery(), 1, EnumSet.of(Exhaust), connection); assertEquals(Arrays.asList(new Document(\"_id\", 0)), firstBatch.getResults());  finally  connection.release();', 'public void copyResources( File sourceDirectory, File webappDirectory, String includes, String excludes, String webXml ) throws IOException  if (!sourceDirectory.equals( webappDirectory ) )  getLog().info( \"Copy webapp resources to \" + webappDirectory.getAbsolutePath() ); if ( new File( warSourceDirectory ).exists() )  //TODO : Use includes and excludes FileUtils.copyDirectoryStructure( sourceDirectory, webappDirectory );  if ( webXml!= null &&!\"\".equals( webXml ) )  FileUtils.copyFileToDirectory( new File( webXml ), new File( webappDirectory, WEB_INF ) );', 'public void putSource( String groupId, String artifactId, Source source )  mappedSources.computeIfAbsent( new DefaultTransformerContext.GAKey( groupId, artifactId ), k -> new HashSet>() ).add( source );', 'private boolean invokeParameterizedScenarioIfPossible(int row)  if (table.getColumnCountInRow(row) == 1) String firstNameCell = table.getCellContents(0, row); for (ScenarioTable scenario : getScenariosWithMostArgumentsFirst())  String[] arguments = scenario.matchParameters(firstNameCell); if (arguments!= null)  scenario.call(arguments, this, row); return true;    return false;', 'public void takeSnapshot() throws IOException  synchronized (_prevalentSystem)  _snapshotManager.writeSnapshot(_prevalentSystem, _systemVersion);', \"public ArtifactResolutionResult collect( Set artifacts, Artifact originatingArtifact, Map managedVersions, ArtifactRepository localRepository, List remoteRepositories, ArtifactMetadataSource source, ArtifactFilter filter, List listeners ) throws ArtifactResolutionException  Map resolvedArtifacts = new LinkedHashMap(); ResolutionNode root = new ResolutionNode( originatingArtifact, remoteRepositories ); root.addDependencies( artifacts, remoteRepositories, filter ); recurse( root, resolvedArtifacts, managedVersions, localRepository, remoteRepositories, source, filter, listeners ); Set set = new LinkedHashSet(); for ( Iterator i = resolvedArtifacts.values().iterator(); i.hasNext(); )  ResolutionNode node = (ResolutionNode) j.next(); if (!node.equals( root ) && node.isActive() )  Artifact artifact = node.getArtifact(); if ( node.filterTrail( filter ) )  // If it was optional and not a direct dependency, // we don't add it or its children, just allow the update of the version and scope if ( node.isChildOfRootNode() ||!artifact.isOptional() )  artifact.setDependencyTrail( node.getDependencyTrail() ); set.add( node );     ArtifactResolutionResult result = new ArtifactResolutionResult(); result.setArtifactResolutionNodes( set ); return result;\", 'static T> void configureClassModelBuilder(final ClassModelBuilderT> classModelBuilder, final ClassT> clazz)  classModelBuilder.type(notNull(\"clazz\", clazz)); if (clazz.equals(Object.class))  return;  ArrayListAnnotation> annotations = new ArrayListAnnotation>(); Class?> type = clazz; while (type!= null)  for (Annotation annotation : type.getDeclaredAnnotations())  annotations.add(annotation);  type = type.getSuperclass();  reverse(annotations); classModelBuilder.annotations(annotations); TypeResolver resolver = new TypeResolver(); MemberResolver memberResolver = new MemberResolver(resolver); ResolvedType resolved = resolver.resolve(clazz); ResolvedTypeWithMembers resolvedType = memberResolver.resolve(resolved, new AnnotationConfiguration.StdConfiguration(AnnotationInclusion.INCLUDE_AND_INHERIT_IF_INHERITED), null); ListString> genericTypeNames = new ArrayListString>(); for (TypeVariableClassT>> classTypeVariable : clazz.getTypeParameters())  genericTypeNames.add(classTypeVariable.getName());  ListResolvedField> resolvedFields = new ArrayListResolvedField>(asList(resolvedType.getMemberFields())); MapString, TypeParameterMap> fieldTypeParameterMap = new HashMapString, TypeParameterMap>(); for (final ResolvedField resolvedField : resolvedFields)  if (resolvedField.isTransient())  continue;  String name = resolvedField.getName(); fieldTypeParameterMap.put(', 'public void executeMojo( MavenSession session, MojoDescriptor mojoDescriptor ) throws ArtifactResolutionException, PluginManagerException, MojoExecutionException  PlexusContainer pluginContainer = null; if ( mojoDescriptor.isDependencyResolutionRequired()!= null )  ArtifactResolver artifactResolver = null; MavenProjectBuilder mavenProjectBuilder = null; try  artifactResolver = (ArtifactResolver) container.lookup( ArtifactResolver.ROLE ); mavenProjectBuilder = (MavenProjectBuilder) container.lookup( MavenProjectBuilder.ROLE ); resolveTransitiveDependencies( session, artifactResolver, mavenProjectBuilder, mojoDescriptor.isDependencyResolutionRequired() ); downloadDependencies( session, artifactResolver );  catch ( ComponentLookupException e )  throw new PluginManagerException( \"Internal configuration error in plugin manager\", e );  finally  if ( artifactResolver!= null )  releaseComponent( artifactResolver );  if ( mavenProjectBuilder!= null )  releaseComponent( mavenProjectBuilder );    Mojo plugin = null; String goalName = mojoDescriptor.getFullGoalName(); try  String pluginKey = mojoDescriptor.getPluginLookupKey(); pluginContainer = container.getChildContainer( pluginKey ); if ( pluginContainer == null )  throw new PluginConfigurationException( \"Cannot find PlexusContainer for plugin: \" + pluginKey );  plugin = (Mojo) pluginContainer.lookup( Mojo.ROLE, mojoDescriptor.getRoleHint', 'public SocketStore getSocketStore(int nodeId, String storeName)  NodeStore nodeStore = new NodeStore(nodeId, storeName); if(!nodeStoreSocketCache.containsKey(nodeStore))  SocketStore socketStore = null; Node node = getAdminClientCluster().getNodeById(nodeId); try  // Can clientConfig.getRequestFormatType() default to // something? socketStore = clientPool.create(storeName, node.getHost(), node.getSocketPort(), clientConfig.getRequestFormatType(), RequestRoutingType.IGNORE_CHECKS);  catch(Exception e)  clientPool.close(); throw new VoldemortException(e);  nodeStoreSocketCache.putIfAbsent(nodeStore, socketStore);  return nodeStoreSocketCache.get(nodeStore);', 'private void executeSingleWriteRequest(@Nullable final AsyncClientSession clientSession, final AsyncWriteOperationBulkWriteResult> writeOperation, final WriteRequest.Type type, final SingleResultCallbackBulkWriteResult> callback)  Executor.execute(writeOperation, readConcern, clientSession, (result, t) ->  if (t instanceof MongoBulkWriteException)  MongoBulkWriteException e = (MongoBulkWriteException) t; if (e.getWriteErrors().isEmpty())  callback.onResult(null, new MongoWriteConcernException(e.getWriteConcernError(), translateBulkWriteResult(type, e.getWriteResult()), e.getServerAddress()));  else  callback.onResult(null, new MongoWriteException(new WriteError(e.getWriteErrors().get(0)), e.getServerAddress()));   else  callback.onResult(result, t);  );', 'private void createPythonNodeTypeLibrary(Attributes attributes) throws SAXException  String moduleName = requireAttribute(\"library\", attributes, \"module\"); library.setPythonModuleName(moduleName); // Load python module // TODO: This is the central versioning problem. // To properly handle this, we need several system states. PyString libraryPath = new PyString(library.getPath()); PyString pyString = PyString.getSystemState(); PyString python = new PyString(libraryPath); PyString python = (PyModule) imp.importName(moduleName.intern(), true);', 'public static Embedded createCatalina( final int port, final int sessionTimeout, final String memcachedNodes, final String jvmRoute, final LoginType loginType, final String transcoderFactoryClassName ) throws MalformedURLException, UnknownHostException, LifecycleException  final Embedded catalina = new Embedded(); final StandardServer server = new StandardServer(); catalina.setServer( server ); try  final NamingContext globalNamingContext = new NamingContext( new HashtableObject, Object>(), \"ctxt\" ); server.setGlobalNamingContext( globalNamingContext ); globalNamingContext.bind( USER_DATABASE, createUserDatabase() );  catch ( final NamingException e )  throw new RuntimeException( e );  final Engine engine = catalina.createEngine(); catalina.addEngine( engine ); /* we must have a unique name for mbeans */ engine.setName( \"engine-\" + port ); engine.setDefaultHost( DEFAULT_HOST ); engine.setJvmRoute( jvmRoute ); final UserDatabaseRealm realm = new UserDatabaseRealm(); realm.setResourceName( USER_DATABASE ); engine.setRealm( realm ); final URL root = new URL( TestUtils.class.getResource( \"/\" ), \"../resources\" ); final String fileSeparator = File.separator.equals( \"\" )? \"\" : File.separator; final String docBase = root.getFile() + File.separator + TestUtils.class.getPackage().getName().replaceAll( \".\", fileSeparator ); final Host host = catalina.createHost( DEFAULT_HOST, docBase ); engine', 'public ModelAndView news( HttpServletRequest request, TopicListRequest topicListForm, HttpServletResponse response ) throws Exception  topicListForm.setSection(Section.SECTION_NEWS); ModelAndView modelAndView = mainTopicsFeedHandler(request, topicListForm, response, null); modelAndView.addObject(\"url\", \"/news/\"); modelAndView.addObject(\"ptitle\", calculatePTitle(sectionService.getSection(Section.SECTION_NEWS), topicListForm)); modelAndView.addObject(\"rssLink\", \"section-rss.jsp?section=1\"); return modelAndView;', 'public MaybeSymbol> parse(Symbol current, Parser parser)  String name = parser.parseToAsString(SymbolType.CloseBrace); if (parser.atEnd() || name.length() == 0) return Symbol.nothing; if (!ScanString.isVariableName(name)) return Symbol.nothing; current.add(name); MaybeString> variableValue = parser.getVariableSource().findVariable(name); if (variableValue.isNothing())  current.add(new Symbol(SymbolType.Meta).add(\"undefined variable: \" + name));  else  Symbol variableValueSymbol = parser.parseWithParent(variableValue.getValue(), null); current.add(variableValueSymbol);  return new MaybeSymbol>(current);', 'private void execute( MavenProject project, MavenSession session, MojoExecution mojoExecution ) throws MojoFailureException, MojoExecutionException, PluginConfigurationException, PluginManagerException  MavenProject executionProject = null; ListMojoExecution> forkedExecutions = mojoExecution.getForkedExecutions(); if (!forkedExecutions.isEmpty() )  if ( logger.isDebugEnabled() )  logger.debug( \"Forking execution for \" + mojoExecution.getMojoDescriptor().getId() );  executionProject = project.clone(); session.setCurrentProject( executionProject ); try  for ( MojoExecution forkedExecution : forkedExecutions )  execute( executionProject, session, forkedExecution );   finally  session.setCurrentProject( project );  if ( logger.isDebugEnabled() )  logger.debug( \"Completed forked execution for \" + mojoExecution.getMojoDescriptor().getId() );   project.setExecutionProject( executionProject ); logger.info( executionDescription( mojoExecution, project ) ); pluginManager.executeMojo( session, mojoExecution );', 'protected TDocument> void writeAdditionalFields(final BsonDocumentWriter writer, final ClassTDocument> tDocumentClass, final CodecRegistry codecRegistry)  Bson position = options.getPosition(); if (position!= null)  writer.writeInt32(\"$position\", position);  Bson slice = options.getSlice(); if (slice!= null)  writer.writeInt32(\"$slice\", slice);  if (options.getSort()!= null)  writer.writeInt32(\"$sort\", options.getSort());  else if (sortDocument!= null)  writer.writeName(\"$sort\"); encodeValue(writer, options.getSortDocument(), codecRegistry);', 'public void actionPerformed( WikiEvent event )  if(!(event instanceof WikiPageEvent) )  return;  String pageName = ((WikiPageEvent) event).getPageName(); if( pageName == null )  return;  try  switch( event.getType() )  // =============================================================================================================== // If page was saved, update all references case (ContentEvent.NODE_SAVED ):  WikiPath path = resolvePage( WikiPath.valueOf( pageName ) ); // Get new linked pages, and set refersTo/referencedBy links ListWikiPath> referenced = extractLinks( path ); setLinks( path, referenced ); m_cm.getCurrentSession().save(); break;  // ====================================================================================================================================================================================', 'public void execute() throws MojoExecutionException  touch( new File( outputDirectory ), \"touch.txt\" ); // This parameter should be aligned to the basedir as the parameter type is specified // as java.io.File if ( basedirAlignmentDirectory.getPath().equals( \"target/test-basedir-alignment\" ) )  throw new MojoExecutionException( \"basedirAlignmentDirectory not aligned\" );  touch( basedirAlignmentDirectory, \"touch.txt\" ); // Test parameter setting if ( pluginItem!= null )  touch( new File( outputDirectory ), pluginItem );  if ( goalItem!= null )  touch( new File( outputDirectory ), goalItem );  project.getBuild().setFinalName( \"coreitified\" );', 'private void transformPomToReleaseVersionPom( MavenProject project ) throws MojoExecutionException  if (!getReleaseProgress().verifyCheckpoint( ReleaseProgressTracker.CP_POM_TRANSFORMED_FOR_RELEASE ) )  if (!isSnapshot( project.getVersion() ) )  throw new MojoExecutionException( \"The project \" + project.getGroupId() + \":\" + project.getArtifactId() + \" isn\\'t a snapshot (\" + project.getVersion() + \").\" );  Model model = project.getOriginalModel(); //Rewrite parent version if ( project.hasParent() )  Artifact parentArtifact = project.getParentArtifact(); if ( isSnapshot( parentArtifact.getBaseVersion() ) )  String version = resolveVersion( parentArtifact, \"parent\", project ); model.getParent().setVersion( version );   //Rewrite dependencies section Map artifactMap = project.getArtifactMap(); List dependencies = model.getDependencies(); if ( dependencies!= null )  for ( Iterator i = dependencies.iterator(); i.hasNext(); )  Dependency dep = (Dependency) i.next(); String conflictId = ArtifactUtils.artifactId( dep.getGroupId(), dep.getArtifactId(), dep.getType(), dep.getClassifier(), dep.getVersion() ); Artifact artifact = (Artifact) artifactMap.get( conflictId ); String version = resolveVersion( artifact, \"dependency\", project ); dep.setVersion( version );   Build build = model.getBuild(); if ( build!= null )  //Rewrite plugins', 'public void setUp() throws Throwable  _portTomcat1 = 8888; final int port = 21211; _map = new PartitionedHashMapString,byte[]>( \"tcp.xml\", getClass().getSimpleName() ); _connector = new MemcachedConnector( InetAddress.getLocalHost(), port, _map); _connector.setThreadPoolCoreThreads(1); _connector.setThreadPoolMaxThreads(5); _map.start(); _connector.start(); try  final String memcachedNodes = \"localhost:\" + port; _tomcat1 = createCatalina( _portTomcat1, memcachedNodes ); _tomcat1.start();  catch( Throwable e )  LOG.error( \"could not start tomcat.\", e ); throw e;  _memcached = new MemcachedClient( new SuffixLocatorConnectionFactory( _tomcat1.getContainer().getManager() ), Arrays.asList( new InetSocketAddress( \"localhost\", port ) ) ); _connectionManager = new SimpleHttpConnectionManager( true ); _httpClient = new HttpClient( _connectionManager );', 'private ListClass?>> categories(Description description)  ArrayListClass?>> categories= new ArrayListClass?>>(); Arrays.asList(directCategories(description))); categories.addAll(Arrays.asList(directCategories(description))); categories.addAll(Arrays.asList(directCategories(parentDescription(description))); return categories;', 'public void dependencyManagementWithScopeAndClassifier() throws IOException  ListModelProperty> mp = new ArrayListModelProperty>(); mp.add(new ModelProperty(ProjectUri.xUri, null)); mp.add(new ModelProperty(ProjectUri.DependencyManagement.xUri, null)); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.xUri, null)); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.groupId, \"gid\")); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.artifactId, \"aid\")); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.version, \"v1\")); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.scope, \"test\")); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.classifier, \"tests\")); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.xUri, null)); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.groupId, \"gid\")); mp.add(new ModelProperty(ProjectUri.DependencyManagement.Dependencies.Dependency.artifactId, \"aid\")); mp.add(new ModelProperty(ProjectUri.Dependency', 'static ReplicaPlan.ForRangeRead replicaPlan(ConsistencyLevel consistencyLevel, EndpointsForRange replicas)  ReplicaPlan consistencyPlan = replicaPlan(ks, consistencyLevel, replicas, replicas); return replicaPlan(replicaPlan, replicas);', 'public String makeHtml(FitNesseContext context)  WikiPage page = pageData.getWikiPage(); HtmlPage html = context.pageFactory.newPage(); WikiPagePath fullPath = page.getPageCrawler().getFullPath(page); String fullPathName = PathParser.render(fullPath); html.setTitle(fullPathName); html.setPageTitle(new PageTitle(fullPath).notLinked()); html.setNavTemplate(\"wikiNav.vm\"); html.put(\"actions\", new WikiPageActions(page)); SetupTeardownAndLibraryIncluder.includeInto(pageData, true); html.setMainTemplate(\"wikiPage\"); html.setFooterTemplate(\"wikiFooter\"); html.put(\"content\", new WikiPageRenderer()); html.put(\"footerContent\", new WikiPageFooterRenderer()); handleSpecialProperties(html, page); return html.html();', 'public void oldVersionsAreRemovedOnCommit() throws Exception  PageData data = page.getData(); Calendar modificationTime = Calendar.getInstance(); modificationTime.add(Calendar.DATE, -1); String timeIndex1 = format(modificationTime); data.getProperties().setLastModificationTime(dateFormat().parse(timeIndex1)); page.commit(data); modificationTime.add(Calendar.DATE, -1); String timeIndex2 = format(modificationTime); data.getProperties().setLastModificationTime(dateFormat().parse(timeIndex2)); page.commit(data); modificationTime.add(Calendar.DATE, -1); data.getProperties().setLastModificationTime(dateFormat().parse(format(modificationTime))); page.commit(data); modificationTime.add(Calendar.DATE, -1); data.getProperties().setLastModificationTime(dateFormat().parse(format(modificationTime))); page.commit(data); CollectionVersionInfo> versions = page.getVersions(); assertEquals(MAX_HISTORY_DEPTH, versions.size()); ListVersionInfo> versionsList = new LinkedListVersionInfo>(versions); Collections.sort(versionsList); assertTrue(versionsList.toString(), versionsList.get(0).toString().endsWith(timeIndex2)); assertTrue(versionsList.toString(), versionsList.get(1).toString().endsWith(timeIndex1)); assertEquals(versionsList.get(2), firstVersion);', 'private void addExtension( Artifact extensionArtifact, Artifact projectArtifact, List remoteRepositories, ArtifactRepository localRepository, ActiveArtifactResolver activeArtifactResolver, Map projectSessions, String projectId ) throws ExtensionManagerException  getLogger().debug( \"Starting extension-addition process for: \" + extensionArtifact ); // create a new MavenProjectSession instance for the current project. // This session instance will house the plugin and extension realms that // pertain to this specific project, along with containing the project-level // realm to use as a lookupRealm in the lifecycle executor and plugin manager. MavenProjectSession projectSession = (MavenProjectSession) projectSessions.get( projectId ); if ( projectSession == null )  try  projectSession = new MavenProjectSession( projectId, container );  catch ( PlexusContainerException e )  throw new ExtensionManagerException( \"Failed to create project realm for: \" + projectId, projectId, e );  projectSessions.put( projectId, projectSession );  ArtifactFilter coreFilter = artifactFilterManager.getArtifactFilter(); // if the extension is null, // if it\\'s already been added to the current project-session, // or if it\\'s excluded by the core filter, // // skip it. if ( ( extensionArtifact!= null ) &&!projectSession.containsExtensionRealm( extensionArtifact ) && coreFilter.include( extensionArtifact ) )  ArtifactFilter filter = new ProjectArtifactExceptionFilter( coreFilter, projectArtifact ); ResolutionGroup resolutionGroup; try  resolutionGroup = artifactMetadataSource.retrieve( extensionArti', 'public synchronized ProjectRealmCache.CacheRecord createProjectRealm( MavenProject project, Model model, ProjectBuildingRequest request ) throws PluginResolutionException, PluginVersionResolutionException  ClassRealm projectRealm; ListPlugin> extensionPlugins = new ArrayListPlugin>(); Build build = model.getBuild(); if ( build!= null )  for ( Extension extension : build.getExtensions() )  Plugin plugin = new Plugin(); plugin.setGroupId( extension.getGroupId() ); plugin.setArtifactId( extension.getArtifactId() ); plugin.setVersion( extension.getVersion() ); extensionPlugins.add( plugin );  for ( Plugin plugin : build.getPlugins() )  if ( plugin.isExtensions() )  extensionPlugins.add( plugin );    if ( extensionPlugins.isEmpty() )  if ( logger.isDebugEnabled() )  logger.debug( \"Extension realms for project \" + model.getId() + \": (none)\" );  return new ProjectRealmCache.CacheRecord( null, null );  ListClassRealm> extensionRealms = new ArrayListClassRealm>(); MapClassRealm, ListString>> exportedPackages = new HashMapClassRealm, ListString>>(); MapClassRealm, ListString>> exportedArtifacts = new HashMapClassRealm, ListString>>(); ListArtifact> publicArtifacts = new ArrayListArtifact>(); for ( Plugin plugin : extensionPlugins )  if ( plugin.getVersion() == null )  PluginVersionRequest versionRequest = new DefaultPluginVersionRequest( plugin, request.getRepositorySession(), project', 'private String toId( Model model )  StringBuilder buffer = new StringBuilder( 64 ); if ( model.getGroupId()!= null )  buffer.append( model.getGroupId() );  else if ( model.getParent()!= null && model.getParent().getGroupId()!= null )  buffer.append( model.getParent().getGroupId() );  else  buffer.append( \"[unknown-group-id]\" );  buffer.append( \\':\\' ); if ( model.getArtifactId()!= null )  buffer.append( model.getArtifactId() );  else  buffer.append( \"[unknown-artifact-id]\" );  buffer.append( \\':\\' ); if ( model.getVersion()!= null )  buffer.append( model.getVersion() );  else if ( model.getParent()!= null && model.getParent().getVersion()!= null )  buffer.append( model.getParent().getVersion() );  else  buffer.append( \"[unknown-version]\" );  return buffer.toString();', 'public void testSlimTablesCreation() throws PluginException  TestProperties.setProperty(ConfigurationParameter.SLIM_TABLES.getKey(), \"test:\" + TestSlimTable.class.getName()); loader.loadSlimTables(); HtmlTable table = makeMockTable(\"test\"); SlimTable slimTable = new SlimTableFactory().makeSlimTable(table, \"foo\", new SlimTestContextImpl()); assertSame(TestSlimTable.class, slimTable.getClass());', 'private int updateGroups(Uri uri, ContentValues values, String selectionWithId, String[] selectionArgs, boolean callerIsSyncAdapter)  mGroupIdCache.clear(); ContentValues updatedValues; if (!callerIsSyncAdapter &&!values.containsKey(Groups.DIRTY))  updatedValues = mValues; updatedValues.clear(); updatedValues.putAll(values); updatedValues.put(Groups.DIRTY, 1);  else  updatedValues = values;  int count = mActiveDb.get().update(Tables.GROUPS, updatedValues, selectionWithId, selectionArgs); if (updatedValues.containsKey(Groups.GROUP_VISIBLE))  mVisibleTouched = true;  // TODO: This will not work for groups that have a data set specified, since the content // resolver will not be able to request a sync for the right source (unless it is updated // to key off account with data set). if (updatedValues.containsKey(Groups.SHOULD_SYNC) && updatedValues.getAsInteger(Groups.SHOULD_SYNC)!= 0)  Cursor c = mActiveDb.get().query(Tables.GROUPS, new String[]Groups.ACCOUNT_NAME, Groups.ACCOUNT_TYPE, selectionWithId, selectionArgs, null, null, null); String accountName; String accountType; try  while (c.moveToNext())  accountName = c.getString(0); accountType = c.getString(1); if (!TextUtils.isEmpty(accountName) &&!TextUtils.', 'public BinaryPackageControlFile createSignedDeb(Compression compression, final PGPSignatureGenerator signatureGenerator, PGPSigner signer ) throws PackagingException  File tempData = null; File tempControl = null; try  tempData = File.createTempFile(\"deb\", \"data\"); tempControl = File.createTempFile(\"deb\", \"control\"); console.debug(\"Building data\"); DataBuilder dataBuilder = new DataBuilder(console); StringBuilder md5s = new StringBuilder(); BigInteger size = dataBuilder.buildData(dataProducers, tempData, md5s, compression); console.info(\"Building conffiles\"); ListString> tempConffiles = populateConffiles(conffilesProducers); console.debug(\"Building control\"); ControlBuilder controlBuilder = new ControlBuilder(console, variableResolver, openReplaceToken, closeReplaceToken); BinaryPackageControlFile packageControlFile = controlBuilder.createPackageControlFile(new File(control, \"control\"), size); if (packageControlFile.get(\"Package\") == null)  packageControlFile.set(\"Package\", packageName);  if (packageControlFile.get(\"Depends\") == null)  packageControlFile.set(\"Depends\", depends);  if (packageControlFile.get(\"Section\") == null)  packageControlFile.set(\"Section\", section);  if (packageControlFile.get(\"Description\") == null)  packageControlFile.set(\"Description\", description);  if (packageControlFile.get(\"Homepage\") == null)  packageControlFile.set(\"Homepage\", homepage);  controlBuilder.buildControl(packageControlFile, control.listFiles(), tempConffiles, md5s, tempControl', 'public void execute() throws MojoExecutionException  if (skip)  getLog().info(\"Test execution is skipped\");  else  final File[] testSourceDirectories = getSourceDirectories(SourceDirectory.TEST); final File[] allSourceDirectories = getSourceDirectories(SourceDirectory.TEST, SourceDirectory.COMPILE); if (testScript == null || \"\".equals(testScript) ||!(new File(testScript).exists()))  // Generate test script try  new File(testOutputDirectory).mkdir(); NamespaceInFile[] ns = new NamespaceDiscovery(getLog(), testDeclaredNamespaceOnly).discoverNamespacesIn(testNamespaces, testSourceDirectories); File testFile = File.createTempFile(\"run-test\", \".clj\"); final PrintWriter writer = new PrintWriter(new FileWriter(testFile)); for (NamespaceInFile namespace : ns)  writer.println(\"(require \\'\" + namespace.getName() + \")\");  StringWriter testCljWriter = new StringWriter(); copy(ClojureRunTestWithJUnitMojo.class.getResourceAsStream(\"/default_test_script.clj\"), testCljWriter); StringBuilder runTestLine = new StringBuilder(); for (NamespaceInFile namespace : ns)  if (xmlEscapeOutput)  // Assumes with-junit-output uses with-test-out internally when necessary. xml escape anything sent to *out*. runTestLine.append(\"(with-open [writer (clojure.java.io/writer \"\" + escapeFilePath(testOutputDirectory, namespace.getName() + \".xml\") + \"\") \"); runTestLine', 'public int delete(Uri url, String where, String[] whereArgs)  final UserHandle callerUserHandle = Binder.getCallingUserHandle(); final int callerUid = Binder.getCallingUid(); final long token = Binder.clearCallingIdentity(); String selectionBySubIds; try  // Filter SMS based on subId. selectionBySubIds = ProviderUtil.getSelectionBySubIds(getContext(), callerUserHandle);  finally  Binder.restoreCallingIdentity(token);  int count; int match = sURLMatcher.match(url); SQLiteDatabase db = getWritableDatabase(match); boolean notifyIfNotDefault = true; switch (match)  case SMS_ALL: if (selectionBySubIds == null)  // No subscriptions associated with user, return 0. return 0;  where = DatabaseUtils.concatenateWhere(where, selectionBySubIds); count = db.delete(TABLE_SMS, where, whereArgs); if (count!= 0)  // Don\\'t update threads unless something changed. MmsSmsDatabaseHelper.updateThreads(db, where, whereArgs);  break; case SMS_ALL_ID: try  int message_id = Integer.parseInt(url.getPathSegments().get(0)); count = MmsSmsDatabaseHelper.deleteOneSms(db, message_id);  catch (Exception e)  throw new IllegalArgumentException( \"Bad message id: \" + url.getPathSegments().get(0));  break; case SMS_CONVERSATIONS_ID: int threadID; try  threadID = Integer.parse', 'public CollectionWikiPage> getAllPages( String space ) throws ProviderException  SetWikiPage> result = new TreeSetWikiPage>(); try  Session session = m_sessionManager.getSession(); QueryManager mgr = session.getWorkspace().getQueryManager(); Query q = mgr.createQuery( \"/jcr:root/\"+JCR_PAGES_NODE+((space!= null)? (\"/\"+space) : \"\")+\"/*\", Query.XPATH ); QueryResult qr = q.execute(); for( NodeIterator ni = qr.getNodes(); ni.hasNext(); )  Node n = ni.nextNode(); // Hack to make sure we don\\'t add the space root node. if(!isSpaceRoot(n) )  result.add( new JCRWikiPage( getEngine(), n ) );    catch( RepositoryException e )  throw new ProviderException(\"getAllPages()\",e);  catch( WikiException e )  throw new ProviderException(\"getAllPages()\",e);  return result;', 'public void execute() throws PluginExecutionException  if ( deploymentRepository == null )  String msg = \"Deployment failed: repository element was not specified in the pom inside\" + \" distributionManagement element\"; throw new PluginExecutionException( msg );  if ( deploymentRepository.getAuthenticationInfo() == null )  getLog().warn( \"Deployment repository id: \\'\" + deploymentRepository.getId() + \"\\' has no associated authentication info!\" );  // Deploy the POM Artifact artifact = new DefaultArtifact( project.getGroupId(), project.getArtifactId(), project.getVersion(), project.getPackaging() ); if (!\"pom\".equals( project.getPackaging() ) )  File pom = new File( project.getFile().getParentFile(), \"pom.xml\" ); ArtifactMetadata metadata = new ModelMetadata( artifact, pom ); artifact.addMetadata( metadata );  try  deployer.deploy( project.getBuild().getDirectory(), artifact, deploymentRepository, localRepository );  catch ( ArtifactDeploymentException e )  // TODO: deployment exception that does not give a trace throw new PluginExecutionException( \"Error deploying artifact\", e );', 'public void testObjectOverrides() throws UnknownHostException  ServerDescription.Builder builder = builder().address(new ServerAddress()).type(ServerType.SHARD_ROUTER).tagSet(new TagSet(asList(new Tag(\"dc\", \"ny\")))).setName(\"test\").maxDocumentSize(100).roundTripTime(50000, java.util.concurrent.TimeUnit.NANOSECONDS).primary(\"localhost:27017\").canonicalAddress(\"localhost:27017\").hosts(new HashSetString>(asList(\"localhost:27017\", \"localhost:27018\"))).passives(new HashSetString>(asList(\"localhost:27019\"))).ok(true).state(CONNECTED).version(new ServerVersion(asList(2, 4, 1))).minWireVersion(1).lastWriteDate(new Date()).maxWireVersion(2).electionId(new ObjectId()).setVersion(new Integer(2)); assertEquals(builder.build(), builder.build()); assertEquals(builder.build(), builder.build().hashCode(), builder.build().hashCode()); assertTrue(builder.build().toString().startsWith(\"ServerDescription\"));', 'public static Set createArtifacts( ArtifactFactory artifactFactory, List dependencies, String inheritedScope, ArtifactFilter dependencyFilter, MavenProject project ) throws InvalidDependencyVersionException  Set projectArtifacts = new LinkedHashSet( dependencies.size() ); for ( Iterator i = dependencies.iterator(); i.hasNext(); )  Dependency d = (Dependency) i.next(); String scope = d.getScope(); if ( StringUtils.isEmpty( scope ) )  scope = Artifact.SCOPE_COMPILE; d.setScope( scope );  VersionRange versionRange; try  versionRange = VersionRange.createFromVersionSpec( d.getVersion() );  catch ( InvalidVersionSpecificationException e )  throw new InvalidDependencyVersionException( \"Unable to parse version \\'\" + d.getVersion() + \"\\' for dependency \\'\" + d.getManagementKey() + \"\\': \" + e.getMessage(), e );  Artifact artifact = artifactFactory.createDependencyArtifact( d.getGroupId(), d.getArtifactId(), versionRange, d.getType(), d.getClassifier(), scope, inheritedScope, d.isOptional() ); if ( Artifact.SCOPE_SYSTEM.equals( scope ) )  artifact.setFile( new File( d.getSystemPath() ) );  if ( artifact!= null && ( dependencyFilter == null || dependencyFilter.include( artifact ) ) )  if ( d.getExclusions()!= null &&!d.getExclusions().is', 'private void generateReleasePoms() throws MojoExecutionException  if (!getReleaseProgress().verifyCheckpoint( ReleaseProgressTracker.CP_GENERATED_RELEASE_POM ) )  String canonicalBasedir; try  canonicalBasedir = trimPathForScmCalculation( new File( basedir ) );  catch ( IOException e )  throw new MojoExecutionException( \"Cannot canonicalize basedir: \" + basedir, e );  for ( Iterator it = reactorProjects.iterator(); it.hasNext(); )  MavenProject project = (MavenProject) it.next(); MavenProject releaseProject = new MavenProject( project ); Model releaseModel = releaseProject.getOriginalModel(); fixNullValueInModel( releaseModel, project.getOriginalModel() ); // Remove parent /* TODO: put this back after it is properly resolved again if ( releaseModel.getParent()!= null )  releaseModel.setParent( null );  */ Set artifacts = releaseProject.getArtifacts(); if ( artifacts!= null )  //Rewrite dependencies section List newdeps = new ArrayList(); for ( Iterator i = releaseProject.getArtifacts().iterator(); i.hasNext(); )  Artifact artifact = (Artifact) i.next(); Dependency newdep = new Dependency(); newdep.setArtifactId( artifact.getArtifactId() ); newdep.setGroupId( artifact.getGroupId() ); newdep.setVersion( artifact.getVersion() ); newdep.setType( artifact.getType() ); newdep.setScope( artifact.getScope', 'protected Set createExtensionArtifacts( String projectId, List extensions ) throws ProjectBuildingException  Set extensionArtifacts = new HashSet(); if ( extensions!= null )  for ( Iterator i = extensions.iterator(); i.hasNext(); )  Extension ext = (Extension) i.next(); String version; if ( StringUtils.isEmpty( ext.getVersion() ) )  version = \"RELEASE\";  else  version = ext.getVersion();  Artifact artifact; try  artifact = artifactFactory.createExtensionArtifact( ext.getGroupId(), ext.getArtifactId(), VersionRange.createFromVersionSpec( version ) );  catch ( InvalidVersionSpecificationException e )  throw new ProjectBuildingException( projectId, \"Unable to parse extension version\", e );  if ( artifact!= null )  extensionArtifacts.add( artifact );    return extensionArtifacts;', 'public void testQueryContactStrequent()  ContentValues values1 = new ContentValues(); createContact(values1, \"Noah\", \"Tever\", \"18004664411\", \"a@acme.com\", StatusUpdates.OFFLINE, 0, 0, StatusUpdates.CAPABILITY_HAS_CAMERA | StatusUpdates.CAPABILITY_HAS_VIDEO); ContentValues values2 = new ContentValues(); createContact(values2, \"Sam\", \"Times\", \"18004664412\", \"b@acme.com\", StatusUpdates.INVISIBLE, 3, 0, 0, StatusUpdates.CAPABILITY_HAS_CAMERA); ContentValues values3 = new ContentValues(); createContact(values3, \"Lotta\", \"Calling\", \"18004664413\", \"c@acme.com\", StatusUpdates.AWAY, 5, 0, 0, StatusUpdates.CAPABILITY_HAS_VIDEO); ContentValues values4 = new ContentValues(); createContact(values4, \"Fay\", \"Veritt\", \"18004664414\", \"d@acme.com\", StatusUpdates.AVAILABLE, 0, 1, 0, StatusUpdates.CAPABILITY_HAS_VIDEO | StatusUpdates.CAPABILITY_HAS_VOICE); Cursor c = mResolver.query(Contacts.CONTENT_STREQUENT_URI, null, null, null, Contacts._ID); assertEquals(3, c.getCount()); c.moveToFirst(); assertCursorValues(c, values4); c.moveToNext(); assertCursorValues(c, values3); c.moveToNext(); assertCursorValues(c, values3); c.close(); Uri filterUri = Uri.withAppendedPath(Contacts.CONTENT_STREQUENT_FILTER_URI, \"fay\"); c = mResolver.query(filterU', 'public MaybeSymbol> parse(Symbol current, Parser parser)  Symbol next = parser.moveNext(1); if (!next.isType(SymbolType.Whitespace)) return Symbol.nothing; next = parser.moveNext(1); String option = \"\"; if ((next.isType(SymbolType.Text) && next.getContent().startsWith(\"-\")) || next.isType(SymbolType.DateFormatOption))  option = next.getContent() + (next.isType(SymbolType.DateFormatOption)? parser.moveNext(1).getContent() : \"\"); next = parser.moveNext(1); if (!next.isType(SymbolType.Whitespace)) return Symbol.nothing; next = parser.moveNext(1);  if (!next.isType(SymbolType.Text) &&!next.isType(WikiWord.symbolType)) return Symbol.nothing; next = parser.moveNext(1);  if (!next.isType(SymbolType.Text) &&!next.isType(WikiWord.symbolType)) return Symbol.nothing; current.add(option).add(next); MaybeSourcePage> includedPage = parser.getPage().getNamedPage().findIncludedPage(next.getContent()); if (includedPage.isNothing())  current.add(\"\").add(new Symbol(SymbolType.Style, \"error\").add(includedPage.because()));  else if (includeHelpOption.equals(option))  String helpText = includedPage.getValue().getProperty(PageData.PropertyHELP); current.add(\"\").add(Parser.make( parser.getPage', 'private ModelAndView getMessage( WebRequest webRequest, HttpServletRequest request, HttpServletResponse response, Message message, Group group, int page, String filter ) throws Exception  Template tmpl = Template.getTemplate(request); MapString, Object> params = new HashMapString, Object>(); params.put(\"showAdsense\",!tmpl.isSessionAuthorized() ||!tmpl.getProf().getBoolean(DefaultProfile.HIDE_ADSENSE)); params.put(\"msgid\", message.getId()); params.put(\"page\", page); boolean showDeleted = request.getParameter(\"deleted\")!= null; if (showDeleted)  page = -1;  boolean rss = request.getParameter(\"output\")!=null && \"rss\".equals(request.getParameter(\"output\")); if (showDeleted &&!\"POST\".equals(request.getMethod()))  return new ModelAndView(new RedirectView(message.getLink()));  if (page==-1 &&!tmpl.isSessionAuthorized())  return new ModelAndView(new RedirectView(message.getLink()));  if (showDeleted)  if (!tmpl.isSessionAuthorized())  throw new BadInputException(\"’  2»   ??14\");   params.put(\"showDeleted\", showDeleted); Connection db = null; try  db = LorDataSource.getConnection(); tmpl.initCurrentUser(db', 'public void exit()  if ( values.isEmpty() )  throw new IllegalStateException();  values.remove( 0 );', 'public void testUpdateOwnPackageVoicemail_retainDirtyStatus_notDirty()  ContentValues values = getTestVoicemailValues(); values.put(Voicemails.DIRTY, \"0\"); final Uri uri = mResolver.insert(voicemailUri(), values); ContentValues retainDirty = new ContentValues(); retainDirty.put(Voicemails.TRANSCRIPTION, \"foo\"); retainDirty.put(Voicemails.DIRTY, Voicemails.DIRTY_RETAIN); mResolver.update(uri, retainDirty, null, null); ContentValues newValues = getTestVoicemailValues(); newValues.put(Voicemails.DIRTY, \"0\"); newValues.put(Voicemails.TRANSCRIPTION, \"foo\"); assertStoredValues(uri, newValues);', 'public static ListJCAnnotation> findCopyableAnnotations(JavacNode node)  JCAnnotation anno = null; String annoName = null; for (JavacNode child : node.down())  if (child.getKind() == Kind.ANNOTATION)  if (anno!= null)  annoName = \"\"; break;  JCAnnotation annotation = (JCAnnotation) child.get(); annoName = annotation.annotationType.toString(); anno = annotation;   if (annoName == null) return List.nil(); java.util.ListTypeName> configuredCopyable = node.getAst().readConfiguration(ConfigurationKeys.COPYABLE_ANNOTATIONS); if (!annoName.isEmpty())  for (TypeName cn : configuredCopyable) if (cn!= null && typeMatches(cn.toString(), node, anno.annotationType)) return List.of(anno); for (String bn : BASE_COPYABLE_ANNOTATIONS) if (typeMatches(bn, node, anno.annotationType)) return List.of(anno);  ListBufferJCAnnotation> result = new ListBufferJCAnnotation>(); for (JavacNode child : node.down())  if (child.getKind() == Kind.ANNOTATION)  JCAnnotation annotation = (JCAnnotation) child.get(); boolean match = false; for (TypeName cn : configuredCopyable) if (cn!= null && typeMatches(cn.toString(), node, annotation.annotationType))  result.append(annotation); match = true; break;  if (!match) for (String bn : BASE', 'public void downloadDependencies( Collection dependencies ) throws DownloadFailedException  for ( Iterator j = dependencies.iterator(); j.hasNext(); )  Dependency dep = (Dependency) j.next(); if (!downloadedArtifacts.contains( dep ) )  File destinationFile = localRepository.getArtifactFile( dep ); // The directory structure for this project may // not exists so create it if missing. File directory = destinationFile.getParentFile(); if ( directory.exists() == false )  directory.mkdirs();  if ( dep.getGroupId().equals( \"org.apache.maven\" ) )  //skip our own continue;  if ( destinationFile.exists() && dep.getVersion().indexOf( SNAPSHOT_SIGNATURE )  0 )  continue;  getRemoteArtifact( dep, destinationFile ); if (!destinationFile.exists() )  throw new DownloadFailedException( \"Failed to download \" + dep );  downloadedArtifacts.add( dep );', 'private static Properties cleanTestProps( final Properties props )  final String pageDir = props.getProperty( \"jspwiki.fileSystemProvider.pageDir\" ); final String stripNumbers = pageDir.substring( pageDir.lastIndexOf( \\'/\\' ) ); final String testDir = pageDir.substring( 0, pageDir.lastIndexOf( \\'/\\' ) ) + stripNumbers.replaceAll( \"d\", StringUtils.EMPTY ) + System.currentTimeMillis(); props.put( AuthenticationManager.PROP_LOGIN_THROTTLING, \"false\" ); props.setProperty( \"jspwiki.fileSystemProvider.pageDir\", testDir ); props.setProperty( \"jspwiki.basicAttachmentProvider.storageDir\", testDir ); return props;', 'private MavenExecutionResult addExceptionToResult( MavenExecutionResult result, Throwable e )  if (!result.getExceptions().contains( e ) )  result.addException( e );  return result;', \"private MavenProject build( String pomLocation, Model model, ArtifactRepository localRepository, List remoteArtifactRepositories, List externalProfiles ) throws ProjectBuildingException  Model superModel = getSuperModel(); LinkedList lineage = new LinkedList(); List aggregatedRemoteWagonRepositories; if ( remoteArtifactRepositories == null || remoteArtifactRepositories.isEmpty() )  aggregatedRemoteWagonRepositories = ProjectUtils.buildArtifactRepositories( superModel.getRepositories(), artifactRepositoryFactory, container );  else  aggregatedRemoteWagonRepositories = new ArrayList( remoteArtifactRepositories );  for ( Iterator i = externalProfiles.iterator(); i.hasNext(); )  Profile externalProfile = (Profile) i.next(); for ( Iterator repoIterator = externalProfile.getRepositories().iterator(); repoIterator.hasNext(); )  Repository mavenRepo = (Repository) repoIterator.next(); ArtifactRepository artifactRepo = ProjectUtils.buildArtifactRepository( mavenRepo, artifactRepositoryFactory, container ); if (!aggregatedRemoteWagonRepositories.contains( artifactRepo ) )  aggregatedRemoteWagonRepositories.add( artifactRepo );    MavenProject project = assembleLineage( model, lineage, aggregatedRemoteWagonRepositories, localRepository ); // we don't have to force the collision exception for superModel here, it's already been done in getSuperModel() Model previous = superModel; for ( Iterator i = lineage.iterator(); i.hasNext\", 'public void onAccountsUpdated(Account[] accounts)  // TODO : Check the unit test. boolean accountsChanged = false; HashSetAccount> existingAccounts = new HashSetAccount>(); mDb.beginTransaction(); try  findValidAccounts(existingAccounts); // Add a row to the ACCOUNTS table for each new account for (Account account : accounts)  if (!existingAccounts.contains(account))  accountsChanged = true; mDb.execSQL(\"INSERT INTO \" + Tables.ACCOUNTS + \" (\" + RawContacts.ACCOUNT_NAME + \", \" + RawContacts.ACCOUNT_TYPE + \") VALUES (?,?)\", new String[] account.name, account.type);   // Remove all valid accounts from the existing account set. What is left // in the accountsToDelete set will be extra accounts whose data must be deleted. HashSetAccount> accountsToDelete = new HashSetAccount>(existingAccounts); for (Account account : accountsToDelete)  Log.d(TAG, \"removing data for removed account \" + account); String[] params = new String[] account.name, account.type; mDb.execSQL( \"DELETE FROM \" + Tables.GROUPS + \" WHERE \" + Groups.ACCOUNT_NAME + \" =?\" + \" AND \" + Groups.ACCOUNT_TYPE + \" =?\", params); mDb.execSQL( \"DELETE FROM \" + Tables.PRESENCE + \" WHERE \" + PresenceColumns.RAW_CONTACT_ID + \" IN (\" + \"SELECT \" + RawContacts.RAW_CONTACT_ID + \" IN (\" + \"SELECT \" + RawContacts.', 'public ListNode> routeRequest(String key)  ListNode> routeRequest = this.serializer.toBytes(key); if (routeRequest.isEmpty())  return this.routeRequest;  else  routeRequest.routeRequest.routeRequest.get(route);', 'private MapString, ListString>> parseOptions(final String optionsPart)  MapString, ListString>> optionsMap = new HashMapString, ListString>>(); if (optionsPart.length() == 0)  return optionsMap;  for (final String part : optionsPart.split(\"&|;\"))  if (part.length() == 0)  continue;  int idx = part.indexOf(\"=\"); if (idx >= 0)  String key = part.substring(0, idx).toLowerCase(); String value = part.substring(idx + 1); ListString> valueList = optionsMap.get(key); if (valueList == null)  valueList = new ArrayListString>(1);  valueList.add(urldecode(value)); optionsMap.put(key, valueList);  else  throw new IllegalArgumentException(format(\"The connection string contains an invalid option \\'%s\\'. \" + \"\\'%s\\' is missing the value delimiter eg \\'%s=value\\'\", optionsPart, part, part));   // handle legacy wtimeout settings if (optionsMap.containsKey(\"wtimeout\") &&!optionsMap.containsKey(\"wtimeoutms\"))  optionsMap.put(\"wtimeoutms\", optionsMap.remove(\"wtimeout\")); if (LOGGER.isWarnEnabled())  LOGGER.warn(\"Uri option \\'wtimeout\\' has been deprecated, use \\'wtimeoutms\\' instead.\");   // handle legacy slaveok settings String slaveok = getLastValue(optionsMap, \"slaveok\"); if (slaveok!= null &&!optionsMap.containsKey(\"readpreference\"))  String read', 'public ListString> getRuntimeClasspathElements() throws DependencyResolutionRequiredException  ListString> list = new ArrayListString>( getArtifacts().size() + 1 ); List.add( getBuild().getOutputDirectory() ); for ( Artifact a : getArtifacts() )  if ( a.getArtifactHandler().isAddedToClasspath() )  // TODO: let the scope handler deal with this if ( Artifact.SCOPE_COMPILE.equals( a.getScope() ) || Artifact.SCOPE_RUNTIME.equals( a.getScope() ) )  addArtifactPath( a, list );    return list;', 'public String getCurrentUtcDate()  TimeZone timezone = TimeZone.getTimeZone( \"UTC\" ); DateFormat fmt = new SimpleDateFormat( \"yyyyMMddHHmmss\" ); DateTimeZone timezone = TimeZone.getTimeZone( timezone ); DateFormat fmt = new DateFormat( \"yyyyMMddHHmmss\" ); fmt.setTimeZone( timezone ); return fmt.format( new Date() );', 'public ClusterableServer create(final ServerAddress serverAddress, final ServerDescriptionChangedListener serverDescriptionChangedListener, final ServerListener serverListener, final ClusterClock clusterClock)  ConnectionPool connectionPool = new DefaultConnectionPool(new ServerId(clusterId, serverAddress), new InternalStreamConnectionFactory(clusterSettings.getMode(), streamFactory, credential, applicationName, mongoDriverInformation, compressorList, commandListener, serverApi), connectionPoolSettings); connectionPool.start(); // no credentials, compressor list, or command listener for the server monitor factory ServerMonitorFactory serverMonitorFactory = new DefaultServerMonitorFactory(new ServerId(clusterId, serverAddress), serverSettings, clusterClock, new InternalStreamConnectionFactory(clusterSettings.getMode(), heartbeatStreamFactory, null, applicationName, mongoDriverInformation, Collections.MongoCompressor>emptyList(), null, serverApi), connectionPool, serverApi); return new DefaultServer(new ServerId(clusterId, serverAddress), clusterSettings.getMode(), connectionPool, new DefaultConnectionFactory(), serverMonitorFactory, serverDescriptionChangedListener, serverListener, commandListener, clusterClock);', 'public int addMessage( HttpServletRequest request, AddTopicRequest form, String message, Group group, User user, Screenshot scrn, Topic previewMsg, SetUser> userRefs ) throws IOException, ScriptErrorException  final int msgid = topicDao.saveNewMessage( previewMsg, user, message, request.getHeader(\"User-Agent\"), group ); Section section = sectionService.getSection(group.getSectionId()); if (section.isImagepost() && scrn == null)  throw new ScriptErrorException(\"scrn==null!?\");  if (scrn!=null)  Screenshot screenShot = scrn.moveTo(configuration.getHTMLPathPrefix() + \"/gallery\", Integer.toString(msgid)); imageDao.saveImage( msgid, \"gallery/\" + screenShot.getMainFile().getName(), \"gallery/\" + screenShot.getIconFile().getName() );  if (section.isPollPostAllowed())  pollDao.createPoll(Arrays.asList(form.getPoll()), form.isMultiSelect(), msgid);  if (!userRefs.isEmpty())  userEventService.addUserRefEvent(userRefs.toArray(new User[userRefs.size()]), msgid);  if (form.getTags()!= null)  ListString> tags = tagService.parseSanitizeTags(form.getTags()); topicTagService.updateTags(msgid, tags); tagService.updateCounters(Collections.String>emptyList(), tags', 'void authenticateAsync(final InternalConnection connection, final ConnectionDescription connectionDescription, final SingleResultCallbackVoid> callback)  try  validateUserName(connectionDescription); executeCommandAsync(getMongoCredential().getSource(), getAuthCommand(getMongoCredential().getUserName()), connection, new SingleResultCallbackBsonDocument>()  @Override public void onResult(final BsonDocument nonceResult, final Throwable t)  if (t!= null)  callback.onResult(null, translateThrowable(t));  else  callback.onResult(null, null);   );  catch (Throwable t)  callback.onResult(null, t);', 'public void testRowsLessThanNodes() throws Exception  MapString, String> values = new HashMapString, String>(); File testDir = TestUtils.createTempDir(); File tempDir = new File(testDir, \"temp\"); File outputDir = new File(testDir, \"output\"); // write test data to text file File inputFile = File.createTempFile(\"input\", \".txt\", testDir); inputFile.deleteOnExit(); StringBuilder contents = new StringBuilder(); for(Map.EntryString, String> entry: values.entrySet()) contents.append(entry.getKey() + \"t\" + entry.getValue() + \"n\"); FileUtils.writeStringToFile(inputFile, contents.toString()); String storeName = \"test\"; SerializerDefinition serDef = new SerializerDefinition(\"string\"); Cluster cluster = ServerTestUtils.getLocalCluster(10); // Test backwards compatibility StoreDefinition def = new StoreDefinitionBuilder().setName(storeName).setType(ReadOnlyStorageConfiguration.TYPE_NAME).setKeySerializer(serDef).setValueSerializer(serDef).setRoutingPolicy(RoutingTier.CLIENT).setRoutingStrategyType(RoutingStrategyType.CONSISTENT_STRATEGY).setReplicationFactor(1).setPreferredReads(1).setRequiredReads(1).setPreferredWrites(1).setRequiredWrites(1).build(); HadoopStoreBuilder builder = new HadoopStoreBuilder(\"testRowsLessThanNodes\", new Props(), new JobConf(), TextStoreMapper.class, TextInputFormat.class, cluster, def,', 'public MavenProject build( File projectDescriptor, ArtifactRepository localRepository, ProfileManager profileManager ) throws ProjectBuildingException  ProjectBuilderConfiguration config = new DefaultProjectBuilderConfiguration().setLocalRepository( localRepository ).setGlobalProfileManager( profileManager ); return build( projectDescriptor, config );', 'private void writeWarSpecificResources( XMLWriter writer, File basedir, MavenProject project, List referencedReactorArtifacts, ArtifactRepository localRepository )  String warSourceDirectory = EclipseUtils.getPluginSetting( project, \"maven-war-plugin\", //$NON-NLS-1$ \"warSourceDirectory\", //$NON-NLS-1$ \"/src/main/webapp\" ); //$NON-NLS-1$ writer.startElement( \"wb-resource\" ); //$NON-NLS-1$ writer.addAttribute( \"deploy-path\", \"/\" ); //$NON-NLS-1$ //$NON-NLS-2$ writer.addAttribute( \"source-path\", //$NON-NLS-1$ EclipseUtils.toRelativeAndFixSeparator( basedir, warSourceDirectory, false ) ); writer.endElement(); // dependencies for ( Iterator it = project.getArtifacts().iterator(); it.hasNext(); )  Artifact artifact = (Artifact) it.next(); addDependency( writer, artifact, referencedReactorArtifacts, localRepository );', 'private BsonDocument createIsMasterCommand(final Authenticator authenticator, final InternalConnection connection)  BsonDocument isMasterCommandDocument = new BsonDocument(getHandshakeCommandName(), new BsonInt32(1)).append(\"helloOk\", BsonBoolean.TRUE); if (clientMetadataDocument!= null)  isMasterCommandDocument.append(\"client\", clientMetadataDocument);  if (!requestedCompressors.isEmpty())  BsonArray compressors = new BsonArray(); for (MongoCompressor cur : this.requestedCompressors)  compressors.add(new BsonString(cur.getName()));  isMasterCommandDocument.append(\"compression\", compressors);  if (checkSaslSupportedMechs)  MongoCredential credential = authenticator.getMongoCredential(); isMasterCommandDocument.append(\"saslSupportedMechs\", new BsonString(credential.getSource() + \".\" + credential.getUserName()));  if (authenticator instanceof SpeculativeAuthenticator)  BsonDocument speculativeAuthenticateDocument = ((SpeculativeAuthenticator) authenticator).createSpeculativeAuthenticateCommand(connection); if (speculativeAuthenticateDocument!= null)  isMasterCommandDocument.append(\"speculativeAuthenticate\", speculativeAuthenticateDocument);   return isMasterCommandDocument;', 'private Node pushTextNode(ParserAutomatonState automatonState, Node currentNode, String text)  if (!currentNode.allows(\"text\"))  if (!text.trim().isEmpty())  if (currentNode.allows(\"p\"))  TagNode node = new TagNode(currentNode, parserParameters, \"p\", \"\", automatonState.getRootNode()); currentNode = node;  else if (currentNode.allows(\"div\"))  currentNode.addChildren(new TagNode(currentNode, parserParameters, \"div\", \"\", automatonState.getRootNode())); currentNode = currentNode.lastChildren();  else  currentNode = currentNode.getParent();  currentNode = pushTextNode(automatonState, currentNode, text);   else  Matcher matcher = P_REGEXP.matcher(text); boolean isParagraph = false; boolean isAllow = true; boolean isParagraphed = false; if (TagNode.class.isInstance(currentNode))  TagNode tempNode = (TagNode) currentNode; SetString> disallowedParagraphTags = parserParameters.getDisallowedParagraphTags(); SetString> paragraphedTags = parserParameters.getParagraphedTags(); if (disallowedParagraphTags.contains(tempNode.getBbtag().getName()))  isAllow = false;  if (paragraphedTags.contains(tempNode.getBbtag().getName()))  isParagraphed = true;  if (\"p\".', 'public ModelAndView tagPage( HttpServletRequest request, @PathVariable String tag ) throws Exception  Template tmpl = Template.getTemplate(request); TagName.checkTag(tag); ModelAndView mv = new ModelAndView(\"tag-page\"); mv.addObject(\"tag\", tag); mv.addObject(\"title\", WordUtils.capitalize(tag)); mv.addObject(\"counter\", tagService.getCounter(tag)); if (tmpl.isSessionAuthorized())  mv.addObject( \"showFavoriteTagButton\",!userTagService.hasFavoriteTag(tmpl.getCurrentUser(), tag) ); mv.addObject( \"showUnFavoriteTagButton\", userTagService.hasFavoriteTag(tmpl.getCurrentUser(), tag) ); mv.addObject( \"showUnFavoriteTagButton\", userTagService.hasFavoriteTag(tmpl.getCurrentUser(), tag));  int tagId = tagService.getTagId(tag); mv.addObject(\"favsCount\", userTagService.countFavs(tagId)); ListTagRef> relatedTags = tagService.getRelatedTags(tagId); if (relatedTags.size()>1)  mv.addObject(\"relatedTags\", relatedTags);  mv.addAllObjects(getNewsSection(request, tag)); mv.addAllObjects(getGallerySection(tag, tagId, tmpl)); mv.addAllObjects(getForumSection(tag, tagId)); return mv;', 'public BulkWriteResult bulkWrite(final List? extends WriteModel? extends T>> requests, final BulkWriteOptions options)  ListWriteRequest> writeRequests = new ArrayListWriteRequest>(requests.size()); for (WriteModel? extends T> writeModel : requests)  WriteRequest writeRequest; if (writeModel instanceof InsertOneModel)  InsertOneModelT> insertOneModel = (InsertOneModelT>) writeModel; if (getCodec() instanceof CollectibleCodec)  ((CollectibleCodecT>) getCodec()).generateIdIfAbsentFromDocument(insertOneModel.getDocument());  writeRequest = new InsertRequest(asBson(insertOneModel.getDocument()));  else if (writeModel instanceof ReplaceOneModel)  ReplaceOneModelT> replaceOneModel = (ReplaceOneModelT>) writeModel; writeRequest = new UpdateRequest(asBson(replaceOneModel.getFilter()), asBson(replaceOneModel.getReplacement()), WriteRequest.Type.REPLACE).upsert(replaceOneModel.getOptions().isUpsert());  else if (writeModel instanceof UpdateOneModel)  UpdateOneModelT> updateOneModel = (UpdateOneModelT>) writeModel; writeRequest = new UpdateRequest(asBson(updateOneModel.getFilter()), asBson(updateOneModel.getUpdate()), WriteRequest.Type.UPDATE).multi(false).upsert(updateOneModel.getOptions().isUpsert());  else if (writeModel instanceof UpdateManyModel)  UpdateManyModelT> updateManyModel = (UpdateManyModelT>) writeModel; writeRequest = new UpdateRequest(asBson(updateManyModel.', 'protected ArtifactRepository getReleaseArtifactRepository()  if ( getDistributionManagement().getRepository()!= null )  try  setReleaseArtifactRepository( repositorySystem.buildArtifactRepository( getDistributionManagement().getRepository() ) );  catch ( InvalidRepositoryException e )    return releaseArtifactRepository;', 'public void produceDir( final DataConsumer consumer, final String dirName ) throws IOException  TarArchiveEntry entry = Producers.defaultDirEntryWithName(dirName); entry = map(entry); entry.setSize(0); Producers.produceDirEntry(consumer, entry);', 'public MavenProject getParent()  if ( parent == null )  /* * TODO: This is suboptimal. Without a cache in the project builder, rebuilding the parent chain currently * causes O(n2) parser invocations for an inheritance hierarchy of depth n. */ if ( parentFile!= null )  try  parent = mavenProjectBuilder.build( parentFile, projectBuilderConfiguration ).getProject();  catch ( ProjectBuildingException e )  if ( logger!= null )  logger.debug( \"Failed to build parent project for \" + getId(), e );    else if ( model.getParent()!= null )  try  parent = mavenProjectBuilder.build( getParentArtifact(), projectBuilderConfiguration ).getProject();  catch ( ProjectBuildingException e )  if ( logger!= null )  logger.debug( \"Failed to build parent project for \" + getId(), e );     return parent;', 'public HtmlTag makeSuitesHtml(PageData pageData) throws Exception  HtmlTag div = new HtmlTag(\"div\"); div.addAttribute(\"style\", \"float: left;\"); div.add(\"Suites:\"); String suites = \"\"; WikiPageProperty suitesProp = pageData.getProperties().getProperty(SUITES); if(suitesProp!= null)  suites = suitesProp.getValue();  div.add(HtmlUtil.BR); div.add(HtmlUtil.makeInputTag(\"text\", \"Suites\", suites)); return div;', 'public Cursor query(Uri uri, String[] projection, String selection, String[] selectionArgs, String sortOrder)  if (VERBOSE_LOGGING)  Log.v(TAG, \"query: \" + uri);  final SQLiteDatabase db = mDbHelper.getReadableDatabase(); SQLiteQueryBuilder qb = new SQLiteQueryBuilder(); String groupBy = null; String limit = getLimit(uri); // TODO: Consider writing a test case for RestrictionExceptions when you // write a new query() block to make sure it protects restricted data. final int match = sUriMatcher.match(uri); switch (match)  case SYNCSTATE: return mDbHelper.getSyncState().query(db, projection, selection, selectionArgs, sortOrder); case CONTACTS:  setTablesAndProjectionMapForContacts(qb, uri, projection); break;  case CONTACTS_ID:  long contactId = ContentUris.parseId(uri); setTablesAndProjectionMapForContacts(qb, uri, projection); selectionArgs = insertSelectionArg(selectionArgs, String.valueOf(contactId)); qb.appendWhere(Contacts._ID + \"=?\"); break;  case CONTACTS_LOOKUP: case CONTACTS_LOOKUP_ID:  ListString> pathSegments = uri.getPathSegments(); int segmentCount = pathSegments.size(); if (segmentCount  3)  throw new IllegalArgumentException(mDbHelper.exceptionMessage( \"Missing a lookup key\", uri));  String lookupKey = pathSegments.get(2); if (segmentCount == 4)  // TODO: pull this out into a method and generalize to not require contactId long', 'public void transformForResolve( Artifact artifact, List remoteRepositories, ArtifactRepository localRepository ) throws ArtifactMetadataRetrievalException  Matcher m = SnapshotArtifactMetadata.VERSION_FILE_PATTERN.matcher( artifact.getBaseVersion() ); if ( m.matches() )  // This corrects the base version, but ensure it is not resolved again artifact.setBaseVersion( m.group( 1 ) + \"-SNAPSHOT\" );  else if ( isSnapshot( artifact ) )  SnapshotArtifactMetadata localMetadata; try  localMetadata = SnapshotArtifactMetadata.readFromLocalRepository( artifact, localRepository );  catch ( ArtifactPathFormatException e )  throw new ArtifactMetadataRetrievalException( \"Error reading local metadata\", e );  catch ( IOException e )  throw new ArtifactMetadataRetrievalException( \"Error reading local metadata\", e );  boolean alreadyResolved = alreadyResolved( artifact ); if (!alreadyResolved )  boolean checkedUpdates = false; for ( Iterator i = remoteRepositories.iterator(); i.hasNext(); )  ArtifactRepository remoteRepository = (ArtifactRepository) i.next(); String snapshotPolicy = remoteRepository.getSnapshotPolicy(); // TODO: should be able to calculate this less often boolean checkForUpdates = false; if ( ArtifactRepository.SNAPSHOT_POLICY_ALWAYS.equals( snapshotPolicy ) )  checkForUpdates = true;  else if ( ArtifactRepository.SNAPSHOT_POLICY_DAILY.equals( snapshotPolicy', 'private void insertPhoto(Cursor c, SQLiteStatement insert, SQLiteStatement photoIdUpdate)  if (c.isNull(PhotosQuery.DATA))  return;  long personId = c.getLong(PhotosQuery.PERSON); insert.bindLong(PhotoInsert.RAW_CONTACT_ID, personId); insert.bindLong(PhotoInsert.MIMETYPE_ID, mPhotoMimetypeId); insert.bindBlob(PhotoInsert.PHOTO, c.getBlob(PhotosQuery.DATA)); String account = c.getString(PhotosQuery._SYNC_ACCOUNT); if (!TextUtils.isEmpty(account))  insert.bindString(PhotoInsert.SYNC1, c.getString(PhotosQuery._SYNC_ID));  else  insert.bindNull(PhotoInsert.SYNC1);  long rowId = insert(insert); photoIdUpdate.bindLong(PhotoIdUpdate.PHOTO_ID, rowId); photoIdUpdate.bindLong(PhotoIdUpdate.CONTACT_ID, personId); photoIdUpdate.execute();', 'public void testHintedHandoff() throws Exception  SetInteger> failedNodes = getFailedNodes(); SetByteArray> failedKeys = populateStore(failedNodes); MapByteArray, byte[]> dataInSlops = Maps.newHashMap(); SetByteArray> slopKeys = Sets.newHashSet(); byte[] opCode = new byte[]  Slop.Operation.PUT.getOpCode() ; byte[] spacer = new byte[]  (byte) 0 ; byte[] storeName = ByteUtils.getBytes(STORE_NAME, \"UTF-8\"); for(ByteArray key: failedKeys) slopKeys.add(new ByteArray(ByteUtils.cat(opCode, spacer, storeName, spacer, key.get()))); for(StoreByteArray, Slop> slopStore: slopStores.values())  MapByteArray, ListVersionedSlop>>> entry: res.entrySet())  Slop slop = entry.getValue().get(0).getValue(); dataInSlops.put(slop.getKey(), slop.getValue()); if(logger.isTraceEnabled()) logger.trace(slop);   for(ByteArray failedKey: failedKeys)  byte[] expected = keyValues.get(failedKey).get(); byte[] actual = dataInSlops.get(failedKey); assertNotNull(\"data should be stored in the slop\", actual); assertEquals(\"correct should be stored in the slop\", actual); assertEquals(\"correct should be stored in the slop\", actual); assertEquals(\"correct should be stored in the s', 'private void executeAsync(final AsyncWriteBinding e)  final AsyncWriteBinding e = getOpQuerySlaveOkFlagBit(); final AsyncWriteBinding e = getOpQuerySlaveOkFlagBit(); final AsyncWriteBinding e = getOpQuerySlaveOkFlagBit(); if (!isEmpty())  if (!isEmpty())  if (!isEmpty())  e == null? e;  else  if (e!= null)  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsync()  e.getAsy', 'public String toString()  String coordinateReferenceSystem = getCoordinateReferenceSystem(); if (coordinateReferenceSystem == null)  coordinateReferenceSystem = getCoordinateReferenceSystem();  return \"GeometryCollection\" + \"geometries=\" + geometries + ((geometryCollectionSystem == null)? \"\" : \", coordinateReferenceSystem=\" + coordinateReferenceSystem) + \\'\\';', 'public void onDraw()  int fg, bg; synchronized (buffer)  boolean entireDirty = buffer.update[0] || fullRedraw; boolean isWideCharacter = false; // walk through all lines in the buffer for (int l = 0; l  buffer.height; l++)  // check if this line is dirty and needs to be repainted // also check for entire-buffer dirty flags if (!entireDirty &&!buffer.update[l + 1]) continue; // reset dirty flag for this line buffer.update[l + 1] = false; // walk through all characters in this line for (int c = 0; c  buffer.width; c++)  int addr = 0; long currAttr = buffer.charAttributes[buffer.windowBase + l][c];  int fgcolor = defaultFg; // check if foreground color attribute is set if ((currAttr & VDUBuffer.COLOR_FG)!= 0) fgcolor = (int) ((currAttr & VDUBuffer.COLOR_FG) >> VDUBuffer.COLOR_FG_SHIFT) - 1; if (fgcolor  8 && (currAttr & VDUBuffer.BOLD)!= 0) fg = color[fgcolor + 8]; else fg = color[fgcolor];  // check if background color attribute is set if ((currAttr & VDUBuffer.COLOR_BG)!= 0) bg = color[(int) ((currAttr & VDUBuffer.COLOR_BG) >> VDUBuffer.COLOR_BG_SHIFT) - 1]; else bg = color[defaultBg]; // support character inversion by', 'public PwGroup search(Database db, String qStr)  Cursor cursor; cursor = mDb.query(true, SEARCH_TABLE, new String[] KEY_UUID, SEARCH_TABLE + \" match?\", new String[] qStr, null, null, null, null); PwGroupV3 group = new PwGroupV3(); group.name = \"Search results\"; group.childEntries = new ArrayListPwEntry>(); group.setGroups(new ArrayListPwGroup>()); cursor.moveToFirst(); while (! cursor.isAfterLast() )  String sUUID = cursor.getString(0); UUID uuid = UUID.fromString(sUUID); Log.d(\"TAG\", uuid.toString()); PwEntry entry = (PwEntry) db.entries.get(uuid); group.childEntries.add(entry); cursor.moveToNext();  cursor.close(); return group;', 'private boolean build( ListProjectBuildingResult> results, ListInterimResult> interimResults, ListFile> pomFiles, boolean isRoot, boolean recursive, ProjectBuildingRequest config, ReactorModelPool reactorModelPool, ReactorModelCache modelCache )  boolean errors = false; for ( File pomFile : pomFiles )  ModelBuildingRequest request = getModelBuildingRequest( config, reactorModelPool ); request.setPomFile( pomFile ); request.setTwoPhaseBuilding( true ); request.setModelCache( modelCache ); DefaultModelBuildingListener listener = new DefaultModelBuildingListener( projectBuildingHelper, config ); request.setModelBuildingListener( listener ); try  ModelBuildingResult result = modelBuilder.build( request ); Model model = result.getEffectiveModel(); interimResults.add( new InterimResult( pomFile, request, result, listener, isRoot ) ); if ( recursive &&!model.getModules().isEmpty() )  File basedir = pomFile.getParentFile(); ListFile> moduleFiles = new ArrayListFile>(); for ( String module : model.getModules() )  if ( StringUtils.isEmpty( module ) )  continue;  File moduleFile = new File( basedir, module ); if ( moduleFile.isDirectory() )  moduleFile = new File( moduleFile, Maven.POMv4 );  if (!moduleFile.isFile() )  ModelProblem problem = new DefaultModelProblem( \"Child module \" + moduleFile + \" of \" + pomFile + \" does not exist\", ModelProblem.S', 'public void configure(JobConf conf)  super.configure(conf); md5er = ByteUtils.getDigest(\"md5\"); keySerializerDefinition = getStoreDef().getKeySerializer(); valueSerializerDefinition = getStoreDef().getValueSerializer(); keySerializer = (SerializerObject>) new DefaultSerializerFactory().getSerializer(keySerializerDefinition); valueSerializer = (SerializerObject>) new DefaultSerializerFactory().getSerializer(valueSerializerDefinition); keyCompressor = new CompressionStrategyFactory().get(keySerializerDefinition.getCompression()); valueCompressor = new CompressionStrategyFactory().get(valueSerializerDefinition.getCompression()); routingStrategy = new ConsistentRoutingStrategy(getCluster().getNodes(), getStoreDef().getReplicationFactor());', 'public boolean onOptionsItemSelected(MenuItem item)  int itemId = item.getItemId(); if (id == R.id.get_new_key_icon)  startActivity(new Intent(this, GeneratePubkeyActivity.class)); return true;  else  itemId = R.id.import_existing_key_icon;  else  return super.onOptionsItemSelected(item);', 'public void initialize( WikiEngine engine, Properties properties ) throws NoRequiredPropertyException, IOException  log.debug(\"Initing CachingProvider\"); // engine is used for getting the search engine m_engine = engine; if (m_cacheManager.cacheExists(CACHE_NAME))  m_cache = m_cacheManager.getCache(CACHE_NAME);  else  log.info(\"cache with name \" + CACHE_NAME + \" not found in ehcache.xml, creating it with defaults.\"); m_cache = new Cache(CACHE_NAME, DEFAULT_CACHECAPACITY, false, false, 0, 0); m_cacheManager.addCache(m_cache);  if (m_cacheManager.cacheExists(TEXTCACHE_NAME))  m_textCache= m_cacheManager.getCache(TEXTCACHE_NAME);  else  log.info(\"cache with name \" + TEXTCACHE_NAME + \" not found in ehcache.xml, creating it with defaults.\"); m_textCache = new Cache(TEXTCACHE_NAME, DEFAULT_CACHECAPACITY, false, false, 0, 0); m_cacheManager.addCache(m_textCache);  if (m_cacheManager.cacheExists(HISTORYCACHE_NAME))  m_historyCache= m_cacheManager.getCache(HISTORYCACHE_NAME);  else  log.info(\"cache with name \" + HISTORYCACHE_NAME + \" not found in ehcache.xml, creating it with defaults.\"); m_historyCache = new Cache(HISTORYCACHE_NAME, DEFAULT_CACHECAPACITY, false, false, 0, 0); m_', 'public ListMojoExecution> calculateLifecyclePlan( String lifecyclePhase, MavenSession session ) throws LifecycleExecutionException  // Extract the project from the session MavenProject project = session.getCurrentProject(); // 1. // // Based on the lifecycle phase we are given, let\\'s find the corresponding lifecycle. // Lifecycle lifecycle = phaseToLifecycleMap.get( lifecyclePhase ); // 2. // // If we are dealing with the \"clean\" or \"site\" lifecycle then there are currently no lifecycle mappings but there are default phases // that need to be run instead. // // Now we need to take into account the packaging type of the project. For a project of type WAR, the lifecycle where mojos are mapped // on to the given phases in the lifecycle are going to be a little different then, say, a project of type JAR. // MapString, String> lifecyclePhasesForPackaging; if ( lifecyclePhase.equals( \"clean\" ) )  lifecyclePhasesForPackaging = new HashMapString,String>(); for( String phase : lifecycle.getDefaultPhases() )  lifecyclePhasesForPackaging.put( \"clean\", \"org.apache.maven.plugins:maven-clean-plugin:clean\" );   else  LifecycleMapping lifecycleMappingForPackaging = lifecycleMappings.get( project.getPackaging() ); lifecyclePhasesForPackaging = lifecycleMappingForPackaging.getLifecycles().get( lifecycle.getId() ).getPhases();  // 3. // // Once we have the lifecycle mapping for the given packaging, we need to know whats phases we need to worry about executing. // // Create an ordered Map of the phases in the lifecycle to a list of mojos to execute. MapString,ListString>> phaseToMojoMapping = new LinkedHashMapString,ListString>>(); // 4. for (', 'public void onCreate(Bundle icicle)  super.onCreate(icicle); if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD)  StrictModeSetup.run();  hardKeyboard = getResources().getConfiguration().keyboard == Configuration.KEYBOARD_QWERTY; clipboard = (ClipboardManager) getSystemService(CLIPBOARD_SERVICE); prefs = PreferenceManager.getDefaultSharedPreferences(this); titleBarHide = prefs.getBoolean(PreferenceConstants.TITLEBARHIDE, false); if (titleBarHide)  supportRequestWindowFeature(Window.FEATURE_ACTION_BAR_OVERLAY);  this.setContentView(R.layout.act_console); // hide status bar if requested by user if (prefs.getBoolean(PreferenceConstants.FULLSCREEN, false))  getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN, WindowManager.LayoutParams.FLAG_FULLSCREEN);  // TODO find proper way to disable volume key beep if it exists. setVolumeControlStream(AudioManager.STREAM_MUSIC); // handle requested console from incoming intent if (icicle == null)  requested = getIntent().getData();  else  String uri = icicle.getString(STATE_SELECTED_URI); if (uri!= null)  requested = Uri.parse(uri);   inflater = LayoutInflater.from(this); toolbar = (Toolbar) findViewById(R.id.toolbar); pager = (ViewPager) findViewById(R.id.console_flip); registerForContextMenu(pager); pager', 'private void removeReleasePoms() throws MojoExecutionException  if (!getReleaseProgress().verifyCheckpoint( ReleaseProgressTracker.CP_REMOVED_RELEASE_POM ) )  File currentReleasePomFile = null; try  String canonicalBasedir = trimPathForScmCalculation( new File( basedir ) ); for ( Iterator it = reactorProjects.iterator(); it.hasNext(); )  MavenProject project = (MavenProject) it.next(); currentReleasePomFile = new File( project.getFile().getParentFile(), RELEASE_POM ); String releasePom = trimPathForScmCalculation( currentReleasePomFile ); releasePom = releasePom.substring( canonicalBasedir.length() ); getScm().remove( \"Removing for next development iteration.\", releasePom ); currentReleasePomFile.delete();   catch ( ScmException e )  throw new MojoExecutionException( \"Cannot remove \" + currentReleasePomFile + \" from development HEAD.\", e );  catch ( IOException e )  throw new MojoExecutionException( \"Cannot remove \" + currentReleasePomFile + \" from development HEAD.\", e );  try  getReleaseProgress().checkpoint( basedir, ReleaseProgressTracker.CP_REMOVED_RELEASE_POM );  catch ( IOException e )  getLog().warn( \"Error writing checkpoint.\", e );', 'public void setState(RawContactDelta state, AccountType type, ViewIdGenerator vig, boolean isProfile)  mState = state; // Remove any existing sections mFields.removeAllViews(); // Bail if invalid state or account type if (state == null || type == null) return; setId(vig.getId(state, null, null, ViewIdGenerator.NO_VIEW_INDEX)); // Make sure we have a StructuredName RawContactModifier.ensureKindExists(state, type, StructuredName.CONTENT_ITEM_TYPE); mRawContactId = state.getRawContactId(); final AccountDisplayInfo account = mAccountDisplayInfoFactory.getAccountDisplayInfoFor(state); final String accountTypeLabel; final String accountNameLabel; if (isProfile)  accountTypeLabel = EditorUiUtils.getAccountHeaderLabelForMyProfile( getContext(), account); accountNameLabel = account.getNameLabel().toString();  else  accountTypeLabel = account.getTypeLabel().toString(); accountNameLabel = account.getNameLabel().toString();  if (!account.hasDistinctName())  // Hide this view so the other view will be centered vertically mAccountHeaderNameTextView.setVisibility(View.GONE);  else  mAccountHeaderNameTextView.setVisibility(View.VISIBLE); mAccountHeaderNameTextView.setText(accountNameLabel);  mAccountHeaderTypeTextView.setText(accountTypeLabel); updateAccountHeaderContentDescription(); mAccountIconImageView.setImageDrawable(state.getRawContactAccountType(getContext()).getDisplayIcon(getContext())', 'private MongoIterableTResult> execute()  ListBsonDocument> aggregateList = createBsonDocumentList(pipeline); BsonValue outCollection = aggregateList.size() == 0? null : aggregateList.get(aggregateList.size() - 1).get(\"$out\"); if (outCollection!= null)  AggregateToCollectionOperation operation = new AggregateToCollectionOperation(namespace, aggregateList).maxTime(maxTimeMS, MILLISECONDS).allowDiskUse(allowDiskUse).bypassDocumentValidation(bypassDocumentValidation); executor.execute(operation); return new FindIterableImplTDocument, TResult>(new MongoNamespace(namespace.getDatabaseName(), outCollection.asString().getValue()), documentClass, resultClass, codecRegistry, readPreference, readConcern, executor, new BsonDocument(), new FindOptions()).batchSize(batchSize);  else  return new OperationIterableTResult>(new AggregateOperationTResult>(namespace, aggregateList, codecRegistry.get(resultClass)).maxTime(maxTimeMS, MILLISECONDS).allowDiskUse(allowDiskUse).batchSize(batchSize).useCursor(useCursor).readConcern(readConcern), readPreference, executor);', 'private PlexusContainer container( CliRequest cliRequest ) throws Exception  if ( cliRequest.classWorld == null )  cliRequest.classWorld = new ClassWorld( \"plexus.core\", Thread.currentThread().getContextClassLoader() );  DefaultPlexusContainer container; String cc = new DefaultContainerConfiguration().setClassWorld( cliRequest.classWorld ).setRealm( setupContainerRealm( cliRequest ) ).setClassPathScanning( PlexusConstants.SCANNING_INDEX ).setAutoWiring( true ).setName( \"maven\" ); container = new DefaultPlexusContainer( cc, new AbstractModule()  @Override protected void configure()  bind( ILoggerFactory.class ).toInstance( slf4jLoggerFactory );   ); // NOTE: To avoid inconsistencies, we\\'ll use the TCCL exclusively for lookups container.setLookupRealm( null ); container.setLoggerManager( plexusLoggerManager ); customizeContainer( container ); container.getLoggerManager().setThresholds( cliRequest.request.getLoggingLevel() ); Thread.currentThread().setContextClassLoader( container.getContainerRealm() ); eventSpyDispatcher = container.lookup( EventSpyDispatcher.class ); DefaultEventSpyContext eventSpyContext = new DefaultEventSpyContext(); MapString, Object> data = eventSpyContext.getData(); data.put( \"plexus\", container ); data.put( \"workingDirectory\", cliRequest.workingDirectory ); data.put( \"systemProperties\", cliRequest.systemProperties', 'private Node pushTagNode(RootNode rootNode, Node currentNode, String name, String parameter, boolean renderCut, String cutUrl) if(!currentNode.allows(name)) Tag newTag = allTagsDict.get(name); if(newTag.isDiscardable()) return currentNode; else if(currentNode == rootNode || blockLevelTags.contains(((TagNode)currentNode).getBbtag().getName()) && newTag.getImplicitTag()!= null) currentNode = pushTagNode(rootNode, currentNode, newTag.getImplicitTag(), \"\", renderCut, cutUrl); currentNode = pushTagNode(rootNode, currentNode, name, parameter, renderCut, cutUrl); else currentNode = currentNode.getParent(); currentNode = pushTagNode(rootNode, currentNode, name, parameter, renderCut, cutUrl); else TagNode node = new TagNode(currentNode, this, name, parameter); if(\"cut\".equals(name)) ((CutTag)(node.getBbtag())).setRenderOptions(renderCut, cutUrl);  currentNode.getChildren().add(node); if(!node.getBbtag().isSelfClosing()) currentNode = descend(currentNode);   return currentNode;', 'public void testServerSideRouting() throws Exception  Cluster localCluster = ServerTestUtils.getLocalCluster(2, new int[][]   0, 1, 2, 3,  ); Cluster localTargetCluster = ServerTestUtils.getLocalCluster(2, new int[][] ,  0, 1, 2, 3  ); // start servers 0, 1 only final ListInteger> serverList = Arrays.asList(0, 1); final Cluster updatedCluster = startServers(localCluster, storeDefFile, serverList, null); final Cluster targetCluster = updateCluster(localTargetCluster); ExecutorService executors = Executors.newFixedThreadPool(2); final AtomicBoolean rebalancingToken = new AtomicBoolean(false); final ListException> exceptions = Collections.synchronizedList(new ArrayListException>()); // populate data now. populateData(updatedCluster, Arrays.asList(0)); Node node = updatedCluster.getNodeById(0); final StoreByteArray, byte[]> serverSideRoutingStore = getSocketStore(testStoreName, node.getHost(), node.getSocketPort(), true); // start get operation. executors.execute(new Runnable()  public void run()  try  ListString> keys = new ArrayListString>(testEntries.keySet()); int nRequests = 0; while(!rebalancingToken.get())  // should always able to get values. int index = (int) (Math.random() * keys.size()); // should get a valid value try  nRequests++; ListVersionedbyte[]>> values = serverSideRoutingStore.get(new ByteArray(Byte', 'public void testRebalanceCleanPrimarySecondary() throws Exception  logger.info(\"Starting testRebalanceCleanPrimary\"); try  Cluster currentCluster = ServerTestUtils.getLocalZonedCluster(6, 2, new int[]  0, 0, 0, 1, 1, 1, new int[][]   0,  1, 6,  2,  3,  4, 7,  5  ); Cluster targetCluster = RebalanceUtils.createUpdatedCluster(currentCluster, 2, Lists.newArrayList(7)); targetCluster = RebalanceUtils.createUpdatedCluster(targetCluster, 5, Lists.newArrayList(6)); /** * original server partition ownership * * [s0 : p0,p3,p4,p5,p6,p7] [s1 : p1-p7] [s2 : p1,p2] [s3 : * p0,p1,p2,p3,p6,p7] [s4 : p1-p7] [s5 : p4,p5] * * target server partition ownership * * [s0 : p0,p2,p3,p4,p5,p6,p7] [s1 : p0,p1] [s2 : p1-p7] [s3 : * p0.p1,p2,p3,p5,p6,p7] [s4 : p0,p1,p2,p3,p4,p7] [s5 : p4,p5,p6] */ // start servers ListInteger> serverList = Arrays.asList(0, 1, 2, 3, 4, 5); MapString, String> configProps = new HashMapString, String>(); configProps.put(\"enable.repair\", \"true\"); currentCluster = startServers(currentCluster, rwStoreDefFileWithReplication, serverList, config', 'private Cursor getTrackCursor(TrackListAdapter.TrackQueryHandler queryhandler, String filter, boolean async)  if (queryhandler == null)  throw new IllegalArgumentException();  Cursor ret = null; mSortOrder = MediaStore.Audio.Media.TITLE_KEY; StringBuilder where = new StringBuilder(); where.append(MediaStore.Audio.Media.TITLE + \"!= \\'\\'\"); // Add in the filtering constraints String [] keywords = null; if (filter!= null)  String [] searchWords = filter.split(\" \"); keywords = new String[searchWords.length]; Collator col = Collator.getInstance(); col.setStrength(Collator.PRIMARY); for (int i = 0; i  searchWords.length; i++)  String key = MediaStore.Audio.keyFor(searchWords[i]); key = key.replace(\"\", \"\"); key = key.replace(\"%\", \"%\"); key = key.replace(\"_\", \"_\"); keywords[i] = \\'%\\' + key + \\'%\\';  for (int i = 0; i  searchWords.length; i++)  where.append(\" AND \"); where.append(MediaStore.Audio.Media.ARTIST_KEY + \"||\"); where.append(MediaStore.Audio.Media.TITLE_KEY + \" LIKE? ESCAPE \\'\\'\");   if (mGenre!= null)  mSortOrder = MediaStore.Audio.Genres.Members.DEFAULT_SORT_ORDER; ret = queryhandler.doQuery(MediaStore.Audio.Genres.', 'public MapByteArray, ListVersionedbyte[]>>> getAll(IterableByteArray> keys, MapByteArray, byte[]> transforms) throws VoldemortException  StoreUtils.assertValidKeys(keys); GetAllPipelineData pipelineData = new GetAllPipelineData(); if(zoneRoutingEnabled) pipelineData.setZonesRequired(storeDef.getZoneCountReads()); else pipelineData.setZonesRequired(null); Pipeline pipeline = new Pipeline(Operation.GET_ALL, timeoutMs, TimeUnit.MILLISECONDS); pipeline.addEventAction(Event.STARTED, new GetAllConfigureNodes(pipelineData, Event.CONFIGURED, failureDetector, storeDef.getPreferredReads(), storeDef.getRequiredReads(), routingStrategy, keys, transforms, clientZone)); pipeline.addEventAction(Event.CONFIGURED, new PerformParallelGetAllRequests(pipelineData, Event.INSUFFICIENT_SUCCESSES, failureDetector, timeoutMs, nonblockingStores)); pipeline.addEventAction(Event.INSUFFICIENT_SUCCESSES, new PerformSerialGetAllRequests(pipelineData, repairReads? Event.RESPONSES_RECEIVED : Event.COMPLETED, keys, failureDetector, innerStores, storeDef.getPreferredReads(), storeDef.getRequiredReads())); if(repairReads) pipeline.addEventAction(Event.RESPONSES_RECEIVED, new GetAllReadRepair(pipelineData, Event.COMPLETED, storeDef.getPreferredReads(), nonblockingStores, readRepairer)); pipeline.addEvent(Event.STARTED); pipeline.execute(); if(pipelineData.getFatalError()', 'public Cursor query(Uri uri, String[] inProjection, String selection, String[] selectionArgs, String sortOrder)  final String fileName = extractFileName(uri); DataExporter.hasNext(); final String fileName = extractFileName(uri); DataExporter.hasNext(); final String[] projection = (inProjection!= null)? inProjection : new String[] OpenableColumns.DISPLAY_NAME, OpenableColumns.SIZE; final MatrixCursor c = new MatrixCursor(projection); // Result will always have one row. final MatrixCursor.RowBuilder b = c.newRow(); for (int i = 0; i  c.getColumnCount(); i++)  final String column = projection[i]; if (OpenableColumns.DISPLAY_NAME.equals(column))  // Just return the requested path as the display name. We don\\'t care if the file // really exists. b.add(fileName);  else if (OpenableColumns.SIZE.equals(column))  final File file = DataExporter.getOutputFile(getContext(), fileName); if (file.exists())  b.add(file.length());  else  // File doesn\\'t exist -- return null for \"unknown\". b.add(null);   else  throw new IllegalArgumentException(\"Unknown column \" + column);   return c;', 'public static String formatInetAddr(InetSocketAddress addr)  InetAddress ia = addr.getAddress(); if (ia == null)  return String.format(\"%s:%s\", addr.getHostString(), addr.getPort());  if (ia instanceof Inet6Address)  String[] %s = addr.getHostAddress(); return String.format(\"[%s]:%s\", ia.getHostAddress(), addr.getPort());  else  return String.format(\"%s:%s\", ia.getHostAddress(), addr.getPort());', 'public void testComplete(TestPage test, TestSummary testSummary)  increaseCompletedTests(); if (firstFailure!= null)  notifier.fireTestFailure(new Failure(descriptionFor(test), firstFailure));  else if (testSummary.getExceptions() > 0)  notifier.fireTestFailure(new Failure(descriptionFor(test), new Exception(\"Exception occurred on page \" + test.getFullPath())));  else if (testSummary.getWrong() > 0)  notifier.fireTestFailure(new Failure(descriptionFor(test), new AssertionError(\"Test failures occurred on page \" + test.getFullPath())));  fireTestFinishedFor(test);', 'public final void validateProfile( WikiContext context, UserProfile profile )  boolean isNew = profile.isNew(); WikiSession session = context.getWikiSession(); InputValidator validator = new InputValidator( SESSION_MESSAGES, context ); ResourceBundle rb = context.getBundle( InternationalizationManager.CORE_BUNDLE ); // // Query the SpamFilter first // ListPageFilter> ls = m_engine.getFilterManager().getFilterList(); for( PageFilter pf : ls )  if( pf instanceof SpamFilter )  if( ((SpamFilter)pf).isValidUserProfile( context, profile ) == false )  session.addMessage( SESSION_MESSAGES, \"Invalid userprofile\" ); return;  break;   // If container-managed auth and user not logged in, throw an error if ( m_engine.getAuthenticationManager().isContainerAuthenticated() &&!context.getWikiSession().isAuthenticated() )  session.addMessage( SESSION_MESSAGES, rb.getString(\"security.user.loginname\") ); validator.validateNotNull( profile.getLoginName(), rb.getString(\"security.user.fullname\") ); validator.validateNotNull( profile.getFullname(), rb.getString(\"security.user.fullname\") ); validator.validate( profile.getEmail(), rb.getString(\"security.user.email\"), InputValidator.EMAIL ); // If new profile, passwords must match and can\\'t be null if (!m_engine.getAuthenticationManager().isContainerAuthenticated() )', 'public void recordTx()  if (threadCnt == 0)  threadCnt = 1;  int txType = thread.currentOperation; DriverContext.TimingInfo timingInfo = thread.driverContext.timingInfo; endTime = timingInfo.respondTime; int responseTime = endTime - timingInfo.invokeTime - timingInfo.pauseTime; if (responseTime  0)  thread.logger.warning(thread.name + \":Pause time too large - invoke : \" + timingInfo.invokeTime + \", respond : \" + endTime + \", pause : \" + timingInfo.pauseTime); responseTime = 0; // Set it to 0 in this case so it does not // destroy the whole run.  int elapsedTime = endTime - RunInfo.getInstance().benchStartTime; if(elapsedTime > 0)  if ((elapsedTime / graphBucketSize) >= graphBuckets)  thruputGraph[txType][graphBuckets - 1]++; respGraph[txType][graphBucketSize]++; respGraph[txType][elapsedTime / graphBucketSize] += responseTime;  else  thruputGraph[txType][elapsedTime / graphBucketSize]++; respGraph[txType][elapsedTime / graphBucketSize] += responseTime;   txCntTotal[txType]++; respGraph[txType][elapsedTime / graphBucketSize] += responseTime; if (!thread.inRamp)  txCntStdy[txType]++; respSumTotal[txType] += responseTime; // post in histogram of response times if ((responseTime / respBucketSize) >= RES', 'protected long createGroup(Account account, String sourceId, String title)  String accountId = accountId.getAccountId(); return createGroup(account, sourceId, title, 1);', 'public final int doWikiStartTag() throws IOException  WikiEngine engine = m_wikiContext.getEngine(); Principal user = m_wikiContext.getCurrentUser(); UserDatabase database = engine.getUserDatabase(); UserProfile profile = null; try  profile = database.find( user.getName() ); if ( \"email\".equals( m_prop ) )  pageContext.getOut().print( profile.getEmail() );  else if ( \"fullname\".equals( m_prop ) )  pageContext.getOut().print( profile.getFullname() );  else if ( \"loginname\".equals( m_prop ) )  pageContext.getOut().print( profile.getLoginName() );  else if ( \"wikiname\".equals( m_prop ) )  pageContext.getOut().print( profile.getWikiName() );   catch( NoSuchPrincipalException e )  pageContext.getOut().print( BLANK );  return SKIP_BODY;', 'private void getPublicMembers(Class aClass, List publicFields, List publicMethods)  Field[] fields = aClass.getDeclaredFields(); for(int i = 0; i  fields.length; i++)  Field field = fields[i]; if(Modifier.isPublic(field.getModifiers())) publicFields.add(field);  Method[] methods = aClass.getDeclaredMethods(); for(int i = 0; i  methods.length; i++)  Method method = methods[i]; if(Modifier.isPublic(method.getModifiers())) publicMethods.add(method);', 'protected ArtifactRepository getSnapshotArtifactRepository()  if ( getDistributionManagement().getSnapshotRepository()!= null )  try  SetSnapshotArtifactRepository( repositorySystem.buildArtifactRepository( getDistributionManagement().getSnapshotRepository() ) );  catch ( InvalidRepositoryException e )    return snapshotArtifactRepository;', 'private boolean fetch(FileSystem fs, Path source, File dest, AsyncOperationStatus status, HdfsCopyStats stats, String storeName, long pushVersion, MetadataStore metadataStore) throws IOException  AdminClient adminClient = null; try  adminClient = new AdminClient(metadataStore.getCluster(), new AdminClientConfig(), new ClientConfig()); FetchStrategy fetchStrategy = new BasicFetchStrategy(this, fs, stats, status, bufferSize); if(!fs.isFile(source))  Utils.mkdirs(dest); HdfsDirectory directory = new HdfsDirectory(fs, source); HdfsFile metadataFile = directory.getMetadataFile(); Long estimatedDiskSize = -1L; if(metadataFile!= null)  File copyLocation = new File(dest, metadataFile.getPath().getName()); fetchStrategy.fetch(metadataFile, copyLocation, null); directory.initializeMetadata(copyLocation); String diskSizeInBytes = (String) directory.getMetadata().get(ReadOnlyStorageMetadata.DISK_SIZE_IN_BYTES); estimatedDiskSize = (diskSizeInBytes!= null && diskSizeInBytes!= \"\")? (Long.parseLong(diskSizeInBytes)) : -1L;  VersionedString> diskQuotaSizeInKB = adminClient.quotaMgmtOps.getQuotaForNode(storeName, QuotaType.STORAGE_SPACE, metadataStore.getNodeId()); /* * Only check quot', 'private void ensureThrottled()  int numGetExceptions = 0; int numPutExceptions = 0; for(int i = 0; i  1000; i++)  try  // do a put storeClient.put(\"key\", \"value\");  catch(QuotaExceededException qee)  /** * occurs only when a serial put to master node fails due to * exceeding quota */ numPutExceptions++;  try  // do a get storeClient.get(\"key\");  catch(QuotaExceededException qee)  numGetExceptions++;   assertTrue(\"No get operations rate limited\", numGetExceptions > 0); assertTrue(\"No put operations rate limited\", numPutExceptions > 0);', 'public void loadFromFile(InputStream is) throws IOException, InvalidDBVersionException  LEDataInputStream dis = new LEDataInputStream(is); int sig1 = dis.readInt(); int sig2 = dis.readInt(); if (! matchesHeader(sig1, sig2) )  throw new InvalidDBVersionException();  long version = dis.readUInt(); if (! validVersion(version) )  throw new InvalidDBVersionException();  boolean done = false; while (! done )  done = readHeaderField(dis);', 'private void prepareResponse(Request request)  response = new SimpleResponse(); if (resultsDirectory == null) resultsDirectory = context.getTestHistoryDirectory(); history = new TestHistory(); pageName = request.getResource(); history.readPageHistoryDirectory(resultsDirectory, pageName); pageHistory = history.getPageHistory(pageName); page = context.pageFactory.newPage(); WikiPage root=context.root; WikiPagePath path = PathParser.parse(pageName); PageCrawler crawler = context.root.getPageCrawler(); WikiPage wikiPage = crawler.getPage(root, path); PageData pageData = wikiPage.getData(); pageTitle = new PageTitle(\"Test History\", PathParser.parse(request.getResource()), pageData.getAttribute(PageData.PropertySUITES)); page.setPageTitle(pageTitle);', 'public ModelAndView getMessageNew( int section, WebRequest webRequest, HttpServletRequest request, HttpServletResponse response, int page, String filter, String groupName, int msgid ) throws Exception  Connection db = null; try  db = LorDataSource.getConnection(); Message message = new Message(db, msgid); Group group = new Group(db, message.getGroupId()); if (!group.getUrlName().equals(groupName) || group.getSectionId()!= section)  return new ModelAndView(new RedirectView(message.getLink()));  return getMessage(db, webRequest, request, response, message, group, page, filter);  finally  if (db!=null)  db.close();', 'public DBObject findAndModify(final DBObject query, final DBCollectionFindAndModifyOptions options)  notNull(\"query\", query); notNull(\"options\", options); WriteConcern optionsWriteConcern = optionsWriteConcern()!= null? optionsWriteConcern : getWriteConcern(); WriteOperationDBObject> operation; if (options.isRemove())  operation = new FindAndDeleteOperationDBObject>(getNamespace(), writeConcern, retryWrites, objectCodec).filter(wrapAllowNull(query)).projection(wrapAllowNull(options.getProjection())).sort(wrapAllowNull(options.getSort())).maxTime(options.getMaxTime(MILLISECONDS), MILLISECONDS).collation(options.getCollation());  else  notNull(\"options#getUpdate\", options.getUpdate()); if (!options.getUpdate().keySet().isEmpty() && options.getUpdate().keySet().iterator().next().charAt(0) == \\'$\\')  operation = new FindAndUpdateOperationDBObject>(getNamespace(), writeConcern, retryWrites, objectCodec).filter(wrap(query)).projection(wrapAllowNull(options.getProjection())).sort(wrapAllowNull(options.getSort())).returnOriginal(!options.returnNew()).upsert(options.isUpsert()).maxTime(options.getMaxTime(MILLISECONDS), MILLISECONDS).bypassDocumentValidation(options.getBypassDocumentValidation())', 'public void testAggregationModeSuspendedSeparateTransactions()  // Setting aggregation mode to SUSPENDED should prevent aggregation from happening long rawContactId1 = RawContactUtil.createRawContact(mResolver, ACCOUNT_1); storeValue(RawContacts.CONTENT_URI, rawContactId1, RawContacts.AGGREGATION_MODE, RawContacts.AGGREGATION_MODE_SUSPENDED); Uri name1 = DataUtil.insertStructuredName(mResolver, rawContactId1, \"THE\", \"SAME\"); long rawContactId2 = RawContactUtil.createRawContact(mResolver, ACCOUNT_2); storeValue(RawContacts.CONTENT_URI, rawContactId2, RawContacts.AGGREGATION_MODE, RawContacts.AGGREGATION_MODE_SUSPENDED); DataUtil.insertStructuredName(mResolver, rawContactId2, \"THE\", \"SAME\"); assertNotAggregated(rawContactId1, rawContactId2); // Changing aggregation mode to DEFAULT should change nothing storeValue(RawContacts.CONTENT_URI, rawContactId1, RawContacts.AGGREGATION_MODE, RawContacts.AGGREGATION_MODE_DEFAULT); storeValue(RawContacts.CONTENT_URI, rawContactId2, RawContacts.AGGREGATION_MODE, RawContacts.AGGREGATION_MODE_DEFAULT); assertNotAggregated(rawContactId1, rawContactId2); // Changing the name should trigger aggregation storeValue(name1, StructuredName.GIVEN_NAME, \"the\"); assertAggregated(rawContactId1, rawContactId2);', 'public ReadOnlyPageData getDecoratedData()  StringBuilder decoratedContent = new StringBuilder(1024); includeScenarioLibraries(decoratedContent); decorate(getSetUp(), decoratedContent); addPageContent(decoratedContent); decorate(getTearDown(), decoratedContent); return new PageData(getData(), decoratedContent.toString());', 'public static ReadOnlyStorageEngineTestInstance create(SearchStrategy strategy, File baseDir, int testSize, int numNodes, int repFactor, SerializerDefinition keySerDef, SerializerDefinition valueSerDef, ReadOnlyStorageFormat type) throws Exception  // create some test data MapString, String> data = createTestData(testSize); JsonReader reader = makeTestDataReader(data, baseDir); // set up definitions for cluster and store ListNode> nodes = new ArrayListNode>(); for(int i = 0; i  numNodes; i++)  nodes.add(new Node(i, \"localhost\", 8080 + i, 6666 + i, 7000 + i, Arrays.asList(4 * i, 4 * i + 1, 4 * i + 2, 4 * i + 3)));  Cluster cluster = new Cluster(\"test\", nodes); StoreDefinition storeDef = new StoreDefinitionBuilder().setName(\"test\").setType(ReadOnlyStorageConfiguration.TYPE_NAME).setKeySerializer(keySerDef).setValueSerializer(valueSerDef).setRoutingPolicy(RoutingTier.CLIENT).setRoutingStrategyType(RoutingStrategyType.CONSISTENT_STRATEGY).setReplicationFactor(repFactor).setPreferredReads(1).setRequiredReads(1).setPreferredWrites(1).setRequiredWrites(1).build(); RoutingStrategy router = new RoutingStrategyFactory().updateRoutingStrategy(storeDef, cluster); // build store files in outputDir File outputDir = TestUtils.createTempDir(baseDir); JsonStoreBuilder', 'public boolean process(ProcessingContext ctx)  boolean success = updateRenderedNode(ctx); if (success)  Canvas c = new Canvas(asFloat(\"width\"), asFloat(\"height\")); Object outputValue = getRenderedNode().getOutputValue(); if (outputValue instanceof Grob)  c.add((Grob) outputValue); setOutputValue(c);  else  throw new AssertionError(getAbsolutePath() + \": output of rendered node is not Grob, but \" + outputValue);   else  getOutputParameter().revertToDefault();  return success;', 'private void configureMojoPhaseBinding( MojoDescriptor mojoDescriptor, Map phaseMap, Settings settings ) throws LifecycleExecutionException  if ( settings.getActiveProfile().isOffline() && mojoDescriptor.requiresOnline() )  String goal = mojoDescriptor.getGoal(); getLogger().warn( goal + \" requires online mode, but maven is currently offline. Disabling \" + goal + \".\" );  else  if ( mojoDescriptor.getPhase()!= null )  Phase phase = (Phase) phaseMap.get( mojoDescriptor.getPhase() ); if ( phase == null )  throw new LifecycleExecutionException( \"Required phase \\'\" + mojoDescriptor.getPhase() + \"\\' not found\" );  phase.getGoals().add( mojoDescriptor.getFullGoalName() );', 'public final int doWikiStartTag() throws IOException  WikiSession session = m_wikiContext.getWikiSession(); String status = session.getStatus(); AuthenticationManager mgr = m_wikiContext.getEngine().getAuthenticationManager(); boolean containerAuth = mgr.isContainerAuthenticated(); boolean cookieAssertions = AuthenticationManager.allowsCookieAssertions(); if( m_status!= null )  if ( ANONYMOUS.equals( m_status ))  if (status.equals(WikiSession.ANONYMOUS))  return EVAL_BODY_INCLUDE;   else if( AUTHENTICATED.equals( m_status ))  return EVAL_BODY_INCLUDE;   else if( ASSERTED.equals( m_status ))  if (status.equals(WikiSession.AUTHENTICATED))  return EVAL_BODY_INCLUDE;   else if( ASSERTED.equals( m_status ))  return EVAL_BODY_INCLUDE;   else if( ASSERTIONS_ALLOWED.equals( m_status ))  if ( cookieAssertions )  return EVAL_BODY_INCLUDE;  return SKIP_BODY;  else if( ASSERTIONS_NOT_ALLOWED.equals( m_status ))  if ( cookieAssertions )  return EVAL_BODY_INCLUDE;  return SKIP_BODY;  else if( CONTAINER_AUTH.equals( m_status ))  if ( containerAuth )  return EVAL_BODY_INCLUDE', 'public static String getDefaultDatabaseName()  String baseName = getDefaultDatabaseName(); if (baseName == null)  return DEFAULT_DATABASE_NAME;  return baseName;', 'public void writeBuffer(byte[] buf, String tag) throws IOException  printCommaUnlessFirst(); stream.print(escapeBuffer(buf)); throwExceptionOnError(tag);', 'private PluginRegistry readPluginRegistry( File registryFile ) throws IOException, XmlPullParserException  PluginRegistry registry = null; if ( registryFile.exists() && registryFile.isFile() )  FileReader reader = null; try  reader = new FileReader( registryFile ); PluginRegistryXpp3Reader modelReader = new PluginRegistryXpp3Reader(); registry = modelReader.read( reader ); registry.setFile( registryFile );  finally  IOUtil.close( reader );   return registry;', 'private void executeTaskSegments( List taskSegments, ReactorManager rm, MavenSession session, MavenProject rootProject, EventDispatcher dispatcher ) throws LifecycleExecutionException, BuildFailureException  for ( Iterator it = taskSegments.iterator(); it.hasNext(); )  TaskSegment segment = (TaskSegment) it.next(); if ( segment.aggregate() )  if (!rm.isBlackListed( rootProject ) )  line(); getLogger().info( \"Building \" + rootProject.getName() ); getLogger().info( \" \" + segment ); line(); //!! This is ripe for refactoring to an aspect. // Event monitoring. String event = MavenEvents.PROJECT_EXECUTION; long buildStartTime = System.currentTimeMillis(); String target = rootProject.getId() + \" ( \" + segment + \" )\"; dispatcher.dispatchStart( event, target ); // only call once, with the top-level project (assumed to be provided as a parameter)... for ( Iterator goalIterator = segment.getTasks().iterator(); goalIterator.hasNext(); )  String task = (String) goalIterator.next(); executeGoalAndHandleFailures( task, session, rootProject, dispatcher, event, rm, buildStartTime, target );  rm.registerBuildSuccess( rootProject, System.currentTimeMillis() - buildStartTime ); dispatcher.dispatchEnd( event, target );  else  line(); getLogger().info( \"SKIPPING \" + rootProject.getName() ); getLogger().info( \" \" + segment ); getLogger().info( \"This project has been banned from further executions', 'private void generateSources( String model, String mode, String dir, String modelVersion, String packageWithVersion ) throws Exception  IsolatedClassLoader modelloClassLoader = new IsolatedClassLoader(); for ( Iterator i = Arrays.asList( modelloDeps ).iterator(); i.hasNext(); )  String dependency = (String) i.next(); File f = new File( repoLocal, dependency ); if (!f.exists() )  throw new FileNotFoundException( \"Missing dependency: \" + dependency + (!online? \"; run again online\" : \"; there was a problem downloading it earlier\" ) );  modelloClassLoader.addURL( f.toURL() );  Class c = modelloClassLoader.loadClass( \"org.codehaus.modello.ModelloCli\" ); Object generator = c.newInstance(); Method m = c.getMethod( \"main\", new Class[]String[].class ); String[] args = new String[]model, mode, dir, modelVersion, packageWithVersion; ClassLoader old = Thread.currentThread().getContextClassLoader(); Thread.currentThread().setContextClassLoader( modelloClassLoader ); m.invoke( generator, new Object[]args ); Thread.currentThread().setContextClassLoader( old );', 'public boolean onOptionsItemSelected(MenuItem item)  final int id = item.getItemId(); if (id == android.R.id.home)  mActivity.onBackPressed(); return true;  case R.id.menu_add:  startGroupAddMemberActivity(); return true;  case R.id.menu_multi_send_email:  final long[] ids = mActionBarAdapter.isSelectionMode()? getAdapter().getSelectedContactIdsArray() : GroupUtil.convertStringSetToLongArray(mGroupMemberContactIds); sendToGroup(ids, ContactsUtils.SCHEME_MAILTO, getString(R.string.menu_sendEmailOption)); return true;  case R.id.menu_multi_send_message:  final long[] ids = mActionBarAdapter.isSelectionMode()? getAdapter().getSelectedContactIdsArray() : GroupUtil.convertStringSetToLongArray(mGroupMemberContactIds); sendToGroup(ids, ContactsUtils.SCHEME_SMSTO, getString(R.string.menu_sendMessageOption)); return true;  case R.id.menu_rename_group:  GroupNameEditDialogFragment.newInstanceForUpdate( new AccountWithDataSet(mGroupMetaData.accountName, mGroupMetaData.accountType, mGroupMetaData.dataSet), GroupUtil.ACTION_UPDATE_GROUP, mGroupMetaData.groupId, mGroupMetaData.groupName).show(getFragmentManager(), TAG_GROUP_NAME_EDIT_DIALOG); return true;  case R.id.menu_delete_', 'protected void callClojureWith( ExecutionMode executionMode, File[] sourceDirectory, File outputDirectory, ListString> compileClasspathElements, String mainClass, String[] clojureArgs) throws MojoExecutionException  outputDirectory.mkdirs(); String cp = \"\"; for (File directory : sourceDirectory)  cp = cp + directory.getPath() + File.pathSeparator;  cp = cp + outputDirectory.getPath() + File.pathSeparator; for (Object classpathElement : compileClasspathElements)  cp = cp + File.pathSeparator + classpathElement;  cp = cp.replaceAll(\"s\", \" \"); final String javaExecutable = getJavaExecutable(); getLog().debug(\"Java exectuable used: \" + javaExecutable); getLog().debug(\"Clojure classpath: \" + cp); CommandLine cl = null; if (ExecutionMode.INTERACTIVE == executionMode && SystemUtils.IS_OS_WINDOWS)  cl = new CommandLine(\"cmd\"); cl.addArgument(\"/c\"); cl.addArgument(\"start\"); cl.addArgument(javaExecutable);  else  cl = new CommandLine(javaExecutable);  if (vmargs!= null)  cl.addArguments(vmargs, false);  cl.addArgument(\"-cp\"); cl.addArgument(cp, false); cl.addArgument(\"-Dclojure.compile.path=\" + outputDirectory.getPath(), false); if (war', \"public Cursor queryDocument(String docId, String[] projection) throws FileNotFoundException  // Delegate to real provider final long token = Binder.clearCallingIdentity(); Cursor cursor = null; try  if (RawDocumentsHelper.isRawDocId(docId))  return super.queryDocument(docId, projection);  final DownloadsCursor result = new DownloadsCursor(projection, getContext().getContentResolver()); if (DOC_ID_ROOT.equals(docId))  includeDefaultDocument(result);  else  cursor = mDm.query(new Query().setFilterById(Long.parseLong(docId))); copyNotificationUri(result, cursor); SetString> filePaths = new HashSet>(); if (cursor.moveToFirst())  // We don't know if this queryDocument() call is from Downloads (manage) // or Files. Safely assume it's Files. includeDownloadFromCursor(result, cursor, filePaths, null /* queryArgs */);   result.start(); return result;  finally  IoUtils.closeQuietly(cursor); Binder.restoreCallingIdentity(token);\", 'public String toString()  if (elements.isEmpty())  return \"defaultPath\";  else  String separator = StringUtils.join(elements, separator); if (elements.isEmpty())  return toString(elements);   return toString(elements);', 'public long insertPresence(ContentValues values)  final String handle = values.getAsString(Presence.IM_HANDLE); if (TextUtils.isEmpty(handle) ||!values.containsKey(Presence.PROTOCOL))  throw new IllegalArgumentException(\"PROTOCOL and IM_HANDLE are required\");  final long protocol = values.getAsLong(Presence.PROTOCOL); String customProtocol = null; if (protocol == Im.PROTOCOL_CUSTOM)  customProtocol = values.getAsString(Presence.CUSTOM_PROTOCOL); if (TextUtils.isEmpty(customProtocol))  throw new IllegalArgumentException( \"CUSTOM_PROTOCOL is required when PROTOCOL=PROTOCOL_CUSTOM\");   // TODO: generalize to allow other providers to match against email boolean matchEmail = Im.PROTOCOL_GOOGLE_TALK == protocol; StringBuilder selection = new StringBuilder(); String[] selectionArgs; if (matchEmail)  selection.append( \"(\" + MimetypesColumns.MIMETYPE + \"=\\'\" + Im.CONTENT_ITEM_TYPE + \"\\'\" + \" AND \" + Im.PROTOCOL + \"=?\" + \" AND \" + Im.DATA + \"=?\"); if (customProtocol!= null)  selection.append(\" AND \" + Im.CUSTOM_PROTOCOL + \"=\"); DatabaseUtils.appendEscapedSQLString(selection, customProtocol);  selection.append(\") OR (\" + MimetypesColumns.MIMETYPE + \"=\\'\" + Email.CONTENT_ITEM_TYPE + \"\\'\" + \" AND \" + Email.DATA + \"=?\" + \"))\"); selectionArgs = new String[]', 'RunExecutor getUpdatesRunExecutor(final ListUpdateRequest> updates, final Boolean bypassDocumentValidation, final Connection connection)  return new RunExecutor(connection)  @Override WriteConcernResult executeWriteProtocol(final int index)  return connection.update(namespace, ordered, writeConcern, singletonList(updates.get(index)));  @Override BulkWriteResult executeWriteCommandProtocol()  return connection.updateCommand(namespace, ordered, writeConcern, bypassDocumentValidation, updates);  @Override WriteRequest.Type getType()  return UPDATE;  ;', 'Cursor doQuery(boolean sync, String filterstring)  // Cancel any pending queries mQueryHandler.cancelOperation(MY_QUERY_TOKEN); StringBuilder where = new StringBuilder(); where.append(MediaStore.Audio.Media.TITLE + \"!= \\'\\'\"); // Add in the filtering constraints String [] keywords = null; if (filterstring!= null)  String [] searchWords = filterstring.split(\" \"); keywords = new String[searchWords.length]; Collator col = Collator.getInstance(); col.setStrength(Collator.PRIMARY); for (int i = 0; i  searchWords.length; i++)  String key = MediaStore.Audio.keyFor(searchWords[i]); key = key.replace(\"%\", \"%\"); key = key.replace(\"%\", \"%\", \"%\", \"%\", \"%\");  for (int i = 0; i  searchWords.length; i++)  where.append(\" AND \"); where.append(MediaStore.Audio.Media.ARTIST_KEY + \"||\"); where.append(MediaStore.Audio.Media.ALBUM_KEY + \"||\"); where.append(MediaStore.Audio.Media.TITLE_KEY + \" LIKE?\");   // We want to show all audio files, even recordings. Enforcing the // following condition would hide recordings. //where.append(\" AND \" + MediaStore.Audio.Media.IS_MUSIC + \"=1\"); if (sync)  try  return getContentResolver().query(mBaseUri, CURSOR_COLS, where.toString(), keywords, mSortOrder);  catch (UnsupportedOperationException ex)', 'private Session loadFromMemcached( final String sessionId )  if (!isValidSessionIdFormat( sessionId ) )  return null;  final String nodeId = _sessionIdFormat.extractMemcachedId( sessionId ); if (!_nodeAvailabilityCache.isNodeAvailable( nodeId ) )  _log.debug( \"Asked for session \" + sessionId + \", but the related\" + \" memcached node is still marked as unavailable (won\\'t load from memcached).\" );  else  _log.debug( \"Loading session from memcached: \" + sessionId ); try  final Session session = (Session) _memcached.get( sessionId ); if ( _log.isDebugEnabled() )  if ( session == null )  _log.debug( \"Session \" + sessionId + \" not found in memcached.\" );  else  _log.debug( \"Found session with id \" + sessionId );   _nodeAvailabilityCache.setNodeAvailable( nodeId, true ); return session;  catch ( final NodeFailureException e )  _log.warn( \"Could not load session with id \" + sessionId + \" from memcached.\" ); _nodeAvailabilityCache.setNodeAvailable( nodeId, false );  catch ( final Exception e )  _log.warn( \"Could not load session with id \" + sessionId + \" from memcached.\" ); _nodeAvailabilityCache.setNodeAvailable( nodeId, false );   return null;', 'public MavenProject build( File pomFile, ProjectBuilderConfiguration configuration ) throws ProjectBuildingException  //Do inheritance DomainModel domainModel; try  domainModel = build( \"unknown\", pomFile, configuration );  catch (IOException e)  throw new ProjectBuildingException(\"\", \"\", e);  //Profiles ListProfile> projectProfiles; Properties props = new Properties(); props.putAll(configuration.getExecutionProperties()); try  projectProfiles = DefaultProfileManager.getActiveProfilesFrom(configuration.getGlobalProfileManager(), props, domainModel.getModel() );  catch ( ProfileActivationException e )  throw new ProjectBuildingException( \"\", \"Failed to activate pom profiles.\");  catch(IOException e)  throw new ProjectBuildingException( \"\", \"Failed to activate pom profiles.\");  try  ListProfile> externalProfiles = new ArrayListProfile>(); for(Profile p : projectProfiles)  if(!\"pom\".equals(p.getSource()))  logger.debug(\"Merging profile into model (build): Model = \" + domainModel.getId() + \", Profile = \" + p.getId() ); externalProfiles.add(p);   domainModel = ProcessorContext.mergeProfilesIntoModel( externalProfiles, domainModel );  catch ( IOException e )  throw new ProjectBuildingException(\"\", \"\");  //Interpolation & Management MavenProject project; try  Model model = interpolateDomainModel( domainModel, configuration, pomFile ); ListPlugin> plns = new ArrayListPlugin>(); SetPlugin> plugins = lifecycle.getPluginsBoundByDefaultToAllLifecycles(model.getPackaging()); addPluginsToModel(model, plugins); ProcessorContext.processManagementNodes(model); project = this', 'public void shouldThrowMethodNotAllowedExceptionWhenNoRoutesMatchTheURIWithGivenHttpMethod() throws Exception  final Route route = mockery.mock(Route.class); final Route route = mockery.canHandle(with(any(String.class))); will(returnValue(true)); allowing(route).allowedMethods(); will(returnValue(EnumSet.of(HttpMethod.GET))); ignoring(anything());  ); router.add(route); try  router.parse(\"any uri\", HttpMethod.DELETE, request); Assert.fail(\"MethodNotAllowedException is expected\");  catch (MethodNotAllowedException e)  assertThat(e.getAllowedMethods(), is((SetHttpMethod>)EnumSet.of(HttpMethod.GET))); mockery.assertIsSatisfied();', 'public void shouldFormatFileNamesForCommit()  GitFileVersionsController versionsController = new GitFileVersionsController(); String formatted = versionsController.formatFiles(new File[] new File(\"simple.txt\"), new File(\"middle.xml\"), new File(\"complex/name.txt\")); assertEquals(\"simple.txt, middle.xml and complex/name.txt\", formatted);', 'public static boolean hasNonNullAnnotations(JavacNode node, ListJCAnnotation> anns)  if (anns == null) return false; for (JCAnnotation ann : anns)  for (String nn : NONNULL_ANNOTATIONS) if (typeMatches(nn, node, ann)) return true;  return false;', 'private void checkin( String path, int currentVersion ) throws RepositoryException  Session copierSession = null; try  copierSession = m_sessionManager.newSession(); // If the item does not exist yet, there is nothing to copy. if(!copierSession.itemExists( path ) ) return; Node nd = (Node)copierSession.getItem( path ); Node versions; if(!nd.hasNode( WIKI_VERSIONS ) )  versions = nd.addNode( WIKI_VERSIONS );  else  versions = nd.getNode( WIKI_VERSIONS );  Node newVersion = versions.addNode( Integer.toString( currentVersion ) ); newVersion.addMixin( \"mix:referenceable\" ); copyProperties( nd, newVersion ); copierSession.save();  finally  if( copierSession!= null ) copierSession.logout();', 'private Poll buildNewPoll(Topic message, EditTopicRequest form) throws PollNotFoundException  Poll poll = pollDao.getPollByTopicId(message.getId()); ListPollVariant> newVariants = new ArrayList>(); for (PollVariant v : poll.getVariants())  String label = form.getPoll().get(v.getId()); if (!Strings.isNullOrEmpty(label))  newVariants.add(new PollVariant(v.getId(), label));   for (String label : form.getNewPoll())  if (!Strings.isNullOrEmpty(label))  newVariants.add(new PollVariant(0, label));   return poll.createNew(newVariants);', 'public int update(final Uri uri, final ContentValues values, final String where, final String[] whereArgs)  if (shouldRestrictVisibility())  Helpers.validateSelection(where, sAppReadableColumnsSet);  final Context context = getContext(); final ContentResolver resolver = context.getContentResolver(); final SQLiteDatabase db = mOpenHelper.getWritableDatabase(); int count; boolean updateSchedule = false; boolean isCompleting = false; ContentValues filteredValues; if (Binder.getCallingPid()!= Process.myPid())  filteredValues = new ContentValues(); copyString(Downloads.Impl.COLUMN_APP_DATA, values, filteredValues); copyInteger(Downloads.Impl.COLUMN_VISIBILITY, values, filteredValues); Integer i = values.getAsInteger(Downloads.Impl.COLUMN_CONTROL); if (i!= null)  filteredValues.put(Downloads.Impl.COLUMN_CONTROL, i); updateSchedule = true;  copyInteger(Downloads.Impl.COLUMN_CONTROL, values, filteredValues); copyString(Downloads.Impl.COLUMN_TITLE, values, filteredValues); copyString(Downloads.Impl.COLUMN_MEDIAPROVIDER_URI, values, filteredValues); copyString(Downloads.Impl.COLUMN_DESCRIPTION, values, filteredValues); copyInteger(Downloads.Impl.COLUMN_DELETED, values, filteredValues);  else  filteredValues = values; String filename = values.', 'void addHtmlFormatter() throws Exception  BaseFormatter formatter = new SuiteHtmlFormatter(context, page, context.htmlPageFactory)  protected void writeData(String output) throws Exception  addToResponse(output);  ; formatters.add(formatter);', 'public void forward(String result)  try  Resource resource = method.getResource(); if (!Info.isOldComponent(resource))  request.getRequestDispatcher(resolver.pathFor(method, result)).forward(request, response); return;  String key = Info.getComponentName(resource.getType()) + \".\" + Info.getLogicName(method.getMethod()) + \".\" + result; String path = config.getForwardFor(key); if (path == null)  request.getRequestDispatcher(resolver.pathFor(method, result)).forward(request, response);  else  try  result = evaluator.parseExpression(path, webRequest);  catch (ExpressionEvaluationException e)  throw new ServletException(\"Unable to redirect while evaluating expression \\'\" + path + \"\\'.\", e);  if (logger.isDebugEnabled())  logger.debug(\"overriden view found for \" + key + \" : \" + path + \" expressed as \" + result);  if (result.startsWith(\"redirect:\"))  response.sendRedirect(result.substring(9));  else  request.getRequestDispatcher(result).forward(request, response);    catch (ServletException e)  throw new ResultException(e);  catch (IOException e)  throw new ResultException(e);', 'private static String retrieveLocalRepo()  String repo = System.getProperty( \"maven.repo.local\" ); if ( repo == null )  UserModelReader userModelReader = new UserModelReader(); try  String userHome = System.getProperty( \"user.home\" ); File userXml = new File( userHome, \".m2/settings.xml\" ); if ( userXml.exists() )  userModelReader.parse( userXml ); repo = new File( userModelReader.getLocalRepository() ).getAbsolutePath();   catch ( Exception e )  e.printStackTrace();   if ( repo == null )  String userHome = System.getProperty( \"user.home\" ); String m2LocalRepoPath = \"/.m2/repository\"; File repoDir = new File( userHome, m2LocalRepoPath ); if (!repoDir.exists() )  repoDir.mkdirs();  repo = repoDir.getAbsolutePath(); System.out.println( \"Using default local repository: \" + repoDir.getAbsolutePath() );  return repo;', 'public void setup() throws Exception  mockery = new Mockery(); this.stream = new ByteArrayOutputStream(); response = mockery.mock(HttpServletResponse.class); mockery.checking(new Expectations()   one(response).setContentType(\"application/json\"); allowing(response).getWriter(); will(returnValue(new PrintWriter(stream)));  ); this.serialization = new XStreamJSONSerialization(response, new DefaultTypeNameExtractor());', 'private void writeDigestFile( File digestFile, byte[] digestData ) throws IOException  Writer out = null; try  out = new FileWriter( digestFile ); for ( int i = 0; i  digestData.length; i++ )  out.write( Integer.toHexString( digestData[i] ) );   finally  IOUtil.close( out );', 'public MemcachedNode getPrimary( final String key )  final MemcachedNode result = _nodesMap.get( getNodeId( key ) ); if ( result == null )  throw new IllegalArgumentException( \"No node found for key \" + key );  return result;', 'private void paintPortTooltip(Graphics2D g)  if (overInput!= null)  Rectangle r = inputPortRect(overInput.node, overInput.port); Point2D pt = new Point2D.Double(r.getX(), r.getY() + 11); String text = String.format(\"%s (%s)\", overInput.port.getName(), overInput.port.getType()); paintTooltip(g, pt, text);  else if (overOutput!= null && connectionOutput == null)  Rectangle r = outputPortRect(overOutput); Point2D pt = new Point2D.Double(r.getX(), r.getY() + 11); String text = String.format(\"output (%s)\", overOutput.getOutputType()); paintTooltip(g, pt, text);', 'public String toHtml(Translator translator, Symbol symbol)  if (symbol.childAt(0).childAt(0).getType() == SymbolType.WikiWord) return translator.translate(symbol.childAt(0)); String linkBody = translator.translate(symbol.childAt(0)); if (symbol.childAt(1).getType() == SymbolType.WikiWord) return new WikiWordBuilder().buildLink(translator.getPage(), symbol.childAt(1).getContent(), linkBody); HtmlTag alias = new HtmlTag(\"a\", linkBody); alias.addAttribute(\"href\", symbol.childAt(1).getContent()); return alias.htmlInline();', 'private REQ, RSP> void maybeEnqueue(MessageREQ> message, InetAddressAndPort to, @Nullable RequestCallbackRSP> callback)  CallbackContext cb; if (callback!= null)  if (callbacks.containsKey(message.id())) throw new AssertionError(\"Message id \" + message.id() + \" already has a callback\"); cb = new CallbackContext(callback); callbacks.put(message.id(), cb);  else  cb = null;  boolean toSelf = this.broadcastAddressAndPort.equals(to); Node node = nodes.get(to); SetFaults> allowedFaults = allowedMessageFaults.apply(node, message); if (allowedFaults.isEmpty())  // enqueue so stack overflow doesn\\'t happen with the inlining desderedScheduled.submit(() -> node.handle(message));  else  Runnable enqueue = () ->  if (!allowedFaults.contains(Faults.DELAY))  unorderedScheduled.submit(() -> node.handle(message));  else  if (toSelf) unorderedScheduled.submit(() -> node.handle(message)); else unorderedSchedule(() -> node.handle(message), networkJitterNanos(to), TimeUnit.NANOSECONDS);  ; if (!allowedFaults.contains(Faults.DROP)) enqueue.run(); else  if (!toSelf && networkDrops(to))  // logger.warn(\"D', 'public void headingAdded( WikiContext context, Heading hd )  log.debug(\"HD: \"+hd.m_level+\", \"+hd.m_titleText+\", \"+hd.m_titleAnchor); switch( hd.m_level )  case Heading.HEADING_SMALL: m_buf.append(\"li class=\"toclevel-3\">\"); m_level3Index++; break; case Heading.HEADING_MEDIUM: m_buf.append(\"li class=\"toclevel-2\">\"); m_level2Index++; break; case Heading.HEADING_LARGE: m_buf.append(\"li class=\"toclevel-1\">\"); m_level1Index++; break; default: throw new InternalWikiException(\"Unknown depth in toc! (Please submit a bug report.)\");  if (m_level1Index  m_starting)  // in case we never had a large heading... m_level1Index++;  if ((m_lastLevel == Heading.HEADING_SMALL) && (hd.m_level!= Heading.HEADING_SMALL))  m_level3Index = 0;  if ( ((m_lastLevel == Heading.HEADING_SMALL) || (m_lastLevel == Heading.HEADING_MEDIUM)) && (hd.m_level == Heading.HEADING_LARGE) )  m_level3Index = 0; m_level2Index = 0;  String url = context.getURL( WikiContext.VIEW, context.getPage().getName() ); String sectref = \"#section-\"+context.getEngine().encodeName(context.getPage().getName())+\"-\"+', 'private void setTablesAndProjectionMapForEntities(SQLiteQueryBuilder qb, Uri uri, String[] projection)  StringBuilder sb = new StringBuilder(); sb.append(Views.ENTITIES); sb.append(\" data\"); appendContactPresenceJoin(sb, projection, ContactsColumns.CONTACT_ID); appendContactStatusUpdateJoin(sb, projection, ContactsColumns.LAST_STATUS_UPDATE_ID); appendDataPresenceJoin(sb, projection, ContactsColumns.LAST_STATUS_UPDATE_ID); appendDataStatusUpdateJoin(sb, projection, ContactsColumns.LAST_STATUS_UPDATE_ID); appendDataPresenceJoin(sb, projection, Contacts.Entity.DATA_ID); qb.setTables(sb.toString()); qb.setProjectionMap(sEntityProjectionMap); appendAccountFromParameter(qb, uri);', 'private ListCommandEvent> getExpectedEvents(final BsonArray expectedEventDocuments)  ListCommandEvent> expectedEvents = new ArrayListCommandEvent>(expectedEventDocuments.size()); for (IteratorBsonValue> iterator = expectedEventDocuments.iterator(); iterator.hasNext();)  BsonDocument curExpectedEventDocument = iterator.next().asDocument(); String eventType = curExpectedEventDocument.keySet().iterator().next(); BsonDocument eventDescriptionDocument = curExpectedEventDocument.getDocument(eventType); CommandEvent commandEvent; if (eventType.equals(\"command_started_event\"))  commandEvent = new CommandStartedEvent(1, null, databaseName, eventDescriptionDocument.getString(\"command_name\").getValue(), eventDescriptionDocument.getDocument(\"command\"));  else if (eventType.equals(\"command_succeeded_event\"))  BsonDocument replyDocument = eventDescriptionDocument.get(\"reply\").asDocument(); commandEvent = new CommandSucceededEvent(1, null, eventDescriptionDocument.getString(\"command_name\").getValue(), replyDocument, 1);  else if (eventType.equals(\"command_failed_event\"))  commandEvent = new CommandFailedEvent(1, null, eventDescriptionDocument.getString(\"command_name\").getValue(), replyDocument, 1);  else  throw new UnsupportedOperationException(\"Unsupported command event type: \" + eventType);  expectedEvents.add(commandEvent);  return expectedEvents;', 'public void removeChildPage(final String name) throws Exception  File fileToBeDeleted = new File(getFileSystemPath() + \"/\" + name); FileUtil.deleteFileSystemDirectory(fileToBeDeleted);', 'public String getExpandedVersion( Artifact artifact )  String classifier = artifact.getClassifier(); String version = version.getVersion(); if ( version == null )  version = version.getVersion();  return versions.get( version ).getVersion();', 'public boolean visit(Symbol node)  try  if (node.isType(SymbolType.WikiWord))  wikiWordRenamePageIfReferenced(node, subjectPage, newName);  else if (node.isType(Alias.symbolType))  if (new WikiWordPath().findLength(wikiWordPath.childAt(1).childAt(0).getContent()) > 0)  WikiWordPath wikiWord = new WikiWordPath().findLength(wikiWordAt(1).childAt(0).getContent(); wikiWordRenamePageIfReferenced(node.childAt(1).childAt(0), subjectPage, newName);    catch (Exception e)  e.printStackTrace(); throw new RuntimeException(e);  return true;', 'public void rebalance()  assert servers!= null && servers.size() > 1; VoldemortConfig config = servers.get(0).getVoldemortConfig(); adminClient = RebalanceUtils.createTempAdminClient(config, cluster, 4); HashMapInteger, ListInteger>> replicaToPartitionList = Maps.newHashMap(); replicaToPartitionList.put(0, ImmutableList.of(0, 1)); int req = adminClient.storeMntOps.migratePartitions(0, 1, testStoreNameRW, replicaToPartitionList, null, null, false); adminClient.rpcOps.waitForCompletion(1, req, 5, TimeUnit.SECONDS); VersionedCluster> versionedCluster = adminClient.metadataMgmtOps.getRemoteCluster(0); Node node0 = versionedCluster.getValue().getNodeById(0); Node node1 = versionedCluster.getValue().getNodeById(1); Node newNode0 = new Node(node0.getId(), node0.getHost(), node0.getHttpPort(), node0.getSocketPort(), node0.getAdminPort(), ImmutableList.Integer> of()); Node newNode1 = new Node(node1.getId(), node1.getHost(), node1.getHttpPort(), node1.getSocketPort(), node1.getAdminPort(), ImmutableList.of(0, 1)); adminClient.storeMntOps.deletePartitions(0, testStoreNameRW, ImmutableList.of(0, 1), null); newCluster = new Cluster(cluster.getName(), ImmutableList.of(newNode0,', 'private ListObject> readList(final BsonReader reader, final DecoderContext decoderContext)  reader.readStartArray(); ListObject> list = new ArrayListObject>(); while (reader.readBsonType()!= BsonType.END_OF_DOCUMENT)  ListObject> readValue = readValue(reader, decoderContext); ListObject> readList = new ArrayListObject>(); while (reader.readBsonType()!= BsonType.END_OF_DOCUMENT)  list.add(readValue(reader, decoderContext));  reader.readEndArray(); return list;', 'public String getHtml()  String content = getDecoratedContent(); WikiSourcePage sourcePage = new WikiSourcePage(sourcePage); ParsingPage parsingPage = new ParsingPage(sourcePage), variableSource); Symbol syntaxTree = Parser.make(parsingPage, content).parse(); return new HtmlTranslator(parsingPage.getPage(), parsingPage).translateTree(syntaxTree);', 'public void testGetStatsCacheDump() throws Exception  // There needs to at least have been one value set or there // won\\'t be anything to dump client.set(\"dumpinitializer\", 0, \"hi\"); MapSocketAddress, MapString, String>> stats = client.getStats(\"cachedump 1 10000\"); System.out.println(\"Stats: \" + stats); assertEquals(1, stats.size()); MapString, String> oneStat=stats.values().iterator().next(); String val = oneStat.get(\"dumpinitializer\"); assertTrue(val + \"doesn\\'t match\", val.matches(\"[2 b; d+ s]\"));', 'public void setUp() throws Exception  root = InMemoryPage.makeRoot(\"RooT\"); context = FitNesseUtil.makeTestContext(root); page = root.getPageCrawler().addPage(root, PathParser.parse(\"ComparedPage\"), \"original content\"); PageData data = page.getData(); firstVersion = page.commit(data).getName(); WikiPageProperties properties = data.getProperties(); properties.set(PageData.PropertySUITES, \"New Page tags\"); data.setContent(\"new stuff\"); secondVersion = page.commit(data).getName(); data.setContent(\"even newer stuff\"); page.commit(data); request = new MockRequest(); request.setResource(\"ComparedPage\"); mockedComparer = mock(VersionComparer.class); responder = new VersionComparerResponder(mockedComparer); responder.testing = true;', 'public ListSuggestion> getSuggestions()  final ArrayListSuggestion> list = Lists.newArrayList(); if (mDataCursor!= null && mAccountFilter!= null)  Suggestion suggestion = null; long currentRawContactId = -1; mDataCursor.moveToPosition(-1); while (mDataCursor.moveToNext())  final long rawContactId = mDataCursor.getLong(DataQuery.RAW_CONTACT_ID); if (rawContactId!= currentRawContactId)  suggestion = new Suggestion(); suggestion.rawContactId = rawContactId; suggestion.photoId = mDataCursor.getLong(DataQuery.PHOTO_ID); suggestion.contactId = mDataCursor.getLong(DataQuery.CONTACT_ID); suggestion.contactLookupKey = mDataCursor.getString(DataQuery.LOOKUP_KEY); final String accountName = mDataCursor.getString(DataQuery.ACCOUNT_NAME); final String accountType = mDataCursor.getString(DataQuery.ACCOUNT_TYPE); final String dataSet = mDataCursor.getString(DataQuery.DATA_SET); final AccountWithDataSet account = new AccountWithDataSet( accountName, accountType, dataSet); if (mAccountFilter.equals(account))  list.add(suggestion);  currentRawContactId;  final String mimetype = mDataCursor.getString(DataQuery.MIMETYPE); if (Phone.CONTENT_ITEM_TYPE.equals(mimetype))  final String data = mDataCursor.getString(DataQuery.MIMETYPE); if (Phone.CONTENT_ITEM_TYPE.equals(mimetype))  final', 'public void testSettingACommentInsertsCommentIntoProfileCollectionWhenProfilingIsTurnedOn()  assumeThat(isSharded(), is(false)); // given database.command(new BasicDBObject(\"profile\", 2)); String expectedComment = \"test comment\"; // when DBCursor cursor = new DBCursor(collection, new BasicDBObject(), new BasicDBObject(), ReadPreference.primary()).comment(expectedComment); while (cursor.hasNext())  cursor.next();  // then DBCollection profileCollection = database.getCollection(\"system.profile\"); assertEquals(1, profileCollection.count()); assertEquals(expectedComment, ((DBObject) profileCollection.findOne().get(\"query\")).get(serverVersionAtLeast(asList(3, 1, 8))? \"comment\" : \"$comment\")); // finally database.command(new BasicDBObject(\"profile\", 0)); profileCollection.drop();', 'public final void fireEvent( int type, WikiContext context )  if ( WikiEventManager.isListening(this) && WikiPageEvent.isValidType(type) )  WikiPagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePagePage = new WikiPageEvent(m_engine,type,pagePage.getName() );', 'public void assertPasses(String pageName, String pageType, String suiteFilter, String excludeSuiteFilter) throws Exception  FitNesseContext context = ContextConfigurator.systemDefaults().withRootPath(fitNesseRootPath).withPort(port).makeFitNesseContext(); JavaFormatter testFormatter = new JavaFormatter(pageName); testFormatter.setResultsRepository(new JavaFormatter.FolderResultsRepository(outputDir)); MultipleTestsRunner testRunner = createTestRunner(initChildren(pageName, suiteFilter, excludeSuiteFilter, context), context, debugMode); testRunner.addTestSystemListener(testFormatter); testRunner.addTestSystemListener(resultsListener); testRunner.addExecutionLogListener(new ConsoleExecutionLogListener()); testRunner.executeTestPages(); TestSummary summary = testFormatter.getTotalSummary(); assertEquals(\"wrong\", 0, summary.getWrong()); assertEquals(\"exceptions\", 0, summary.getExceptions()); assertTrue(msgAtLeastOneTest(pageName, summary), summary.getRight() > 0);', 'private void constructFixture()  Expectation expectation = new Expectation(\"OK\", getInstructionNumber(), 0,0); expectations.add(expectation); ListObject> makeInstruction = prepareInstruction(); makeInstruction.add(\"make\"); makeInstruction.add(getTableName()); String tableHeader = table.getCellContents(0,0); String fixtureName = tableHeader.split(\":\")[1]; makeInstruction.add(fixtureName); int columnCount = table.getColumnCountInRow(0); for (int col = 1; col  columnCount; col++) makeInstruction.add(table.getCellContents(col, 0)); addInstruction(makeInstruction);', 'public void extend(Shape s)  PathIterator pi = s.getPathIterator(new AffineTransform()); while (!pi.isDone())  float[] points = new float[6]; int cmd = pi.currentSegment(points); if (cmd == PathIterator.SEG_MOVETO)  moveto(points[0], points[1]);  else if (cmd == PathIterator.SEG_LINETO)  lineto(points[0], points[1]);  else if (cmd == PathIterator.SEG_QUADTO)  throw new RuntimeException(\"Can\\'t handle quad to segments.\");  else if (cmd == PathIterator.SEG_CUBICTO)  curveto(points[0], points[1], points[2], points[3], points[4], points[5]);  else if (cmd == PathIterator.SEG_CLOSE)  close();  else  throw new AssertionError(\"Unknown path command \" + cmd);  pi.next();', \"private void injectHandlerPluginConfiguration( MavenProject project, String groupId, String artifactId )  // TODO: use the model injector, or just lookup the versions from the project? // They need to be injected, but we should track the required plugins first, then just sweep through. // TODO: this is a bit of a hack to get the version from plugin management - please fix Plugin plugin = findPlugin( project.getPlugins(), groupId, artifactId ); plugin.setGroupId( groupId ); plugin.setArtifactId( artifactId ); project.addPlugin( plugin );  // TODO: shouldn't have to call all the time modelDefaultsInjector.injectDefaults( project.getModel() ); // TODO: remove - should discover the version plugin = findPlugin( project.getPlugins(), groupId, artifactId ); if ( plugin.getVersion() == null )  plugin.setVersion( PluginDescriptor.getDefaultPluginVersion() );\", 'public MavenProject build( File projectDescriptor, ProjectBuilderConfiguration config ) throws ProjectBuildingException  MavenProject project = readModelFromLocalPath( \"unknown\", projectDescriptor, new DefaultPomArtifactResolver( config.getLocalRepository(), mavenTools.buildArtifactRepositories( getSuperModel() ), artifactResolver ), config ); project.setFile( projectDescriptor ); project = buildWithProfiles( project.getModel(), config, projectDescriptor, project.getParentFile(), true ); Build build = project.getBuild(); // NOTE: setting this script-source root before path translation, because // the plugin tools compose basedir and scriptSourceRoot into a single file. project.addScriptSourceRoot( build.getScriptSourceDirectory() ); project.addCompileSourceRoot( build.getSourceDirectory() ); project.addTestCompileSourceRoot( build.getTestSourceDirectory() ); project.setFile( projectDescriptor ); setBuildOutputDirectoryOnParent( project ); return project;', 'protected void read(SelectionKey selectionKey) throws IOException  int count = 0; long startNs = -1; if(logger.isDebugEnabled()) startNs = System.nanoTime(); if((count = socketChannel.read(inputStream.getBuffer())) == -1) throw new EOFException(\"EOF for \" + socketChannel.socket()); if(logger.isTraceEnabled()) traceInputBufferState(\"Read \" + count + \" bytes\"); if(count == 0) return; // Take note of the position after we read the bytes. We\\'ll need it in // case of incomplete reads later on down the method. final int position = inputStream.getBuffer().position(); // Flip the buffer, set our limit to the current position and then set // the position to 0 in preparation for reading in the RequestHandler. inputStream.getBuffer().flip(); // We have to do this on the first request as we don\\'t know the protocol // yet. if(requestHandler == null)  if(!initRequestHandler(selectionKey))  return;   if(streamRequestHandler!= null)  // We\\'re continuing an existing streaming request from our last pass // through. So handle it and return. handleStreamRequest(selectionKey); return;  if(!requestHandler.isCompleteRequest(inputStream.getBuffer()))  // Ouch - we\\'re missing some data for a full request, so handle that // and return. handleIncompleteRequest(position); return;  // At this point we have the full request (and it\\'s not streaming), so // rewind the buffer for reading and execute the request. inputStream.getBuffer().rewind(); if(logger.isTraceEnabled()) logger.trace(\"Starting execution for \" + socketChannel.socket()); streamRequestHandler =', 'public void executeAsync(final AsyncReadBinding binding, final SingleResultCallbackMapReduceAsyncBatchCursorT>> callback)  withConnection(binding, new AsyncCallableWithConnection()  @Override public void call(final AsyncConnection connection, final Throwable t)  SingleResultCallbackMapReduceAsyncBatchCursorT>> errHandlingCallback = errorHandlingCallback(callback, LOGGER); if (t!= null)  errHandlingCallback.onResult(null, t);  else  final SingleResultCallbackMapReduceAsyncBatchCursorT>> wrappedCallback = releasingCallback( errHandlingCallback, connection); checkValidReadConcern(connection, readConcern, new AsyncCallableWithConnection()  @Override public void call(final AsyncConnection connection, final Throwable t)  if (t!= null)  wrappedCallback.onResult(null, t);  else  executeWrappedCommandProtocolAsync(binding, namespace.getDatabaseName(), getCommand(), CommandResultDocumentCodec.create(decoder, \"results\"), connection, asyncTransformer(connection), wrappedCallback);   );', 'protected abstract boolean isSupportedBy(Index index, ColumnMetadata def); public static class EQRestriction extends MultiColumnRestriction  protected final Term value; public EQRestriction(ListColumnMetadata> columnDefs, Term value)  super(columnDefs); this.value = value;  @Override public boolean isEQ()  return true;  @Override public void addFunctionsTo(ListFunction> functions)  value.addFunctionsTo(functions);  @Override public String toString()  return String.format(\"EQ(%s)\", value);  @Override public SingleRestriction doMergeWith(SingleRestriction otherRestriction)  throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes an Equal\", getColumnsInCommons(otherRestriction));  @Override protected boolean isSupportedBy(Index index, ColumnMetadata column)  return index.supportsExpression(column, Operator.EQ);  @Override public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)  Tuples.Value t = ((Tuples.Value) value.bind(options)); ListByteBuffer> values = t.getElements(); for (int i = 0, m = values.size(); i  m; i++)  builder.addElementToAll(values.get(i)); checkFalse(builder.containsNull(), \"Invalid null value for column %s\", columnDefs.get(i).name);  return builder;  @Override public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)  Tuples.Valu', 'protected Method findMethodWithArgs(int args) throws NoSuchMethodException  Parse methodCell = cells.more; if (isNullOrBlank(methodCell)) throw new FitFailureException(\"You must specify a method.\"); return findMethodWithArgs(camel(methodCell.text()), args);', 'private void checkCorrectVersionsOfScalaLibrary(String scalaDefVersion) throws Exception  getLog().debug(\"Checking for multiple versions of scala\"); //TODO - Make sure we handle bad artifacts.... // TODO: note that filter does not get applied due to MNG-3236 VersionNumber sv = new VersionNumber(scalaDefVersion); VersionNumber requiredScalaVersion = StringUtils.isNotEmpty(scalaCompatVersion)? new VersionNumberMask(scalaCompatVersion) : sv; if (requiredScalaVersion.compareTo(sv)!= 0)  String msg = String.format(\"Scala library detected %s doesn\\'t match scala.compat.version : %s\", sv, requiredScalaVersion); if(failOnMultipleScalaVersions)  getLog().error(msg); throw new MojoFailureException(msg);  getLog().warn(msg);  checkArtifactForScalaVersion(requiredScalaVersion, dependencyTreeBuilder.buildDependencyGraph(project.getProjectBuildingRequest(), null));', 'private static void createContactsViews(SQLiteDatabase db)  db.execSQL(\"DROP VIEW IF EXISTS \" + Views.CONTACTS_ALL + \";\"); db.execSQL(\"DROP VIEW IF EXISTS \" + Views.CONTACTS_RESTRICTED + \";\"); db.execSQL(\"DROP VIEW IF EXISTS \" + Views.DATA_ALL + \";\"); db.execSQL(\"DROP VIEW IF EXISTS \" + Views.DATA_RESTRICTED + \";\"); db.execSQL(\"DROP VIEW IF EXISTS \" + Views.RAW_CONTACTS_ALL + \";\"); db.execSQL(\"DROP VIEW IF EXISTS \" + Views.RAW_CONTACTS_RESTRICTED + \";\"); String dataColumns = Data.IS_PRIMARY + \", \" + Data.IS_SUPER_PRIMARY + \", \" + Data.DATA_VERSION + \", \" + PackagesColumns.PACKAGE + \" AS \" + Data.RES_PACKAGE + \",\" + MimetypesColumns.MIMETYPE + \" AS \" + Data.MIMETYPE + \", \" + Data.DATA1 + \", \" + Data.DATA2 + \", \" + Data.DATA3 + \", \" + Data.DATA4 + \", \" + Data.DATA5 + \", \" + Data.DATA6 + \", \" + Data.DATA7 + \", \" + Data.DATA8 + \", \" + Data.DATA9 + \", \" + Data.DATA10 + \", \" + Data.DATA11 + \", \" + Data.DATA13 + \", \" + Data.DATA14 + \", \" + Data.SYNC1 + \", \" + Data.SYNC2 + \", \" + Data.SYNC2 + \", \" + Data.SYNC2 + \",', 'public void produce( final DataConsumer pReceiver ) throws IOException  String user = Producers.ROOT_NAME; int uid = Producers.ROOT_UID; String group = Producers.ROOT_NAME; int gid = Producers.ROOT_UID; int filemode = TarEntry.DEFAULT_FILE_MODE; int dirmode = TarEntry.DEFAULT_DIR_MODE; String prefix = \"\"; if (fileset instanceof Tar.TarFileSet)  Tar.TarFileSet tarfileset = (Tar.TarFileSet) fileset; user = tarfileset.getUserName(); uid = tarfileset.getUid(); group = tarfileset.getGroup(); gid = tarfileset.getGid(); filemode = tarfileset.getMode(); dirmode = tarfileset.getDirMode(tarfileset.getProject()); prefix = tarfileset.getPrefix(tarfileset.getProject());  final DirectoryScanner scanner = fileset.getDirectoryScanner(fileset.getProject()); scanner.scan(); final File basedir = scanner.getBasedir(); for (String directory : scanner.getIncludedDirectories())  String name = directory.replace(\\'\\', \\'/\\'); pReceiver.onEachDir(prefix + \"/\" + name, null, user, uid, group, gid, dirmode, 0);  for (String filename : scanner.getIncludedFiles())  final String name = filename.replace(\\'\\', \\'/\\'); final File file = new File(basedir, name); final InputStream inputStream = new FileInputStream(file); try  pReceiver.onEachFile(inputStream, prefix + \"/\" +', 'private void setVariables(int row)  SetString> varKeys = vars.keySet(); for (String var : varKeys)  int col = vars.get(var); setVariableExpectation(col, row); ListObject> setInstruction = prepareInstruction(); addCall(setInstruction, \"set\" + \" \" + var); setInstruction.add(table.getCellContents(col, row)); addInstruction(setInstruction);', 'private Map bindLifecycleForPackaging( MavenSession session, String selectedPhase, MavenProject project ) throws ArtifactResolutionException, LifecycleExecutionException  Map mappings; String packaging = project.getPackaging(); try  LifecycleMapping m = (LifecycleMapping) session.lookup( LifecycleMapping.ROLE, packaging ); mappings = m.getPhases();  catch ( ComponentLookupException e )  getLogger().error( \"No lifecycle mapping for type \\'\" + packaging + \"\\': using defaults\" ); mappings = defaultPhases;  Map lifecycleMappings = new HashMap(); boolean finished = false; for ( Iterator i = phases.iterator(); i.hasNext() &&!finished; )  String phase = (String) i.next(); String phaseTasks = (String) mappings.get( phase ); if ( phaseTasks!= null )  for ( StringTokenizer tok = new StringTokenizer( phaseTasks, \",\" ); tok.hasMoreTokens(); )  String goal = tok.nextToken().trim(); MojoDescriptor mojoDescriptor = getMojoDescriptor( goal, session, project ); addToLifecycleMappings( lifecycleMappings, phase, new MojoExecution( mojoDescriptor ), session.getSettings() );   if ( phase.equals( selectedPhase ) )  finished = true;   return lifecycleMappings;', 'public void testEmailFilterSortOrderWithFeedback()  long rawContactId1 = createRawContact(); insertEmail(rawContactId1, \"address1@email.com\"); long rawContactId2 = createRawContact(); insertEmail(rawContactId2, \"address2@email.com\"); long dataId = ContentUris.parseId(insertEmail(rawContactId2, \"address3@email.com\")); ContentValues v1 = new ContentValues(); v1.put(Email.ADDRESS, \"address1@email.com\"); ContentValues v2 = new ContentValues(); v2.put(Email.ADDRESS, \"address2@email.com\"); ContentValues v3 = new ContentValues(); v3.put(Email.ADDRESS, \"address3@email.com\"); Uri filterUri1 = Uri.withAppendedPath(Email.CONTENT_FILTER_URI, \"address\"); Uri filterUri2 = Email.CONTENT_FILTER_URI.buildUpon().appendPath(\"address\").appendQueryParameter(DataUsageFeedback.USAGE_TYPE, DataUsageFeedback.USAGE_TYPE_CALL).build(); Uri filterUri3 = Email.CONTENT_FILTER_URI.buildUpon().appendPath(\"address\").appendQueryParameter(DataUsageFeedback.USAGE_TYPE, DataUsageFeedback.USAGE_TYPE_LONG_TEXT).build(); Uri filterUri4 = Email.CONTENT_FILTER_URI.buildUpon().appendPath(\"address\").appendQueryParameter(DataUsageFeedback.USAGE_TY', 'public void testPhoneUpdate()  ContentValues values = new ContentValues(); Uri rawContactUri = mResolver.insert(RawContacts.CONTENT_URI, values); long rawContactId = ContentUris.parseId(rawContactUri); insertStructuredName(rawContactId, \"Hot\", \"Tamale\"); Uri phoneUri = insertPhoneNumber(rawContactId, \"18004664411\"); Uri lookupUri1 = Uri.withAppendedPath(PhoneLookup.CONTENT_FILTER_URI, \"8004664411\"); assertStoredValue(lookupUri1, PhoneLookup.DISPLAY_NAME, \"Hot Tamale\"); values.clear(); values.put(Phone.NUMBER, \"18004664422\"); mResolver.update(phoneUri, values, null, null); Uri lookupUri2 = Uri.withAppendedPath(PhoneLookup.CONTENT_FILTER_URI, \"8004664422\"); assertStoredValue(lookupUri2, PhoneLookup.DISPLAY_NAME, \"Hot Tamale\"); // Setting number to null will remove the phone lookup record values.clear(); values.putNull(Phone.NUMBER); mResolver.update(phoneUri, values, null, null); assertEquals(0, getCount(lookupUri2, null, null)); // Let\\'s restore that phone lookup record values.clear(); values.put(Phone.NUMBER, \"18004664422\"); mResolver.update(phoneUri, values, null, null); assertStoredValue(lookupUri2, PhoneLookup.DISPLAY_NAME, \"Hot Tamale\"); assertNetworkNotified(true);', 'private ListT> fixAndFilterDocumentsFromSystemNamespace(final ListBsonDocument> unstripped)  if (unstripped == null)  return null;  ListT> stripped = new ArrayListT>(unstripped.size()); String prefix = databaseName + \".\"; for (BsonDocument cur : unstripped)  String name = cur.getString(\"name\").getValue(); if (name.startsWith(prefix))  String collectionName = name.substring(prefix.length()); if (!collectionName.contains(\"$\"))  cur.put(\"name\", new BsonString(collectionName)); stripped.add(decoder.decode(new BsonDocumentReader(cur), DecoderContext.builder().build()));    return stripped;', 'public void shouldNotAcceptStringReturn() throws Exception  Method method = FakeResource.class.getMethod(\"string\"); ResourceMethod recursive = new DownloadInterceptor(null, null, null); Assert.assertFalse(recursive); accepts(recursive);', 'public String getTrace()  StringWriter stringWriter = new StringWriter(); PrintWriter writer = new PrintWriter(stringWriter); getException().printStackTrace(writer); StringBuffer buffer = stringWriter.getBuffer(); return buffer.toString();', 'public T> T getConfiguredMojo(ClassT> mojoInterface, MavenSession session, MojoExecution mojoExecution) throws PluginConfigurationException, PluginContainerException  MojoDescriptor mojoDescriptor = mojoExecution.getMojoDescriptor(); PluginDescriptor pluginDescriptor = mojoDescriptor.getPluginDescriptor(); ClassRealm pluginRealm = pluginDescriptor.getClassRealm(); if (pluginRealm == null)  try  setupPluginRealm(pluginDescriptor, session, null, null, null);  catch (PluginResolutionException e)  String msg = \"Cannot setup plugin realm [mojoDescriptor=\" + mojoDescriptor.getId() + \", pluginDescriptor=\" + pluginDescriptor.getId() + \"]\"; throw new PluginConfigurationException(pluginDescriptor, msg, e);  pluginRealm = pluginDescriptor.getClassRealm();  if (logger.isDebugEnabled())  logger.debug(\"Loading mojo \" + mojoDescriptor.getId() + \" from plugin realm \" + pluginRealm);  // We are forcing the use of the plugin realm for all lookups that occur in contextualize calls in line with the right realm. ClassRealm oldLookupRealm = container.setLookupRealm(pluginRealm); ClassLoader oldClassLoader = Thread.currentThread().getContextClassLoader(); Thread.currentThread().setContextClassLoader(pluginRealm); try  T mojo; try  mojo = container.lookup(mojoInterface, mojoDescriptor.getRoleHint()', 'public String getMechanismName()  String token = getCredential(); String token = getAuthenticationMechanism(); return token.getMechanismName();', 'private Node processTag(ParserAutomatonState automatonState, Node currentNode, boolean tagNameIsCode)  if (automatonState.isCode() &&!tagNameIsCode)  currentNode = pushTextNode(automatonState, currentNode, automatonState.getWholematch());  else if (tagNameIsCode)  automatonState.setCode(true); automatonState.setFirstCode(true); currentNode = pushTagNode(automatonState, currentNode, automatonState.getTagname(), automatonState.getParameter());  else  if (\"url\".equals(automatonState.getTagname()) &&! StringUtils.isEmpty(automatonState.getParameter()))  //? °»12°? €342€o°  »? [url]? °€°14€3414 currentNode = pushTagNode(automatonState, currentNode, \"url2\", automatonState.getParameter());  else  currentNode = pushTagNode(automatonState, currentNode, automatonState.getTagname(), automatonState.getParameter());   return currentNode;', 'private int updateAggregationException(SQLiteDatabase db, ContentValues values)  int exceptionType = values.getAsInteger(AggregationExceptions.TYPE); long contactId = values.getAsInteger(AggregationExceptions.CONTACT_ID); long rawContactId = values.getAsInteger(AggregationExceptions.RAW_CONTACT_ID); // First, we build a list of rawContactID-rawContactID pairs for the given contact. ArrayListRawContactPair>(); Cursor c = db.query(ContactsQuery.TABLE, ContactsQuery.PROJECTION, RawContacts.CONTACT_ID + \"=\" + contactId, null, null, null, null); try  while (c.moveToNext())  long aggregatedContactId = c.getLong(ContactsQuery.RAW_CONTACT_ID); if (aggregatedContactId!= rawContactId)  pairs.add(new RawContactPair(aggregatedContactId, rawContactId));    finally  c.close();  // Now we iterate through all contact pairs to see if we need to insert/delete/update // the corresponding exception ContentValues exceptionValues = new ContentValues(3); exceptionValues.put(AggregationExceptions.TYPE, exceptionType); for (RawContactPair pair : pairs)  final String whereClause = AggregationExceptionColumns.RAW_CONTACT_ID1 + \"=\" + pair.rawContactId1 + \" AND \" + AggregationExceptionColumns.RAW_CONTACT_ID2 + \"=\" + pair.rawContactId2; if (exceptionType == AggregationExceptions.TYPE_AUTOMATIC)  db.delete(Tables.', 'public DependencyCollectorResult collect(@Nonnull DependencyCollectorRequest request) throws DependencyCollectorException, IllegalArgumentException  nonNull(request, \"request\"); InternalSession session = InternalSession.from(request.getSession()); Artifact rootArtifact = request.getRootArtifact().orElse(null); Dependency root = request.getRoot().orElse(null); DependencyCollector root = request.getRoot().map(d -> session.toDependency(d, false)).orElse(null); CollectRequest collectRequest = new CollectRequest().setRootArtifact(rootArtifact).setRoot(root).setDependencies(session.toDependencies(request.getDependencies(), false)).setManagedDependencies(session.toDependencies(managedDependencies, true)).setRepositories(session.toRepositories(session.getRemoteRepositories())); RepositorySystemSession systemSession = session.getSession(); if (request.getVerbose())  systemSession = new DefaultRepositorySystemSession(systemSession).setConfigProperty(ConflictResolver.CONFIG_PROP_VERBOSE, true).setConfigProperty(DependencyManagerUtils.CONFIG_PROP_VERBOSE, true);  try  final CollectResult result = session.getRepositorySystem().collectDependencies(systemSession, collectRequest); return new DependencyCollectorResult()  @Override public ListException> getExceptions()  return result.getExceptions();  @Override public Node getRoot()  return session.getNode(result.getRoot', 'private int writeObject(TransactionTimestamp timestamp) throws IOException  synchronized (_writeLock)  if (_closed)  throw new IOException(\"already closed\");  try  ByteArrayOutputStream bytes = new ByteArrayOutputStream(); _serializer.writeObject(bytes, timestamp); Chunking.writeChunk(_active, new Chunk(bytes.toByteArray()));  catch (IOException exception)  internalClose(); throw exception;  _objectsWritten++; return _objectsWritten;', 'public Response makeResponse(FitNesseContext context, Request request)  String resource = request.getResource(); String tags = \"\"; if(context.getRootPage()!= null) WikiPagePath path = PathParser.parse(resource); WikiPage wikiPage = context.getRootPage().getPageCrawler().getPage(path); if(wikiPage!= null)  PageData pageData = wikiPage.getData(); tags = pageData.getAttribute(PageData.PropertySUITES);   HtmlPage page = context.pageFactory.newPage(); page.setMainTemplate(\"refactorForm\"); page.setTitle(\"Refactor: \" + resource); page.setPageTitle(new PageTitle(\"Refactor\", PathParser.parse(resource), tags)); page.put(\"refactoredRootPage\", resource); page.put(\"request\", request); page.put(\"type\", request.getInput(\"type\")); page.put(\"viewLocation\", request.getResource()); page.put(\"suiteMap\", AutoCompleteUtil.createSuiteList(context)); SimpleResponse response = new SimpleResponse(); response.setContent(page.html()); return response;', 'protected void paint(PPaintContext paintContext)  Graphics2D g2 = paintContext.getGraphics(); g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON); if (getNode() == null) return; Object outputValue = getNode().getOutputValue(); // Draw the canvas bounds if (outputValue instanceof Canvas)  Canvas c = (Canvas) outputValue; Rectangle2D canvasBounds = c.getBounds().getRectangle2D(); g2.setColor(Color.DARK_GRAY); g2.setStroke(new BasicStroke(1f)); g2.draw(canvasBounds);  if (outputValue instanceof Grob)  Shape oldClip = g2.getClip(); ((Grob) outputValue).draw(g2); g2.setClip(oldClip);  else if (outputValue!= null)  String s = outputValue.toString(); g2.setColor(Theme.TEXT_NORMAL_COLOR); g2.setFont(Theme.EDITOR_FONT); g2.drawString(s, 5, 20);  // Draw the handle. if (hasVisibleHandle())  // Create a canvas with a transparent background nodebox.graphics.Canvas canvas = new nodebox.graphics.Canvas(); canvas.setBackground(new nodebox.graphics.Color(0, 0, 0, 0)); CanvasContext ctx = new CanvasContext(canvas); handle.draw(ctx); ctx.getCanvas().draw(g2);  // Draw the points. if (showPoints && outputValue instanceof IGeometry)  //', 'public boolean onLongClick(View view)  CharSequence title = null; String mime = null; String query = null; CharSequence artist = mArtistName.getText(); CharSequence album = mAlbumName.getText(); CharSequence song = mTrackName.getText(); if (view.equals(mArtistName.getParent()))  title = artist; query = artist.toString(); mime = MediaStore.Audio.Artists.ENTRY_CONTENT_TYPE;  else if (view.equals(mAlbumName.getParent()))  title = album; query = artist + \" \" + album; mime = MediaStore.Audio.Albums.ENTRY_CONTENT_TYPE;  else if (view.equals(mTrackName.getParent()))  title = song; query = artist + \" \" + song; mime = \"audio/*\"; // the specific type doesn\\'t matter, so don\\'t bother retrieving it  else  throw new RuntimeException(\"shouldn\\'t be here\");  title = getString(R.string.mediasearch, title); Intent i = new Intent(); i.setAction(MediaStore.INTENT_ACTION_MEDIA_SEARCH); i.putExtra(SearchManager.QUERY, query); i.putExtra(MediaStore.EXTRA_MEDIA_ARTIST, artist); i.putExtra(MediaStore.EXTRA_MEDIA_ALBUM, album); i.putExtra(MediaStore.EXTRA_MEDIA_TITLE, song); i.putExtra(MediaStore.EXTRA_MEDIA_FOCUS, mime); startActivity(Intent.createChooser(i, title)); return true;', 'private Element handleBar( boolean newLine ) throws IOException  Element el = null; if(!m_istable &&!newLine )  return null;  if( newLine )  if(!m_istable )  startBlockLevel(); el = pushElement( new Element(\"table\").setAttribute(\"class\",\"wikitable\").setAttribute(\"border\",\"1\") ); m_istable = true;  el = pushElement( new Element(\"tr\") ); // m_closeTag = m_renderer.closeTableItem()+m_renderer.closeTableRow();  int ch = nextToken(); if( ch == \\'|\\' )  if(!newLine )  el = popElement(\"th\");  el = pushElement( new Element(\"th\") );  else  if(!newLine )  el = popElement(\"td\");  el = pushElement( new Element(\"td\") ); pushBack( ch );  return el;', 'public final void testLogout()  TestHttpServletRequest request = new TestHttpServletRequest(); request.setUserPrincipal( new WikiPrincipal( \"Andrew Jaquith\" ) ); try  CallbackHandler handler = new WebContainerCallbackHandler( request, db ); LoginContext context = new LoginContext( \"JSPWiki-container\", subject, handler ); context.login(); Set principals = subject.getPrincipals(); assertEquals( 3, principals.size() ); assertTrue( principals.contains( new WikiPrincipal( \"Andrew Jaquith\" ) ) ); assertTrue( principals.contains( Role.AUTHENTICATED ) ); assertTrue( principals.contains( Role.ALL ) ); context.logout(); assertEquals( 0, principals.size() );  catch( LoginException e )  System.err.println( e.getMessage() ); assertTrue( false );', 'BsonDocument getInsertManyResult(final BsonDocument arguments)  ListBsonDocument> documents = new ArrayListBsonDocument>(); for (BsonValue document : arguments.getArray(\"documents\"))  documents.add(document.asDocument());  BsonBooleanBooleanBooleanBooleanBooleanBoolean = arguments.getBoolean(\"ordered\", BsonBoolean.TRUE).getValue()); return toResult((BsonValue) null);', 'protected long createGroup(Account account, String sourceId, String title)  String accountId = accountId.getAccountId(); return createGroup(account, sourceId, title, 1);', 'public void connectionRemoved(final ConnectionEvent event)  ConnectionStatistics statistics = getStatistics(event); if (statistics!= null)  statistics.connectionRemoved(event);', 'private String contentsWithPages(String name1, String name2, String nested)  return \"div class=\"toc1\">\" + HtmlElement.endl + \"tdiv class=\"contents\">\" + HtmlElement.endl + \"ttb>Contents:/b>\" + HtmlElement.endl + \"ttul>\" + HtmlElement.endl + \"ttttta href=\"PageOne.PageThree\">\" + name1 + \"/a>\" + HtmlElement.endl + \"ttt/li>\" + HtmlElement.endl + \"ttt/li>\" + HtmlElement.endl + \"tttta href=\"PageOne.PageTwo\">\" + name2 + \"/a>\" + HtmlElement.endl + \"ttttta href=\"PageOne.PageTwo\">\" + name2 + \"/a>\" + HtmlElement.endl + \"ttt/li>\" + HtmlElement.endl + \"tt/ul>\" + HtmlElement.endl + \"tt/ul>\" + HtmlElement.endl + \"tt/div>\" + HtmlElement.endl;', 'public void evaluate() throws Throwable  FutureTaskThrowable> task = new FutureTaskThrowable>(new CallableStatement()); threadGroup = new ThreadGroup(\"FailOnTimeoutGroup\"); Thread thread = new Thread(threadGroup, task, \"Time-limited test\"); thread.setDaemon(true); thread.start(); Throwable throwable = getResult(task, thread); if (throwable!= null)  throw throwable;', 'public void create(final String collectionName, final CreateCollectionOptions options)  drop(namespace); new CreateCollectionOperation(namespace.getDatabaseName(), collectionName).capped(options.isCapped()).sizeInBytes(options.getSizeInBytes()).autoIndex(options.isAutoIndex()).maxDocuments(options.getMaxDocuments()).usePowerOf2Sizes(options.isUsePowerOf2Sizes()).execute(getBinding());', 'public void shouldCallCloseOnClosableTestSystemListener() throws IOException, InterruptedException  WikiPage testPage = addTestPage(suite, \"TestPage1\", \"!define TEST_SYSTEM A\"); ClosableTestSystemListener listener = mock(ClosableTestSystemListener.class); ListTestSystem> runner = new MultipleTestsRunner(asList(testPage), context, testingTracker, testSystemFactory); runner.addTestSystemListener(listener); runner.executeTestPages(); verify(listener).close();', 'public void createPoll(ListString> pollList, boolean multiSelect, int msgid)  final int voteid = getNextPollId(); jdbcTemplate.update(\"INSERT INTO votenames (id, multiselect, topic) values (?,?,?)\", voteid, multiSelect, msgid); try  final Poll poll = getPoll(voteid); for (String variant : pollList)  if (variant.trim().length() == 0)  continue;  addNewVariant(poll, variant);   catch (PollNotFoundException e)  throw new RuntimeException(e);', 'private void findComponentsFromWebInfClasses(MapString, SetString>> index, SetString> stereotypeNames, SetString> results)  for (String stereotype : stereotypeNames)  String result = index.get(stereotype); results.addAll( result);', 'public void executeMojo( MavenSession session, MojoExecution mojoExecution ) throws MojoFailureException, MojoExecutionException, PluginConfigurationException, PluginManagerException  MavenProject project = session.getCurrentProject(); MojoDescriptor mojoDescriptor = mojoExecution.getMojoDescriptor(); Mojo mojo = null; ClassRealm pluginRealm = getPluginRealm( session, mojoDescriptor.getPluginDescriptor() ); ClassRealm oldLookupRealm = container.getLookupRealm(); ClassLoader oldClassLoader = Thread.currentThread().getContextClassLoader(); MavenSession oldSession = legacySupport.getSession(); try  mojo = mavenPluginManager.getConfiguredMojo( Mojo.class, session, mojoExecution ); Thread.currentThread().setContextClassLoader( pluginRealm ); legacySupport.setSession( session ); // NOTE: DuplicateArtifactAttachmentException is currently unchecked, so be careful removing this try/catch! // This is necessary to avoid creating compatibility problems for existing plugins that use // MavenProjectHelper.attachArtifact(..). try  mojo.execute();  catch ( ClassCastException e )  // to be processed in the outer catch block throw e;  catch ( RuntimeException e )  throw new PluginExecutionException( mojoExecution, project, e );   catch ( PluginManagerException e )  throw new PluginExecutionException( mojoExecution, project, e );  catch ( LinkageError e )  ByteArrayOutputStream os = new ByteArrayOutputStream( 1024 ); PrintStream ps = new Print', 'public boolean isFiltersAppended ()  boolean isDoingIt = false; try  isDoingIt = \"true\".equals(parent.getVariable(FILTER_TOC));  catch (Exception e)  isDoingIt = false;  return isDoingIt || isFiltered;', 'public void actionPerformed( WikiEvent event )  if(!(event instanceof WikiPageEvent) )  return;  WikiPath path = ((WikiPageEvent) event).getPath(); if(!isWikiPage( path ) )  return;  try  switch( event.getType() )  // =============================================================================================================== // If page was saved, update all references case (ContentEvent.NODE_SAVED ):  path = resolvePage( path ); // Get new linked pages, and set refersTo/referencedBy links ListWikiPath> referenced = extractLinks( path ); setLinks( path, referenced ); m_cm.getCurrentSession().save(); break;  // ==============================================================================================================================================================================================', 'protected boolean delete(final ByteArray key, final Version version, long deleteOpTimeout) throws VoldemortException  StoreUtils.assertValidKey(key); long startTimeMs = -1; long startTimeNs = -1; if(logger.isDebugEnabled())  startTimeMs = System.currentTimeMillis(); startTimeNs = System.nanoTime();  BasicPipelineDataBoolean> pipelineData = new BasicPipelineDataBoolean>(); if(zoneRoutingEnabled) pipelineData.setZonesRequired(storeDef.getZoneCountWrites()); else pipelineData.setZonesRequired(null); pipelineData.setStoreName(getName()); pipelineData.setStats(stats); Pipeline pipeline = new Pipeline(Operation.DELETE, deleteOpTimeout, TimeUnit.MILLISECONDS); pipeline.setEnableHintedHandoff(isHintedHandoffEnabled()); HintedHandoff hintedHandoff = null; if(isHintedHandoffEnabled()) hintedHandoff = new HintedHandoff(failureDetector, slopStores, nonblockingSlopStores, handoffStrategy, pipelineData.getFailedNodes(), deleteOpTimeout); pipeline.addEventAction(Event.STARTED, new ConfigureNodesBoolean, BasicPipelineDataBoolean>>(pipelineData, Event.CONFIGURED, failureDetector, storeDef.getRequiredWrites(), routingStrategy, key, clientZone)); pipeline.addEventAction(Event.CONFIGURED, new PerformParallelDeleteRequestsBoolean, BasicPipelineDataBoolean>>(pipelineData, isHintedHandoffEnabled()? Event.RESPONSES_REC', 'public Artifact relocatedTarget(RepositorySystemSession session, ArtifactDescriptorRequest request, Model model)  Relocations relocations = (Relocations) session.getData().computeIfAbsent(getClass().getName() + \".relocations\", () -> parseRelocations(session)); if (relocations!= null)  Relocation relocation = relocations.getRelocation(request.getArtifact()); if (relocation!= null && (isProjectContext(request.getRequestContext()) || relocation.global))  Artifact result = new RelocatedArtifact( request.getArtifact(), isAny(relocation.target.getGroupId())? null : relocation.target.getGroupId(), isAny(relocation.target.getArtifactId())? null : relocation.target.getArtifactId(), isAny(relocation.target.getClassifier())? null : relocation.target.getClassifier(), isAny(relocation.target.getExtension())? null : relocation.target.getExtension(), isAny(relocation.target.getVersion())? null : relocation.target.getVersion(), relocation.global? \"User global relocation\" : \"User project relocation\"); LOGGER.debug( \"The artifact  has been relocated to : \", request.getArtifact(), result, relocation.global? \"User global relocation\" : \"User global relocation\"); return result;   return null;', 'public static String convertToDebianVersion( String version, Date timestamp )  Pattern pattern1 = Pattern.compile(\"(.*)[-+]SNAPSHOT\"); Matcher matcher = pattern1.matcher(version); if (matcher.matches())  version = matcher.group(1) + \"\"; if (timestamp!= null)  version += new SimpleDateFormat(\"yyyyMMddHHmmss\").format(timestamp);  else  version += \"SNAPSHOT\";   Pattern pattern2 = Pattern.compile(\"(.*?)([.-_]?)(alpha|beta|rc)(.*)\", Pattern.CASE_INSENSITIVE); matcher = pattern2.matcher(version); if (matcher.matches())  version = matcher.group(1) + \"\" + matcher.group(3) + matcher.group(4);  version = version.replace(\\'-\\', \\'+\\'); return version;', 'public ListVersionedbyte[]>> get(final ByteArray key)  StoreUtils.assertValidKey(key); BasicPipelineDataListVersionedbyte[]>>> pipelineData = new BasicPipelineDataListVersionedbyte[]>>>(); final Pipeline pipeline = new Pipeline(Operation.GET); NonblockingStoreRequest nonblockingStoreRequest = new NonblockingStoreRequest()  public void request(Node node, NonblockingStore store)  NonblockingStoreCallback callback = new BasicResponseCallbackByteArray>(pipeline, node, key); store.submitGetRequest(key, callback);  ; BlockingStoreRequestListVersionedbyte[]>>> blockingStoreRequest = new BlockingStoreRequestListVersionedbyte[]>>>()  public ListVersionedbyte[]>> request(Node node, StoreByteArray, byte[]> store)  return store.get(key);  ; Action configureNodes = new ConfigureNodesListVersionedbyte[]>>, BasicPipelineDataListVersionedbyte[]>>>(pipelineData, Event.CONFIGURED, failureDetector, storeDef.getRequiredReads(), routingStrategy, key); Action performRequests = new PerformParallelRequestsListVersionedbyte[]>>, BasicPipelineDataListVersionedbyte[]>>>(pipelineData, Event.COMPLETED, storeDef.getPreferredReads(), nonblockingStores, nonblockingStoreRequest); Action acknowledgeResponse = new AcknowledgeResponseListVersionedbyte[]>>, BasicPipelineDataListVersionedbyte[]>>>(pipelineData,', 'public Plugin getPluginDefinitionForPrefix( String prefix, MavenSession session, MavenProject project ) throws PluginManagerException  // TODO: since this is only used in the lifecycle executor, maybe it should be moved there? There is no other // use for the mapping manager in here return pluginMappingManager.getByPrefix( prefix, session.getSettings().getPluginGroups(), project.getPluginArtifactRepositories(), session.getLocalRepository() );', 'public void testResolveTree() throws IOException  ArtifactMavenArtifact mad = MercuryAdaptor.toMavenArtifactMetadata( new ArtifactMetadata( \"asm:asm-xml:3.0\" ) ); MetadataResolutionRequest request = new MetadataResolutionRequest(); request.setLocalRepository( _localRepo ); request.setRemoteRepostories( _remoteRepos ); request.setArtifactMetadata( mad ); request.setAsResolvedTree( true ); request.setScope( \"compile\" ); MetadataResolutionResult res = _mrs.resolveMetadata( request ); assertNotNull( res ); MetadataGraph resGraph = res.getResolvedTree(); assertNotNull( resGraph ); assertNotNull( resGraph ); assertNotNull( resGraph.getNodes() ); assertEquals( 4, resGraph.getNodes().size() ); // // assertTrue( checkExists( as, \"asm:asm:asm-util:3.0\" ) ); // // assertTrue( checkExists( as, \"asm:asm:asm-tree:3.0\" ) ); // // assertTrue( checkExists( as, \"asm:asm:asm:3.0\" ) ); // // assertFalse( checkExists( as, \"asm:asm-parent:3.0\" ) ); // // for( Artifact a : as ) //  // assertTrue( a.getFile().exists() ); // // System.out.println( a.getFile().getCanonicalPath()+ \" : \"+ a.getFile().length()+\" bytes\");', 'public boolean isInputConnectedTo(Node outputNode)  // Check parameters for upstream connections. for (Parameter p : parameters.values())  if (p.isConnectedTo(outputNode)) return true;  return false;', 'public void invokeSwap(final String storeName, final MapNode, Response> fetchResponseMap)  // do swap MapNode, String> previousDirs = Maps.newHashMap(); HashMapNode, Exception> exceptions = Maps.newHashMap(); for(Map.EntryNode, Response> entry: fetchResponseMap.entrySet())  Response response = entry.getValue(); if (response.isSuccessful())  Node node = entry.getKey(); Integer nodeId = node.getId(); String dir = response.getResponse(); try  logger.info(\"Attempting swap for \" + node.briefToString() + \", dir = \" + dir); previousDirs.put(node, adminClient.readonlyOps.swapStore(nodeId, storeName, dir)); logger.info(\"Swap succeeded for \" + node.briefToString());  catch(Exception e)  exceptions.put(node, e);    if(!exceptions.isEmpty())  if(rollbackFailedSwap)  // Rollback data on successful nodes for(Node node: previousDirs.keySet())  try  int successfulNodeId = node.getId(); logger.info(\"Rolling back data on successful \" + node.briefToString()); adminClient.readonlyOps.rollbackStore(successfulNodeId, storeName, ReadOnlyUtils.getVersionId(new File(previousDirs.get(node)))); logger.info(\"Rollback succeeded for \" + node.briefToString());  catch(Exception e)  logger.error(\"Exception thrown during rollback ( after swap', 'private long insertRawContact(Uri uri, ContentValues values)  mValues.clear(); mValues.putAll(values); mValues.putNull(RawContacts.CONTACT_ID); if (!resolveAccount(uri, mValues))  return -1;  if (values.containsKey(RawContacts.DELETED) && values.getAsInteger(RawContacts.DELETED)!= 0)  mValues.put(RawContacts.AGGREGATION_MODE, RawContacts.AGGREGATION_MODE_DISABLED);  long rawContactId = mDb.insert(Tables.RAW_CONTACTS, RawContacts.CONTACT_ID, mValues); mContactAggregator.markNewForAggregation(rawContactId); // Trigger creation a Contact based on this RawContact at the end of transaction mInsertedRawContacts.put(rawContactId, mAccount); return rawContactId;', 'private static void writeKeysAscii(IteratorByteArray> keyIterator, File outputFile, StoreDefinition storeDefinition) throws IOException  BufferedWriter writer = null; CompressionStrategy keysCompressionStrategy = null; FileWriter fileWriter = null; if(outputFile!= null)  writer = new BufferedWriter(new FileWriter(outputFile));  else  writer = new BufferedWriter(new OutputStreamWriter(System.out));  SerializerDefinition serializerDef = storeDefinition.getKeySerializer(); if(null!= serializerDef && serializerDef.hasCompression())  keysCompressionStrategy = new CompressionStrategyFactory().get(serializerDef.getCompression());  SerializerFactory serializerFactory = new DefaultSerializerFactory(); StringWriter stringWriter = new StringWriter(); JsonGenerator generator = new JsonFactory(new ObjectMapper()).createJsonGenerator(stringWriter); @SuppressWarnings(\"unchecked\") SerializerObject> serializer = (SerializerObject>) serializerFactory.getSerializer(storeDefinition.getKeySerializer()); try  while(keyIterator.hasNext())  // Ugly hack to be able to separate text by newlines vs. spaces byte[] keyBytes = keyIterator.next().get(); Object keyObject = serializer.toObject((null == keysCompressionStrategy)? keyBytes : keysCompressionStrategy.inflate(keyBytes)); generator.writeObject(keyObject); StringBuffer buf = stringWriter.getBuffer(); if(buf.charAt(0) ==\\'\\')', 'public String render() throws Exception  if(WikiWordWidget.isWikiWord(href))  WikiWordWidget www = new WikiWordWidget(new BlankParentWidget(this, \"\"), href); String theWord = www.getWikiWord(); WikiPagePath wikiWordPath = PathParser.parse(theWord); WikiPagePath fullPathOfWikiWord = parentPage.getPageCrawler().getFullPathOfChild(parentPage, wikiWordPath); String qualifiedName = PathParser.render(fullPathOfWikiWord); if(parentPage.getPageCrawler().pageExists(parentPage, PathParser.parse(theWord))) return (\"a href=\"\" + qualifiedName + \"\">\" + childHtml() + \"/a>\"); else if(getWikiPage() instanceof ProxyPage) return makeAliasLinkToNonExistentRemotePage(theWord); else return (childHtml() + \"a href=\"\" + qualifiedName + \"?edit\">?/a>\");  else return (\"a href=\"\" + href + \"\">\" + childHtml() + \"/a>\");', 'public S> void set(final S value, final PropertyModelS> propertyModel)  if (newInstance!= null)  propertyModel.getPropertyAccessor().set(newInstance, value);  else  if (!properties.isEmpty())  Integer index = properties.get(propertyModel.getName()); if (index!= null)  params[index] = value;  properties.remove(propertyModel.getName());  if (properties.isEmpty())  constructInstanceAndProcessCachedValues();  else  cachedValues.put(propertyModel, value);', 'private Node pushTextNode(ParserAutomatonState automatonState, Node currentNode, String text)  if (text.trim().isEmpty())  return currentNode;  while (!currentNode.allows(\"text\"))  if (currentNode.allows(\"p\"))  TagNode node = new TagNode(currentNode, parserParameters, \"p\", \"\", automatonState.getRootNode()); currentNode.addChildren(node); currentNode = node;  else if (currentNode.allows(\"div\"))  TagNode node = new TagNode(currentNode, parserParameters, \"div\", \"\", automatonState.getRootNode()); currentNode.addChildren(node); currentNode = node;  else  currentNode = currentNode.getParent();   boolean isParagraph = false; boolean isAllow = true; boolean isParagraphed = false; if (TagNode.class.isInstance(currentNode))  TagNode tempNode = (TagNode) currentNode; SetString> disallowedParagraphTags = parserParameters.getDisallowedParagraphTags(); SetString> paragraphedTags = parserParameters.getParagraphedTags(); if (disallowedParagraphTags.contains(tempNode.getBbtag().getName()))  isAllow = false;  if (paragraphedTags.contains(tempNode.getBbtag().getName()))  isParagraphed = true;  if (\"p\".equals(tempNode.getBbtag().getName()))  isParagraph = true;   /** *', 'private String getMonthNaviLink( final Calendar day, final String txt, String queryString )  final String result; queryString = TextUtil.replaceEntities( queryString ); final Calendar nextMonth = Calendar.getInstance(); nextMonth.set( Calendar.DATE, 1 ); nextMonth.add( Calendar.DATE, -1); nextMonth.add( Calendar.MONTH, 1 ); // Now move to 1st day of next month if ( day.before( nextMonth ) )  final Page thePage = m_wikiContext.getPage(); final String pageName = thePage.getName(); final String calendarDate = m_dateFormat.format( day.getTime() ); String url = m_wikiContext.getURL( ContextEnum.PAGE_VIEW.getRequestContext(), pageName,\"calendar.date=\"+calendarDate ); if( queryString!= null && queryString.length() > 0 )  // // Ensure that the \\'calendar.date=ddMMyy\\' has been removed from the queryString // // FIXME: Might be useful to have an entire library of // routines for this. Will fail if it\\'s not calendar.date // but something else. final int pos1 = queryString.indexOf(\"calendar.date=\"); if( pos1 >= 0 )  String tmp = queryString.substring( 0, pos1 ); // FIXME: Will this fail when we use & instead of &amp? // FIXME: should use some parsing routine final int pos2 = queryString.indexOf(\"&\", pos1 ) + 1; if ( ( pos2 > 0 ) && ( pos2  queryString.length() ) )  tmp = tmp + queryString.substring(pos2);  queryString = tmp;  if( queryString.', 'private File getFileForMediaStoreDownload(String docId)  final long token = Binder.clearCallingIdentity(); try  String filePath = null; try (Cursor cursor = getContext().getContentResolver().query( getMediaStoreUri(docId), null, null, null))  if (cursor.moveToNext())  File filePath = cursor.getString(cursor.getColumnIndex(FileColumns.DATA));   if (filePath == null)  throw new IllegalStateException(\"Filepath could not be found for\" + \" mediastore docId: \" + docId);  return new File(filePath);  finally  Binder.restoreCallingIdentity(token);', 'public void testSetSendToVoicemailAndRingtone()  long rawContactId = RawContactUtil.createRawContactWithName(mResolver); long contactId = queryContactId(rawContactId); updateSendToVoicemailAndRingtone(contactId, true, \"foo\"); long contactId = queryContactId(rawContactId); updateSendToVoicemailAndRingtone(contactId, true, \"foo\"); assertNetworkNotified(true); assertDirty(ContentUris.withAppendedId(RawContacts.CONTENT_URI, rawContactId), true); updateSendToVoicemailAndRingtoneWithSelection(contactId, false, \"bar\"); assertSendToVoicemailAndRingtone(contactId, false, \"bar\"); assertNetworkNotified(true); assertDirty(ContentUris.withAppendedId(RawContacts.CONTENT_URI, rawContactId), true);', 'public void testComplete(TestPage test, TestSummary testSummary)  increaseCompletedTests(); if (firstFailure!= null)  notifier.fireTestFailure(new Failure(descriptionFor(test), firstFailure));  else if (testSummary.getExceptions() > 0)  notifier.fireTestFailure(new Failure(descriptionFor(test), new Exception(\"Exception occurred on page \" + test.getFullPath())));  else if (testSummary.getWrong() > 0)  notifier.fireTestFailure(new Failure(descriptionFor(test), new AssertionError(\"Test failures occurred on page \" + test.getFullPath())));  fireTestFinishedFor(test);', 'public byte[] loadFromFile(InputStream is) throws IOException, InvalidDBVersionException  MessageDigest md; try  md = MessageDigest.getInstance(\"SHA-256\");  catch (NoSuchAlgorithmException e)  throw new IOException(\"No SHA-256 implementation\");  DigestInputStream dis = new DigestInputStream(is, md); LEDataInputStream lis = new LEDataInputStream(dis); int sig1 = lis.readInt(); int sig2 = lis.readInt(); if (! matchesHeader(sig1, sig2) )  throw new InvalidDBVersionException();  long version = lis.readUInt(); if (! validVersion(version) )  throw new InvalidDBVersionException();  boolean done = false; while (! done )  done = readHeaderField(lis);  return md.digest();', 'public void testActivatingProperSearch() throws Exception  TestableSearcher searcher = new TestableSearcher(); responder.setSearcher(searcher); responder.setRequest(request); request.addInput(\"searchType\", \"something with the word title in it\"); new MockResponseSender(responder.makeResponse(new FitNesseContext(root), request)); assertTrue(searcher.titleSearchCalled); request.addInput(\"searchType\", \"something with the word content in it\"); new MockResponseSender(responder.makeResponse(new FitNesseContext(root), request)); assertTrue(searcher.contentSearchCalled);', \"public Cursor loadInBackground()  // Get the id of the contact we're looking at. final Cursor cursor = getContext().getContentResolver().query(mContactUri, new String[]  Contacts._ID, Contacts.IS_USER_PROFILE, null, null, null); if (cursor == null)  return null;  if (cursor.getCount()  1)  cursor.close(); return null;  cursor.moveToFirst(); final long contactId = cursor.getLong(/* Contacts._ID */ 0); mIsUserProfile = cursor.getInt(/* Contacts.IS_USER_PROFILE */ 1) == 1; cursor.close(); // Update selection arguments and uri. setSelectionArgs(new String[] Long.toString(contactId) ); if (mIsUserProfile)  setUri(ContactsContract.Profile.CONTENT_RAW_CONTACTS_URI);  else  setUri(RawContacts.CONTENT_URI);  return super.loadInBackground();\", 'private void executeOperations(final BsonArray operations, final boolean throwExceptions)  FailPoint failPoint = null; ServerAddress currentPrimary = null; try  for (BsonValue cur : operations)  final BsonDocument operation = cur.asDocument(); String operationName = operation.getString(\"name\").getValue(); BsonValue expectedResult = operation.get(\"result\"); String receiver = operation.getString(\"object\").getValue(); ClientSession clientSession = receiver.startsWith(\"session\")? sessionsMap.get(receiver) : null; if (clientSession == null)  clientSession = operation.getDocument(\"arguments\", new BsonDocument()).containsKey(\"session\")? sessionsMap.get(operation.getDocument(\"arguments\").getString(\"session\").getValue()) : null;  try  if (operationName.equals(\"startTransaction\"))  BsonDocument arguments = operation.getDocument(\"arguments\", new BsonDocument()); if (arguments.containsKey(\"options\"))  TransactionOptions transactionOptions = createTransactionOptions(arguments.getDocument(\"options\")); nonNullClientSession(clientSession).startTransaction(transactionOptions);  else  nonNullClientSession(clientSession).startTransaction(transactionOptions);  else  nonNullClientSession(clientSession).startTransaction();   else if (operationName.equals(\"commitTransaction\"))  nonNullClientSession(clientSession).commitTransaction();  else if (operationName.equals(\"abort', 'public String createNewSessionId( final String sessionId, final String newMemcachedId )  final int idxDash = sessionId.indexOf( \\'-\\' ); if ( idxDash  0 )  return sessionId + \"-\" + newMemcachedId;  final int idxDot = sessionId.indexOf( \\'.\\' ); if ( idxDot  0 )  return sessionId.substring( 0, idxDash + 1 ) + newMemcachedId;  else  return sessionId.substring( 0, idxDash + 1 ) + newMemcachedId + sessionId.substring( idxDot );', 'public OperationResult executeChangeStream(final BsonDocument operation)  String entityName = operation.getString(\"object\").getValue(); ChangeStreamIterableBsonDocument> iterable; if (entities.hasCollection(entityName))  iterable = entities.getCollection(entityName).watch();  else if (entities.hasDatabase(entityName))  iterable = entities.getDatabase(entityName).watch(BsonDocument.class);  else if (entities.hasClient(entityName))  iterable = entities.getClient(entityName).watch(BsonDocument.class);  else  throw new UnsupportedOperationException(\"No entity found for id: \" + entityName);  for (Map.EntryString, BsonValue> cur : operation.getDocument(\"arguments\", new BsonDocument()).entrySet())  //noinspection SwitchStatementWithTooFewBranches switch (cur.getKey())  case \"batchSize\": iterable.batchSize(cur.getValue().asNumber().intValue()); break; default: throw new UnsupportedOperationException(\"Unsupported argument: \" + cur.getKey());   return resultOf(() ->  MongoCursorBsonDocument> cursor = iterable.withDocumentClass(BsonDocument.class).cursor(); entities.addChangeStream(operation.getString(\"saveResultAsEntity\").getValue(), cursor); return null; );', 'public void merge( Model target, Model source )  if ( target.getBuild() == null )  target.setBuild( new Build() );  MapObject, Object> context = Collections.singletonMap( PLUGIN_MANAGEMENT, target.getBuild().getPluginManagement() ); mergePluginContainer_Plugins( target.getBuild(), source.getBuild(), false, context );', 'public ProjectBuildingResult build( Artifact artifact, boolean allowStubModel, ProjectBuildingRequest configuration ) throws ProjectBuildingException  if (!artifact.getType().equals( \"pom\" ) )  artifact = repositorySystem.createProjectArtifact( artifact.getGroupId(), artifact.getArtifactId(), artifact.getVersion() );  ArtifactResolutionRequest request = new ArtifactResolutionRequest().setArtifact( artifact ).setCache( configuration.getRepositoryCache() ).setLocalRepository( configuration.getLocalRepository() ).setRemoteRepositories( configuration.getRemoteRepositories() ).setOffline( configuration.isOffline() ).setForceUpdate( configuration.isForceUpdate() ); request.setTransferListener( configuration.getTransferListener() ); ArtifactResolutionResult result = repositorySystem.resolve( request ); if ( result.hasMissingArtifacts() && allowStubModel )  return build( null, createStubModelSource( artifact ), configuration );  try  resolutionErrorHandler.throwErrors( request, result );  catch ( ArtifactResolutionException e )  throw new ProjectBuildingException( artifact.getId(), \"Error resolving project artifact: \" + e.getMessage(), e );  boolean localProject = artifact.getRepository()!= null && artifact.getRepository().isProjectAware(); File pomFile = artifact.getFile(); return build( localProject? pomFile : null, new FileModelSource( pomFile ), configuration );', 'public void setUp() throws Exception  cluster = getNineNodeCluster(); storeDef = getStoreDef(STORE_NAME, REPLICATION_FACTOR, P_READS, R_READS, P_WRITES, R_WRITES, RoutingStrategyType.CONSISTENT_STRATEGY); for(Node node: cluster.getNodes())  VoldemortException e = new UnreachableStoreException(\"Node down\"); InMemoryStorageEngineByteArray, byte[], byte[]> storageEngine = new InMemoryStorageEngineByteArray, byte[], byte[]>(STORE_NAME); LoggingStoreByteArray, byte[], byte[]> loggingStore = new LoggingStoreByteArray, byte[], byte[]>(storageEngine); subStores.put(node.getId(), new ForceFailStoreByteArray, byte[], byte[]>(loggingStore, e, node.getId()));  setFailureDetector(subStores); for(Node node: cluster.getNodes())  int nodeId = node.getId(); StoreRepository storeRepo = new StoreRepository(); storeRepo.addLocalStore(subStores.get(nodeId)); for(int i = 0; i  NUM_NODES_TOTAL; i++) storeRepo.addNodeStore(i, subStores.get(i)); SlopStorageEngine slopStorageEngine = new SlopStorageEngine(new InMemoryStorageEngineByteArray, byte[], byte[]>(SLOP_STORE_NAME), cluster); StorageEngineByteArray, Slop, byte[]> storageEngine = slopStorageEngine.asSlopStore();', 'private void processPluginConfiguration( MavenProject project, MavenSession mavenSession ) throws Exception  for ( Iterator i = project.getPlugins().iterator(); i.hasNext(); )  Plugin plugin = (Plugin) i.next(); // TODO: should this flag be used in verifyPlugin, completely disabling the plugin? if (!plugin.isDisabled() )  if ( pluginManager.verifyPlugin( plugin.getId(), mavenSession ) )  PluginDescriptor pluginDescriptor = pluginManager.getPluginDescriptor( plugin.getId() ); for ( Iterator j = pluginDescriptor.getMojos().iterator(); j.hasNext(); )  MojoDescriptor mojoDescriptor = (MojoDescriptor) j.next(); // TODO: check if the goal exists in the configuration and is disabled if ( mojoDescriptor.getPhase()!= null )  Phase phase = (Phase) phaseMap.get( mojoDescriptor.getPhase() ); phase.getGoals().add( mojoDescriptor.getId() );', 'private DateFormat getDateFormat( WikiContext context, MapString,Object> params )  String formatString = get(params, DEFAULT_DATE_FORMAT, PARAM_DATE_FORMAT); if (\"\".equals(formatString.trim())) return Preferences.getDateFormat( context, TimeFormat.DATE ); return new SimpleDateFormat(formatString);', 'public void execute(Pipeline pipeline)  MapNode, ListByteArray>> nodeToKeysMap = Maps.newHashMap(); MapByteArray, ListNode>> keyToExtraNodesMap = Maps.newHashMap(); for(ByteArray key: keys)  ListNode> nodes = null; try  nodes = getNodes(key);  catch(VoldemortException e)  pipelineData.setFatalError(e); pipeline.addEvent(Event.ERROR); return;  ListNode> preferredNodes = Lists.newArrayListWithCapacity(preferred); ListNode> extraNodes = Lists.newArrayListWithCapacity(3); if(pipelineData.getZonesRequired()!= null)  if(pipelineData.getZonesRequired() > this.clientZone.getProximityList().size())  throw new VoldemortException(\"Number of zones required should be less than the total number of zones\");  if(pipelineData.getZonesRequired() > required)  throw new VoldemortException(\"Number of zones required should be less than the total number of zones\");  if(pipelineData.getZonesRequired() > required)  throw new VoldemortException(\"Number of zones required should be less than the required number of \" + pipeline.getOperation().getSimpleName() + \"s\");  // Create zone id to node mapping MapInteger, ListNode>> zoneIdToNode = new HashMapInteger, ListNode>>(); for(Node node: nodes)  ListNode> nodesList = null; if(zoneIdToNode.containsKey(node.getZoneId()))  nodesList = zoneIdToNode.get', 'public NetworkInfo getActiveNetworkInfo(int uid)  NetworkInfo mActiveNetwork = mActiveNetworkType; if (mActiveNetwork == null)  return null;  else  NetworkInfo mActiveNetwork = mActiveNetworkType; if (mActiveNetwork == null)  return new NetworkInfo(mActiveNetworkType, 0, null, null);', 'public void process(CliRequest cliRequest) throws Exception  CommandLine commandLine = cliRequest.getCommandLine(); String workingDirectory = cliRequest.getWorkingDirectory(); MavenExecutionRequest request = cliRequest.getRequest(); File userSettingsFile; if (commandLine.hasOption(CLIManager.ALTERNATE_USER_SETTINGS))  userSettingsFile = new File(commandLine.getOptionValue(CLIManager.ALTERNATE_USER_SETTINGS)); userSettingsFile = resolveFile(userSettingsFile, workingDirectory); if (!userSettingsFile.isFile())  throw new FileNotFoundException(\"The specified user settings file does not exist: \" + userSettingsFile);   else  userSettingsFile = DEFAULT_USER_SETTINGS_FILE;  File globalSettingsFile; if (commandLine.hasOption(CLIManager.ALTERNATE_GLOBAL_SETTINGS))  globalSettingsFile = new File(commandLine.getOptionValue(CLIManager.ALTERNATE_GLOBAL_SETTINGS)); globalSettingsFile = resolveFile(globalSettingsFile, workingDirectory); if (!globalSettingsFile.isFile())  throw new FileNotFoundException( \"The specified global settings file does not exist: \" + globalSettingsFile);   else  globalSettingsFile = DEFAULT_GLOBAL_SETTINGS_FILE;  request.setGlobalSettingsFile(globalSettingsFile); request.setUserSettingsFile(userSettingsFile); SettingsBuildingRequest settingsRequest = new DefaultSettingsBuildingRequest(); settingsRequest.setGlobalSettingsFile(globalSettingsFile); settingsRequest.setUser', 'public void testProxyPutDuringRebalancing() throws Exception  logger.info(\"Starting testProxyPutDuringRebalancing\"); try  Cluster currentCluster = ServerTestUtils.getLocalZonedCluster(6, 2, new int[]  0, 0, 0, 1, 1, 1, new int[][]   0,  1, 6,  2,  3,  4, 7,  5  ); Cluster targetCluster = RebalanceUtils.createUpdatedCluster(currentCluster, 2, Lists.newArrayList(7)); targetCluster = RebalanceUtils.createUpdatedCluster(targetCluster, 5, Lists.newArrayList(6)); /** * original server partition ownership * * [s0 : p0,p3,p4,p5,p6,p7] [s1 : p1-p7] [s2 : p1,p2] [s3 : * p0,p1,p2,p3,p6,p7] [s4 : p1-p7] [s5 : p4,p5] * * target server partition ownership * * [s0 : p0,p2,p3,p4,p5,p6,p7] [s1 : p0,p1] [s2 : p1-p7] [s3 : * p0.p1,p2,p3,p5,p6,p7] [s4 : p0,p1,p2,p3,p4,p7] [s5 : p4,p5,p6] */ ListInteger> serverList = Arrays.asList(0, 1, 2, 3, 4, 5); MapString, String> configProps = new HashMapString, String>(); configProps.put(\"admin.max.threads\", \"5\"); final Cluster updatedCurrentCluster = startServers(currentCluster, rwStoreDefFileWithReplication, serverList, configPro', 'public void transformForResolve( Artifact artifact, List remoteRepositories, ArtifactRepository localRepository ) throws ArtifactMetadataRetrievalException  if ( isSnapshot( artifact ) )  // TODO: this mostly works, however... // - we definitely need the manual/daily check as this is quite slow given the large number of snapshots inside m2 presently SnapshotArtifactMetadata localMetadata; try  localMetadata = SnapshotArtifactMetadata.readFromLocalRepository( artifact, localRepository );  catch ( ArtifactPathFormatException e )  throw new ArtifactMetadataRetrievalException( \"Error reading local metadata\", e );  catch ( IOException e )  throw new ArtifactMetadataRetrievalException( \"Error reading local metadata\", e );  String version = localMetadata.constructVersion(); if (!alreadyResolved( artifact ) )  boolean checkedUpdates = false; for ( Iterator i = remoteRepositories.iterator(); i.hasNext(); )  ArtifactRepository remoteRepository = (ArtifactRepository) i.next(); String snapshotPolicy = remoteRepository.getSnapshotPolicy(); // TODO: should be able to calculate this less often boolean checkForUpdates = false; if ( ArtifactRepository.SNAPSHOT_POLICY_ALWAYS.equals( snapshotPolicy ) )  checkForUpdates = true;  else if ( ArtifactRepository.SNAPSHOT_POLICY_DAILY.equals( snapshotPolicy ) )  // Note that if last modified is 0, it didn\\'t exist, so this will be true if ( getMidnightBoundary().after( new Date( localMetadata.getLastModified() )', 'private MavenProject assembleLineage( Model model, LinkedList lineage, ArtifactRepository localRepository, File projectDir, List parentSearchRepositories, Set aggregatedRemoteWagonRepositories ) throws ProjectBuildingException  if (!model.getRepositories().isEmpty() )  List respositories = buildArtifactRepositories( model ); for ( Iterator it = respositories.iterator(); it.hasNext(); )  ArtifactRepository repository = (ArtifactRepository) it.next(); if (!aggregatedRemoteWagonRepositories.contains( repository ) )  aggregatedRemoteWagonRepositories.add( repository );    ProfileManager profileManager = new DefaultProfileManager( container ); List activeProfiles; try  profileManager.addProfiles( model.getProfiles() ); loadProjectExternalProfiles( profileManager, projectDir ); activeProfiles = injectActiveProfiles( profileManager, model );  catch ( ProfileActivationException e )  throw new ProjectBuildingException( \"Failed to activate local (project-level) build profiles.\", e );  MavenProject project = new MavenProject( model ); project.setActiveProfiles( activeProfiles ); lineage.addFirst( project ); Parent parentModel = model.getParent(); if ( parentModel!= null )  if ( StringUtils.isEmpty( parentModel.getGroupId() ) )  throw new ProjectBuildingException( \"Missing groupId element from parent element\" );  else if ( StringUtils.isEmpty( parentModel.getArtifactId() ) )  throw new ProjectBuildingException( \"Missing artifactId element from parent element\" );  else if ( StringU', 'public static Embedded createCatalina( final int port, final int sessionTimeout, final String memcachedNodes, final String jvmRoute ) throws MalformedURLException, UnknownHostException, LifecycleException  final Embedded catalina = new Embedded(); final Engine engine = catalina.createEngine(); /* we must have a unique name for mbeans */ engine.setName( \"engine-\" + port ); engine.setDefaultHost( \"localhost\" ); engine.setJvmRoute( jvmRoute ); final URL root = new URL( TestUtils.class.getResource( \"/\" ), \"../resources\" ); final String docBase = root.getFile() + File.separator + TestUtils.class.getPackage().getName().replaceAll( \".\", File.separator ); final Host host = catalina.createHost( \"localhost\", docBase ); engine.addChild( host ); new File( docBase ).mkdirs(); final MemcachedBackupSessionManager sessionManager = new MemcachedBackupSessionManager(); engine.setManager( sessionManager ); final Context context = catalina.createContext( \"/\", \"webapp\" ); context.setManager( sessionManager ); context.setBackgroundProcessorDelay( 1 ); new File( docBase + File.separator + \"webapp\" ).mkdirs(); host.addChild( context ); /* we must set the maxInactiveInterval after the context, * as setContainer(context) uses the session timeout set on the context */ sessionManager.setMemcachedNodes( memcachedNodes ); sessionManager.setMaxInactiveInterval( sessionTimeout ); // 1 second sessionManager.setProcessExpiresFrequency( 1 ); // 1 second (factor for context.setBackgroundPro', \"private void printProperty( StringBuilder b, Object key, Object value )  b.append( key.toString() ); b.append('' ); b.append( '=' ); b.append('' ); b.append( value.toString() ); b.append( m_br );\", 'public static void rm(IterableFile> files)  if(files!= null)  for(File f: files)  if(f.isDirectory())  Arrays.asList(f.listFiles())); f.delete();  else  f.delete();', 'public VersionRange restrict( VersionRange restriction )  ArtifactVersion version = max( recommendedVersion, restriction.getRecommendedVersion() ); // TODO return new VersionRange( version, Collections.EMPTY_LIST );', 'public V checkout(K key) throws Exception  checkNotClosed(); long startNs = System.nanoTime(); long timeoutNs = resourcePoolConfig.getTimeout(TimeUnit.NANOSECONDS); long timeoutNs = totalElapsedNs; long timeoutNs = totalElapsedNs; long timeoutNs = totalElapsedNs; long timeoutNs = totalElapsedNs; long timeRemainingNs = resourcePool.blockingGet(timeRemainingNs); if(resource == null)  resource = resourcePool.blockingGet(timeRemainingNs);  if(resource == null)  long totalElapsedNs = System.nanoTime() - nonBlockingElapsedNs; String errorMessage = String.format(\"Timeout while checking out resource (%s). Configured time (%d) ns NonBlocking time (%d) ns Blocking time (%d) ns \", key, resourcePoolConfig.getTimeout(TimeUnit.NANOSECONDS), nonBlockingElapsedNs, blockingElapsedNs); throw new TimeoutException(errorMessage);   if(!objectFactory.validate(key, resource)) throw new ExcessiveInvalidResourcesException(1);  catch(Exception e)  destroyResource(key, resourcePool, resource); throw e;  return resource;', 'public void setUp() throws IOException  cluster = ServerTestUtils.getLocalCluster(2, new int[][]   0, 1, 2, 3,  4, 5, 6, 7  ); servers = new VoldemortServer[2]; storeDefs = new StoreDefinitionsMapper().readStoreList(new File(storesXmlfile)); Properties serverProperties = new Properties(); serverProperties.setProperty(\"client.max.connections.per.node\", \"20\"); servers[0] = ServerTestUtils.startVoldemortServer(socketStoreFactory, ServerTestUtils.createServerConfig(useNio, 0, TestUtils.createTempDir().getAbsolutePath(), null, storesXmlfile, serverProperties), cluster); servers[1] = ServerTestUtils.startVoldemortServer(socketStoreFactory, ServerTestUtils.createServerConfig(useNio, 1, TestUtils.createTempDir().getAbsolutePath(), null, storesXmlfile, serverProperties), cluster); Properties adminProperties = new Properties(); adminProperties.setProperty(\"max_connections\", \"20\"); adminClient = new AdminClient(cluster, new AdminClientConfig(adminProperties));', 'public void publish()  String content = buildLogContent(); WikiPage errorLogPage = crawler.addPage(root, errorLogPagePath); PageData data = errorLogPage.getData(); WikiPagePath wpp=new WikiPagePath(errorLogPagePath.getRest()); WikiPage wikiPage = crawler.getPage(root, wpp); PageData pageData = wikiPage.getData(); data.setAttribute(PageData.PropertySUITES,pageData.getAttribute(PageData.PropertySUITES)); data.setContent(content); errorLogPage.commit(data);', 'private void generateProjectReportsPage( InputStream siteDescriptor ) throws Exception  File siteDirectory = new File( siteDirectory ); XhtmlSink sink = siteRenderer.createSink( siteDirectory ), \"maven-reports.html\", outputDirectory, siteDescriptor, flavour ); String title = \"Maven Generated Reports\"; sink.head(); sink.title(); sink.text( title ); sink.title_(); sink.head_(); sink.body(); sink.section1(); sink.sectionTitle1(); sink.text( title ); sink.sectionTitle1_(); sink.paragraph(); sink.text( \"This document provides an overview of the various reports that are automatically generated by \" ); sink.link( \"http://maven.apache.org\" ); sink.text( \"Maven\" ); sink.link_(); sink.text( \". Each report is briefly described below.\" ); sink.paragraph_(); sink.section2(); sink.sectionTitle2(); sink.text( \"Overview\" ); sink.sectionTitle2_(); sink.table(); sink.tableRow(); sink.tableHeaderCell(); sink.text( \"Document\" ); sink.tableHeaderCell_(); sink.tableHeaderCell(); sink.text( \"Description\" ); sink.tableHeaderCell_(); sink.tableRow_(); for ( Iterator i = projectReports.iterator(); i.hasNext(); )  MavenReport report = (MavenReport) i.next(); sink.tableRow(); sink.tableCell(); sink.link( report.getOutputName() + \".html\" ); sink.text( report.getName() ); sink.link_(); sink.tableCell_(); sink.tableCell(); sink.text( report.getDescription() ); sink.tableCell_(); sink.tableRow_();', 'public static boolean removeWikiEventListener( final WikiEventListener listener )  // get the Map.entry object for the entire Map, then check match on entry (listener) final WikiEventManager mgr = getInstance(); final Map Object, WikiEventDelegate > sources = mgr.getDelegates(); synchronized( sources )  // get an iterator over the Map.Enty objects in the map for( final Map.Entry Object, WikiEventDelegate > entry : sources.entrySet() )  // the entry value is the delegate final WikiEventDelegate delegate = entry.getValue(); // now see if we can remove the listener from the delegate (delegate may be null because this is a weak reference) if( delegate!= null && delegate.removeWikiEventListener( listener ) )  return true; // was removed    return false;', 'protected long createGroup(Account account, String sourceId, String title)  String accountId = accountId.getAccountId(); return createGroup(account, sourceId, title, 1);', 'public void start()  String command = buildCommand(descriptor); MapString, String> environmentVariables = createClasspathEnvironment(descriptor.getClassPath()); CommandRunningFitClient.CommandRunningStrategy runningStrategy = fastTest? new CommandRunningFitClient.InProcessCommandRunner(descriptor) : new CommandRunningFitClient.OutOfProcessCommandRunner(command, environmentVariables); this.client = new CommandRunningFitClient(this, context.port, context.socketDealer, runningStrategy); setExecutionLog(new ExecutionLog(page, client.commandRunner)); client.start();', 'private void executeTaskSegments( List taskSegments, ReactorManager rm, MavenSession session, MavenProject rootProject, EventDispatcher dispatcher, MavenExecutionResponse response ) throws ArtifactNotFoundException, MojoExecutionException, LifecycleExecutionException, MojoFailureException, ArtifactResolutionException  for ( Iterator it = taskSegments.iterator(); it.hasNext(); )  TaskSegment segment = (TaskSegment) it.next(); if ( segment.aggregate() )  if (!rm.isBlackListed( rootProject ) )  line(); getLogger().info( \"Building \" + rootProject.getName() ); getLogger().info( \" \" + segment ); line(); //!! This is ripe for refactoring to an aspect. // Event monitoring. String event = MavenEvents.PROJECT_EXECUTION; long buildStartTime = System.currentTimeMillis(); dispatcher.dispatchStart( event, rootProject.getId() + \" ( \" + segment + \" )\" ); try  // only call once, with the top-level project (assumed to be provided as a parameter)... for ( Iterator goalIterator = segment.getTasks().iterator(); goalIterator.hasNext(); )  String task = (String) goalIterator.next(); try  executeGoal( task, session, rootProject, response );  catch ( MojoExecutionException e )  // TODO: should this be removed? handleExecutionFailure( rm, rootProject, e, task, buildStartTime );  catch ( ArtifactResolutionException e )  // TODO: should this be removed? handleExecutionFailure(', 'public void getConverterForClass_should_return_Object_Converter_as_last_resort() throws SecurityException, NoSuchMethodException  try  ConverterRegistry.addConverter(Object.class, new MyObjectConverter()); Converter?> converter = ConverterRegistry.getConverterForClass(ConverterRegistryTest.class); assertNotNull(\"no converter retunred\", converter); assertEquals(MyObjectConverter.class, converter.getClass());  finally  // cleanup ConverterRegistry.addConverter(Object.class, null); assertNull(ConverterRegistry.getConverterForClass(Object.class));', 'private Object findExtension( MavenProject project, String role, String roleHint, Settings settings, ArtifactRepository localRepository ) throws ArtifactResolutionException, ArtifactNotFoundException, LifecycleExecutionException  for ( Iterator i = project.getBuildPlugins().iterator(); i.hasNext(); )  Plugin plugin = (Plugin) i.next(); if ( plugin.isExtensions() )  verifyPlugin( plugin, project, settings, localRepository ); // TODO: if moved to the plugin manager we already have the descriptor from above and so do can lookup the container directly try  return pluginManager.getPluginComponent( plugin, role, roleHint );  catch ( ComponentLookupException e )  getLogger().debug( \"Unable to find the lifecycle component in the extension\", e );  catch ( PluginManagerException e )  throw new LifecycleExecutionException( \"Error getting extensions from the plugin \\'\" + plugin.getKey() + \"\\'\", e );    return null;', 'protected void configure()  bindScope(RequestScoped.class, GuiceProvider.REQUEST); bindScope(SessionScoped.class, GuiceProvider.SESSION); bindScope(ApplicationScoped.class, GuiceProvider.APPLICATION); MatcherTypeLiteral?>> isApplication = type(annotatedWith(ApplicationScoped.class)); MatcherTypeLiteral?>> isSession = type(annotatedWith(SessionScoped.class)); bindListener(isApplication, new ScopeLifecycleListener(GuiceProvider.APPLICATION)); bindListener(isSession, new ScopeLifecycleListener(GuiceProvider.SESSION)); bindListener(not(isApplication).and(not(isSession)), new ScopeLifecycleListener(GuiceProvider.REQUEST)); bindListener(not(isApplication).and(not(isSession)), new ScopeLifecycleListener(GuiceProvider.REQUEST)); bind(MutableRequest.class).toProvider(new ProviderMutableRequest>()  public RequestInfo get()  return VRaptorRequestHolder.currentRequest();  ).in(GuiceProvider.REQUEST); bind(HttpSession.class).toProvider(new ProviderHttpSession>()  public HttpSession get()  return VRaptorRequestHolder.currentRequest().getRequest().getSession();  ).in(GuiceProvider.REQUEST); bind(HttpSession.class).toProvider(new ProviderHttpSession>()  public HttpSession get()  return VRaptor', 'public void report(long startTime, long endTime)  String sysfile = run.getOutDir() + \"system.report\"; PrintStream syslog = null; DateFormat df = DateFormat.getDateTimeInstance( DateFormat.MEDIUM, DateFormat.LONG, Locale.US); String start = df.format(new Date(startTime)); String end = df.format(new Date(endTime)); String startMon = start.substring(0, 3); int ind = start.indexOf(\\',\\'); String startDay = start.substring(4, ind); ind = ind + 7; // skip over\\'1999\\'to get to time String stime = start.substring(ind, start.indexOf(\\' \\', ind)); logger.fine(\"Run started Month = \" + startMon + \" Day = \" + startDay + \" Time = \" + stime); String endMon = end.substring(0, 3); ind = end.indexOf(\\',\\'); String endDay = end.substring(4, ind); ind = ind + 7; // skip over\\'1999\\'to get to time String etime = end.substring(ind, end.indexOf(\\' \\', ind)); logger.fine(\"Run ended Month = \" + endMon + \" Day = \" + endDay + \" Time = \" + etime); // Now, get /var/adm/messages and look for messages between // start and end CommandHandle handle; Command c = new Command(\"messages\", \"\"\" + startMon + \" \" + startDay + \" \" + stime + \"\"\", \"\"\" + endMon + \" \" + endDay + \" \" + etime + \"\"\"); c.setStreamHandling(Command.STDOUT, Command.CAPTURE); logger.fine(\"Getting system messages\"); for (ParamRepository.HostConfig hostConfig : hostConfigs)  for(String', 'private static void executeSingleCommand(Arguments arguments, FitNesse fitnesse, FitNesseContext context) throws Exception  TestTextFormatter.finalErrorCount = 0; System.out.println(\"Executing command: \" + arguments.getCommand()); System.out.println(\"-----Command Output-----\"); fitnesse.executeSingleCommand(arguments.getCommand(), System.out); System.out.println(\"-----Command Complete-----\"); fitnesse.stop(); if (shouldExitAfterSingleCommand())  TestTextFormatter.finalErrorCount = 0; System.out.println(\"Executing command: \" + arguments.getCommand()); System.out.println(\"-----CommandOutput-----\"); fitnesse.executeSingleCommand(arguments.getCommand(), System.out); System.out.println(\"-----Command Complete-----\"); fitnesse.stop(); if (shouldExitAfterSingleCommand())  TestTextFormatter.finalErrorCount = 0; System.exit(TestTextFormatter.finalErrorCount);', 'public static void reloadPreferences( PageContext pageContext )  Preferences prefs = new Preferences(); Properties props = PropertyReader.loadWebAppProps( pageContext.getServletContext() ); WikiContext ctx = WikiContext.findContext( pageContext ); prefs.put(\"SkinName\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.skinname\", \"PlainVanilla\" ) ); prefs.put(\"DateFormat\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.dateformat\", ctx.getEngine().getInternationalizationManager().get( InternationalizationManager.CORE_BUNDLE, getLocale( ctx ), \"common.datetimeformat\" ) ) ); prefs.put(\"TimeZone\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.timezone\", java.util.TimeZone.getDefault().getID() ) ); prefs.put(\"Orientation\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.orientation\", \"fav-left\" ) ); prefs.put(\"Sidebar\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.sidebar\", \"active\" ) ); prefs.put(\"Layout\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.layout\", \"fluid\" ) ); prefs.put(\"Language\", TextUtil.getStringProperty( props, \"jspwiki.defaultprefs.template.language\", getLocal', \"public ResolutionGroup retrieve(MetadataResolutionRequest request) throws ArtifactMetadataRetrievalException  Artifact artifact = request.getArtifact(); // // If we have a system scoped artifact then we do not want any searching in local or remote repositories // and we want artifact resolution to only return the system scoped artifact itself. // if (artifact.getScope()!= null && artifact.getScope().equals(Artifact.SCOPE_SYSTEM))  return new ResolutionGroup(null, null, null);  ResolutionGroup cached = cache.get( artifact, request.isResolveManagedVersions(), request.getLocalRepository(), request.getRemoteRepositories()); if (cached!= null // if the POM has no file, we cached a missing artifact, only return the cached data if no update forced && (!request.isForceUpdate() || hasFile(cached.getPomArtifact())))  return cached;  ListDependency> dependencies; ListDependency> managedDependencies = null; ListArtifactRepository> pomRepositories = null; Artifact pomArtifact; Artifact relocatedArtifact = null; // TODO hack: don't rebuild model if it was already loaded during reactor resolution final WorkspaceReader workspace = legacySupport.getRepositorySession().getWorkspaceReader(); Model model = null; if (workspace instanceof MavenWorkspaceReader)  model = ((MavenWorkspaceReader) workspace).findModel(RepositoryUtils.toArtifact(artifact));  if (model!= null)  pomArtifact = artifact; dependencies = model.getDependencies(); DependencyManagement dependencyManagement = model.getDependencyManagement(); managedDependencies\", 'protected void onLayout(boolean changed, int left, int top, int right, int bottom)  final int height = bottom - top; final int width = right - left; // Determine the vertical bounds by laying out the header first. int topBound = 0; int bottomBound = height; int leftBound = getPaddingLeft(); int rightBound = width - getPaddingRight(); final boolean isLayoutRtl = ViewUtil.isViewLayoutRtl(this); // Put the section header on the left side of the contact view. if (mIsSectionHeaderEnabled)  if (mHeaderTextView!= null)  int headerHeight = mHeaderTextView.getMeasuredHeight(); int headerTopBound = (bottomBound + topBound - headerHeight) / 2 + mTextOffsetTop; mHeaderTextView.layout( isLayoutRtl? rightBound - mHeaderWidth : leftBound + mHeaderWidth, headerTopBound + headerHeight);  if (isLayoutRtl)  rightBound -= mHeaderWidth;  else  leftBound += mHeaderWidth;   mBoundsWithoutHeader.set(left + leftBound, topBound, left + rightBound, bottomBound); mLeftOffset = left + leftBound; mRightOffset = left + rightBound; if (mIsSectionHeaderEnabled)  if (isLayoutRtl)  rightBound -= mHeaderWidth;   mBoundsWithoutHeader.set(', 'private static String toMessage( String modelId, ListModelProblem> problems )  StringWriter buffer = new StringWriter( 1024 ); PrintWriter writer = new PrintWriter( buffer ); writer.print( problems.size() ); writer.print( ( problems.size() == 1 )? \" problem was \" : \" problems were \" ); writer.print( \"encountered while building the effective model\" ); if ( modelId!= null && modelId.length() > 0 )  writer.print( \" for \" ); writer.print( modelId );  writer.println(); for ( ModelProblem problem : problems )  writer.print( \"[\" ); writer.print( problem.getSeverity() ); writer.print( \"] \" ); writer.print( problem.getMessage() ); writer.print( \" @ \" ); writer.println( ModelProblemUtils.formatLocation( problem, modelId ) );  return buffer.toString();', 'public PwDatabaseV4 openDatabase(InputStream inStream, String password, String keyfile, UpdateStatus status) throws IOException, InvalidKeyFileException, InvalidPasswordException, InvalidDBSignatureException, InvalidDBVersionException  PwDatabaseV4 db = new PwDatabaseV4(); PwDbHeaderV4 header = new PwDbHeaderV4(db); header.loadFromFile(inStream); db.setMasterKey(password, keyfile); db.makeFinalKey(header.masterSeed, header.transformSeed, (int)db.numKeyEncRounds); // Attach decryptor Cipher cipher; try  cipher = CipherFactory.getInstance(db.dataCipher, db.finalKey, header.encryptionIV);  catch (NoSuchAlgorithmException e)  throw new IOException(\"Invalid algorithm.\");  catch (NoSuchPaddingException e)  throw new IOException(\"Invalid algorithm.\");  catch (InvalidKeyException e)  throw new IOException(\"Invalid algorithm.\");  catch (InvalidAlgorithmParameterException e)  throw new IOException(\"Invalid algorithm.\");  InputStream decrypted = new CipherInputStream(inStream, cipher); LEDataInputStream dataDecrypted = new LEDataInputStream(decrypted); byte[] storedStartBytes = dataDecrypted.readBytes(32); if ( storedStartBytes == null || storedStartBytes.length!= 32 )  throw new IOException(\"Invalid data.\");  if (! Arrays.equals(storedStartBytes, header.streamStartBytes) )  // TODO: Probably need a special error here. This would probably', 'private void addMultiPointFields(ParameterSignature sig, ListPotentialAssignment> list)  for (final Field field : getDataPointsFields(sig))  Class?> type = field.getType(); if (sig.canAcceptArrayType(type))  try  addArrayValues(field.getName(), list, getStaticFieldValue(field));  catch (Throwable e)  // ignore and move on', \"public String resolvePluginVersion( String groupId, String artifactId, MavenProject project, Settings settings, ArtifactRepository localRepository ) throws PluginVersionResolutionException  // first pass...if the plugin is specified in the pom, try to retrieve the version from there. String version = getVersionFromPluginConfig( groupId, artifactId, project ); // we're NEVER going to persist POM-derived plugin versions. String updatedVersion = null; // we're not going to prompt the user to accept a plugin update until we find one. boolean promptToPersist = false; // second pass...if the plugin is listed in the settings.xml, use the version from useVersion/>. if ( StringUtils.isEmpty( version ) )  // 1. resolve existing useVersion. version = resolveExistingFromPluginRegistry( groupId, artifactId ); if ( StringUtils.isNotEmpty( version ) )  boolean forceUpdate = settings.getRuntimeInfo().isPluginUpdateForced(); // 2. check for updates. Determine whether this is the right time to attempt to update the version. if ( forceUpdate || shouldCheckForUpdates( groupId, artifactId ) )  updatedVersion = resolveReleaseVersion( groupId, artifactId, project.getRemoteArtifactRepositories(), localRepository ); if ( StringUtils.isNotEmpty( updatedVersion ) &&!updatedVersion.equals( version ) )  // see if this version we've resolved is on our previously rejected list. boolean isRejected = checkForRejectedStatus( groupId, artifactId, updatedVersion ); // we should only prompt to use this version if the user has not previously rejected it. promptToPersist =!isRejected; // if we've rejected this version previously, forget about updating. if ( isRe\", 'public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException  if (!(req instanceof HttpServletRequest) ||!(res instanceof HttpServletResponse))  throw new ServletException( \"VRaptor must be run inside a Servlet environment. Portlets and others aren\\'t supported.\");  final HttpServletRequest baseRequest = (HttpServletRequest) req; final HttpServletResponse baseResponse = (HttpServletResponse) res; if (staticHandler.requestingStaticFile(baseRequest))  staticHandler.deferProcessingToContainer(chain, baseRequest, baseResponse);  else  logger.debug(\"VRaptor received a new request\"); logger.trace(\"Request: \", req); VRaptorRequest mutableRequest = new VRaptorRequest(baseRequest); VRaptorResponse mutableResponse = new VRaptorResponse(baseResponse); final RequestInfo request = new RequestInfo(servletContext, chain, mutableRequest, mutableResponse); provider.provideForRequest(request, new ExecutionObject>()  public Object insideRequest(Container container)  container.instanceFor(EncodingHandler.class).setEncoding(baseRequest, baseResponse); container.instanceFor(RequestExecution.class).execute(); return null;  ); logger.debug(\"VRaptor ended the request\");', 'public void callsTestingTrackerBeforeAndAfterTestExecution() throws IOException, InterruptedException  final String stopId = \"42\"; WikiPage testPage = addTestPage(suite, \"TestPage1\", \"!define TEST_SYSTEM A\"); ClosableTestSystemListener listener = mock(ClosableTestSystemListener.class); when(testingTracker.addStartedProcess(any(Stoppable.class))).thenReturn(stopId); MultipleTestsRunner runner = new MultipleTestsRunner(asList(testPage), context, testingTracker, testSystemFactory); runner.addTestSystemListener(listener); runner.executeTestPages(); verify(testingTracker).addStartedProcess(runner); verify(testingTracker).removeEndedProcess(stopId);', 'private String disgraceName()  disgracedName = new StringBuffer(); for (char c : name.toCharArray()) appendCharInProperCase(c); String s = disgracedName.toString(); s.add(s);', 'public Message getPreviousMessage(Connection db) throws SQLException  PreparedStatement pst; int scrollMode = Section.getScrollMode(getSectionId()); switch (scrollMode)  case Section.SCROLL_SECTION: pst = db.prepareStatement(\"SELECT topics.id as msgid FROM topics WHERE topics.groupid=groups.id AND topics.commitdate=(SELECT max(commitdate) FROM topics, groups, sections WHERE sections.id=groups.section AND topics.commitdate? AND topics.groupid=groups.id AND groups.section=? AND (topics.moderate OR NOT sections.moderate) AND NOT deleted)\"); pst.setTimestamp(1, commitDate); pst.setInt(2, sectionid); break; case Section.SCROLL_GROUP: pst = db.prepareStatement(\"SELECT topics.id as msgid FROM topics WHERE topics.id=(SELECT max(topics.id) FROM topics, groups, sections WHERE sections.id=groups.section AND topics.id? AND topics.groupid=? AND topics.groupid=groups.id AND (topics.moderate OR NOT sections.moderate) AND NOT deleted)\"); pst.setInt(1, msgid); pst.setInt(2, guid); break; case Section.SCROLL_NOSCROLL: default: return null;  try  ResultSet rs = pst.executeQuery(); if (!rs.next())  return null;  return new Message(db, rs.getInt(\"msgid\"));  catch (MessageNotFoundException e)  throw new RuntimeException(e);  finally  pst.', 'public void initHelper() throws Exception  TestRunner testRunner = new FitNesseRepository(\".\"); helper = new JUnitHelper(new TestRunner(new FitNesseRepository(\".\"), new FitTestEngine(), new File(System.getProperty(\"java.io.tmpdir\"), \"fitnesse\").getAbsolutePath());', 'protected Message.Response execute(QueryState state, long queryStartNanoTime, boolean traceRequest)  String cqlVersion = options.get(CQL_VERSION); if (cqlVersion == null) throw new ProtocolException(\"Missing value CQL_VERSION in STARTUP message\"); try  if (new CassandraVersion(cqlVersion).compareTo(new CassandraVersion(\"2.99.0\"))  0) throw new ProtocolException(String.format(\"CQL version %s is not supported by the binary protocol (supported version are >= 3.0.0)\", cqlVersion));  catch (IllegalArgumentException e)  throw new ProtocolException(e.getMessage());  if (options.containsKey(COMPRESSION))  String compression = options.get(COMPRESSION).toLowerCase(); if (compression.equals(\"snappy\"))  if (Compressor.SnappyCompressor.instance == null) throw new ProtocolException(\"This instance does not support Snappy compression\"); if (getSource().header.version.isGreaterOrEqualTo(ProtocolVersion.V5)) throw new ProtocolException(\"Snappy compression is not supported in protocol V5\"); connection.setCompressor(Compressor.SnappyCompressor.instance);  else if (compression.equals(\"lz4\"))  connection.setCompressor(Compressor.LZ4Compressor.instance);  else  throw new ProtocolException(String.format(\"Unknown compression algorithm: %s\", compression));   connection.setThrowOnOverload(\"1\".equals(options.get(THROW_ON_OVERLOAD))); ClientState clientState = state.getClientState(); clientState.setClientOption', 'public void getAsync(final SingleResultCallbackInternalConnection> callback)  InternalConnection connection = null; try  connection = getPooledConnection(0, MILLISECONDS);  catch (MongoTimeoutException e)  // fall through  if (connection!= null)  callCallback(connection, callback);  else if (waitQueueSize.incrementAndGet() > settings.getMaxWaitQueueSize())  waitQueueSize.decrementAndGet(); callback.onResult(null, createWaitQueueFullException());  else  final long startTimeMillis = System.currentTimeMillis(); connectionPoolListener.waitQueueEntered(new ConnectionPoolWaitQueueEvent(serverId, currentThread().getId())); getAsyncGetter().submit(new Runnable()  @Override public void run()  try  if (getRemainingWaitTime() = 0)  callCallback(createTimeoutException(), callback);  else  callCallback(getPooledConnection(getRemainingWaitTime(), MILLISECONDS), callback);   catch (Throwable t)  callCallback(t, callback);  finally  waitQueueSize.decrementAndGet(); connectionPoolListener.waitQueueExited(new ConnectionPoolWaitQueueEvent(serverId, currentThread().getId()));   private long getRe', 'public Model read( Reader input, MapString,?> options ) throws IOException  Objects.requireNonNull( input, \"input cannot be null\" ); try ( Reader in = input )  String source = isStrict( options ); return read( source );', 'public T> T is(final ClassT> type)  Method method = new MethodInvocationT>(); return proxifier.proxify(type, method, Object[] args, SuperMethod superMethod); boolean alreadySetTheStrategy =!strategy.getClass().equals(NoStrategy.class); if (alreadySetTheStrategy)  // the virtual machine might be invoking the finalize return null;  is(type, method); return null;  );', 'private void populatePluginFields( Object mojo, MojoDescriptor mojoDescriptor, ClassRealm pluginRealm, PlexusConfiguration configuration, ExpressionEvaluator expressionEvaluator ) throws PluginConfigurationException  ComponentConfigurator configurator = null; String configuratorId = mojoDescriptor.getComponentConfigurator(); if ( StringUtils.isEmpty( configuratorId ) )  configuratorId = \"basic\";  try  // TODO: could the configuration be passed to lookup and the configurator known to plexus via the descriptor // so that this method could entirely be handled by a plexus lookup? configurator = container.lookup( ComponentConfigurator.class, configuratorId ); ConfigurationListener listener = new DebugConfigurationListener( logger ); logger.debug( \"Configuring mojo \\'\" + mojoDescriptor.getId() + \"\\' with \" + configuratorId + \" configurator -->\" ); configurator.configureComponent( mojo, configuration, expressionEvaluator, pluginRealm, listener ); logger.debug( \"-- end configuration --\" );  catch ( ComponentConfigurationException e )  String message = \"Unable to parse configuration of mojo \" + mojoDescriptor.getId(); if ( e.getFailedConfiguration()!= null )  message += \" for parameter \" + e.getFailedConfiguration().getName();  message += \": \" + e.getMessage(); throw new PluginConfigurationException( mojoDescriptor.getPluginDescriptor(), message, e );  catch ( ComponentLookupException e )  throw new PluginConfigurationException( mojoDescriptor.getPluginDescriptor(), \"Unable to retrieve component configurator \" + configuratorId + \" for configuration of mojo \" + mojoDescriptor.get', 'protected void addCall(ListObject> instruction, String instanceName, String functionName)  String functionName = functionName.getDisgracer(); if (functionName.isEmpty())  instruction.add(\"call\"); instruction.add(instanceName); instruction.add(Disgracer.disgraceMethodName(functionName));', 'private void buildData( final Data srcData, final TarOutputStream outputStream, final StringBuffer md5sum ) throws Exception  final File src = srcData.getFile(); if (!src.exists())  return;  // FIXME: merge both cases via visitor if (src.isFile())  final TarInputStream inputStream = new TarInputStream(new GZIPInputStream(new FileInputStream(src))); final MessageDigest digest = MessageDigest.getInstance(\"MD5\"); while(true)  final TarEntry entry = inputStream.getNextEntry(); if (entry == null)  break;  entry.setName(srcData.getPrefix() + stripPath(srcData.getStrip(), entry.getName())); outputStream.putNextEntry(entry); digest.reset(); copy(inputStream, new DigestOutputStream(outputStream, digest)); log(\"adding data file name:\" + entry.getName() + \" size:\" + entry.getSize() + \" mode:\" + entry.getMode() + \" linkname:\" + entry.getLinkName() + \" username:\" + entry.getUserName() + \" userid:\" + entry.getUserId() + \" groupname:\" + entry.getGroupName() + \" groupid:\" + entry.getGroupId() + \" modtime:\" + entry.getModTime() + \" md5: \" + toHex(digest.digest()) ); outputStream.closeEntry(); md5sum.append(entry.getName()).append(\" \").append(toHex(digest.digest())).append(\\'n\\');  inputStream.close();  else  final MessageDigest digest = MessageDigest.getInstance(\"MD5\"); iterate(s', 'private QueryBuilder processQueryString(Client client, String queryText)  QueryStringQueryBuilder esQuery = queryString(queryText); esQuery.lenient(true); ValidateQueryResponse response = client.admin().indices().prepareValidateQuery(SearchQueueListener.MESSAGES_TYPE).setQuery(esQuery).execute().actionGet(); if (response.isValid())  return esQuery;  else  String fixedText = queryText.replaceAll(\"((?:[)|(?:]))\", \"$1\"); logger.info(\"Rewitter \\'\\' to \\'\\'\", queryText, fixedText); QueryStringQueryBuilder fixedQuery = queryString(fixedText); fixedQuery.lenient(true); return fixedQuery;', 'public void buildProject( String basedir ) throws Exception  System.out.println( \"Building project in \" + basedir ); if (!reader.parse( new File( basedir, \"pom.xml\" ) ) )  System.err.println( \"Could not parse pom.xml\" ); System.exit( 1 );  String sources = new File( basedir, SOURCES ).getAbsolutePath(); String resources = new File( basedir, RESOURCES ).getAbsolutePath(); String classes = new File( basedir, CLASSES ).getAbsolutePath(); String testSources = new File( basedir, TEST_SOURCES ).getAbsolutePath(); String testResources = new File( basedir, TEST_RESOURCES ).getAbsolutePath(); String testClasses = new File( basedir, TEST_CLASSES ).getAbsolutePath(); String generatedSources = new File( basedir, GENERATED_SOURCES ).getAbsolutePath(); String buildDir = new File( basedir, BUILD_DIR ).getAbsolutePath(); // ---------------------------------------------------------------------------------------------- // Download deps // ---------------------------------------------------------------------------------------------------------------------------------------- if ( online )  System.out.println( \"Downloading dependencies...\" ); downloadDependencies( reader.getDependencies() );  // -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 'private HtmlTag makeRenamePageForm() throws Exception  HtmlTag form = HtmlUtil.makeFormTag(\"get\", resource); form.add(HtmlUtil.makeInputTag(\"hidden\", \"responder\", \"renamePage\")); form.add(\" New Name: \"); form.add(HtmlUtil.makeInputTag(\"text\", \"newName\", \"\")); form.add(HtmlUtil.BR); form.add(HtmlUtil.makeInputTag(\"checkbox\", \"refactorReferences\")); form.add(\" - Find all references to this page and change them accordingly (May take several minutes)\"); form.add(HtmlUtil.BR); form.add(HtmlUtil.makeInputTag(\"submit\", \"\", \"Rename Page\")); return form;', 'private void assertMechanismProperties(final MongoCredential credential)  BsonValue expected = getExpectedValue(\"credential.mechanism_properties\"); if (!expected.isNull())  BsonDocument document = expected.asDocument(); for (String key : document.keySet())  if (document.get(key).isString())  String expectedValue = document.getString(key).getValue(); // If the mechanism is \"GSSAPI\", the default SERVICE_NAME, which is stated as \"mongodb\" in the specification, // is set to null in the driver. if (credential.getMechanism().equals(\"GSSAPI\") && key.equals(\"SERVICE_NAME\") && expectedValue.equals(\"mongodb\"))  assertNull(credential.getMechanismProperty(key, null));  else  assertEquals(expectedValue, credential.getMechanismProperty(key, null));   else  assertEquals(expectedValue, credential.getMechanismProperty(key, null));   else  assertEquals(document.getBoolean(key).getValue(), credential.getMechanismProperty(key, (Boolean) null).booleanValue());', 'private PluginDescriptor verifyVersionedPlugin( Plugin plugin, MavenProject project, ArtifactRepository localRepository ) throws PluginVersionResolutionException, PluginManagerException, ArtifactNotFoundException, ArtifactResolutionException  // TODO: this might result in an artifact \"RELEASE\" being resolved continuously // FIXME: need to find out how a plugin gets marked as \\'installed\\' // and no ChildContainer exists. The check for that below fixes // the \\'Can\\'t find plexus container for plugin: xxx\\' error. if (!pluginCollector.isPluginInstalled( plugin ) || container.getChildContainer( plugin.getKey() ) == null )  try  VersionRange versionRange = VersionRange.createFromVersionSpec( plugin.getVersion() ); List remoteRepositories = new ArrayList(); remoteRepositories.addAll( project.getPluginArtifactRepositories() ); remoteRepositories.addAll( project.getRemoteArtifactRepositories() ); checkRequiredMavenVersion( plugin, localRepository, remoteRepositories ); Artifact pluginArtifact = artifactFactory.createPluginArtifact( plugin.getGroupId(), plugin.getArtifactId(), versionRange ); addPlugin( plugin, pluginArtifact, project, localRepository ); project.addPlugin( plugin );  catch ( ArtifactNotFoundException e )  String groupId = plugin.getGroupId(); String artifactId = plugin.getArtifactId(); String version = plugin.getVersion(); if ( groupId == null || artifactId == null || version == null )  throw new PluginNotFoundException( e );  else if ( groupId.equals( e.getGroupId() ) && artifactId.', 'public static KdfParameters deserialize(byte[] data) throws IOException  ByteArrayInputStream bis = new ByteArrayInputStream(data); LEDataInputStream lis = new LEDataInputStream(bis); VariantDictionary d = VariantDictionary.deserialize(lis); if (d == null)  assert(false); return null;  UUID uuid = Types.bytestoUUID(d.getByteArray(ParamUUID)); if (uuid == null)  assert(false); return null;  KdfParameters kdfP = new KdfParameters(uuid); kdfP.copyTo(d); return kdfP;', 'public void openAsync(final AsyncCompletionHandlerVoid> handler)  isTrue(\"unopened\", getChannel() == null); initializeSocketChannel(handler, new LinkedListSocketAddress>(serverAddress.getSocketAddresses()));', 'private void reportSessionCollectedValidationIssues(MavenSession mavenSession)  if (!logger.isWarnEnabled())  return; // nothing can be reported  ValidationReportLevel validationReportLevel = validationReportLevel(mavenSession.getRepositorySession()); if (validationReportLevel == ValidationReportLevel.NONE || validationReportLevel == ValidationReportLevel.INLINE)  return; // we were asked to not report anything OR reporting already happened inline  ConcurrentHashMapString, PluginValidationIssues> issuesMap = pluginIssues(mavenSession.getRepositorySession()); if (!issuesMap.isEmpty())  logger.warn(\"\"); logger.warn(\"Plugin validation issues were detected in  plugin(s)\", issuesMap.size()); logger.warn(\"\"); if (validationReportLevel == ValidationReportLevel.BRIEF)  return;  for (Map.EntryString, PluginValidationIssues> entry : issuesMap.entrySet())  logger.warn(\" * \", entry.getKey()); if (validationReportLevel == ValidationReportLevel.VERBOSE)  PluginValidationIssues issues = entry.getValue(); if (!issues.pluginDeclarations.isEmpty())  logger.warn(\" Declared at location(s):\"); for (String pluginDeclaration : issues.pluginDeclarations)  logger.warn(\" * \", pluginDeclaration);   if (!issues.pluginOccurrences.isEmpty())  logger.warn(\" Used in module(s):\"); for (String', 'public ResolutionGroup retrieve( Artifact artifact, ArtifactRepository localRepository, List remoteRepositories ) throws ArtifactMetadataRetrievalException  MavenProject p; Artifact pomArtifact; boolean done = false; do  // TODO: can we just modify the original? pomArtifact = artifactFactory.createProjectArtifact( artifact.getGroupId(), artifact.getArtifactId(), artifact.getVersion(), artifact.getScope() ); try  p = mavenProjectBuilder.buildFromRepository( pomArtifact, remoteRepositories, localRepository );  catch ( ProjectBuildingException e )  throw new ArtifactMetadataRetrievalException( \"Unable to read the metadata file\", e );  Relocation relocation = null; if ( p.getDistributionManagement()!= null )  relocation = p.getDistributionManagement().getRelocation();  if ( relocation!= null )  if ( relocation.getGroupId()!= null )  artifact.setGroupId( relocation.getGroupId() );  if ( relocation.getArtifactId()!= null )  artifact.setArtifactId( relocation.getArtifactId() );  if ( relocation.getVersion()!= null )  artifact.setVersion( relocation.getVersion() );  String message = \"n This artifact has been relocated to \" + artifact.getGroupId() + \":\" + artifact.getArtifactId() + \":\" + artifact.getVersion() + \".n\"; if ( relocation.getMessage()!= null )  message += \" \" + relocation.getMessage() + \"n\";  getLogger().warn(', 'static void encodeCoordinateReferenceSystem(final BsonWriter writer, final Geometry geometry, final EncoderContext encoderContext, final CodecRegistry registry)  BsonWriter bsonWriter = bsonWriter.getBsonWriter(); if (bsonWriter!= null)  writer.writeName(\"crs\"); if (bsonWriter!= null)  writer.writeName(\"crs\"); Codec codec = registry.get(geometry.getCoordinateReferenceSystem().getClass()); encoderContext.encodeWithChildContext(codec, writer, geometry.getCoordinateReferenceSystem());', 'public org.apache.maven.model.Plugin getByPrefix( String pluginPrefix, List groupIds, List pluginRepositories, ArtifactRepository localRepository )  // if not found, try from the remote repository if (!pluginDefinitionsByPrefix.containsKey( pluginPrefix ) )  logger.info( \"Searching repository for plugin with prefix: \\'\" + pluginPrefix + \"\\'.\" ); loadPluginMappings( groupIds, pluginRepositories, localRepository );  org.apache.maven.model.Plugin result = (org.apache.maven.model.Plugin) pluginDefinitionsByPrefix.get( pluginPrefix ); if ( result == null )  logger.debug( \"Failed to resolve plugin from prefix: \" + pluginPrefix, new Throwable() );  return result;', 'public void onCreate(Bundle icicle)  super.onCreate(icicle); if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD)  StrictModeSetup.run();  hardKeyboard = getResources().getConfiguration().keyboard == Configuration.KEYBOARD_QWERTY; clipboard = (ClipboardManager) getSystemService(CLIPBOARD_SERVICE); prefs = PreferenceManager.getDefaultSharedPreferences(this); titleBarHide = prefs.getBoolean(PreferenceConstants.TITLEBARHIDE, false); if (titleBarHide)  supportRequestWindowFeature(Window.FEATURE_ACTION_BAR_OVERLAY);  this.setContentView(R.layout.act_console); // hide status bar if requested by user if (prefs.getBoolean(PreferenceConstants.FULLSCREEN, false))  getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN, WindowManager.LayoutParams.FLAG_FULLSCREEN);  // TODO find proper way to disable volume key beep if it exists. setVolumeControlStream(AudioManager.STREAM_MUSIC); // handle requested console from incoming intent if (icicle == null)  requested = getIntent().getData();  else  requested = Uri.parse(icicle.getString(STATE_SELECTED_URI));  inflater = LayoutInflater.from(this); toolbar = (Toolbar) findViewById(R.id.toolbar); pager = (ViewPager) findViewById(R.id.console_flip); registerForContextMenu(pager); pager.addOnPageChangeListener( new ViewPager.SimpleOn', 'private List readArray(final BsonReader reader, final DecoderContext decoderContext, final ListString> path)  reader.readStartArray(); BasicDBList list = new BasicDBList(); while (reader.readBsonType()!= BsonType.END_OF_DOCUMENT)  List readValue(reader, decoderContext, null, path));  reader.readEndArray(); return list;', 'public final InputStream transform( Path pomFile, TransformerContext context ) throws IOException, org.apache.maven.model.building.TransformerException  final TransformerHandler transformerHandler = getTransformerHandler( pomFile ); final AbstractSAXFilter filter; try  filter = getSAXFilter( pomFile, context ); filter.setLexicalHandler( transformerHandler ); // By default errors are written to stderr. // Hence set custom errorHandler to reduce noice filter.setErrorHandler( new ErrorHandler()  @Override public void fatalError( SAXParseException exception ) throws SAXException  throw exception;  @Override public void error( SAXParseException exception ) throws SAXException  throw exception;  @Override public void error( SAXParseException exception ) throws SAXException  throw exception;   );  catch ( TransformerConfigurationException | ParserConfigurationException e )  throw new org.apache.maven.model.building.TransformerException( e );  final SAXSource transformSource = new SAXSource( filter, new org.xml.sax.InputSource( new FileInputStream( pomFile.toFile() ) ) ); final PipedOutputStream pout = new PipedOutputStream(); final PipedInputStream pipedInputStream = new PipedInputStream( pout ); OutputStream out = filterOutputStream( pout, pomFile ); final javax.xml.transform.Result result; if ( transformerHandler == null )  result = new StreamResult( out );  else  result = new SAXResult( transformerHandler ); ( (SAXResult) result ).setLexicalHandler( new CommentRenormalizer( transformerHandler ) ); transformerHand', 'private String figureOutURL() throws ProviderException  String url = null; WikiEngine engine = m_wikiContext.getEngine(); if( m_pageName == null )  WikiPage page = m_wikiContext.getPage(); m_pageName = page.getName();  if( m_jsp!= null )  url = engine.getURL( WikiContext.NONE, m_jsp, null, false );  else if( m_ref!= null )  int interwikipoint; if( JSPWikiMarkupParser.isExternalLink(m_ref) )  url = m_ref;  else if( (interwikipoint = m_ref.indexOf(\":\"))!= -1 )  String extWiki = m_ref.substring( 0, interwikipoint ); String wikiPage = m_ref.substring( interwikipoint+1 ); url = engine.getInterWikiURL( extWiki ); if( url!= null )  url = TextUtil.replaceString( url, \"%s\", wikiPage );   else if( m_ref.startsWith(\"#\") )  // Local link  else if( TextUtil.isNumber(m_ref) )  // Reference  else  int hashMark = -1; String parms = (m_version!= null)? \"version=\"+getVersion() : null; // // Internal wiki link, but is it an attachment link? // WikiPage p = engine.getPage( m_pageName ); if( p instanceof Attachment )  url = m_wikiContext.getURL( WikiContext.ATTACH, m_pageName );  else if( (hash', 'public void close()  if (asNIO()!= null)  release(asNIO()); release(asNIO()); super.close();', 'public void testShouldMergePluginConfigurationSubItemsWithMergeAttributeSet() throws XmlPullParserException, IOException  String parentConfigStr = \"configuration>items>item>one/item>item>two/item>/items>/configuration>\"; Xpp3Dom parentConfig = Xpp3DomBuilder.build( new StringReader( parentConfigStr ) ); Plugin parentPlugin = createPlugin( \"group\", \"artifact\", \"1\", null ); parentPlugin.setConfiguration( parentConfig ); String childConfigStr = \"configuration>items combine.children=\"append\">item>three/item>/items>/configuration>\"; Xpp3Dom childConfig = Xpp3DomBuilder.build( new StringReader( childConfigStr ) ); Plugin childPlugin = createPlugin( \"group\", \"artifact\", \"1\", null ); childPlugin.setConfiguration( childConfig ); ModelUtils.mergePluginDefinitions( childPlugin, parentPlugin, true ); Xpp3Dom result = (Xpp3Dom) childPlugin.getConfiguration(); Xpp3Dom items = result.getChild( \"items\" ); assertEquals( 3, items.getChildCount() ); Xpp3Dom[] item = items.getChildren(); assertEquals( \"one\", item[0].getValue() ); assertEquals( \"two\", item[1].getValue() ); assertEquals( \"three\", item[2].getValue() );', 'public static int primaryReplica(ListNode> nodes, Range range)  for (int i = 0; i  nodes.size(); i++)  if (range.end!= Long.MIN_VALUE && nodes.get(i).token() >= range.end) return i;  return -1;', 'protected void addPlugin( Plugin plugin, Artifact pluginArtifact, MavenProject project, ArtifactRepository localRepository ) throws PluginManagerException, InvalidPluginException  PlexusContainer child; try  child = container.createChildContainer( plugin.getKey(), Collections.singletonList( pluginArtifact.getFile() ), Collections.EMPTY_MAP, Collections.singletonList( pluginCollector ) );  catch ( PlexusContainerException e )  throw new PluginManagerException( \"Failed to create plugin container for plugin \\'\" + plugin + \"\\': \" + e.getMessage(), e );  // this plugin\\'s descriptor should have been discovered in the child creation, so we should be able to // circle around and set the artifacts and class realm PluginDescriptor addedPlugin = pluginCollector.getPluginDescriptor( plugin ); addedPlugin.setClassRealm( child.getContainerRealm() ); // we\\'re only setting the plugin\\'s artifact itself as the artifact list, to allow it to be retrieved // later when the plugin is first invoked. Retrieving this artifact will in turn allow us to // transitively resolve its dependencies, and add them to the plugin container... addedPlugin.setArtifacts( Collections.singletonList( pluginArtifact ) ); try  Set artifacts = MavenMetadataSource.createArtifacts( artifactFactory, plugin.getDependencies(), null, null, project ); addedPlugin.setIntroducedDependencyArtifacts( artifacts );  catch ( InvalidDependencyVersionException e )  throw new InvalidPluginException( \"Plugin \\'\" + plugin + \"\\' is invalid: \" + e.getMessage(), e );', 'public void keyPressed(KeyEvent e)  if (e.getKeyCode() == KeyEvent.VK_SHIFT)  isShiftPressed = true;  else if (e.getKeyCode() == KeyEvent.VK_SPACE)  isPanningView = true; setCursor(panCursor);', 'private static String readString(String filename)  try  String sourceAsStream = VoldemortTestConstants.class.getResourceAsStream(filename); return IOUtils.toString(sourceAsStream);  catch(IOException e)  throw new RuntimeException(e);', 'public List decode(final BSONReader reader)  reader.readStartArray(); List list = new ArrayList(); while (reader.readBSONType()!= BSONType.END_OF_DOCUMENT)  List decoder = register.get(bsonTypeClassMap.get(reader.getCurrentBSONType()); list.add(decoder);  reader.readEndArray(); return list;', 'private static T> T newInstanceFromPublicConstructor( final ClassT> cls, final javolution.xml.XMLFormat.InputElement xml ) throws XMLStreamException  try  final Constructor?>[] constructors = cls.getConstructors(); for ( final Constructor?> constructor : constructors )  final Class?>[] parameterTypes = constructor.getParameterTypes(); if ( parameterTypes.length == 0 )  return (T) constructor.newInstance();  else if ( parameterTypes.length == 1 && parameterTypes[0] == int.class )  return (T) constructor.newInstance( xml.getAttribute( SIZE ).toInt() );   if ( LOG.isDebugEnabled() && constructors.length > 0 )  LOG.debug( \"No suitable constructor found for map \" + cls + \", available constructors:n\" + Arrays.asList( constructors ) );   catch ( final SecurityException e )  // ignore  catch ( final IllegalArgumentException e )  throw new XMLStreamException( e ); // not expected  catch ( final InstantiationException e )  throw new XMLStreamException( e ); // not expected  catch ( final IllegalAccessException e )  throw new XMLStreamException( e ); // not expected  catch ( final InvocationTargetException e )  // ignore - constructor threw exception LOG.info( \"Tried to invoke int constructor on \" + cls.getName() + \", this threw an exception.\", e.getTargetException() );  return null;', 'protected Document luceneIndexPage( final WikiPage page, final String text, final IndexWriter writer ) throws IOException  if( log.isDebugEnabled() )  log.debug( \"Indexing \"+page.getName()+\"...\" );  // make a new, empty document final Document doc = new Document(); if( text == null )  return doc;  // Raw name is the keyword we\\'ll use to refer to this document for updates. Field field = new Field( LUCENE_ID, page.getName(), StringField.TYPE_STORED ); doc.add( field ); // Body text. It is stored in the doc for search contexts. field = new Field( LUCENE_PAGE_CONTENTS, text, TextField.TYPE_STORED ); doc.add( field ); // Allow searching by page name. Both beautified and raw final String unTokenizedTitle = StringUtils.replaceChars( page.getName(), MarkupParser.PUNCTUATION_CHARS_ALLOWED, c_punctuationSpaces ); field = new Field( LUCENE_PAGE_NAME, TextUtil.beautifyString( page.getName() ) + \" \" + unTokenizedTitle, TextField.TYPE_STORED ); doc.add( field ); // Allow searching by authorname if( page.getAuthor()!= null )  field = new Field( LUCENE_AUTHOR, page.getAuthor(), TextField.TYPE_STORED ); doc.add( field );  // Now add the names of the attachments of this page try  final List Attachment > attachments = m_engine.getAttachmentManager().listAttachments(page); String attachmentNames = \"\"; for( final Attachment att : attachments )  attachmentNames += att.getName() + \";\";  field = new Field( L', 'public void deleteHost(HostBean host)  if (host.getId()  0)  return;  mDb.beginTransaction(); try  mDb.delete(TABLE_HOSTS, \"_id =?\", new String[] String.valueOf(host.getId())); mDb.setTransactionSuccessful();  finally  mDb.endTransaction();', 'private String[] buildArgs()  List list = new LinkedList(); if(args!= null &&!args.equals(\"\"))  String[] startingArg = args.split(\" \"); for(int i = 0; i  startingArg.length; i++) list.add(startingArg[i]);  list.add(\"localhost\"); list.add(FitnesseFixtureContext.context.port + \"\"); list.add(pageName); return (String[])list.toArray(new String[]);', 'public void intercept(InterceptorStack invocation, ResourceMethod method, Object resourceInstance) throws IOException, InterceptionException  try  Method reflectionMethod = method.getMethod(); Object[] parameters = this.parameters.getValues(); reflectionMethod.invoke(resourceInstance, parameters);  catch (ValidationError e)  // finished just fine  catch (IllegalArgumentException e)  throw new InterceptionException(e);  catch (IllegalAccessException e)  throw new InterceptionException(e);  catch (InvocationTargetException e)  throw new InterceptionException(e.getCause());', 'private HtmlTag makeNoForm()  HtmlTag noForm = HtmlUtil.makeFormTag(\"get\", \"/\" + resource); noForm.add(HtmlUtil.makeInputTag(\"submit\", \"\", \"No\")); return noForm;', 'public int wikiWordPathLength(String text)  String candidate = text + \".\"; int offset = \">.\".indexOf(candidate.substring(0, 1)) >= 0? 1 : 0; while (offset  candidate.length())  int dot = candidate.indexOf(\".\", offset); int word = wikiWordLength(candidate.substring(offset, dot)); if (word == 0) return offset > 1? offset - 1 : 0; if (offset + word  dot) return offset + word; offset = dot + 1;  return text.length();', 'private static String retrieveLocalRepo()  String repo = System.getProperty( \"maven.repo.local\" ); if ( repo == null )  try  File mavenPropertiesFile = new File( System.getProperty( \"user.home\" ), \".m2/maven.properties\" ); Properties mavenProperties = new Properties(); mavenProperties.load( new FileInputStream( mavenPropertiesFile ) ); repo = mavenProperties.getProperty( \"maven.repo.local\" );  catch ( Exception e )  System.err.println( \"WARNING: failed to parse user pom (ignoring): \" + e.getMessage() );   if ( repo == null )  repo = System.getProperty( \"user.home\" ) + \"/.m2/repository\";  return repo;', 'private void initialize( Properties props ) throws WikiException  m_startTime = new Date(); m_properties = props; // // Initialized log4j. However, make sure that // we don\\'t initialize it multiple times. Also, if // all of the log4j statements have been removed from // the property file, we do not do any property setting // either.q // if(!c_configured )  if( props.getProperty(\"log4j.rootCategory\")!= null )  PropertyConfigurator.configure( props );  c_configured = true;  log.info(\"**************************************\"); log.info(Release.APPNAME+\" \"+Release.getVersionString()+\" starting. Whee!\"); fireEvent( WikiEngineEvent.INITIALIZING ); // begin initialization log.debug(\"Java version: \"+System.getProperty(\"java.runtime.version\")); log.debug(\"Java vendor: \"+System.getProperty(\"java.vm.vendor\")); log.debug(\"OS: \"+System.getProperty(\"os.name\")+\" \"+System.getProperty(\"os.version\")+\" \"+System.getProperty(\"os.arch\")); log.debug(\"Default server locale: \"+Locale.getDefault()); log.debug(\"Default server timezone: \"+TimeZone.getDefault().getDisplayName(true, TimeZone.LONG)); if( m_servletContext!= null )  log.info(\"Servlet container: \"+m_servletContext.getServerInfo() ); if( m_servletContext.getMajorVersion() == 2 && m_servletContext.getMin', 'protected ListSlimAssertion> assertionsFromScenario(int row) throws TestExecutionException  int lastCol = table.getColumnCountInRow(row) - 1; String actionName = getActionNameStartingAt(0, lastCol, row); ScenarioTable scenario = getTestContext().getScenario(Disgracer.disgraceClassName( actionName.replace(SEQUENTIAL_ARGUMENT_PROCESSING_SUFFIX, \"\"))); ListSlimAssertion> assertions = new ArrayList>(); if (scenario!= null)  scenario.setCustomComparatorRegistry(customComparatorRegistry); String[] args = getArgumentsStartingAt(1, lastCol, row, assertions); assertions.addAll(scenario.call(args, this, row));  else if (lastCol == 0)  String firstNameCell = table.getCellContents(0, row); ScenarioTable s = getTestContext().getScenarioByPatternMatching(firstNameCell, customComparatorRegistry); if (s!= null)  String[] args = s.matchParameters(firstNameCell); assertions.addAll(s.call(args, this, row));   return assertions;', 'void logError(Throwable e, BenchmarkDefinition.Operation op)  String message = name + \".\" + op.m.getName() + \": \" + e.getMessage(); if (inRamp)  message += \"nNote: Error not counted in result.\" + \"nEither transaction start or end time is not \" + \"within steady state.\";  Level level; if (e instanceof ExpectedException)  level = Level.FINER;  else  level = Level.WARNING;  logger.log(level, message, e);', 'public static void start(Context ctx)  ctx.startService(new Intent(ctx, TimeoutService.class)); long triggerTime = System.currentTimeMillis() + DEFAULT_TIMEOUT; AlarmManager am = (AlarmManager) ctx.getSystemService(Context.ALARM_SERVICE); Log.d(TAG, \"Timeout start\"); am.set(AlarmManager.RTC, triggerTime, buildIntent(ctx));', 'public Object[] getActualValues(boolean nullsOk, int start, int stop) throws CouldNotGenerateValueException  Object[] values= new Object[stop - start]; for (int i= start; i  stop; i++)  values[i]= fAssigned.get(i).getValue(); if (values[i] == null &&!nullsOk) throw new CouldNotGenerateValueException();  return values;', 'private void publishWithoutWorryingAboutNewSubscriptions(Transaction transaction)  Turn myTurn = nextTurn(); Date executionTime = realTime(myTurn); //TODO realTime() and approve in the same turn. long systemVersion = approve(transaction, executionTime, myTurn); _journal.append(transaction, executionTime, myTurn); notifySubscribers(transaction, systemVersion, executionTime, myTurn);', 'private MapString, ArtifactRepository> getVersions( RepositorySystemSession session, VersionRangeResult result, VersionRangeRequest request )  MapString, ArtifactRepository> versionIndex = new HashMapString, ArtifactRepository>(); Metadata metadata = new DefaultMetadata( request.getArtifact().getGroupId(), request.getArtifact().getArtifactId(), MAVEN_METADATA_XML, Metadata.Nature.RELEASE_OR_SNAPSHOT ); ListMetadataRequest> metadataRequests = new ArrayListMetadataRequest>( request.getRepositories().size() ); for ( RemoteRepository repository : request.getRepositories() )  MetadataRequest metadataRequest = new MetadataRequest( metadata, repository, request.getRequestContext() ); metadataRequest.setDeleteLocalCopyIfMissing( true ); metadataRequests.add( metadataRequest );  ListMetadataResult> metadataResults = metadataResolver.resolveMetadata( session, metadataRequests ); WorkspaceReader workspace = session.getWorkspaceReader(); if ( workspace!= null )  ListString> versions = workspace.findVersions( request.getArtifact() ); for ( String version : versions )  versionIndex.put( version, workspace.getRepository() );   LocalRepositoryManager lrm = session.getLocalRepositoryManager(); File localMetadataFile = new File( lrm.getRepository().getBasedir(), lrm.getPathForLocalMetadata( metadata ) ); if ( localMetadataFile.isFile() )  metadata = metadata.setFile( localMetadataFile ); Versioning versioning = readVersions( session, metadata, result ); for ( String version : versioning.getVersions', 'public static Dialog getSelectAccountDialog(Context context, int resId, DialogInterface.OnClickListener onClickListener, DialogInterface.OnCancelListener onCancelListener)  final AccountTypeManager accountTypes = AccountTypeManager.getInstance(context); final ListAccountWithDataSet> writableAccountList = accountTypes.getAccounts(true); Log.i(LOG_TAG, \"The number of available accounts: \" + writableAccountList.size()); // Assume accountList.size() > 1 // Wrap our context to inflate list items using correct theme final Context dialogContext = new ContextThemeWrapper( context, android.R.style.Theme_Light); final LayoutInflater dialogInflater = (LayoutInflater)dialogContext.getSystemService(Context.LAYOUT_INFLATER_SERVICE); final ArrayAdapterAccountWithDataSet> accountAdapter = new ArrayAdapterAccountWithDataSet>( context, R.layout.account_selector_list_item_condensed, parent, false);  @Override public View getView(int position, View convertView, ViewGroup parent)  if (convertView == null)  convertView = dialogInflater.inflate( R.layout.account_selector_list_item_condensed, parent, false);  final TextView text1 = (TextView) convertView.findViewById(android.R.id.text1); final TextView text2 = (TextView) convertView.findViewById(android.R.id.text2); final ImageView icon = (ImageView) convertView.findViewById(android.R.id.icon); final AccountWithDataSet account = this.getItem(position); final AccountType accountType = accountTypes.getAccountType( account.type, account.dataSet); final Context context = get', 'public static void main(String args[]) throws IOException  OptionParser parser = new OptionParser(); parser.accepts(\"help\", \"print help information\"); parser.accepts(\"cluster-xml\", \"[REQUIRED] Path to cluster-xml\").withRequiredArg().describedAs(\"xml\").ofType(String.class); parser.accepts(\"stores-xml\", \"[REQUIRED] Path to stores-xml\").withRequiredArg().describedAs(\"xml\").ofType(String.class); parser.accepts(\"output-dir\", \"[REQUIRED] The output directory where we\\'ll store / retrieve the keys\").withRequiredArg().describedAs(\"output-dir\").ofType(String.class); parser.accepts(\"op\", \"Operation type (0 - gets keys [default], 1 - checks the keys\").withRequiredArg().describedAs(\"op\").ofType(Integer.class); parser.accepts(\"num-keys\", \"Number of keys per store [ Default: 100 ]\").withRequiredArg().describedAs(\"keys\").ofType(Long.class); OptionSet options = parser.parse(args); if(options.has(\"help\"))  parser.printHelpOn(System.out); System.exit(0);  SetString> missing = CmdUtils.missing(options, \"cluster-xml\", \"stores-xml\", \"output-dir\"); if(missing.size() > 0)  System.err.println(\"Missing required arguments: \" + Joiner.on(\", \").join(missing)); parser.printHelpOn(System.err); System.exit', 'public ModelAndView listDel( ServletRequest request, @RequestParam int id ) throws Exception  Template tmpl = Template.getTemplate(request); if (!tmpl.isSessionAuthorized())  throw new AccessViolationException(\"Not authorized\");  Connection db = null; try  db = LorDataSource.getConnection(); db.setAutoCommit(false); User user = tmpl.getCurrentUser(); user.checkAnonymous(); IgnoreList ignoreList = new IgnoreList(db, user.getId()); if (!ignoreList.remove(db, id))  throw new BadInputException(\"122€121 12 o\");  db.commit(); return new ModelAndView(\"ignore-list\", \"ignoreList\", ignoreList.getIgnoreList());  finally  if (db!=null)  db.close();', 'public void testQueuingStats() throws Exception  QueueTestResource> resources = new LinkedListTestResource>(); QueueTestResourceRequest> resourceRequests = new LinkedListTestResourceRequest>(); long deadlineNs = System.nanoTime() + TimeUnit.MILLISECONDS.toNanos(KeyedResourcePoolTest.TIMEOUT_MS); for(int i = 0; i  POOL_SIZE * 10001; i++)  resourceRequests.add(new TestResourceRequest(deadlineNs, resources));  assertEquals(0, this.factory.getCreated()); assertEquals(0, this.queuedPool.getTotalResourceCount()); assertEquals(0, this.queuedPool.getCheckedInResourceCount()); assertEquals(0, this.queuedPool.getRegisteredResourceRequestCount()); assertEquals(0, resources.size()); // Submit initial POOL_SIZE requests for(int i = 0; i  POOL_SIZE; i++)  this.queuedPool.registerResourceRequest(\"a\", resourceRequests.poll());  // Confirm initial requests were handled in nonblocking manner assertEquals(POOL_SIZE, this.factory.getCreated()); assertEquals(POOL_SIZE, this.queuedPool.getTotalResourceCount()); assertEquals(0, this.queuedPool.getCheckedInResourceCount()); assertEquals(0, this.queuedPool.getRegisteredResourceRequestCount()); assertEquals(0, resources.size()); // Register five order of magnitude more resource requests. for(int i = 0;', 'private void putRemoteFile( ArtifactRepository repository, File source, String remotePath, TransferListener downloadMonitor ) throws TransferFailedException  String protocol = repository.getProtocol(); Wagon wagon; try  wagon = getWagon( protocol );  catch ( UnsupportedProtocolException e )  throw new TransferFailedException( \"Unsupported Protocol: \", e );  if ( downloadMonitor!= null )  wagon.addTransferListener( downloadMonitor );  Map checksums = new HashMap( 2 ); Map sums = new HashMap( 2 ); // TODO: configure these on the repository try  ChecksumObserver checksumObserver = new ChecksumObserver( \"MD5\" ); wagon.addTransferListener( checksumObserver ); checksums.put( \"md5\", checksumObserver ); checksumObserver = new ChecksumObserver( \"SHA-1\" ); wagon.addTransferListener( checksumObserver ); checksums.put( \"sha1\", checksumObserver );  catch ( NoSuchAlgorithmException e )  throw new TransferFailedException( \"Unable to add checksum methods\", e );  try  wagon.connect( new Repository( repository.getId(), repository.getUrl() ), getAuthenticationInfo( repository.getId() ), getProxy( protocol ) ); wagon.put( source, remotePath ); wagon.removeTransferListener( downloadMonitor ); // Pre-store the checksums as any future puts will overwrite them for ( Iterator i = checksums.keySet().iterator(); i.hasNext(); )  String extension = (String) i.next(); ChecksumObserver observer = (ChecksumObserver) checksums.get( extension ); sums.put( extension, observer.getActualChecksum() );  // We do this in here so we', 'public void init()  requestWindowFeature(Window.FEATURE_NO_TITLE); setContentView(R.layout.music_library); mNowPlayingView = findViewById(R.id.nowplaying); mTitle = (TextView) mNowPlayingView.findViewById(R.id.title); mArtist = (TextView) mNowPlayingView.findViewById(R.id.artist); findViewById(R.id.browse_button).setOnClickListener(this); findViewById(R.id.albums_button).setOnClickListener(this); findViewById(R.id.tracks_button).setOnClickListener(this); findViewById(R.id.playlists_button).setOnClickListener(this);', 'public void execute(Pipeline pipeline)  MapNode, ListByteArray>> nodeToKeysMap = Maps.newHashMap(); MapByteArray, ListNode>> keyToExtraNodesMap = Maps.newHashMap(); for(ByteArray key: keys)  ListNode> nodes = null; ListNode> originalNodes = null; try  originalNodes = getNodes(key);  catch(VoldemortException e)  pipelineData.setFatalError(e); pipeline.addEvent(Event.ERROR); return;  ListNode> preferredNodes = Lists.newArrayListWithCapacity(preferred); ListNode> extraNodes = Lists.newArrayListWithCapacity(3); if(zoneAffinity!= null && zoneAffinity.isGetAllOpZoneAffinityEnabled())  nodes = new ArrayListNode>(); for(Node node: originalNodes)  if(node.getZoneId() == clientZone.getId())  nodes.add(node);    else  nodes = originalNodes;  if(pipelineData.getZonesRequired()!= null)  if(pipelineData.getZonesRequired() > this.clientZone.getProximityList().size())  throw new VoldemortException(\"Number of zones required should be less than the total number of zones\");  if(pipelineData.getZonesRequired() > required)  throw new VoldemortException(\"Number of zones required should be less than the required number of \" + pipeline.getOperation().getSimpleName() + \"s\");  // Create zone id to node mapping MapInteger, ListNode>> zoneIdToNode = new Has', 'public void testSessionsRefCountHandlingIssue111() throws Exception  _service.setSticky(false); _service.setLockingMode(LockingMode.ALL.name()); final TranscoderService transcoderService = new TranscoderService(new JavaSerializationTranscoder()); _service.setTranscoderService( transcoderService ); _service.setMemcachedClient(_memcachedMock); _service.startInternal(); @SuppressWarnings(\"unchecked\") final OperationFutureBoolean> addResultMock = mock(OperationFuture.class); when(addResultMock.get()).thenReturn(true); when(addResultMock.get(anyLong(), any(TimeUnit.class))).thenReturn(true); when(_memcachedMock.add(anyString(), anyInt(), any(TimeUnit.class))).thenReturn(addResultMock); final MemcachedBackupSession session = createSession( _service ); // the session is now already added to the internal session map assertNotNull(session.getId()); FutureBackupResult> result = _service.backupSession(session.getId(), false, null); assertFalse(_service.getManager().getSessionsInternal().containsKey(session.getId())); // start another request that loads the session from mc _service.getLockingStrategy().onRequestStart(mock(Request.class)); when(_memcachedMock.get(eq(session.getId()))).thenReturn(transcoderService.serialize(session)); final MemcachedBackupSession session2 = _service.findSession(session.getId()); assertTrue(session2.isL', 'public void updateRemoteStoreDefList(int nodeId, ListStoreDefinition> storesList) throws VoldemortException  // get current version. VectorClock oldClock = (VectorClock) metadataMgmtOps.getRemoteStoreDefList(nodeId).getVersion(); MetadataStore.STORES_KEY storeName = new VersionedString>(storeMapper.writeStoreList(storesList), oldClock.incremented(nodeId, 1)));', 'public void onCreate(Bundle icicle)  super.onCreate(icicle); final Intent intent = getIntent(); final String action = intent.getAction(); if (Intent.ACTION_CREATE_SHORTCUT.equals(action))  mCreateShortcut = true;  requestWindowFeature(Window.FEATURE_INDETERMINATE_PROGRESS); requestWindowFeature(Window.FEATURE_NO_TITLE); setVolumeControlStream(AudioManager.STREAM_MUSIC); mToken = MusicUtils.bindToService(this, new ServiceConnection()  public void onServiceConnected(ComponentName classname, IBinder obj)  if (Intent.ACTION_VIEW.equals(action))  long id = Long.parseLong(intent.getExtras().getString(\"playlist\")); if (id == RECENTLY_ADDED_PLAYLIST)  playRecentlyAdded();  else if (id == PODCASTS_PLAYLIST)  playPodcasts();  else if (id == ALL_SONGS_PLAYLIST)  long [] list = MusicUtils.getAllSongs(PlaylistBrowserActivity.this); if (list!= null)  MusicUtils.playAll(PlaylistBrowserActivity.this, list, 0);   else  MusicUtils.playPlaylist(PlaylistBrowserActivity.this, id);  finish(); return;  MusicUtils.updateNowPlaying(PlaylistBrowserActivity.this);  public void onServiceDisconnected(ComponentName classname)    ); IntentFilter f = new IntentFilter(); f.addAction(Intent.ACTION_MEDIA_SCANNER_STARTED);', 'public AsyncReadOperationBsonDocument> asExplainableOperationAsync(final ExplainVerbosity explainVerbosity)  notNull(\"explainVerbosity\", explainVerbosity); return new AsyncReadOperationBsonDocument>()  @Override public void executeAsync(final AsyncReadBinding binding, final SingleResultCallbackBsonDocument> callback)  withConnection(binding, new AsyncCallableWithConnectionAndSource()  @Override public void call(final AsyncConnectionSource connectionSource, final AsyncConnection connection, final Throwable t)  SingleResultCallbackBsonDocument> errHandlingCallback = null; if (t!= null)  callback.onResult(null, t);  else  AsyncReadBinding singleConnectionReadBinding = new AsyncSingleConnectionReadBinding(binding.getReadPreference(), connectionSource.getServerDescription(), connection); if (serverIsAtLeastVersionThreeDotTwo(connection.getDescription()))  new CommandReadOperationBsonDocument>(namespace.getDatabaseName(), new BsonDocument(\"explain\", asCommandDocument()), new BsonDocumentCodec()).executeAsync(singleConnectionReadBinding, releasingCallback(exceptionTransformingCallback(errorHandlingCallback(callback)), singleConnectionReadBinding, connectionSource, connection));  else  createExplainableQueryOperation().executeAsync(singleConnectionReadBinding, releasingCallback(errorHandlingCallback(new ExplainResultCallback(callback)', 'public void initialize( WikiEngine engine, Properties properties ) throws NoRequiredPropertyException, IOException  log.debug(\"Initing CachingProvider\"); // engine is used for getting the search engine m_engine = engine; if (m_cacheManager.cacheExists(CACHE_NAME))  m_cache = m_cacheManager.getCache(CACHE_NAME);  else  log.info(\"cache with name \" + CACHE_NAME + \" not found in ehcache.xml, creating it with defaults.\"); m_cache = new Cache(CACHE_NAME, DEFAULT_CACHECAPACITY, false, false, 0, 0); m_cacheManager.addCache(m_cache);  if (m_cacheManager.cacheExists(TEXTCACHE_NAME))  m_textCache= m_cacheManager.getCache(TEXTCACHE_NAME);  else  log.info(\"cache with name \" + TEXTCACHE_NAME + \" not found in ehcache.xml, creating it with defaults.\"); m_textCache = new Cache(TEXTCACHE_NAME, DEFAULT_CACHECAPACITY, false, false, 0, 0); m_cacheManager.addCache(m_textCache);  if (m_cacheManager.cacheExists(HISTORYCACHE_NAME))  m_historyCache= m_cacheManager.getCache(HISTORYCACHE_NAME);  else  log.info(\"cache with name \" + HISTORYCACHE_NAME + \" not found in ehcache.xml, creating it with defaults.\"); m_historyCache = new Cache(HISTORYCACHE_NAME, DEFAULT_CACHECAPACITY, false, false, 0, 0); m_', 'public Cursor query(Uri uri, String[] projection, String selection, String[] selectionArgs, String sortOrder)  final int callerUid = Binder.getCallingUid(); final boolean accessRestricted = ProviderUtil.isAccessRestricted( getContext(), callerUid); // If access is restricted, we don\\'t allow subqueries in the query. Log.v(TAG, \"accessRestricted=\" + accessRestricted); if (accessRestricted)  SqlQueryChecker.checkQueryParametersForSubqueries(projection, selection, sortOrder);  final String pduTable = getPduTable(accessRestricted); SQLiteQueryBuilder qb = new SQLiteQueryBuilder(); // Generate the body of the query. int match = sURLMatcher.match(uri); if (LOCAL_LOGV)  Log.v(TAG, \"Query uri=\" + uri + \", match=\" + match);  switch (match)  case MMS_ALL: constructQueryForBox(qb, Mms.MESSAGE_BOX_ALL, pduTable); break; case MMS_INBOX: constructQueryForBox(qb, Mms.MESSAGE_BOX_INBOX, pduTable); break; case MMS_SENT: constructQueryForBox(qb, Mms.MESSAGE_BOX_SENT, pduTable); break; case MMS_DRAFTS: constructQueryForBox(qb, Mms.MESSAGE_BOX_DRAFTS, pduTable); break; case MMS_OUTBOX: constructQueryForBox(qb, Mms.MESSAGE', 'protected ListServerDescription> chooseForReplicaSet(final ClusterDescription clusterDescription)  ListServerDescription> selectedServers = Collections.emptyList(); if (getTagSetList().isEmpty())  ClusterDescription clusterDescription = clusterDescription.getSecondaries(); if (!servers.isEmpty())  ClusterDescription clusterDescription = collections.emptyList(); selectedServers = clusterDescription.getSecondaries();  else  for (final TagSet tagSet : getTagSetList())  ListServerDescription> servers = clusterDescription.getSecondaries(tagSet); if (!servers.isEmpty())  selectedServers = servers; break;    return selectedServers;', 'private static void writeKeysAscii(IteratorByteArray> keyIterator, File outputFile, StoreDefinition storeDefinition) throws IOException  BufferedWriter writer = null; CompressionStrategy keysCompressionStrategy = null; FileWriter fileWriter = null; if(outputFile!= null)  writer = new BufferedWriter(new FileWriter(outputFile));  else  writer = new BufferedWriter(new OutputStreamWriter(System.out));  if(storeDefinition.getKeySerializer().hasCompression())  keysCompressionStrategy = new CompressionStrategyFactory().get(storeDefinition.getKeySerializer().getCompression());  SerializerFactory serializerFactory = new DefaultSerializerFactory(); StringWriter stringWriter = new StringWriter(); JsonGenerator generator = new JsonFactory(new ObjectMapper()).createJsonGenerator(stringWriter); @SuppressWarnings(\"unchecked\") SerializerObject> serializer = (SerializerObject>) serializerFactory.getSerializer(storeDefinition.getKeySerializer()); try  while(keyIterator.hasNext())  // Ugly hack to be able to separate text by newlines vs. spaces byte[] keyBytes = keyIterator.next().get(); Object keyObject = serializer.toObject((null == keysCompressionStrategy)? keyBytes : keysCompressionStrategy.inflate(keyBytes)); generator.writeObject(keyObject); StringBuffer buf = stringWriter.getBuffer(); if(buf.charAt(0) ==\\'\\')  buf.setCharAt(0, \\'n\\'', 'protected void runPages(ListWikiPage>pages, final RunNotifier notifier)  MultipleTestsRunner testRunner = createTestRunner(pages); addTestSystemListeners(notifier, testRunner, suiteClass); addExecutionLogListener(notifier, testRunner, suiteClass); System.setProperty(SystemExitSecurityManager.PREVENT_SYSTEM_EXIT, String.valueOf(preventSystemExit)); try  executeTests(testRunner);  catch (AssertionError | Exception e)  notifier.fireTestFailure(new Failure(Description.createSuiteDescription(suiteClass), e));', 'private long refreshNow()  if(mService == null) return 500; try  long pos = mPosOverride  0? mService.position() : mPosOverride; long remaining = 1000 - (pos % 1000); if ((pos >= 0) && (mDuration > 0))  mCurrentTime.setText(MusicUtils.makeTimeString(this, pos / 1000)); if (mService.isPlaying())  mCurrentTime.setVisibility(View.VISIBLE);  else  // blink the counter int vis = mCurrentTime.getVisibility(); mCurrentTime.setVisibility(vis == View.INVISIBLE? View.VISIBLE : View.INVISIBLE); remaining = 500;  mProgress.setProgress((int) (1000 * pos / mDuration));  else  mCurrentTime.setText(\"--:--\"); mProgress.setProgress(1000);  // return the number of milliseconds until the next full second, so // the counter can be updated at just the right time return remaining;  catch (RemoteException ex)   return 500;', 'public ModelAndView forum( HttpServletRequest request, TopicListRequest topicListForm, HttpServletResponse response ) throws Exception  topicListForm.setSection(Section.SECTION_FORUM); ModelAndView modelAndView = mainTopicsFeedHandler(request, topicListForm, response, null); modelAndView.addObject(\"ptitle\", calculatePTitle(sectionService.getSection(Section.SECTION_FORUM), topicListForm)); modelAndView.addObject(\"url\", \"/forum/lenta\"); modelAndView.addObject(\"rssLink\", \"section-rss.jsp?section=2\"); return modelAndView;', 'protected boolean handleResponseError(Exception e, Node node, long requestTime, Pipeline pipeline, FailureDetector failureDetector)  if(e instanceof StoreTimeoutException || e instanceof ObsoleteVersionException || e instanceof UnreachableStoreException)  // Quietly mask all errors that are \"expected\" regularly. if(logger.isEnabledFor(Level.DEBUG))  logger.debug(\"Error in \" + pipeline.getOperation().getSimpleName() + \" on node \" + node.getId() + \" (\" + node.getHost() + \") : \" + e.getMessage());   else  if(logger.isEnabledFor(Level.WARN))  logger.warn(\"Error in \" + pipeline.getOperation().getSimpleName() + \" on node \" + node.getId() + \" (\" + node.getHost() + \")\", e);   if(e instanceof UnreachableStoreException)  if(logger.isTraceEnabled())  logger.trace(\"Adding node [\" + node + \"] to failed nodes list\");  pipelineData.addFailedNode(node); pipelineData.recordFailure(e); failureDetector.recordException(node, requestTime, (UnreachableStoreException) e);  else if(e instanceof VoldemortApplicationException)  pipelineData.setFatalError((VoldemortApplicationException) e); pipeline.abort(); if(logger.isEnabledFor(Level.TRACE)) logger.trace(\"Error is terminal - aborting further pipeline processing\"); return true;  else  pipelineData.recordFailure(e);  return false;', 'protected void createFeed(SyndFeed feed, Map model)  @SuppressWarnings(\"unchecked\") ListShowRepliesController.MyTopicsListItem> list = (ListShowRepliesController.MyTopicsListItem>) model.get(\"topicsList\"); String s = \"2 12° o34141412°€   34»342°»? \" + String.valueOf(model.get(\"nick\")); feed.setTitle(s); feed.setLink(\"http://www.linux.org.ru\"); feed.setDescription(s); Date lastModified = new Date(); if (!list.isEmpty())  Timestamp timestamp = list.get(0).getLastmod(); lastModified = new Date(timestamp.getTime());  feed.setPublishedDate(lastModified); ListSyndEntry> entries = new ArrayListSyndEntry>(); feed.setEntries(entries); for (ShowRepliesController.MyTopicsListItem item : list)  SyndEntry feedEntry = new SyndEntryImpl(); feedEntry.setPublishedDate(new Date(item.getCommentDate().getTime())); feedEntry.setTitle(item.getSubj()); feedEntry.setAuthor(String.valueOf(item.getNick())); feedEntry.setLink(String.format(\"http://www.linux.org.ru/jump-message.jsp?msgid=%s&cid=%s\", String.valueOf(item.getMsgid()), String.valueOf(item.getCid()))); if (item.getMessageText()!', 'protected void invokeTestMethod(Method method, RunNotifier notifier)  Object test; try  test= createTest();  catch (InvocationTargetException e)  notifier.testAborted(methodDescription(method), e.getCause()); return;  catch (Exception e)  notifier.testAborted(methodDescription(method), e); return;  createMethodRunner(test, method, notifier).run();', 'public void recordSuccess(Node node, long requestTime)  checkArgs(node, requestTime); int successDelta = 1; if(requestTime > getConfig().getRequestLengthThreshold())  // Consider slow requests as \"soft\" errors that are counted against // us in our success threshold. if(logger.isTraceEnabled()) logger.trace(node + \" recording success, but request time (\" + requestTime + \") exceeded threshold (\" + getConfig().getRequestLengthThreshold() + \")\"); successDelta = 0;  update(node, successDelta, null);', 'public final InputStream transform( Path pomFile, TransformerContext context ) throws IOException, org.apache.maven.model.building.TransformerException  final TransformerHandler transformerHandler = getTransformerHandler( pomFile ); final PipedOutputStream pout = new PipedOutputStream(); OutputStream out = filterOutputStream( pout, pomFile ); final javax.xml.transform.Result result; final ConsumerLexicalHandler> lexConsumer; if ( transformerHandler == null )  result = new StreamResult( out ); lexConsumer = null;  else  result = new SAXResult( transformerHandler ); lexConsumer = l -> ( (SAXResult) result ).setLexicalHandler( new CommentRenormalizer( l ) ); transformerHandler.setResult( new StreamResult( out ) );  final Transformer transformer; final AbstractSAXFilter filter; try  filter = getSAXFilter( pomFile, context, lexConsumer ); filter.setLexicalHandler( transformerHandler ); // By default errors are written to stderr. // Hence set custom errorHandler to reduce noice filter.setErrorHandler( new ErrorHandler()  @Override public void warning( SAXParseException exception ) throws SAXException  throw exception;  @Override public void fatalError( SAXParseException exception ) throws SAXException  throw exception;  @Override public void error( SAXParseException exception ) throws SAXException  throw exception;   ); transformer = transformerFactory.newTransformer();  catch ( TransformerConfigurationException | SAXException | ParserConfigurationException e )  throw new org.apache.maven.model.building.TransformerException( e );', 'protected void assertLocalArtifactNotPresent( Artifact artifact ) throws ArtifactHandlerNotFoundException  String path = artifactHandlerManager.path( artifact ); File file = new File( localRepository().getBasedir(), path ); if ( file.exists() )  fail( \"Local artifact \" + file + \" should not be present.\" );', 'public void run()  Process.setThreadPriority(Process.THREAD_PRIORITY_BACKGROUND); // Skip when download already marked as finished; this download was // probably started again while racing with UpdateThread. if (mInfo.queryDownloadStatus() == Downloads.Impl.STATUS_SUCCESS)  logDebug(\"Already finished; skipping\"); return;  try  // while performing download, register for rules updates mNetworkPolicy.registerListener(mPolicyListener); logDebug(\"Starting\"); mInfoDelta.mStatus = STATUS_RUNNING; mInfoDelta.writeToDatabase(); // If we\\'re showing a foreground notification for the requesting // app, the download isn\\'t affected by the blocked status of the // requesting app mIgnoreBlocked = mInfo.isVisible(); // Use the caller\\'s default network to make this connection, since // they might be subject to restrictions that we shouldn\\'t let them // circumvent mNetwork = mSystemFacade.getActiveNetwork(mInfo.mUid, mIgnoreBlocked); if (mNetwork == null)  throw new StopRequestException(STATUS_WAITING_FOR_NETWORK, \"No network associated with requesting UID\");  // Remember which network this download started on; used to // determine if errors were due to network changes. final NetworkInfo info = mSystemFacade.getNetworkInfo(mNetwork, mInfo.mUid, mIgnoreBlocked); if (info!= null)  mNetworkType = info.getType();  // Network traffic on this thread should be counted against the // requesting UID, and is tagged with well-known value. TrafficStats.setThreadStatsTag(TrafficStats.TAG_SYSTEM_DOWNLOAD); TrafficStats.setThreadStats', 'private boolean isActive( Profile profile, ProfileActivationContext context, ModelProblemCollector problems )  for ( ProfileActivator activator : activators )  try  if ( activator.isActive( profile, context, problems ) )  return true;   catch ( RuntimeException e )  problems.add( new ModelProblemCollectorRequest( Severity.ERROR, Version.BASE ).setMessage( \"Failed to determine activation for profile \" + profile.getId() ).setLocation( profile.getLocation( \"\" ) ).setException( e ) ); return false;   return false;', 'public ListMethod> getOrderedMethods(ClassLanguage classLanguage)  ListMethod> methods = new ArrayListMethod>(); List?> clazz : classLanguage.getClasses())  Method m2)  Class?> classLanguage = classLanguage.getClasses(); for(Class?> clazz : classLanguage.getClasses())  Methods.addAll(Arrays.asList(clazz.getMethods()));  Collections.sort(methods, new ComparatorMethod>()  public int compare(Method m1, Method m2)  return order(m1) - order(m2);  private int order(Method m)  Order order = m.getAnnotation(Order.class); return (order == null)? Integer.MAX_VALUE : order.value();  ); return methods;', 'public void updateRawContactDisplayName(SQLiteDatabase db, long rawContactId)  if (mNameSplitter == null)  createNameSplitter();  int bestDisplayNameSource = DisplayNameSources.UNDEFINED; NameSplitter.Name bestName = null; String bestDisplayName = null; String bestPhoneticName = null; int bestPhoneticNameStyle = PhoneticNameStyle.UNDEFINED; mSelectionArgs1[0] = String.valueOf(rawContactId); Cursor c = db.rawQuery(RawContactNameQuery.RAW_SQL, mSelectionArgs1); try  while (c.moveToNext())  int mimeType = c.getInt(RawContactNameQuery.MIMETYPE); int source = getDisplayNameSourceForMimeTypeId(mimeType); if (source  bestDisplayNameSource || source == DisplayNameSources.UNDEFINED)  continue;  if (source == bestDisplayNameSource && c.getInt(RawContactNameQuery.IS_PRIMARY) == 0)  continue;  if (mimeType == getMimeTypeIdForStructuredName())  NameSplitter.Name name; if (bestName!= null)  name = new NameSplitter.Name();  else  name = mName; name.clear();  name.prefix = c.getString(RawContactNameQuery.PREFIX); name.givenNames = c.getString(RawContactNameQuery.GIVEN_NAME); name.middleName = c.getString(RawContactNameQuery.MIDDLE_NAME); name.familyName = c.getString(RawContactNameQuery.FAMILY_NAME); name.suffix', 'private static String convertArtifact( String artifact )  StringTokenizer tok = new StringTokenizer( artifact, \":\" ); if ( tok.countTokens()!= 4 )  throw new IllegalArgumentException( \"Artifact must have 4 tokens: \\'\" + artifact + \"\\'\" );  String[] a = new String[4]; for ( int i = 0; i  4; i++ )  a[i] = tok.nextToken();  String ext = a[3]; if ( \"maven-plugin\".equals( a[3] ) )  ext = \"jar\"; classifier = \"it\";  if ( \"coreit-artifact\".equals( a[3] ) )  ext = \"jar\"; classifier = \"it\";  if ( \"test-jar\".equals( a[3] ) )  ext = \"jar\"; classifier = \"tests\";  String repositoryPath; if ( \"legacy\".equals( localRepoLayout ) )  repositoryPath = a[0] + \"/\" + a[3] + \"s/\" + a[1] + \"-\" + a[2]; repositoryPath = repositoryPath + \"/\" + a[1] + \"-\" + a[2]; if ( classifier!= null )  repositoryPath = repositoryPath + \"-\" + classifier;  repositoryPath = repositoryPath + \".\"', 'public void pipe(final BsonReader reader)  if (reader instanceof BsonBinaryReader)  BsonBinaryReader binaryReader = (BsonBinaryReader) reader; if (getState() == State.VALUE)  buffer.write(BsonType.DOCUMENT.getValue()); writeCurrentName();  BsonInputStream bsonInputStream = binaryReader.getBuffer(); int size = bsonInputStream.readInt32(); buffer.writeInt(size); buffer.write(bsonInputStream.readBytes(size - 4)); binaryReader.setState(AbstractBsonReader.State.TYPE); if (getContext() == null)  setState(State.DONE);  else  if (getContext().getContextType() == BsonContextType.JAVASCRIPT_WITH_SCOPE)  backpatchSize(); // size of the JavaScript with scope value setContext(getContext().getParentContext());  setState(getNextState());   else  super.pipe(reader);', 'public void setUp() throws Exception  int clusterSize = ec2RebalancingTestConfig.getInstanceCount(); partitionMap = getPartitionMap(clusterSize, ec2RebalancingTestConfig.partitionsPerNode); originalCluster = ServerTestUtils.getLocalCluster(clusterSize, getPorts(clusterSize), partitionMap); deploy(hostNames, ec2RebalancingTestConfig); startClusterAsync(hostNames, ec2RebalancingTestConfig, nodeIds); nodeIds = generateClusterDescriptor(hostNamePairs, originalCluster, ec2RebalancingTestConfig); testEntries = ServerTestUtils.createRandomKeyValueString(ec2RebalancingTestConfig.numKeys); originalCluster = updateCluster(originalCluster, nodeIds); if (logger.isInfoEnabled()) logger.info(\"Sleeping for 15 seconds to let the Voldemort cluster start\"); Thread.sleep(3000);', 'public void restoreScmInfo( String projectId, Scm scm )  String connection = releaseProperties.getProperty( SCM_INFO_PREFIX + projectId + \".connection\" ); if ( connection == null )  throw new IllegalArgumentException( \"Project \\'\" + projectId + \"\\' has not had its SCM info cached. Cannot restore uncached SCM info.\" );  scm.setConnection( connection ); scm.setDeveloperConnection( releaseProperties.getProperty( SCM_INFO_PREFIX + projectId + \".developerConnection\" ) ); scm.setUrl( releaseProperties.getProperty( SCM_INFO_PREFIX + projectId + \".url\" ) ); scm.setTag( releaseProperties.getProperty( SCM_INFO_PREFIX + projectId + \".tag\" ) );', \"public TextLayout next()  if (first)  first = false;  else  y += ascent * lineHeight;  TextLayout layout = measurer.nextLayout((float) width); x = 0; if (align == Align.RIGHT)  x = (float) (width - layout.getAdvance());  else if (align == Align.CENTER)  x = (float) ((width - layout.getAdvance()) / 2.0F);  else if (align == Align.JUSTIFY)  // Don't justify the last line. if (measurer.getPosition()  text.length())  layout = layout.getJustifiedLayout((float) width);   ascent = layout.getAscent(); // y += layout.getDescent() + layout.getLeading() + layout.getAscent(); return layout;\", 'public List?> renderChild(Node network, Node child, MapPort,?> networkArgumentMap)  // A list of all result objects. ListObject> resultsList = new ArrayListObject>(); // If the node has no input ports, execute the node once for its side effects. if (child.getInputs().isEmpty())  return renderNode(child);  else  // The list of values that need to be processed for this port. MapPort, List?>> portArguments = new LinkedHashMapPort, List?>>(); // Evaluate the port data. for (Port port : child.getInputs())  List?> result = evaluatePort(network, child, port, networkArgumentMap); List?> convertedResult = convertResultsForPort(port, result); portArguments.put(port, convertedResult);  // Data from the network (through published ports) overrides the arguments. for (Map.EntryPort,?> argumentEntry : networkArgumentMap.entrySet())  Port networkPort = argumentEntry.getKey(); checkState(networkPort.isPublishedPort(), \"Given port %s is not a published port.\", networkPort); if (networkPort.getChildNode(network) == child)  Port childPort = networkPort.getChildPort(network); List?> values = preprocessInput(networkPort, childPort, argumentEntry.getValue()); portArguments.put(childPort, values);   // A prepared list of argument lists, each for one invocation of the child node. ListMapPort,?>> argumentMaps = buildArgumentMaps(portArguments); for (MapPort,?> argumentMap : argumentMaps)  List?> results = renderNode(child, argumentMap); resultsList.addAll(results);   return resultsList;', 'private void scanForTables(NodeList nodes)  for (int i = 0; i  nodes.size(); i++)  Node node = nodes.elementAt(i); if (node instanceof TableTag)  TableTag tableTag = deepClone((TableTag) node); tables.add(new HtmlTable(tableTag)); this.nodes.add(tableTag);  else  this.nodes.add(flatClone(node)); NodeList children = node.getChildren(); if (children!= null)  scanForTables(children);  Node endNode = endTag(node); if (endNode!= null)  this.nodes.add(endNode);', 'protected void logFailure( MavenExecutionResponse r, Throwable e, String longMessage )  line(); getLogger().info( \"BUILD FAILURE\" ); line(); getLogger().info( \"Reason: \" + e.getMessage() ); getLogger().info( \"Found these embedded error messages:n\" ); Throwable cause = e.getCause(); int depth = 0; while( cause!= null )  getLogger().info( \"t[\" + ( depth++ ) + \"] \" + cause.getMessage() ); cause = cause.getCause();  line(); if ( longMessage!= null )  getLogger().info( longMessage ); line();  if ( getLogger().isDebugEnabled() )  getLogger().debug( \"Trace\", e ); line();  stats( r.getStart(), r.getFinish() ); line();', 'public void run()  if ( mSuccess )  // Mark group dirty if title changes if (! mBackup.title.equals(mNewE.title) )  PwGroup parent = mBackup.parent; if ( parent!= null )  // Resort entries parent.sortEntriesByName(); // Mark parent group dirty mDb.gDirty.put(parent, new WeakReferencePwGroup>(parent));  // Update search index mDb.searchHelper.updateEntry(mOldE);   else  // If we fail to save, back out changes to global structure mOldE.assign(mBackup);  super.run();', 'private Model readModel( ModelSource modelSource, File pomFile, ModelBuildingRequest request, DefaultModelProblemCollector problems ) throws ModelBuildingException  Model model; if ( modelSource == null )  if ( pomFile!= null )  modelSource = new FileModelSource( pomFile );  else  throw new IllegalArgumentException( \"neither model source nor input file are specified\" );   problems.setSource( modelSource.getLocation() ); try  boolean strict = request.getValidationLevel() >= ModelBuildingRequest.VALIDATION_LEVEL_MAVEN_2_0; MapString, Object> options = new HashMapString, Object>(); options.put( ModelProcessor.IS_STRICT, Boolean.valueOf( strict ) ); options.put( ModelProcessor.LOCATION, modelSource.getLocation() ); options.put( ModelProcessor.SOURCE, modelSource ); try  model = modelProcessor.read( modelSource.getInputStream(), options );  catch ( ModelParseException e )  if (!strict )  throw e;  options.put( ModelProcessor.IS_STRICT, Boolean.FALSE ); model = modelProcessor.read( modelSource.getInputStream(), options ); if ( pomFile!= null )  problems.add( Severity.ERROR, \"Malformed POM \" + modelSource.getLocation() + \": \" + e.getMessage(), e );  else  problems.add( Severity.WARNING, \"Malformed POM \" + modelSource.getLocation() + \": \" + e.getMessage(), e );    catch ( ModelParseException e )  problems.add( Severity.FA', 'public void testColors()  Path p1 = new Path(); Path p2 = new Path(); p1.rect(0, 0, 100, 100); p2.rect(150, 150, 100, 100); Geometry g = new Geometry(); g.add(p1); g.add(p2); assertEquals(2, g.size()); // Each path has 4 points. assertEquals(8, g.getPointCount()); Color red = new Color(1, 0, 0); g.setFill(red); assertEquals(red, p1.getFillColor()); assertEquals(red, p2.getFillColor());', 'public PluginDescriptor loadPlugin( Plugin plugin, ArtifactRepository localRepository, ListArtifactRepository> remoteRepositories ) throws PluginNotFoundException, PluginResolutionException, PluginDescriptorParsingException, CycleDetectedInPluginGraphException  PluginDescriptor pluginDescriptor = getPluginDescriptor( plugin ); // There are cases where plugins are discovered but not actually populated. These are edge cases where you are working in the IDE on // Maven itself so this speaks to a problem we have with the system not starting entirely clean. if ( pluginDescriptor!= null && pluginDescriptor.getClassRealm()!= null )  return pluginDescriptor;  Artifact pluginArtifact = repositorySystem.createPluginArtifact( plugin ); ArtifactResolutionRequest request = new ArtifactResolutionRequest().setArtifact( pluginArtifact ).setLocalRepository( localRepository ).setRemoteRepostories( remoteRepositories ); ArtifactResolutionResult result = repositorySystem.resolve( request ); try  resolutionErrorHandler.throwErrors( request, result );  catch ( ArtifactResolutionException e )  throw new PluginResolutionException( plugin, e );  ClassRealm pluginRealm = pluginClassLoaderCache.get( constructPluginKey( plugin ) ); if ( pluginRealm!= null )  return getPluginDescriptor( plugin );  pluginRealm = container.createChildRealm( pluginKey( plugin ) ); SetArtifact> pluginArtifacts; try  pluginArtifacts = getPluginArtifacts( pluginArtifact, plugin, localRepository, remoteRepositories );  catch ( ArtifactNotFoundException e )  throw new PluginNotFoundException( plugin, e );  catch ( ArtifactResolutionException', 'private DefaultProfileActivationContext getProfileActivationContext( ModelBuildingRequest request )  DefaultProfileActivationContext context = new DefaultProfileActivationContext(); context.setActiveProfileIds( request.getActiveProfileIds() ); context.setInactiveProfileIds( request.getInactiveProfileIds() ); context.setSystemProperties( request.getSystemProperties() ); context.setUserProperties( request.getUserProperties() ); context.setProjectDirectory( ( request.getPomFile()!= null )? request.getPomFile().getParentFile() : null ); return context;', 'public void sendStartedEvent()  if (loggingRequired())  logger.debug( format(\"Sending command \\'%s\\' with request id %d to database %s on connection [%s] to server %s\", getTruncatedJsonCommand(), message.getId(), message.getNamespace().getDatabaseName(), description.getConnectionId(), description.getServerAddress()));  if (eventRequired())  BsonDocument commandDocumentForEvent = redactionRequired? new BsonDocument() : commandDocument; sendCommandStartedEvent(message, message.getNamespace().getDatabaseName(), commandName, commandDocumentForEvent, description, commandListener);  // the buffer underlying the command document may be released after the started event, so set to null to ensure it\\'s not used // when sending the failed or succeeded event commandDocument = null;', 'private Response makePageHistoryResponse(FitNesseContext context) throws Exception  velocityContext.put(\"pageHistory\", pageHistory); Template template = VelocityFactory.getVelocityEngine().getTemplate(\"pageHistory.vm\"); return makeResponseFromTemplate(template);', 'public String getPathSeparator(WikiPage page) throws Exception  String separator = page.getData().getVariable(\"PATH_SEPARATOR\"); if(separator == null) separator = (String) System.getProperties().get(\"path.separator\"); return separator;', 'public MongoClientOptions getOptions()  ReadPreference readPreference = proxied.getReadPreference(); if (readPreference!= null)  builder.readPreference(readPreference);  ReadConcern readConcern = proxied.getReadConcern(); if (readConcern!= null)  builder.readConcern(readConcern);  WriteConcern writeConcern = proxied.getWriteConcern(); if (writeConcern!= null)  builder.writeConcern(writeConcern);  if (proxied.getRetryWrites())  builder.retryWrites(proxied.getRetryWrites());  Integer maxConnectionPoolSize = proxied.getMaxConnectionPoolSize(); if (maxConnectionPoolSize!= null)  builder.connectionsPerHost(maxConnectionPoolSize);  Integer integer = proxied.getMinConnectionPoolSize(); if (integer!= null)  builder.minConnectionsPerHost(integer);  Integer maxWaitTime = proxied.getMaxWaitTime(); if (maxWaitTime!= null)  builder.maxWaitTime(maxWaitTime);  Integer threadsAllowedToBlockForConnectionMultiplier = proxied.getThreadsAllowedToBlockForConnectionMultiplier(); if (threadsAllowedToBlockForConnectionMultiplier!= null)  builder.threadsAllowedToBlockForConnectionMultiplier(threadsAllowedToBlockForConnectionMultiplier);  Integer maxConnectionIdleTime = proxied.getMaxConnectionIdleTime()', 'public void testMng5568()  String key = \"6.1.0-beta\"; String key = \"6.1.0rc3\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; String key = \"6.1.0\"; if (key.isEmpty())  key = \"6.1.0\";  if (key.isEmpty())  key = \"6.1.0\";  if (key.isEmpty())  key = \"6.1.0\";  if (key!= null)  key = \"6.1.0\";  if (key!= null)  key = \"6.1.0\";    RC as of MNG-7559 checkVersionsOrder(key); String key = \"6.1.0rc3\", key); // classical checkVersionsOrder(key); // transitivity', 'private List translateDependencies( List v3Deps )  List deps = new ArrayList(); if ( notEmpty( v3Deps ) )  for ( Iterator it = v3Deps.iterator(); it.hasNext(); )  org.apache.maven.model.v3_0_0.Dependency v3Dep = (org.apache.maven.model.v3_0_0.Dependency) it.next(); String type = v3Dep.getType(); if( \"plugin\".equals( type ) )  String groupId = v3Dep.getGroupId(); if( \"maven\".equals( groupId ) )  groupId = \"org.apache.maven.plugins\";  Plugin plugin = new Plugin(); plugin.setGroupId( groupId ); plugin.setArtifactId( v3Dep.getArtifactId() ); plugin.setVersion( v3Dep.getVersion() ); Xpp3Dom config = new Xpp3Dom( \"configuration\" ); Properties props = v3Dep.getProperties(); if (!props.isEmpty() )  for ( Iterator propertyIterator = props.keySet().iterator(); it.hasNext(); )  String key = (String) propertyIterator.next(); String value = props.getProperty( key ); Xpp3Dom child = new Xpp3Dom( key ); child.setValue( value ); config.addChild( child );   plugin.setConfiguration( config ); this.discoveredPlugins.add( plugin );  else  Dependency dep = new Dependency(); String artifactId = v3Dep.getArtifactId', 'private void assertTopology(final BsonDocument outcome)  BsonDocument topology = outcome.getString(\"topologyType\").getValue(); assertLogicalSessionTimeout(outcome.get(\"logicalSessionTimeoutMinutes\", BsonNull.VALUE)); assertDriverCompatibility(outcome.get(\"compatible\"));', 'public PwGroupV3 newGroup(String name, PwGroupV3 parent)  // Initialize group PwGroupV3 group = new PwGroupV3(); group.parent = parent; group.groupId = newGroupId(); group.imageId = 0; group.name = name; Date now = Calendar.getInstance().getTime(); group.tCreation = new PwDate(now); group.tLastAccess = new PwDate(now); group.tLastMod = new PwDate(now); group.tExpire = new PwDate(PwGroupV3.NEVER_EXPIRE); group.level = parent.level + 1; group.childEntries = new VectorPwEntryV3>(); group.childGroups = new VectorPwGroupV3>(); // Add group PwDatabaseV3 and Parent parent.childGroups.add(group); groups.add(group); return group;', 'private void moveItem(boolean up)  int curcount = mTrackCursor.getCount(); int curpos = mTrackList.getSelectedItemPosition(); if ( (up && curpos  1) || (!up && curpos >= curcount - 1))  return;  if (mTrackCursor instanceof NowPlayingCursor)  NowPlayingCursor c = (NowPlayingCursor) mTrackCursor; c.moveItem(curpos, up? curpos - 1 : curpos + 1); ((TrackListAdapter)getListAdapter()).notifyDataSetChanged(); getListView().invalidateViews(); mDeletedOneRow = true; if (up)  mTrackList.setSelection(curpos - 1);  else  mTrackList.setSelection(curpos + 1);   else  int colidx = mTrackCursor.getColumnIndex(MediaStore.Audio.Playlists.Members.PLAY_ORDER); mTrackCursor.moveToPosition(curpos); int currentplayidx = mTrackCursor.getInt(colidx); if (up)  mTrackCursor.updateInt(colidx, currentplayidx - 1); mTrackCursor.moveToPrevious();  else  mTrackCursor.updateInt(colidx, currentplayidx + 1); mTrackCursor.moveToNext();  mTrackCursor.updateInt(colidx, currentplayidx + 1); mTrackCursor.moveToNext();  mTrackCursor.updateInt(colidx, currentplayidx + 1); mTrackCursor', 'private ServerDescription buildServerDescription(final BsonDocument serverDescription)  ServerDescription.Builder builder = ServerDescription.builder(); builder.address(new ServerAddress(serverDescription.getString(\"address\").getValue())); builder.type(getServerType(serverDescription.getString(\"type\").getValue())); if (serverDescription.containsKey(\"tags\"))  builder.tagSet(buildTagSet(serverDescription.getDocument(\"tags\")));  if (serverDescription.containsKey(\"avg_rtt_ms\"))  builder.roundTripTime(serverDescription.getNumber(\"avg_rtt_ms\").asInt32().getValue(), TimeUnit.MILLISECONDS);  builder.state(ServerConnectionState.CONNECTED); if (serverDescription.containsKey(\"lastWrite\"))  builder.lastWriteDate(new Date(serverDescription.getDocument(\"lastWrite\").getNumber(\"lastWriteDate\").longValue()));  if (serverDescription.containsKey(\"lastUpdateTime\"))  builder.lastUpdateTimeNanos(serverDescription.getNumber(\"lastUpdateTime\").longValue() * 1000000); // convert to nanos  else  builder.lastUpdateTimeNanos(42L);  if (serverDescription.containsKey(\"maxWireVersion\"))  builder.maxWireVersion(serverDescription.getNumber(\"maxWireVersion\").intValue());  builder.ok(true); return builder.build();', 'public int update(final Uri uri, final ContentValues values, final String where, final String[] whereArgs)  Helpers.validateSelection(where, sAppReadableColumnsSet); SQLiteDatabase db = mOpenHelper.getWritableDatabase(); int count; boolean updateSchedule = false; ContentValues filteredValues; if (Binder.getCallingPid()!= Process.myPid())  filteredValues = new ContentValues(); copyString(Downloads.Impl.COLUMN_APP_DATA, values, filteredValues); copyInteger(Downloads.Impl.COLUMN_VISIBILITY, values, filteredValues); Integer i = values.getAsInteger(Downloads.Impl.COLUMN_CONTROL); if (i!= null)  filteredValues.put(Downloads.Impl.COLUMN_CONTROL, i); updateSchedule = true;  copyInteger(Downloads.Impl.COLUMN_CONTROL, values, filteredValues); copyString(Downloads.Impl.COLUMN_TITLE, values, filteredValues); copyString(Downloads.Impl.COLUMN_MEDIAPROVIDER_URI, values, filteredValues); copyString(Downloads.Impl.COLUMN_DESCRIPTION, values, filteredValues); copyInteger(Downloads.Impl.COLUMN_DELETED, values, filteredValues);  else  filteredValues = values; String filename = values.getAsString(Downloads.Impl._DATA); if (filename!= null)  Cursor c = null; try  c = query(uri, new String[]  Download', 'public static void main(String[] args)  // Create the client RESTClientString, String> clientStore = new RESTClientString, String>(\"http://localhost:8080\", \"test\"); // Sample put clientStore.put(\"a\", \"Howdy!!!!\"); clientStore.put(\"b\", \"Partner!!!!\"); // Do a sample operation: System.out.println(\"Received response : \" + clientStore.get(\"a\")); ListString> keyList = new ArrayListString>(); keyList.add(\"a\"); keyList.add(\"b\"); System.out.println(\"Received response : \" + clientStore.getAll(keyList)); clientStore.close();', 'public final String getFinalPageName( String page ) throws ProviderException  boolean isThere = simplePageExists( page ); if (!isThere && m_matchEnglishPlurals )  if ( page.endsWith( \"s\" ) )  page = page.substring( 0, page.length() - 1 );  else  page += \"s\";  isThere = simplePageExists( page );  return isThere? page : null;', 'private S> CodecS> specializePojoCodec(final PropertyModelS> propertyModel)  CodecS> codec = getCodecFromPropertyRegistry(propertyModel); if (codec instanceof PojoCodec)  PojoCodecS> pojoCodec = (PojoCodecS>) codec; ClassModelS> specialized = getSpecializedClassModel(pojoCodec.getClassModel(), propertyModel); if (codecCache.containsKey(specialized))  codec = (CodecS>) codecCache.get(specialized);  else  codec = new LazyPojoCodecS>(specialized, registry, propertyCodecRegistry, discriminatorLookup, codecCache);   return codec;', 'private String formatHTMLLine(String chunk)  StringBuilder out = new StringBuilder(); Matcher m = (outputLorcode?urlRE_UNESCAPED:urlRE).matcher(chunk); int index = 0; while (m.find())  int start = m.start(); int end = m.end(); // 34€°34o° 12°°»12341 °?   34 URL out.append(chunk.substring(index, start)); // 34€°34o° URL String url = chunk.substring(start, end); if (urlHighlight)  String urlchunk = url; if (url.toLowerCase().startsWith(\"www.\"))  url = \"http://\" + url;  else if (url.toLowerCase().startsWith(\"ftp.\"))  url = \"ftp://\" + url;  if (urlchunk.length() > maxlength)  urlchunk = urlchunk.substring(0, maxlength - 3) + \"...\";  if (outputLorcode)  out.append(\"[url=\").append(URLEncoder(url)).append(\\']\\').append(urlchunk).append(\"[/url]\");  else  // ’34»121 o34  ?€°2»12? lor??»34o 12° o341412°€   €°2 »1214', 'private boolean endsRow(Symbol symbol)  if (!Symbol symbol == null) return null; return symbol.getContent().indexOf(\"n\");', 'private void processGoalChain( String task, MavenSession session, Map phaseMap ) throws LifecycleExecutionException, ArtifactResolutionException  if ( phaseMap.containsKey( task ) )  // only execute up to the given phase int index = phases.indexOf( phaseMap.get( task ) ); for ( int j = 0; j = index; j++ )  // TODO: phases should just be strings... Phase p = (Phase) phases.get( j ); p = (Phase) phaseMap.get( p.getId() ); if ( p.getGoals()!= null )  for ( Iterator k = p.getGoals().iterator(); k.hasNext(); )  String goal = (String) k.next(); verifyMojoPhase( goal, session, phaseMap );     else  verifyMojoPhase( task, session, phaseMap );      else  verifyMojoPhase( task, session, phaseMap );', 'public int delete(final Uri uri, final String where, final String[] whereArgs)  if (shouldRestrictVisibility())  Helpers.validateSelection(where, sAppReadableColumnsSet);  final JobScheduler scheduler = getContext().getSystemService(JobScheduler.class); final SQLiteDatabase db = mOpenHelper.getWritableDatabase(); int count; int match = sURIMatcher.match(uri); switch (match)  case MY_DOWNLOADS: case MY_DOWNLOADS_ID: case ALL_DOWNLOADS: case ALL_DOWNLOADS_ID: final SqlSelection selection = getWhereClause(uri, where, whereArgs, match); deleteRequestHeaders(db, selection.getSelection(), selection.getParameters()); try (Cursor cursor = db.query(DB_TABLE, new String[]  _ID, _DATA, COLUMN_MEDIAPROVIDER_URI, selection.getSelection(), selection.getParameters(), null, null, null))  while (cursor.moveToNext())  final long id = cursor.getLong(0); scheduler.cancel((int) id); DownloadStorageProvider.onDownloadProviderDelete(getContext(), id); final String path = cursor.getString(1); if (!TextUtils.isEmpty(path))  try  final File file = new File(path).getCanonicalFile(); if (Helpers.isFilenameValid(getContext(), file))  Log.v(Constants.TAG, \"Deleting \" + file + \" via provider delete\"); file.delete();   catch (IOException ignored)    final String mediaUri = cursor.getString(2)', 'public Settings buildSettings() throws IOException, XmlPullParserException  Settings globalSettings = readSettings( globalSettingsFile ); Settings userSettings = readSettings( userSettingsFile ); if ( userSettings == null )  userSettings = new Settings(); userSettings.setRuntimeInfo( new RuntimeInfo( userSettings ) );  SettingsUtils.merge( userSettings, globalSettings, TrackableBase.GLOBAL_LEVEL ); if ( userSettings.getLocalRepository() == null || userSettings.getLocalRepository().length()  1 )  File mavenUserConfigurationDirectory = new File( userHome, \".m2\" ); if (!mavenUserConfigurationDirectory.exists() )  if (!mavenUserConfigurationDirectory.mkdirs() )  //throw a configuration exception   String localRepository = new File( mavenUserConfigurationDirectory, \"repository\" ).getAbsolutePath(); userSettings.setLocalRepository( localRepository );  return userSettings;', 'private Config parseJSON(JSONObject jsonObject) throws JSONException  // the incoming config could be cache or EP object types, JSON envelope // picked apart if (!jsonObject.has(\"vBucketServerMap\"))  String vBucketServerMap =!jsonObject.getJSONObject(\"vBucketServerMap\"); if (vBucketServerMap.equals(\"vBucketServerMap\"))  return parseCacheJSON(jsonObject);  return parseEpJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));', 'private String sendEmailToAdmin( HttpServletRequest request, Exception exception )  InternetAddress mail; String adminEmailAddress = configuration.getAdminEmailAddress(); try  mail = new InternetAddress(adminEmailAddress, true);  catch (AddressException e)  return EMAIL_NOT_SENT + \"?€°2 »121 e-mail ° €?: \" + adminEmailAddress;  StringBuilder text = new StringBuilder(); if (exception.getMessage() == null)  text.append(exception.getClass().getName());  else  text.append(exception.getMessage());  text.append(\"nn\"); Template tmpl = Template.getTemplate(request); // text.append(\"Main URL: \").append(tmpl.getMainUrl()).append(request.getAttribute(\"javax.servlet.error.request_uri\")); text.append(\"Main URL: \").append(tmpl.getMainUrl()).append(request.getPathInfo()); if (request.getQueryString()!= null)  text.append(\\'?\\').append(request.getQueryString()).append(\\'n\\');  text.append(\\'n\\'); text.append(\"IP: \" + request.getRemoteAddr() + \\'n\\'); text.append(\" Headers: \"); Enumeration enu = request.getHeaderNames(); while (enu.hasMoreElements())  String paramName = (String) enu.nextElement(); text.append(\"n \").append(paramName).append(\": \").append(paramName));  text.append(\"n \").append', 'public final void testLogin()  Principal principal = new WikiPrincipal( \"Andrew Jaquith\" ); principal = new WikiPrincipal( \"Andrew Jaquith\" ); principal = new WikiPrincipal( \"Andrew Jaquith\" ); principal = new WikiPrincipal( principal ); principal = new Principal( principal ); principal.login(); Set principals = subject.getPrincipals(); assertEquals( 3, principals.size() ); assertTrue( principals.contains( principals.size() ) ); assertTrue( principals.contains( principals.contains( Role.AUTHENTICATED ) ); assertTrue( principals.contains( Role.ALL ) ); // Test using remote user (WebContainerLoginModule succeeds) subject = new Subject(); request = new TestHttpServletRequest(); request.setRemoteUser( \"Andrew Jaquith\" ); handler = new WebContainerCallbackHandler( request, db ); context = new LoginContext( \"JSPWiki-container\", subject, handler ); context.login(); principals = subject.getPrincipals(); assertEquals( 3, principals.size() ); assertTrue( principals.contains( principals.contains( new WikiPrincipal( \"Andrew Jaquith\" ) ) ); assertTrue( principals.contains( Role.AUTHENTICATED ) ); assertTrue( principals.contains( Role.ALL ) ); // Test using IP address (AnonymousLoginModule succeeds) subject = new Subject(); request = new TestHttpServletRequest(); request.setRemoteAddr( \"53.33.128.9\" );', 'public void testMillionChanges() throws Exception  String text = \"\"; String name = NAME1; int maxver = 100; // Save 100 versions. for( int i = 0; i  maxver; i++ )  text = text + \".\"; engine.saveText( name, text );  WikiPage pageinfo = engine.getPage( NAME1 ); assertEquals( \"wrong version\", maxver, pageinfo.getVersion() ); // +2 comes from rn. assertEquals( \"wrong text\", maxver+2, engine.getText(NAME1).length() );', 'public static Assignments allUnassigned(Method testMethod, Class?> testClass)  ListPotentialAssignment>> parameters = ParameterSignature.signatures(testMethod); return new Assignments(new ArrayListPotentialAssignment>(), parameters);', 'public long getCursorId()  String serverCursor = cursor.getServerCursor(); return serverCursor.getId();', 'public boolean shouldIncludeScenarioLibraries()  // Should consider all of the decorated content to resolve those variables. boolean isSlim = \"slim\".equalsIgnoreCase(sourcePage.getVariable(WikiPageIdentity.TEST_SYSTEM)); String includeScenarioLibraries = sourcePage.getVariable(\"INCLUDE_SCENARIO_LIBRARIES\"); boolean includeScenarios = \"true\".equalsIgnoreCase(includeScenarioLibraries); boolean notIncludeScenarios = \"false\".equalsIgnoreCase(includeScenarioLibraries); return includeScenarios || (!notIncludeScenarios && isSlim);', 'public void sessionStarted( ExecutionEvent event )  if ( logger.isInfoEnabled() && event.getSession().getProjects().size() > 1 )  infoLine( \\'-\\' ); infoMain( \"Reactor Build Order:\" ); logger.info( \"\" ); for ( MavenProject project : event.getSession().getProjects() )  logger.info( project.getName() );', 'public String toHexString()  StringBuilder buf = new StringBuilder(24); for (final byte b : toByteArray())  buf.append(String.format(\"%02x\", b & 0xff));  return buf.toString();', 'public static SslSettings getSslSettings()  SslSettingsConnectionString connectionString = getConnectionString(); if (connectionString == null)  return SslSettings.builder().applyConnectionString(connectionString).build();', 'public ParcelFileDescriptor openFile(Uri uri, String mode) throws FileNotFoundException  if (!\"r\".equals(mode))  throw new UnsupportedOperationException();  final File file = DataExporter.getOutputFile(getContext(), extractFileName(uri)); final File file = extractFileName(uri); return ParcelFileDescriptor.open(file, ParcelFileDescriptor.MODE_READ_ONLY);', \"public Artifact retrieveRelocatedArtifact( Artifact artifact, ArtifactRepository localRepository, ListArtifactRepository> remoteRepositories ) throws ArtifactMetadataRetrievalException  ProjectRelocation rel = retrieveRelocatedProject( artifact, localRepository, remoteRepositories ); if ( rel == null )  return artifact;  MavenProject project = rel.project; if ( project == null || getRelocationKey( artifact ).equals( getRelocationKey( project.getArtifact() ) ) )  return artifact;  // NOTE: Using artifact information here, since some POMs are deployed // to central with one version in the filename, but another in the version> string! // Case in point: org.apache.ws.commons:XmlSchema:1.1:pom. // // Since relocation triggers a reconfiguration of the artifact's information // in retrieveRelocatedProject(..), this is safe to do. Artifact result = null; if ( artifact.getClassifier()!= null )  result = repositorySystem.createArtifactWithClassifier( artifact.getGroupId(), artifact.getArtifactId(), artifact.getVersion(), artifact.getType(), artifact.getClassifier() );  else  result = repositorySystem.createArtifact( artifact.getGroupId(), artifact.getArtifactId(), artifact.getVersion(), artifact.getScope(), artifact.getType() );  result.setResolved( artifact.isResolved() ); result.setFile( artifact.getFile() ); result.setScope( artifact.getScope() ); result.setArtifactHandler( artifact.getArtifactHandler() ); result.setDependency\", 'public void addPluginAsExtension( Plugin plugin, Model originatingModel, List remoteRepositories, MavenExecutionRequest request ) throws ExtensionManagerException  Parent originatingParent = originatingModel.getParent(); String groupId = originatingModel.getGroupId(); if ( ( groupId == null ) && ( originatingParent!= null ) )  groupId = originatingParent.getGroupId();  String artifactId = originatingModel.getArtifactId(); String version = originatingModel.getVersion(); if ( ( version == null ) && ( originatingParent!= null ) )  version = originatingParent.getVersion();  Artifact pluginArtifact = artifactFactory.createBuildArtifact( plugin.getGroupId(), plugin.getArtifactId(), plugin.getVersion(), \"maven-plugin\" ); getLogger().debug( \"Starting extension-addition process for: \" + pluginArtifact ); ArtifactFilter coreFilter = artifactFilterManager.getArtifactFilter(); MavenRealmManager realmManager = request.getRealmManager(); // if the extension is null, // or if it\\'s excluded by the core filter, // // skip it. if ( ( pluginArtifact!= null ) && coreFilter.include( pluginArtifact ) )  MavenProject dummyProject = new MavenProject( originatingModel ); EventDispatcher dispatcher = new DefaultEventDispatcher( request.getEventMonitors() ); MavenSession session = new MavenSession( container, request, dispatcher, null ); PluginDescriptor pd; try  pd = pluginManager.verifyPlugin( plugin, dummyProject, session ); pluginArtifact = pd.get', 'public void shouldRemoveFlashIncludedParameters() throws Exception  mockery.checking(new Expectations()   one(session).getAttribute(FlashInterceptor.FLASH_INCLUDED_PARAMETERS); will(returnValue(Collections.singletonMap(\"Abc\", 1002))); one(session).removeAttribute(FlashInterceptor.FLASH_INCLUDED_PARAMETERS); ignoring(anything());  ); interceptor.intercept(stack, null, null); mockery.assertIsSatisfied();', 'public synchronized Date time()  Date time time = currentTimeMillis(); if (isPaused) return time; DateTimeMillis currentTimeMillis = currentTimeMillis; if (currentTimeMillis!= millis)  set(new Date(currentTimeMillis));  return time;', 'public void canExecuteConstructorWhenIntArgType() throws Throwable  //given DefaultInteraction defaultInteraction = new DefaultInteraction(); Constructor?> constructor = null; Date dateVal = new Date(); Object args[] = new Object[]1, \"stringVal\", dateVal; //when constructor = defaultInteraction.getConstructor(Testee.class, args); Testee testee = (Testee) constructor.newInstance(args); //then assertEquals(1, testee.getI()); assertEquals(\"stringVal\", testee.getStringVal()); assertEquals(dateVal, testee.getDateVal());', 'private void updateWithLocked(CollectionDownloadInfo> downloads)  final Resources res = mContext.getResources(); // Cluster downloads together final MultimapString, DownloadInfo> clustered = ArrayListMultimap.create(); for (DownloadInfo info : downloads)  final String tag = buildNotificationTag(info); if (tag!= null)  clustered.put(tag, info);   // Build notification for each cluster for (String tag : clustered.keySet())  final int type = getNotificationTagType(tag); final CollectionDownloadInfo> cluster = clustered.get(tag); final Notification.Builder builder = new Notification.Builder(mContext); builder.setColor(res.getColor( com.android.internal.R.color.system_notification_accent_color)); // Use time when cluster was first shown to avoid shuffling final long firstShown; if (mActiveNotifs.containsKey(tag))  firstShown = mActiveNotifs.get(tag);  else  firstShown = System.currentTimeMillis(); mActiveNotifs.put(tag, firstShown);  builder.setWhen(firstShown); // Show relevant icon if (type == TYPE_ACTIVE)  builder.setSmallIcon(android.R.drawable.stat_sys_download);  else if (type == TYPE_WAITING)  builder.setSmallIcon(android.R.drawable.stat_sys_warning);  else if (type == TYPE_COMPLETE)  builder.setSmallIcon(android.R.drawable.stat_sys_download_done);  // Build action intents if (type == TYPE_ACTIVE || type == TYPE_WAITING)  // build a synthetic uri for intent identification', 'public void testProxyGetDuringRebalancing() throws Exception  final Cluster currentCluster = ServerTestUtils.getLocalCluster(2, new int[][]   0, 1, 2, 3,  ); final Cluster targetCluster = ServerTestUtils.getLocalCluster(2, new int[][] ,  0, 1, 2, 3  ); // start servers 0, 1 only final ListInteger> serverList = Arrays.asList(0, 1); final Cluster updatedCluster = startServers(currentCluster, storeDefFile, serverList, null); ExecutorService executors = Executors.newFixedThreadPool(2); final AtomicBoolean rebalancingToken = new AtomicBoolean(false); final ListException> exceptions = Collections.synchronizedList(new ArrayListException>()); // populate data now. populateData(updatedCluster, Arrays.asList(0)); final SocketStoreClientFactory factory = new SocketStoreClientFactory(new ClientConfig().setBootstrapUrls(getBootstrapUrl(updatedCluster, 0)).setSocketTimeout(120, TimeUnit.SECONDS)); final StoreClientString, String> storeClient = new DefaultStoreClientString, String>(testStoreName, null, factory, 3); final boolean[] masterNodeResponded =  false, false ; // start get operation. executors.execute(new Runnable()  public void run()  try  ListString> keys = new ArrayListString>(testEntries.keySet()); int nRequests = 0; while(!rebalancingToken.get())  // should always able to get values. int index = (int) (M', \"public void receive(Transaction transaction, Date executionTime)  synchronized (_prevalentSystem)  _systemVersion++; try  transaction.executeOn(_prevalentSystem, executionTime);  catch (RuntimeException rx)  if (!_ignoreRuntimeExceptions) throw rx; //TODO Guarantee that transactions received from pending transaction recovery don't ever throw RuntimeExceptions. Maybe use a wrapper for that.\", 'public void putSource( String groupId, String artifactId, Source source )  mappedSources.computeIfAbsent( new DefaultTransformerContext.GAKey( groupId, artifactId ), k -> new HashSet>() ).add( source );', 'public void incrementVersion( MavenProject project ) throws MojoExecutionException  String projectVersion = project.getOriginalModel().getVersion(); if ( projectVersion.endsWith( \"SNAPSHOT\" ) )  String projectId = ArtifactUtils.versionlessKey( project.getGroupId(), project.getArtifactId() ); throw new MojoExecutionException( \"The project \" + projectId + \" is a snapshot (\" + projectVersion + \"). It appears that the release version has not been committed.\" );  // TODO: we will need to incorporate versioning strategies here because it is unlikely // that everyone will be able to agree on a standard. This is extremely limited right // now and really only works for the way maven is versioned. // releaseVersion = 1.0-beta-4 // snapshotVersion = 1.0-beta-5-SNAPSHOT // or // releaseVersion = 1.0.4 // snapshotVersion = 1.0.5-SNAPSHOT String nextVersionString = null; if ( projectVersion.indexOf( \"-\" ) > 0 )  nextVersionString = projectVersion.substring( projectVersion.lastIndexOf( \"-\" ) + 1 );  else if ( projectVersion.indexOf( \".\" ) > 0 )  nextVersionString = projectVersion.substring( projectVersion.lastIndexOf( \".\" ) + 1 );  else  nextVersionString = projectVersion;  try  nextVersionString = Integer.toString( Integer.parseInt( nextVersionString ) + 1 ); projectVersion = projectVersion.substring( 0, projectVersion.lastIndexOf( \"-\" ) + 1 ) + nextVersionString + SNAPSHOT_CLASSIFIER;  catch ( NumberFormatException e )  projectVersion = \"\";  String projectId = ArtifactUtils', 'public void setProperty(Map context, Object array, Object key, Object value) throws OgnlException  int index = (Integer) key; int length = Array.getLength(array); if (length = index)  array = copyOf(array, index, length); OgnlContext ctx = (OgnlContext) context; String fieldName = ctx.getCurrentEvaluation().getPrevious().getNode().toString(); Object origin = ctx.getCurrentEvaluation().getPrevious().getSource(); Method setter = ReflectionBasedNullHandler.findMethod(origin.getClass(), \"set\" + Info.capitalize(fieldName), origin.getClass(), null); Container container = (Container) context.get(Container.class); EmptyElementsRemoval removal = container.instanceFor(EmptyElementsRemoval.class); removal.add(array, setter, origin); try  setter.invoke(origin, array);  catch (IllegalArgumentException e)  // TODO better throw new IllegalArgumentException(e);  catch (IllegalAccessException e)  // TODO better throw new IllegalArgumentException(e);  catch (InvocationTargetException e)  // TODO better throw new IllegalArgumentException(e);   super.setProperty(context, array, key, value);', 'private synchronized void ensureOpen(final Mongo mongo) throws IOException  if ( _socket!= null ) return; long sleepTime = 100; long maxAutoConnectRetryTime = CONN_RETRY_TIME_MS; if (_options.maxAutoConnectRetryTime > 0)  maxAutoConnectRetryTime = _options.maxAutoConnectRetryTime;  boolean successfullyConnected = false; final long start = System.currentTimeMillis(); do  try  _socket = _options.socketFactory.createSocket(); _socket.connect( _addr.getSocketAddress(), _options.connectTimeout ); _socket.setTcpNoDelay(! USE_NAGLE ); _socket.setKeepAlive( _options.socketKeepAlive ); _socket.setSoTimeout( _options.socketTimeout ); _in = new BufferedInputStream( _socket.getInputStream() ); _out = _socket.getOutputStream(); if (mongo!= null)  _serverVersion = getVersion(runCommand(mongo.getDB(\"admin\"), new BasicDBObject(\"buildinfo\", 1)));  successfullyConnected = true;  catch ( IOException e ) close(); if (!_options.autoConnectRetry || (provider!= null &&!provider.hasWorked())) throw e; long waitSoFar = System.currentTimeMillis() - start; if (waitSoFar >= maxAutoConnectRetryTime) throw e; if (sleepTime + waitSoFar > maxAutoConnectRetryTime) sleepTime = maxAutoConnectRetryTime - waitSoFar; _logger.log(Level', 'private Cursor getTrackCursor(TrackListAdapter.TrackQueryHandler queryhandler, String filter, boolean async)  if (queryhandler == null)  throw new IllegalArgumentException();  Cursor ret = null; mSortOrder = MediaStore.Audio.Media.TITLE_KEY; StringBuilder where = new StringBuilder(); where.append(MediaStore.Audio.Media.TITLE + \"!= \\'\\'\"); // Add in the filtering constraints String [] keywords = null; if (filter!= null)  String [] searchWords = filter.split(\" \"); keywords = new String[searchWords.length]; Collator col = Collator.getInstance(); col.setStrength(Collator.PRIMARY); for (int i = 0; i  searchWords.length; i++)  String key = MediaStore.Audio.keyFor(searchWords[i]); key = key.replace(\"\", \"\"); key = key.replace(\"%\", \"%\"); key = key.replace(\"_\", \"_\"); keywords[i] = \\'%\\' + key + \\'%\\';  for (int i = 0; i  searchWords.length; i++)  where.append(\" AND \"); where.append(MediaStore.Audio.Media.ARTIST_KEY + \"||\"); where.append(MediaStore.Audio.Media.TITLE_KEY + \" LIKE? ESCAPE \\'\\'\");   if (mGenre!= null)  mSortOrder = MediaStore.Audio.Genres.Members.DEFAULT_SORT_ORDER; ret = queryhandler.doQuery(MediaStore.Audio.Genres.', 'ReadConcern getReadConcern()  ReadConcern readConcern = findOptions.getReadConcern(); if (readConcern!= null)  ReadConcern readConcern = findOptions.getReadConcern();  ReadConcern readConcern = collection.getReadConcern(); if (readConcern!= null)  readConcern = findOptions.getReadConcern();  return readConcern;', 'public static MapString, Object> httpGet(final String url, final long timeoutSeconds)  synchronized (url)  if (responseCache.containsKey(url))  Response r = responseCache.get(url); long timeNow = nowSeconds(); long timeFetched = r.timeFetched; if ((timeNow - timeFetched) = timeoutSeconds)  return r.response;   MapString, Object> r = _httpGet(url); Response res = new Response(nowSeconds(), r); responseCache.put(url, res); return r;', 'public VAdminProto.AsyncOperationStatusResponse handleFetchROStore(VAdminProto.FetchStoreRequest request)  final String fetchUrl = request.getStoreDir(); final String storeName = request.getStoreName(); int requestId = asyncService.getUniqueRequestId(); VAdminProto.AsyncOperationStatusResponse.Builder response = VAdminProto.AsyncOperationStatusResponse.newBuilder().setRequestId(requestId).setComplete(false).setDescription(\"Fetch store\").setStatus(\"started\"); try  if(!metadataStore.getReadOnlyFetchEnabledUnlocked())  throw new VoldemortException(\"Read-only fetcher is disabled in \" + metadataStore.getServerStateUnlocked() + \" state on node \" + metadataStore.getNodeId());  final ReadOnlyStorageEngine store = getReadOnlyStorageEngine(metadataStore, storeRepository, storeName); final long pushVersion; if(request.hasPushVersion())  pushVersion = request.getPushVersion(); if(pushVersion = store.getCurrentVersionId()) throw new VoldemortException(\"Version of push specified (\" + pushVersion + \") should be greater than current version \" + store.getCurrentVersionId() + \" for store \" + storeName + \" on node \" + metadataStore.getNodeId());  else  // Find the max version long maxVersion; File[] storeDirList = ReadOnlyUtils.getVersionDirs(new File(store.getStoreDirPath())); if(storeDirList == null || storeDirList.length == 0)  throw new Voldemor', 'public static void render(Grob g, File file)  String g = g.getBounds(); if (g.getBounds() == null)  render(g, g.getBounds(), null, file);  if (g.getBounds() == null)  render(g, file);', \"public boolean onKey(View v, int keyCode, KeyEvent event)  try  final boolean hardKeyboardHidden = manager.hardKeyboardHidden; // Ignore all key-up events except for the special keys if (event.getAction() == KeyEvent.ACTION_UP)  // There's nothing here for virtual keyboard users. if (!hardKeyboard || (hardKeyboard && hardKeyboardHidden)) return false; // skip keys if we aren't connected yet or have been disconnected if (bridge.isDisconnected() || bridge.transport == null) return false; if (PreferenceConstants.KEYMODE_RIGHT.equals(keymode))  if (keyCode == KeyEvent.KEYCODE_ALT_RIGHT && (metaState & META_SLASH)!= 0)  metaState &= (META_SLASH | META_TRANSIENT); bridge.transport.write('/'); return true;  else if (keyCode == KeyEvent.KEYCODE_SHIFT_RIGHT && (metaState & META_TAB)!= 0)  metaState &= (META_TAB | META_TRANSIENT); bridge.transport.write(0x09); return true;   else if (PreferenceConstants.KEYMODE_LEFT.equals(keymode))  if (keyCode == KeyEvent.KEYCODE_ALT_LEFT && (metaState & META_SLASH)!= 0)  metaState &= (META_TAB | META_TRANSIENT); bridge.transport.write('/'); return true;  else if (keyCode == KeyEvent.KEYCODE_SHIFT_LEFT && (metaState & META_TAB)!= 0)\", 'public void execute() throws MojoExecutionException  outputDirectory.mkdirs(); EnumerationURL> path = null; try  path = Thread.currentThread().getContextClassLoader().getResources(\"clojure\");  catch (IOException e)  throw new MojoExecutionException(e.getMessage());  String cp = srcDirectory.getPath() + File.pathSeparator + outputDirectory.getPath(); while (path.hasMoreElements())  URL url = path.nextElement(); getLog().debug(url.getPath()); if (\"jar\".equals(url.getProtocol()))  cp = cp + File.pathSeparator + url.getPath().replaceFirst(\"file:\", \"\").replaceFirst(\"jar.*\", \"jar\");   ListString> args = new ArrayListString>(); args.add(\"java\"); args.add(\"-cp\"); args.add(cp); args.add(\"clojure.lang.Repl\"); ProcessBuilder pb = new ProcessBuilder(args); pb.environment().put(\"path\", \";\"); pb.environment().put(\"path\", System.getProperty(\"java.home\")); pb.redirectErrorStream(true); try  writeProcessOutput(pb.start());  catch (IOException e)  throw new MojoExecutionException(e.getMessage());', 'private void addToPhaseMap( Map phaseMap, String phase, MojoDescriptor mojoDescriptor ) throws LifecycleExecutionException  if ( phase!= null )  List goals = (List) phaseMap.get( phase ); if ( goals == null )  String message = \"Required phase \\'\" + phase + \"\\' not found\"; throw new LifecycleExecutionException( message );  if (!goals.contains( mojoDescriptor ) )  goals.add( mojoDescriptor );', 'public String execute( WikiContext context, Map params ) throws PluginException  ReferenceManager refmgr = context.getEngine().getReferenceManager(); String pageName = (String)params.get( PARAM_PAGE ); if( pageName == null )  pageName = context.getPage().getName();  WikiPage page = context.getEngine().getPage( pageName ); if( page!= null )  Collection links = refmgr.findReferrers( page.getName() ); String wikitext = \"\"; super.initialize( context, params ); int items = TextUtil.parseIntParameter( (String)params.get( PARAM_MAX ), ALL_ITEMS ); String extras = (String)params.get( PARAM_EXTRAS ); if( extras == null )  extras = \"...and %d more\";  log.debug( \"Fetching referring pages for \"+page.getName()+ \" with a max of \"+items); if( links!= null && links.size() > 0 )  links = filterCollection( links ); wikitext = wikitizeCollection( links, m_separator, items ); if( items  links.size() && items > 0 )  extras = TextUtil.replaceString( extras, \"%d\", \"\"+(links.size()-items) ); wikitext += extras;   // // If nothing was left after filtering or during search // if( links == null || links.size() == 0 )  wikitext = \"...nobody\";  return makeHTML( context, wikitext );  return \"\";', 'public boolean sampleStore(StoreDefinition storeDefinition)  String storeName = storeDefinition.getName(); String fileName = outDir + System.getProperty(\"file.separator\") + storeName + \".keys\"; File file = new File(fileName); if(file.exists())  logger.warn(\"Key file \" + fileName + \" already exists. Skipping sampling store \" + storeName + \".\"); return true;  Writer keyWriter = null; try  keyWriter = new FileWriter(file); MapNode, FutureNodeSampleResult>> results = new HashMapNode, FutureNodeSampleResult>>(); for(Node node: cluster.getNodes())  FutureNodeSampleResult> future = nodeSamplerService.submit(new NodeSampler(node, storeDefinition)); results.put(node, future);  boolean success = true; for(Node node: cluster.getNodes())  FutureNodeSampleResult> future = results.get(node); if(!success)  future.cancel(true); continue;  try  NodeSampleResult nodeSampleResult = future.get(); if(nodeSampleResult.success)  keyWriter.write(nodeSampleResult.keysString);  else  success = false; logger.error(\"Sampling on node \" + node.getHost() + \" of store \" + storeDefinition.getName() + \" failed.\");   catch(ExecutionException ee)  success = false; logger.error(\"Encountered an execution exception on node \" + node.getHost() + \" while sampling \" + storeName + \": \" + ee.getMessage()); ee.printStackTrace();  catch(InterruptedException ie', 'private TestVersionedValue doGet(String key)  String response = null; TestVersionedValue responseObj = null; try  // Create the right URL and Http connection HttpURLConnection conn = null; String base64Key = new String(Base64.encodeBase64(key.getBytes())); URL url = new URL(this.coordinatorURL + \"/\" + STORE_NAME + \"/\" + base64Key); conn = (HttpURLConnection) url.openConnection(); // Set the right headers conn.setRequestMethod(\"GET\"); conn.setDoInput(true); conn.setRequestProperty(VoldemortHttpRequestHandler.X_VOLD_REQUEST_TIMEOUT_MS, \"1000\"); if(conn.getResponseCode() == 404)  return null;  // Check for the right response code if(conn.getResponseCode()!= 200)  System.err.println(\"Illegal response during GET : \" + conn.getResponseMessage()); fail(\"Incorrect response received for a HTTP put request :\" + conn.getResponseCode());  // Buffer the result into a string BufferedReader rd = new BufferedReader(new InputStreamReader(conn.getInputStream())); StringBuilder sb = new StringBuilder(); String line; while((line = rd.readLine())!= null)  sb.append(line);  rd.close(); conn.disconnect(); response = sb.toString(); VectorClock vc = CoordinatorUtils.deserializeVectorClock(conn.getHeaderField(\"ETag\")); responseObj = new TestVersionedValu', 'public Artifact replaceWithActiveArtifact( Artifact pluginArtifact )  if ( getProjectReferences()!= null &&!getProjectReferences().isEmpty() )  String refId = getProjectReferenceId( pluginArtifact.getGroupId(), pluginArtifact.getArtifactId(), pluginArtifact.getVersion() ); MavenProject ref = (MavenProject) getProjectReferences().get( refId ); if ( ref!= null && ref.getArtifact()!= null )  // TODO: if not matching, we should get the correct artifact from that project (attached) if ( ref.getArtifact().getDependencyConflictId().equals( pluginArtifact.getDependencyConflictId() ) )  // if the project artifact doesn\\'t exist, don\\'t use it. We haven\\'t built that far. if ( ref.getArtifact().getFile()!= null && ref.getArtifact().getFile().exists() )  pluginArtifact = new ActiveProjectArtifact( ref, pluginArtifact ); return pluginArtifact;  else  /* TODO... logger.warn( \"Artifact found in the reactor has not been built when it\\'s use was \" + \"attempted - resolving from the repository instead\" ); */   Iterator itr = ref.getAttachedArtifacts().iterator(); while(itr.hasNext())  Artifact attached = (Artifact) itr.next(); if( attached.getDependencyConflictId().equals(pluginArtifact.getDependencyConflictId()) )  /* TODO: if I use the original, I', 'public ListLifecycle> getLifeCycles()  // ensure canonical order of standard lifecycles MapString, Lifecycle> lifecycles = new LinkedHashMapString, Lifecycle>( this.lifecycles ); LinkedHashSetString> lifecycleNames = new LinkedHashSetString>( Arrays.asList( STANDARD_LIFECYCLES ) ); lifecycleNames.addAll( lifecycles.keySet() ); ArrayListLifecycle> result = new ArrayListLifecycle>(); for ( String name : lifecycleNames )  result.add( lifecycles.get( name ) );  return result;', 'public static Intent getCallIntent(String number)  String callUri = getCallUri(number); return callUri;', 'protected String[] getFromProperty( String jcrNode, String property ) throws RepositoryException  if ( jcrNode == null || property == null )  throw new IllegalArgumentException( \"jcrNode and property cannot be null!\" );  // Retrieve the destination node for the page ContentManager cm = m_engine.getContentManager(); Node node = null; try  node = (Node) cm.getCurrentSession().getItem( jcrNode );  catch( PathNotFoundException e )  return NO_VALUES;  // Retrieve the property; re-pack value array into String array String[] stringValues = NO_VALUES; try  Property p = node.getProperty( property ); Value[] values = p.getValues(); stringValues = new String[values.length]; for( int i = 0; i  values.length; i++ )  checkValueString( values[i].getString() ); stringValues[i] = values[i].getString();   catch( PathNotFoundException e )  return NO_VALUES;  return stringValues;', 'public void run()  PwDatabase pm = mDb.pm; byte[] backupKey = new byte[pm.masterKey.length]; System.arraycopy(pm.masterKey, 0, backupKey, 0, backupKey.length); // Set key try  pm.setMasterKey(mPassword, mKeyfile);  catch (InvalidKeyFileException e)  erase(backupKey); finish(false, e.getMessage()); return;  catch (IOException e)  erase(backupKey); finish(false, e.getMessage()); return;  // Save Database mFinish = new AfterSave(backupKey, mFinish); SaveDB save = new SaveDB(ctx, mDb, mFinish, mDontSave); save.run();', 'public void insertOne(final T document)  if (getCodec() instanceof CollectibleCodec)  ((CollectibleCodecT>) getCodec()).generateIdIfAbsentFromDocument(document);  executeSingleWriteRequest(new InsertRequest(asBson(document)));', 'private void executeCreateCollection(@Nullable final ClientSession clientSession, final String collectionName, final CreateCollectionOptions createCollectionOptions)  CreateCollectionOperation operation = new CreateCollectionOperation(name, collectionName, writeConcern).collation(createCollectionOptions.getCollation()).capped(createCollectionOptions.isCapped()).sizeInBytes(createCollectionOptions.getSizeInBytes()).autoIndex(createCollectionOptions.isAutoIndex()).maxDocuments(createCollectionOptions.getMaxDocuments()).usePowerOf2Sizes(createCollectionOptions.isUsePowerOf2Sizes()).storageEngineOptions(toBsonDocument(createCollectionOptions.getStorageEngineOptions())); IndexOptionDefaults indexOptionDefaults = createCollectionOptions.getIndexOptionDefaults(); Bson storageEngine = indexOptionDefaults.getStorageEngine(); if (storageEngine!= null)  operation.indexOptionDefaults(new BsonDocument(\"storageEngine\", toBsonDocument(storageEngine)));  ValidationOptions validationOptions = createCollectionOptions.getValidationOptions(); Bson validator = validationOptions.getValidator(); if (validator!= null)  operation.validator(toBsonDocument(validator));  if (validationOptions.getValidationLevel()!= null)  operation.validationLevel(validationOptions.getValidationLevel());  if (validationOptions.getValidationAction()!= null)  operation.validationAction(validationOptions.getValidationAction());  executor.execute(operation, clientSes', 'public void executeAsync(final AsyncWriteBinding binding, final SingleResultCallbackVoid> callback)  withConnection(binding, new AsyncCallableWithConnection()  @Override public void call(final AsyncConnection connection, final Throwable t)  SingleResultCallbackVoid> errHandlingCallback = errorHandlingCallback(callback, LOGGER); if (t!= null)  errHandlingCallback.onResult(null, t);  else  final SingleResultCallbackVoid> wrappedCallback = releasingCallback(errHandlingCallback, connection); if (serverIsAtLeastVersionTwoDotSix(connection.getDescription()))  executeWrappedCommandProtocolAsync(binding, credential.getSource(), getCommand(), connection, new VoidTransformerBsonDocument>(), wrappedCallback);  else  connection.updateAsync(getNamespace(), true, WriteConcern.ACKNOWLEDGED, asList(getUpdateRequest()), new SingleResultCallbackWriteConcernResult>()  @Override public void onResult(final WriteConcernResult result, final Throwable t)  wrappedCallback.onResult(null, t);  );    );', 'public IteratorMemcachedNode> getSequence( String key )  throw new NodeFailureException( \"The node \" + getNodeId( key ) + \" is not available.\" );', 'public static ServiceToken bindToService(Context context, ServiceConnection callback)  ContextWrapper cw = new ContextWrapper(context); cw.startService(new Intent(cw, MediaPlaybackService.class)); ServiceBinder sb = new ServiceBinder(callback); if (cw.bindService((new Intent()).setClass(cw, MediaPlaybackService.class), sb, 0))  sConnectionMap.put(cw, sb); return new ServiceToken(cw);  return null;', 'private void traverseObjectWithParents( Class?> cls, Object target )  if ( cls == null )  return;  if ( cls.isArray() )  evaluateArray( target );  else if ( isQualifiedForInterpolation( cls ) )  for ( Field currentField : getFields( cls ) )  Class?> type = currentField.getType(); if ( isQualifiedForInterpolation( currentField, type ) )  synchronized ( currentField )  interpolateField( cls, target, currentField, type );    traverseObjectWithParents( cls.getSuperclass(), target );', 'private void getJpgInfo(FileInputStream fileStream) throws IOException, BadImageException  if (fileStream.read() == 0xFF && fileStream.read() == 0xD8)  while (true)  int marker; do  marker = fileStream.read();  while (marker!= 0xFF); do  marker = fileStream.read();  while (marker == 0xFF); if ((marker >= 0xC0) && (marker = 0xC3)) || ((marker >= 0xC5) && (marker = 0xCB)) || ((marker >= 0xC5) && (marker = 0xCB)) || ((marker >= 0xCD) && (marker = 0xCF)))  fileStream.skip(3); height = shortBigEndian((byte) fileStream.read(), (byte) fileStream.read()); width = shortBigEndian((byte) fileStream.read(), (byte) fileStream.read()); break;  else  fileStream.skip(shortBigEndian((byte) fileStream.read(), (byte) fileStream.read()) - 2);    else  throw new BadImageException(\"Bad JPG image: \"+filename);', 'public Builder applyConnectionString(final ConnectionString connectionString)  if (connectionString.getSslEnabled()!= null)  this.enabled = connectionString.getSslEnabled();  String int i = connectionString.getSslInvalidHostnameAllowed = connectionString.getSslInvalidHostnameAllowed(); if (invalidHostnameAllowed!= null)  this.invalidHostNameAllowed = proxied.getSslInvalidHostnameAllowed();  return this;', 'public List getCompileClasspathElements()  List list = new ArrayList( getArtifacts().size() ); for ( Iterator i = getArtifacts().iterator(); i.hasNext(); )  Artifact a = (Artifact) i.next(); // TODO: let the scope handler deal with this if ( Artifact.SCOPE_COMPILE.equals( a.getScope() ) )  // TODO: this assumes resolution, which may not have been the case - improve error reporting in that instance list.add( a.getFile().getPath() );   return list;', 'public ArtifactResolutionResult resolve( ArtifactResolutionRequest request )  if ( request == null ) throw new IllegalArgumentException( LANG.getMessage( \"null.request\" ) ); if ( request.getArtifact() == null ) throw new IllegalArgumentException( LANG.getMessage( \"null.request.artifact\" ) ); ArtifactResolutionResult result = new ArtifactResolutionResult(); ListRepository> repos = MercuryAdaptor.toMercuryRepos( request.getLocalRepository(), request.getRemoteRepostories(), _dependencyProcessor ); try  ListArtifactMetadata> mercuryMetadataList = _mercury.resolve( repos, null, MercuryAdaptor.toMercuryMetadata( request.getArtifact() ) ); Listorg.apache.maven.mercury.artifact.Artifact> mercuryArtifactList = _mercury.read( repos, mercuryMetadataList ); if (!Util.isEmpty( mercuryArtifactList ) ) for ( org.apache.maven.mercury.artifact.Artifact a : mercuryArtifactList ) result.addArtifact( MercuryAdaptor.toMavenArtifact( _artifactFactory, a ) );  catch ( RepositoryException e )  result.addErrorArtifactException( new ArtifactResolutionException( e.getMessage(), request.getArtifact(), request.getRemoteRepostories() ) );  return result;', 'private void addDependency( XMLWriter writer, Artifact artifact )  File path = artifact.getFile(); if ( path == null )  System.err.println( \"The artifacts path was null. Artifact id: \" + artifact.getId() ); return;  writer.startElement( \"classpathentry\" ); writer.addAttribute( \"kind\", \"var\" ); writer.addAttribute( \"path\", \"M2_REPO/\" + toRelative( localRepository, path.getPath() ) ); writer.endElement();', 'public void update(StoreDefinition storeDef)  if(!useOneEnvPerStore) throw new VoldemortException(\"Memory foot print can be set only when using different environments per store\"); Environment environment = environments.get(storeDef.getName()); // change reservation amount of reserved store if(!unreservedStores.contains(environment) && storeDef.hasMemoryFootprint())  EnvironmentMutableConfig mConfig = environment.getMutableConfig(); long currentCacheSize = mConfig.getCacheSize(); long newCacheSize = storeDef.getMemoryFootprintMB() * BYTES_PER_MB; if(currentCacheSize!= newCacheSize)  this.reservedCacheSize = this.reservedCacheSize - currentCacheSize + newCacheSize; adjustCacheSizes(); mConfig.setCacheSize(newCacheSize); environment.setMutableConfig(mConfig); logger.info(\"Setting private cache for store \" + storeDef.getName() + \" to \" + newCacheSize);   else  // we cannot support changing a reserved store to unreserved or vice // versa since the sharedCache param is not mutable throw new VoldemortException(\"Cannot switch between shared and private cache dynamically\");', 'private void addAccountInfo(RawContactDeltaList rawContactDeltas)  mAccountHeaderContainer.setVisibility(View.GONE); mRawContactContainer.setVisibility(View.GONE); if (mPrimaryNameKindSectionData == null) return; final RawContactDelta rawContactDelta = mPrimaryNameKindSectionData.first.getRawContactDelta(); // Get the account information for the primary raw contact delta final PairString,String> accountInfo = mIsUserProfile? EditorUiUtils.getLocalAccountInfo(getContext(), rawContactDelta.getAccountName(), rawContactDelta.getAccountType(mAccountTypeManager)) : EditorUiUtils.getAccountInfo(getContext(), rawContactDelta.getAccountName(), rawContactDelta.getAccountType(mAccountTypeManager)); // Either the account header or selector should be shown, not both. final ListAccountWithDataSet> accounts = AccountTypeManager.getInstance(getContext()).getAccounts(true); if (mHasNewContact &&!mIsUserProfile)  if (accounts.size() > 1)  addAccountSelector(accountInfo, rawContactDelta);  else  addAccountHeader(accountInfo);   else if (mIsUserProfile ||!shouldHideAccountContainer(rawContactDeltas))  addAccountHeader(accountInfo);  // The raw contact selector should only display linked raw contacts that can be edited in // the full editor (i.e. they are not newly created raw contacts) final RawContactAccountListAdapter adapter = new RawContactAccountListAdapter(getContext(), getRawContactDeltaListForSelector(rawContactDeltas)); final', 'private void importDependencyManagement( Model model, ModelBuildingRequest request, ListModelProblem> problems )  DependencyManagement depMngt = model.getDependencyManagement(); if ( depMngt == null )  return;  ModelResolver modelResolver = request.getModelResolver(); ModelBuildingRequest importRequest = null; ListModel> importModels = null; for ( IteratorDependency> it = depMngt.getDependencies().iterator(); it.hasNext(); )  Dependency dependency = it.next(); if (!\"pom\".equals( dependency.getType() ) ||!\"import\".equals( dependency.getScope() ) )  continue;  it.remove(); if ( modelResolver == null )  throw new IllegalArgumentException( \"no model resolver provided, cannot resolve import POM \" + toId( dependency ) + \" for POM \" + toSourceHint( model ) );  ModelSource importSource; try  importSource = modelResolver.resolveModel( dependency.getGroupId(), dependency.getArtifactId(), dependency.getVersion() );  catch ( UnresolvableModelException e )  problems.add( new ModelProblem( \"Non-resolvable import POM \" + toId( dependency ) + \" for POM \" + toSourceHint( model ) + \": \" + e.getMessage(), ModelProblem.Severity.ERROR, toSourceHint( model ), e ) ); continue;  if ( importRequest == null )  importRequest = new DefaultModelBuildingRequest(); importRequest.setValidationLevel( ModelBuildingRequest.VALIDATION_LEVEL_MINIMAL );  importRequest.setModelSource( importSource ); importRequest.setModel', 'private void initForDefaultLocale()  mCurrentLocale = getLocale(); mNameSplitter = mDbHelper.createNameSplitter(); mNameLookupBuilder = new StructuredNameLookupBuilder(mNameSplitter); mPostalSplitter = new PostalSplitter(mCurrentLocale); mCommonNicknameCache = new CommonNicknameCache(mDbHelper.getReadableDatabase()); ContactLocaleUtils.getIntance().setLocale(mCurrentLocale); mContactAggregator = new ContactAggregator(this, mDbHelper, createPhotoPriorityResolver(getContext()), mNameSplitter, mCommonNicknameCache); mContactAggregator.setEnabled(SystemProperties.getBoolean(AGGREGATE_CONTACTS, true)); mDataRowHandlers = new HashMapString, DataRowHandler>(); mDataRowHandlers.put(Email.CONTENT_ITEM_TYPE, new DataRowHandlerForEmail(mDbHelper, mContactAggregator)); mDataRowHandlers.put(Im.CONTENT_ITEM_TYPE, new DataRowHandlerForEmail(mDbHelper, mContactAggregator)); mDataRowHandlers.put(Im.CONTENT_ITEM_TYPE, new DataRowHandlerForCommonDataKind(mDbHelper, mContactAggregator, Im.CONTENT_ITEM_TYPE, Im.TYPE, Im.LABEL)); mDataRowHandlers.put(Nickname.CONTENT_ITEM_TYPE, new DataRowHandlerForCommonDataKind(mDbHelper', 'private CheckSum copyFileWithCheckSum(HdfsFile source, File dest, CheckSumType checkSumType) throws IOException  CheckSum fileCheckSumGenerator = null; logger.debug(\"Starting copy of \" + source + \" to \" + dest); // Check if its Gzip compressed boolean isCompressed = source.isCompressed(); FilterInputStream input = null; OutputStream output = null; long startTimeMS = System.currentTimeMillis(); for (int attempt = 0; attempt  fetcher.getMaxAttempts(); attempt++)  boolean success = true; long totalBytesRead = 0; boolean fsOpened = false; try  // Create a per file checksum generator if (checkSumType!= null)  fileCheckSumGenerator = CheckSum.getInstance(checkSumType);  logger.info(\"Attempt \" + attempt + \" at copy of \" + source + \" to \" + dest); if (!isCompressed)  input = fs.open(source.getPath());  else  // We are already bounded by the \"hdfs.fetcher.buffer.size\" // specified in the Voldemort config, the default value of // which is 64K. Using the same as the buffer size for // GZIPInputStream as well. input = new GZIPInputStream(fs.open(source.getPath()), this.bufferSize);  fsOpened = true; output = new BufferedOutputStream(new FileOutputStream(dest)); while (true)  int read = input.read(buffer); if (read  0)  break;  else  output.write(buffer, 0, read);  // Update the per file checksum if (fileCheckSumGenerator!= null)  fileCheckSumGenerator.update(buffer, 0, read);', 'public void setUp() throws Exception  final int numServers = 1; this.nodeId = 0; this.time = SystemTime.INSTANCE; servers = new VoldemortServer[numServers]; int partitionMap[][] =   0, 1, 2, 3, 4, 5, 6, 7  ; try  // Setup the cluster cluster = ServerTestUtils.startVoldemortCluster(numServers, servers, partitionMap, socketStoreFactory, true, null, storesXmlfile, new Properties());  catch(IOException e)  fail(\"Failure to setup the cluster\");  socketUrl = servers[0].getIdentityNode().getSocketUrl().toString(); ListString> bootstrapUrls = new ArrayListString>(); bootstrapUrls.add(socketUrl); // create a copy of the config file in /tmp and work on that RestClientTestUtils.copyFile(FAT_CLIENT_CONFIG_PATH_ORIGINAL, FAT_CLIENT_CONFIG_PATH); // Setup the Coordinator CoordinatorConfig coordinatorConfig = new CoordinatorConfig(); coordinatorConfig.setBootstrapURLs(bootstrapUrls).setCoordinatorCoreThreads(100).setCoordinatorMaxThreads(100).setFatClientConfigPath(FAT_CLIENT_CONFIG_PATH).setServerPort(9999); try  StoreClientConfigService storeClientConfigs = null; switch(coordinatorConfig.getFatClientConfigSource())  case FILE: storeClientConfigs = new FileBasedStoreClientConfigService(coordinatorConfig); break; case ZOOKEEPER: throw new UnsupportedOperationException(\"Zookeeper-based configs are not', 'public void shouldNotIncludeTheApplicationContextOnTheRootApplicationContextParamIfAlreadySet() throws Exception  ConfigurableWebApplicationContext mock = mockery.mock(ConfigurableWebApplicationContext.class); ConfigurableWebApplicationContext one = new ConfigurableWebApplicationContext(servletContext); one(servletContext).getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE); will(returnValue(mock)); never(servletContext).setAttribute(with(equal(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE)), with(any(WebApplicationContext.class))); ignoring(mock); ); defaultExpectations(); SpringProvider provider = new SpringProvider(); provider.start(servletContext); mockery.assertIsSatisfied();', 'public Outcome execute() throws WikiException  // Retrieve attributes WikiEngine engine = m_context.getEngine(); Workflow workflow = getWorkflow(); // Get the wiki page WikiPage page = m_context.getPage(); // Figure out who the author was. Prefer the author // set programmatically; otherwise get from the // current logged in user if ( page.getAuthor() == null )  Principal wup = m_context.getCurrentUser(); if ( wup!= null ) page.setAuthor( wup.getName() );  // Run the pre-save filters. If any exceptions, add error to list, abort, and redirect String saveText; try  saveText = engine.getFilterManager().doPreSaveFiltering( m_context, m_proposedText );  catch ( FilterException e )  throw e;  // Stash the wiki context, old and new text as workflow attributes workflow.setAttribute( PRESAVE_WIKI_CONTEXT, m_context ); workflow.setAttribute( FACT_PROPOSED_TEXT, saveText ); workflow.setAttribute( PRESAVE_COMPLETE, saveText ); return Outcome.STEP_COMPLETE;', 'private void executeRunsAsync(final IteratorRun> runs, final Connection connection, final BulkWriteBatchCombiner bulkWriteBatchCombiner, final SingleResultCallbackBulkWriteResult> callback)  final Run run = runs.next(); run.executeAsync(connection, new SingleResultCallbackBulkWriteResult>()  @Override public void onResult(final BulkWriteResult result, final Throwable t)  if (t!= null)  if (t instanceof BulkWriteException)  bulkWriteBatchCombiner.addErrorResult((BulkWriteException) t, run.indexMap);  else  callback.onResult(null, t); return;   else if (result.wasAcknowledged())  bulkWriteBatchCombiner.addResult(result, run.indexMap);  // Execute next run or complete if (runs.hasNext() &&!bulkWriteBatchCombiner.shouldStopSendingMoreBatches())  executeRunsAsync(runs, connection, bulkWriteBatchCombiner, callback);  else if (bulkWriteBatchCombiner.hasErrors())  callback.onResult(null, bulkWriteBatchCombiner.getError());  else  callback.onResult(bulkWriteBatchCombiner.getResult(), null);   );', 'public void triggerAggregation(TransactionContext txContext, long rawContactId)  if (!mEnabled)  return;  int aggregationMode = mDbHelper.getAggregationMode(rawContactId); switch (aggregationMode)  case RawContacts.AGGREGATION_MODE_DISABLED: break; case RawContacts.AGGREGATION_MODE_DEFAULT:  markForAggregation(rawContactId, aggregationMode, false); break;  case RawContacts.AGGREGATION_MODE_SUSPENDED:  long contactId = mDbHelper.getContactId(rawContactId); if (contactId!= 0)  updateAggregateData(txContext, contactId);  break;  case RawContacts.AGGREGATION_MODE_IMMEDIATE:  aggregateContact(txContext, mDbHelper.getWritableDatabase(), rawContactId); break;', 'private void rewriteArtifactsAndPoms( List artifacts, ArtifactRepository sourceRepo, ArtifactRepository targetRepo, RepositoryCleanerConfiguration configuration, File reportsBase, File sourceRepositoryBase, File targetRepositoryBase, Reporter repoReporter ) throws Exception  Logger logger = getLogger(); ArtifactPomRewriter artifactPomRewriter = null; try  logger.info(\"Rewriting \" + artifacts.size() + \" artifacts (Should be \" + (artifacts.size() * 2) + \" rewrites including POMs).\"); for ( Iterator it = artifacts.iterator(); it.hasNext(); )  Artifact artifact = (Artifact) it.next(); String artifactReportPath = buildArtifactReportPath(artifact); Reporter artifactReporter = null; try  artifactReporter = new Reporter( reportsBase, artifactReportPath ); boolean errorOccurred = false; File artifactSource = new File( sourceRepo.getBasedir(), sourceRepo.pathOf( artifact ) ); if(artifactSource.exists())  File artifactTarget = new File( targetRepo.getBasedir(), targetRepo.pathOf( artifact ) ); artifact.setFile( artifactSource ); try  if (!configuration.reportOnly() )  if(logger.isDebugEnabled())  logger.debug( \"sourceRepo basedir is: \\'\" + sourceRepo.getBasedir() + \"\\'\" ); logger.debug( \"targetRepo basedir is: \\'\" + targetRepo.getBasedir() + \"\\'\" );  File targetParent =', 'private ListVoldemortService> createServices()  /* Services are given in the order they must be started */ ListVoldemortService> services = new ArrayListVoldemortService>(); SchedulerService scheduler = new SchedulerService(voldemortConfig.getSchedulerThreads(), SystemTime.INSTANCE, voldemortConfig.canInterruptService()); StorageService storageService = new StorageService(storeRepository, metadata, scheduler, voldemortConfig); asyncService = new AsyncOperationService(scheduler, ASYNC_REQUEST_CACHE_SIZE); services.add(storageService); services.add(scheduler); services.add(asyncService); if(voldemortConfig.isHttpServerEnabled())  /* * TODO REST-Server 1. Need to decide on replacing HttpService */ HttpService(this, storageService, storeRepository, RequestFormatType.VOLDEMORT_V1, voldemortConfig.getMaxThreads(), identityNode.getHttpPort()));  if(voldemortConfig.isRestServiceEnabled())  RestService restService = new RestService(voldemortConfig, identityNode.getRestPort(), storeRepository, identityNode.getZoneId(), metadata.getStoreDefList());  if(voldemortConfig.isSocketServerEnabled())  RequestHandlerFactory socketRequestHandlerFactory = new SocketRequestHandlerFactory(storageService, this.storeRepository, this.metadata, this.voldemortConfig, this.asyncService, null); if(voldemortConfig.getUseNioConnector())  logger.info(\"Using NIO Connector.\"); NioSocketService nodeSocketService = new Ni', 'public XmlSerializer include(String... fields)  for (String field : fields)  String class = toSerialize.getClass(); excludes.remove(class, field);  return this;', 'public String renderNodeXhtml(Node node) StringBuilder ret = new StringBuilder(); String url; if(node.lengthChildren() == 0) return \"\";  TextNode txtNode = (TextNode)node.getChildren().iterator().next(); if(node.isParameter()) url = node.getParameter().trim(); else url = txtNode.getText().trim();  String linkText = txtNode.getText().trim(); if(linkText == null || linkText.isEmpty()) linkText = url;  if(url.length()!= 0) ret.append(\"a href=\"\"); ret.append(Parser.escape(url)); ret.append(\"\">\"); ret.append(Parser.escape(linkText)); ret.append(\"/a>\");  return ret.toString();', 'public DependencyNode resolve( Plugin plugin, Artifact pluginArtifact, DependencyFilter dependencyFilter, ListRemoteRepository> repositories, RepositorySystemSession session ) throws PluginResolutionException  if ( pluginArtifact == null )  pluginArtifact = toArtifact( plugin, session );  DependencyFilter collectionFilter = new ScopeDependencyFilter( \"provided\", \"test\" ); DependencyFilter resolutionFilter = new ExclusionsDependencyFilter( artifactFilterManager.getCoreArtifactExcludes() ); resolutionFilter = AndDependencyFilter.newInstance( resolutionFilter, dependencyFilter ); resolutionFilter = new AndDependencyFilter( collectionFilter, resolutionFilter ); DependencyNode node; try  DependencySelector selector = AndDependencySelector.newInstance( session.getDependencySelector(), new WagonExcluder() ); DependencyGraphTransformer transformer = ChainedDependencyGraphTransformer.newInstance( session.getDependencyGraphTransformer(), new PlexusUtilsInjector() ); DefaultRepositorySystemSession pluginSession = new DefaultRepositorySystemSession( session ); pluginSession.setDependencySelector( selector ); pluginSession.setDependencyGraphTransformer( transformer ); CollectRequest request = new CollectRequest(); request.setRequestContext( REPOSITORY_CONTEXT ); request.setRepositories( repositories ); request.setRoot( new org.sonatype.aether.graph.Dependency( pluginArtifact, null ) ); for ( Dependency dependency : plugin.getDependencies() )  org.sonatype.aether.graph.Dependency pluginDep = Re', 'private int buildIndex(SQLiteDatabase db, String selection, boolean replace)  mSb.setLength(0); mSb.append(Data.CONTACT_ID + \", \"); mSb.append(\"(CASE WHEN \" + DataColumns.MIMETYPE_ID + \"=\"); mSb.append(mDbHelper.getMimeTypeId(Nickname.CONTENT_ITEM_TYPE)); mSb.append(\" THEN -4 \"); mSb.append(\" WHEN \" + DataColumns.MIMETYPE_ID + \"=\"); mSb.append(mDbHelper.getMimeTypeId(Organization.CONTENT_ITEM_TYPE)); mSb.append(\" THEN -3 \"); mSb.append(\" WHEN \" + DataColumns.MIMETYPE_ID + \"=\"); mSb.append(mDbHelper.getMimeTypeId(StructuredPostal.CONTENT_ITEM_TYPE)); mSb.append(\" THEN -2\"); mSb.append(\" WHEN \" + DataColumns.MIMETYPE_ID + \"=\"); mSb.append(mDbHelper.getMimeTypeId(Email.CONTENT_ITEM_TYPE)); mSb.append(\" THEN -1\"); mSb.append(\" ELSE \" + DataColumns.MIMETYPE_ID); mSb.append(\" END), \" + Data.IS_SUPER_PRIMARY + \", \" + DataColumns.CONCRETE_ID); int count = 0; Cursor cursor = db.query(Tables.DATA_JOIN_MIMETYPE_RAW_CONTACTS, ContactIndexQuery.COLUMNS, selection, null, null, nu', 'public static Chunk readChunk(InputStream stream) throws IOException  String header = readLine(stream); if (header == null)  return null;  if (!HEADER_PATTERN.matcher(header).matches())  throw new IOException(\"Chunk header corrupted\");  StringTokenizer tokenizer = new StringTokenizer(header, \";=rn\"); int size = Integer.parseInt(tokenizer.nextToken(), 16); Map parameters = new LinkedHashMap(); while (tokenizer.hasMoreTokens())  String name = tokenizer.nextToken(); String value = tokenizer.nextToken(); parameters.put(name, value);  byte[] bytes = new byte[size]; int total = 0; while (total  size)  int read = stream.read(bytes, total, size - total); if (read == -1)  throw new IOException(\"Unexpected end of stream in chunk data\");  total += read;  if (stream.read()!= \\'r\\' || stream.read()!= \\'n\\')  throw new IOException(\"Chunk trailer corrupted\");  return new Chunk(bytes, parameters);', 'public boolean setup(ParamRepository par, String outDir)  cmds = CmdService.getHandle(); /* Get tool related parameters */ List hostClasses = par.getTokenizedParameters(\"fa:hostConfig/fa:host\"); List allTools = par.getParameters(\"fa:hostConfig/fh:tools\"); List enabled = par.getParameters(\"fa:hostConfig/fh:enabled\"); if(hostClasses.size()!= enabled.size())  logger.warning(\"Number of hosts does not match \" + \"Number of enabled node\"); return false;  // HashMap containing exclusive list of tools to start on each machine. HashMapString, HashSetString>> hostMap = new HashMapString, HashSetString>>(); // Temporary tool list for host class being processed. ArrayListString> newTools = new ArrayListString>(); // First we flatten out the classes into host names and tools sets for (int i = 0; i  hostClasses.size(); i++)  // Ignore if the host class is not enabled. if (!Boolean.parseBoolean((String)enabled.get(i)) continue; String toolCmds = ((String) allTools.get(i)).trim(); // Ignore class if no tools to start. if (toolCmds.length() == 0 || toolCmds.toUpperCase().equals(\"NONE\")) continue; // Get the hosts list in the class. String[] hosts = (String[]) hostClasses.get(i); // Get the tools list for this host list. StringTokenizer st = new StringTokenizer(toolCmds, \";\"); while(st.hasMore', 'public MavenProject buildFromRepository( Artifact artifact, ListArtifactRepository> remoteRepositories, ArtifactRepository localRepository ) throws ProjectBuildingException  ProjectBuilderConfiguration configuration = new DefaultProjectBuilderConfiguration().setLocalRepository( localRepository ).setRemoteRepositories( remoteRepositories ); configuration.setProcessPlugins( false ); configuration.setValidationLevel( ModelBuildingRequest.VALIDATION_LEVEL_MINIMAL ); MavenSession session = legacySupport.getSession(); if ( session!= null )  configuration.setSystemProperties( session.getSystemProperties() ); configuration.setUserProperties( session.getUserProperties() );  else  configuration.setSystemProperties( System.getProperties() );  return buildFromRepository( artifact, configuration );', 'public void testSetRenamedUserProfile() throws Exception  // First, count the number of users, groups, and pages int oldUserCount = m_db.getWikiNames().length; GroupManager groupManager = m_engine.getGroupManager(); ContentManager contentManager = m_engine.getContentManager(); AuthorizationManager authManager = m_engine.getAuthorizationManager(); int oldGroupCount = groupManager.getRoles().length; int oldPageCount = contentManager.getTotalPageCount( null ); // Setup Step 1: create a new user with random name WikiSession session = m_engine.guestSession(); long now = System.currentTimeMillis(); String oldLogin = \"TestLogin\" + now; String oldName = \"Test User \" + now; String newLogin = \"RenamedLogin\" + now; String newName = \"Renamed User \" + now; UserProfile profile = m_db.newProfile(); profile.setEmail( \"testuser@testville.com\" ); profile.setLoginName( oldLogin ); profile.setFullname ( oldName ); profile.setPassword ( \"password\" ); m_mgr.setUserProfile( session, profile ); // 1a. Make sure the profile saved successfully and that we\\'re logged in profile = m_mgr.getUserProfile( session ); assertEquals( oldLogin, profile.getLoginName() ); assertEquals( oldName, profile.getFullname() ); assertEquals( oldUserCount+1, m_db.getWikiNames().length ); assertTrue( session.isAuthenticated() ); // Setup Step 2: create a new group with our test user in it Group group = groupManager.parseGroup( m_groupName, \"Alice n Bob n Charlie n \" + oldLogin + \"n\" + oldName, true', 'private HtmlTag makeAttributeRadiosHtml(String label, String[] attributes, String radioGroup, PageData pageData) throws Exception  HtmlTag div = new HtmlTag(\"div\"); div.addAttribute(\"style\", \"float: left; width: 150px;\"); div.add(label); String checkedAttribute = getCheckedAttribute(pageData, attributes); for (String attribute : attributes)  div.add(HtmlUtil.BR); div.add(makeAttributeRadio(radioGroup, attribute, attribute.equals(checkedAttribute), attribute));  div.add(HtmlUtil.BR); div.add(HtmlUtil.BR); div.add(HtmlUtil.BR); div.add(makeAttributeCheckbox(PropertyPRUNE, \"Skip (Recursive)\", pageData)); return div;', 'private ModelAndView getMessage( WebRequest webRequest, HttpServletRequest request, HttpServletResponse response, PreparedTopic preparedMessage, Group group, int page, String filter ) throws Exception  Topic message = preparedMessage.getMessage(); Template tmpl = Template.getTemplate(request); MapString, Object> params = new HashMapString, Object>(); params.put(\"showAdsense\",!tmpl.isSessionAuthorized() ||!tmpl.getProf().isHideAdsense()); params.put(\"page\", page); boolean showDeleted = request.getParameter(\"deleted\")!= null; if (showDeleted)  page = -1;  boolean rss = request.getParameter(\"output\")!= null && \"rss\".equals(request.getParameter(\"output\")); if (showDeleted &&!\"POST\".equals(request.getMethod()))  return new ModelAndView(new RedirectView(message.getLink()));  if (page == -1 &&!tmpl.isSessionAuthorized())  return new ModelAndView(new RedirectView(message.getLink()));  if (showDeleted)  if (!tmpl.isSessionAuthorized())  throw new BadInputException(\"’  2»   ??14\");   params.put(\"showDeleted\", showDeleted); User currentUser = tmpl.getCurrentUser(); if (message.isExpired() && showDeleted &&!tmpl.isModeratorSession())', 'private PreparedComment prepareComment( MessageText messageText, @Nonnull Comment comment, CommentList comments, boolean secure, Template tmpl, Topic topic ) throws UserNotFoundException  User author = userDao.getUserCached(comment.getUserid()); String processedMessage; processedMessage = prepareCommentText(messageText, secure,!topicPermissionService.followAuthorLinks(author)); User replyAuthor = null; Comment reply = null; int replyPage = 0; boolean deletable = false; boolean editable = false; boolean samePage = false; if (comments!= null)  if (comment.getReplyTo()!= 0)  CommentNode replyNode = comments.getNode(comment.getReplyTo()); if (replyNode!=null)  reply = replyNode.getComment(); if(tmpl!= null)  reply = replyNode.getComment(); if(tmpl!= null)  replyPage = comments.getCommentPage(reply, tmpl.getProf());  replyAuthor = userDao.getUserCached(reply.getUserid());  if (tmpl!=null)  samePage = comments.getCommentPage(comment, tmpl.getProf()) == replyPage;   boolean haveAnswers = comments.getNode(comment.getId()).isHaveAnswers(); if(tmpl!= null && topic!= null)  final boolean authored = author.getNick().equals(tmpl.getNick()); final long currentTimestamp = comment.getPostdate().getTime(); final User currentUser =', 'public ListString> getTestClasspathElements() throws DependencyResolutionRequiredException  ListString> list = new ArrayListString>( getArtifacts().size() + 2 ); List.add( getBuild().getTestOutputDirectory() ); List.add( getBuild().getOutputDirectory() ); for ( Artifact a : getArtifacts() )  if ( a.getArtifactHandler().isAddedToClasspath() )  addArtifactPath( a, list );   return list;', 'void cli(CliRequest cliRequest) throws Exception  // // Parsing errors can happen during the processing of the arguments and we prefer not having to check if // the logger is null and construct this so we can use an SLF4J logger everywhere. // slf4jLogger = new Slf4jStdoutLogger(); cliManager = new CLIManager(); ListString> args = new ArrayList>(); CommandLine mavenConfig = null; try  File configFile = new File(cliRequest.multiModuleProjectDirectory, MVN_MAVEN_CONFIG); if (configFile.isFile())  for (String arg : Files.readAllLines(configFile.toPath(), Charset.defaultCharset()))  if (!arg.isEmpty())  args.add(arg);   mavenConfig = cliManager.parse(args.toArray(new String[0])); List?> unrecognized = mavenConfig.getArgList(); if (!unrecognized.isEmpty())  throw new ParseException(\"Unrecognized maven.config entries: \" + unrecognized);    catch (ParseException e)  System.err.println(\"Unable to parse maven.config: \" + e.getMessage()); cliManager.displayHelp(System.out); throw e;  try  if (mavenConfig == null)  cliRequest.commandLine = cliManager.parse(cliRequest.args);  else  cliRequest.commandLine = cliMerge(cliManager.parse(cliRequest.args), mavenConfig);   catch (ParseException e)  System.err.println(\"Unable', 'public void setUp() throws Exception  mockery = new Mockery(); dao = mockery.mock(DvdDao.class); session = mockery.mock(HttpSession.class); //ignoring session mockery.checking(new Expectations()   allowing(session).getAttribute(UserInfo.CURRENT_USER); will(returnValue(null)); ignoring(session);  ); userInfo = new UserInfo(session); userInfo.login(new User()); result = new MockResult(); Validator validator = new MockValidator(); controller = new DvdsController(dao, userInfo, result, validator);', 'private static void processCommands(StoreClientFactory factory, BufferedReader reader, boolean printCommands) throws IOException  for(String line = reader.readLine(); line!= null; line = reader.readLine())  if(line.trim().equals(\"\")) continue; if(printCommands) System.out.println(line); try  if(line.toLowerCase().startsWith(\"put\"))  JsonReader jsonReader = new JsonReader(new StringReader(line.substring(\"put\".length()))); client.put(tightenNumericTypes(jsonReader.read()), tightenNumericTypes(jsonReader.read()));  else if(line.toLowerCase().startsWith(\"getall\"))  JsonReader jsonReader = new JsonReader(new StringReader(line.substring(\"getall\".length()))); ListObject> keys = new ArrayListObject>(); try  while(true) keys.add(jsonReader.read());  catch(EndOfFileException e)  // this is okay, just means we are done reading  MapObject, VersionedObject>> vals = client.getAll(keys); if(vals.size() > 0)  for(Map.EntryObject, VersionedObject>> entry: vals.entrySet())  System.out.print(entry.getKey()); System.out.print(\" => \"); printVersioned(entry.getValue());   else  System.out.println(\"null\");   else if(line.toLowerCase().startsWith(\"get\"))  JsonReader jsonReader = new JsonReader(new StringReader(line', 'private boolean build( ListProjectBuildingResult> results, ListMavenProject> projects, MapString, MavenProject> projectIndex, ListInterimResult> interimResults, ProjectBuildingRequest request, MapFile, Boolean> profilesXmls )  boolean noErrors = true; for ( InterimResult interimResult : interimResults )  try  ModelBuildingResult result = modelBuilder.build( interimResult.request, interimResult.result ); MavenProject project = interimResult.listener.getProject(); initProject( project, projectIndex, result, profilesXmls, request ); ListMavenProject> modules = new ArrayList>(); noErrors = build( results, modules, projectIndex, interimResult.modules, request, profilesXmls ) && noErrors; projects.addAll( modules ); projects.add( project ); project.setExecutionRoot( interimResult.root ); project.setCollectedProjects( modules ); results.add( new DefaultProjectBuildingResult( project, result.getProblems(), null ) );  catch ( ModelBuildingException e )  results.add( new DefaultProjectBuildingResult( e.getModelId(), interimResult.pomFile, e.getProblems() ) ); noErrors = false;   return noErrors;', 'public ListMavenProject> selectProjects( ListFile> files, MavenExecutionRequest request ) throws ProjectBuildingException  ProjectBuildingRequest projectBuildingRequest = request.getProjectBuildingRequest(); ListProjectBuildingResult> results = projectBuilder.build( files, request.isRecursive(), projectBuildingRequest ); ListMavenProject> projects = new ArrayList>( results.size() ); boolean problems = false; for ( ProjectBuildingResult result : results )  projects.add( result.getProject() ); if (!result.getProblems().isEmpty() && LOGGER.isWarnEnabled() )  LOGGER.warn( \"\" ); LOGGER.warn( \"Some problems were encountered while building the effective model for \\'\\'\", result.getProject().getId() ); for ( ModelProblem problem : result.getProblems() )  String loc = ModelProblemUtils.formatLocation( problem, result.getProjectId() ); LOGGER.warn( \"\", problem.getMessage(), ( StringUtils.isNotEmpty( loc )? \" @ \" + loc : \"\" ) );  problems = true;   if ( problems )  LOGGER.warn( \"\" ); LOGGER.warn( \"It is highly recommended to fix these problems\" + \" because they threaten the stability of your build.\" ); LOGGER.warn( \"\" ); LOGGER.warn( \"For this reason, future Maven versions might no\" + \" longer support building such malformed projects.\" ); LOGGER.warn( \"\" );  return projects;', 'public MongoClientOptions getOptions()  ReadPreference readPreference = proxied.getReadPreference(); if (readPreference!= null)  builder.readPreference(readPreference);  ReadConcern readConcern = proxied.getReadConcern(); if (readConcern!= null)  builder.readConcern(readConcern);  WriteConcern writeConcern = proxied.getWriteConcern(); if (writeConcern!= null)  builder.writeConcern(writeConcern);  if (proxied.getRetryWrites())  builder.retryWrites(proxied.getRetryWrites());  Integer maxConnectionPoolSize = proxied.getMaxConnectionPoolSize(); if (maxConnectionPoolSize!= null)  builder.connectionsPerHost(maxConnectionPoolSize);  Integer integer = proxied.getMinConnectionPoolSize(); if (integer!= null)  builder.minConnectionsPerHost(integer);  Integer maxWaitTime = proxied.getMaxWaitTime(); if (maxWaitTime!= null)  builder.maxWaitTime(maxWaitTime);  Integer threadsAllowedToBlockForConnectionMultiplier = proxied.getThreadsAllowedToBlockForConnectionMultiplier(); if (threadsAllowedToBlockForConnectionMultiplier!= null)  builder.threadsAllowedToBlockForConnectionMultiplier(threadsAllowedToBlockForConnectionMultiplier);  Integer maxConnectionIdleTime = proxied.getMaxConnectionIdleTime()', 'private ArtifactTransferEvent wrap( TransferEvent event )  if ( event == null )  return null;  else  String wagon = event.getWagon().getRepository().getUrl(); MavenArtifact artifact = wrap( event.getResource() ); if ( event.getException()!= null )  return new ArtifactTransferEvent( wagon, event.getException(), event.getRequestType(), artifact );  else  return new ArtifactTransferEvent( wagon, event.getEventType(), event.getRequestType(), artifact );', 'public void testQueryContactStrequent()  ContentValues values1 = new ContentValues(); createContact(values1, \"Noah\", \"Tever\", \"18004664411\", \"a@acme.com\", StatusUpdates.OFFLINE, 0, 0, StatusUpdates.CAPABILITY_HAS_CAMERA | StatusUpdates.CAPABILITY_HAS_VIDEO); ContentValues values2 = new ContentValues(); createContact(values2, \"Sam\", \"Times\", \"18004664412\", \"b@acme.com\", StatusUpdates.INVISIBLE, 3, 0, 0, StatusUpdates.CAPABILITY_HAS_CAMERA); ContentValues values3 = new ContentValues(); createContact(values3, \"Lotta\", \"Calling\", \"18004664413\", \"c@acme.com\", StatusUpdates.AWAY, 5, 0, 0, StatusUpdates.CAPABILITY_HAS_VIDEO); ContentValues values4 = new ContentValues(); createContact(values4, \"Fay\", \"Veritt\", \"18004664414\", \"d@acme.com\", StatusUpdates.AVAILABLE, 0, 1, 0, StatusUpdates.CAPABILITY_HAS_VIDEO | StatusUpdates.CAPABILITY_HAS_VOICE); Cursor c = mResolver.query(Contacts.CONTENT_STREQUENT_URI, null, null, null, Contacts._ID); assertEquals(3, c.getCount()); c.moveToFirst(); assertCursorValues(c, values4); c.moveToNext(); assertCursorValues(c, values3); c.moveToNext(); assertCursorValues(c, values3); c.close(); Uri filterUri = Uri.withAppendedPath(Contacts.CONTENT_STREQUENT_FILTER_URI, \"fay\"); c = mResolver.query(filterU', 'public Userpic getUserpic(User user, boolean secure, String avatarStyle)  if (user.getPhoto()!= null)  try  ImageInfo info = new ImageInfo(configuration.getHTMLPathPrefix() + \"/photos/\" + user.getPhoto()); return new Userpic( \"/photos/\" + user.getPhoto(), info.getWidth(), info.getHeight() );  catch (FileNotFoundException e)  logger.warn(\"Userpic not found for : \", user.getNick(), e.getMessage());  catch (BadImageException | IOException e)  logger.warn(\"Bad userpic for \", user.getNick(), e.getMessage());   if (user.hasGravatar())  return new Userpic( user.getGravatar(avatarStyle, 150, secure), 150, 150 );  return new Userpic(\"/img/p.gif\", 1, 1);', 'public static Set createArtifacts( ArtifactFactory artifactFactory, List dependencies, String inheritedScope, ArtifactFilter dependencyFilter, MavenProject project ) throws InvalidDependencyVersionException  Set projectArtifacts = new HashSet( dependencies.size() ); for ( Iterator i = dependencies.iterator(); i.hasNext(); )  Dependency d = (Dependency) i.next(); String scope = d.getScope(); if ( StringUtils.isEmpty( scope ) )  scope = Artifact.SCOPE_COMPILE; d.setScope( scope );  VersionRange versionRange; try  versionRange = VersionRange.createFromVersionSpec( d.getVersion() );  catch ( InvalidVersionSpecificationException e )  throw new InvalidDependencyVersionException( \"Unable to parse version \\'\" + d.getVersion() + \"\\' for dependency \\'\" + d.getManagementKey() + \"\\': \" + e.getMessage(), e );  Artifact artifact = artifactFactory.createDependencyArtifact( d.getGroupId(), d.getArtifactId(), versionRange, d.getType(), d.getClassifier(), scope, inheritedScope, d.isOptional() ); if ( Artifact.SCOPE_SYSTEM.equals( scope ) )  artifact.setFile( new File( d.getSystemPath() ) );  if ( artifact!= null && ( dependencyFilter == null || dependencyFilter.include( artifact ) ) )  if ( d.getExclusions()!= null &&!d.getExclusions().isEmp', 'public static synchronized void addWikiEventListener( Object client, int type, WikiEventListener listener )  // Make sure WikiEventManager exists WikiEventManager.getInstance(); // first, figure out what kind of event is expected to be generated this does // tie us into known types, but WikiEvent.isValidType() will return true so // long as the type was set to any non-ERROR or non-UNKNOWN value if ( WikiEngineEvent.isValidType(type) ) // add listener directly to WikiEngine  WikiEventManager.addWikiEventListener( client, listener );  else if ( WikiPageEvent.isValidType(type) ) // add listener to one of several options  if( type == WikiPageEvent.PAGE_LOCK || type == WikiPageEvent.PAGE_UNLOCK ) // attach to PageManager  if( client instanceof WikiEngine )  WikiEventManager.addWikiEventListener( ((WikiEngine)client).getPageManager(), listener );  else // if ( client instanceof PageManager ) // no filter?  WikiEventManager.addWikiEventListener( client, listener );   else if( type == WikiPageEvent.PAGE_REQUESTED || type == WikiPageEvent.PAGE_DELIVERED ) // attach directly to WikiServletFilter  WikiEventManager.addWikiEventListener( client, listener );  else if( type == WikiPageEvent.PRE_TRANSLATE_BEGIN || type == WikiPageEvent.POST_TRANSLATE_END || type == WikiPageEvent.POST_TRANSLATE_BEGIN || type == WikiPageEvent.POST_TRANSLATE_END || type == Wi', 'public void executeMojo( MavenSession session, String goalName ) throws PluginExecutionException  try  verifyPluginForGoal( goalName, session );  catch ( Exception e )  throw new PluginExecutionException( \"Unable to execute goal: \" + goalName, e );  PluginExecutionRequest request = null; MojoDescriptor mojoDescriptor = getMojoDescriptor( goalName ); if ( mojoDescriptor == null )  throw new PluginExecutionException( \"Unable to find goal: \" + goalName );  try  if ( mojoDescriptor.getRequiresDependencyResolution()!= null )  ArtifactResolver artifactResolver = null; MavenProjectBuilder mavenProjectBuilder = null; try  artifactResolver = (ArtifactResolver) container.lookup( ArtifactResolver.ROLE ); mavenProjectBuilder = (MavenProjectBuilder) container.lookup( MavenProjectBuilder.ROLE ); resolveTransitiveDependencies( session, artifactResolver, mavenProjectBuilder, mojoDescriptor.getRequiresDependencyResolution() ); downloadDependencies( session, artifactResolver );  finally  // TODO: watch out for the exceptions being thrown if ( artifactResolver!= null )  container.release( artifactResolver );  if ( mavenProjectBuilder!= null )  container.release( mavenProjectBuilder );     catch ( Exception e )  throw new PluginExecutionException( \"Unable to resolve required dependencies for goal\", e );  Plugin plugin = null; try  plugin = (Plugin) container.lookup( Plugin.ROLE, goalName );', \"public ArtifactResolutionResult resolve( ArtifactResolutionRequest request )  Artifact rootArtifact = request.getArtifact(); SetArtifact> artifacts = request.getArtifactDependencies(); Map managedVersions = request.getManagedVersionMap(); ListResolutionListener> listeners = request.getListeners(); ArtifactFilter collectionFilter = request.getCollectionFilter(); ArtifactFilter resolutionFilter = request.getResolutionFilter(); //TODO: hack because metadata isn't generated in m2e correctly and i want to run the maven i have in the workspace if ( source == null )  try  source = container.lookup( ArtifactMetadataSource.class );  catch ( ComponentLookupException e )  // won't happen   if ( listeners == null )  listeners = new ArrayListResolutionListener>(); if ( logger.isDebugEnabled() )  listeners.add( new DebugResolutionListener( logger ) );  listeners.add( new WarningResolutionListener( logger ) );  ArtifactResolutionResult result = new ArtifactResolutionResult(); // The root artifact may, or may not be resolved so we need to check before we attempt to resolve. // This is often an artifact like a POM that is taken from disk and we already have hold of the // file reference. But this may be a Maven Plugin that we need to resolve from a remote repository // as well as its dependencies. if ( request.isResolveRoot() /* && rootArtifact.getFile() == null */ )  try  resolve( rootArtifact, request, request.getTransferListener(), false );  catch ( ArtifactResolutionException e )  result.addErrorArtifactException( e ); return result;\", 'public void onClick(View view)  int result = Database.LoadData(getEditText(R.id.pass_filename),getEditText(R.id.pass_password)); switch (result)  case 0: saveDefaultPrefs(); GroupActivity.Launch(mAct, null); break; case -1: errorMessage(\"Unknown error.\"); break; default: errorMessage(result); break;', 'private void readKeyFromFile(Uri uri)  PubkeyBean pubkey = new PubkeyBean(); // find the exact file selected pubkey.setNickname(uri.getLastPathSegment()); byte[] keyData; try  ContentResolver resolver = getContentResolver(); keyData = getBytesFromInputStream(resolver.openInputStream(uri), MAX_KEYFILE_SIZE);  catch (IOException e)  Toast.makeText(PubkeyListActivity.this, R.string.pubkey_import_parse_problem, Toast.LENGTH_LONG).show(); return;  KeyPair kp; if ((kp = readPKCS8Key(keyData))!= null)  pubkey.setType(kp.getPrivate().getAlgorithm()); pubkey.setPrivateKey(kp.getPrivate().getEncoded()); pubkey.setPublicKey(kp.getPublic().getEncoded());  else  try  PEMStructure struct = PEMDecoder.parsePEM(new String(keyData).toCharArray()); boolean encrypted = PEMDecoder.isPEMEncrypted(struct); pubkey.setEncrypted(encrypted); if (!encrypted)  kp = PEMDecoder.decode(struct, null); pubkey.setType(kp.getPrivate().getAlgorithm()); pubkey.setPrivateKey(kp.getPrivate().getEncoded()); pubkey.setPublicKey(kp.getPublic().getEncoded());  else  pubkey.setType(PubkeyDatabase.KEY_TYPE_IMPORTED); pubkey.setPrivateKey(keyData);   catch (IOException e)  Log.e(TAG, \"Pro', 'public TDocument> BsonDocument toBsonDocument(final ClassTDocument> tDocumentClass, final CodecRegistry codecRegistry)  BsonDocumentWriter writer = new BsonDocumentWriter(new BsonDocument()); writer.writeStartDocument(); writer.writeStartDocument(\"$bucket\"); writer.writeName(\"groupBy\"); BuildersHelper.encodeValue(writer, groupBy, codecRegistry); writer.writeStartArray(\"boundaries\"); for (TBoundary boundary : boundaries)  BuildersHelper.encodeValue(writer, boundary, codecRegistry);  writer.writeEndArray(); if (options.getDefaultBucket()!= null)  writer.writeName(\"default\"); BuildersHelper.encodeValue(writer, options.getDefaultBucket(), codecRegistry);  writeBucketOutput(codecRegistry, writer, options.getOutput()); writer.writeEndDocument(); return writer.getDocument();', 'public ServerTuple selectServer(final ServerSelector serverSelector, final OperationContext operationContext)  isTrue(\"open\",!isClosed()); try  CountDownLatch currentPhase = phase.get(); ClusterDescription curDescription = description; ServerSelector compositeServerSelector = getCompositeServerSelector(serverSelector); ServerTuple serverTuple = selectServer(compositeServerSelector, curDescription); boolean selectionFailureLogged = false; long startTimeNanos = System.nanoTime(); long curTimeNanos = startTimeNanos; long maxWaitTimeNanos = getMaxWaitTimeNanos(); while (true)  throwIfIncompatible(curDescription); if (serverTuple!= null)  return serverTuple;  if (curTimeNanos - startTimeNanos > maxWaitTimeNanos)  throw createTimeoutException(serverSelector, curDescription);  if (!selectionFailureLogged)  logServerSelectionFailure(serverSelector, curDescription); selectionFailureLogged = true;  connect(); currentPhase.await(Math.min(maxWaitTimeNanos - startTimeNanos), getMinWaitTimeNanos()), NANOSECONDS); curTimeNanos = System.nanoTime(); currentPhase = phase.get(); curDescription = description; serverTuple = selectServer(compositeServerSelector, curDescription);   catch (InterruptedException e)  throw interruptAndCreateMongoInterruptedException(format(\"Interrupted while waiting for a server that matches %s\", serverSelector), e);', 'public void testRawContactUpdate_updatesContactUpdatedTimestamp()  DatabaseAsserts.ContactIdPair ids = DatabaseAsserts.assertAndCreateContact(mResolver); long baseTime = ContactUtil.queryContactLastUpdatedTimestamp(mResolver, ids.mContactId); ContentValues values = new ContentValues(); values.put(ContactsContract.RawContacts.STARRED, 1); RawContactUtil.update(mResolver, ids.mRawContactId, values); long newTime = ContactUtil.queryContactLastUpdatedTimestamp(mResolver, ids.mContactId); assertTrue(newTime > baseTime); // Clean up RawContactUtil.delete(mResolver, ids.mRawContactId, true);', 'public void roundedRect(double cx, double cy, double width, double height, double rx, double ry)  double halfWidth = width / 2.0; double halfHeight = height / 2.0; double dx = rx; double dy = ry; // rx/ry cannot be greater than half of the width of the rectangle // (required by SVG spec) dx = Math.min(dx, width * 0.5); dy = Math.min(dy, height * 0.5); moveto(cx + dx - halfWidth, cy - halfHeight); if (dx  width * 0.5) lineto(cx + halfWidth - rx, cy - halfHeight); curveto(cx + halfWidth - dx * ONE_MINUS_QUARTER, cy - halfHeight, cy + halfWidth, cy - halfHeight + dy * ONE_MINUS_QUARTER, cx + halfWidth, dy * ONE_MINUS_QUARTER, cx + halfWidth, cy - halfHeight + dy); if (dy  height * 0.5) lineto(cx + halfWidth - dy); curveto(cx + halfWidth, cy + halfHeight - dy * ONE_MINUS_QUARTER, cx + halfWidth - dy * ONE_MINUS_QUARTER, cx + halfWidth - dy); if (dy  height * 0.5) lineto(cx + dx, dy * ONE_MINUS_QUARTER, cx + halfWidth - dx * ONE_MINUS_QUARTER, cx + halfHeight, cx + halfHeight - dy * ONE_MINUS_QUARTER,', 'public T> CASResponse cas(String key, long casId, int exp, T value, TranscoderT> tc)  try  return asyncCAS(key, casId, exp, value, tc).get(operationTimeout, TimeUnit.MILLISECONDS);  catch (InterruptedException e)  throw new RuntimeException(\"Interrupted waiting for value\", e);  catch (ExecutionException e)  throw new RuntimeException(\"Exception waiting for value\", e);  catch (TimeoutException e)  throw new OperationTimeoutException(\"Timeout waiting for value\", e);', 'private ReplyHeader replaceResponseTo(final ReplyHeader header, final int responseTo)  ByteBuffer headerByteBuffer = ByteBuffer.allocate(36); headerByteBuffer.order(ByteOrder.LITTLE_ENDIAN); headerByteBuffer.putInt(header.getMessageLength()); headerByteBuffer.putInt(header.getRequestId()); headerByteBuffer.putInt(responseTo); headerByteBuffer.putInt(1); headerByteBuffer.putInt(header.getResponseFlags()); headerByteBuffer.putLong(header.getCursorId()); headerByteBuffer.putInt(header.getStartingFrom()); headerByteBuffer.putInt(header.getNumberReturned()); headerByteBuffer.flip(); ByteBufferBsonInput headerInputBuffer = new ByteBufferBsonInput(new ByteBufNIO(headerByteBuffer)); MessageHeader messageHeader = new MessageHeader(headerInputBuffer, ConnectionDescription.getDefaultMaxMessageSize()); return new ReplyHeader(headerInputBuffer, messageHeader);', 'private int[][] getPartitionMap(int nodes, int perNode)  int[][] partitionMap = new int[nodes][perNode]; int i, k; partitionMap = partitionMap[i][j] = k++; partitionMap = partitionMap; if (!i=0) partitionMap = partitionMap;  return partitionMap;', 'public static boolean verifySaltedPassword(final byte[] password, final String entry ) throws NoSuchAlgorithmException  // First, extract everything after SSHA and decode from Base64 if(!entry.startsWith( SSHA ) )  throw new IllegalArgumentException( \"Hash not prefixed by SSHA; is it really a salted hash?\" );  final byte[] challenge = Base64.getDecoder().decode( entry.substring( 6 ).getBytes( StandardCharsets.UTF_8 ) ); // Extract the password hash and salt final byte[] passwordHash = extractPasswordHash( challenge ); final byte[] salt = extractSalt( challenge ); // Re-create the hash using the password and the extracted salt final MessageDigest digest = MessageDigest.getInstance( \"SHA\" ); digest.update( password ); final byte[] hash = digest.digest( salt ); // See if our extracted hash matches what we just re-created return Arrays.equals( passwordHash, hash );', 'private void addMatchCandidatesFullName(String givenName, String familyName, int mode, MatchCandidateList candidates)  final String givenNameN = NameNormalizer.normalize(givenName); final String familyNameN = NameNormalizer.normalize(familyName); candidates.add(givenNameN + \".\" + familyNameN, NameLookupType.FULL_NAME); candidates.add(familyNameN + \".\" + givenNameN, NameLookupType.FULL_NAME_REVERSE); candidates.add(givenNameN + familyNameN, NameLookupType.FULL_NAME_CONCATENATED); candidates.add(givenNameN + familyNameN, NameLookupType.FULL_NAME_REVERSE_CONCATENATED); candidates.add(familyNameN + givenNameN, NameLookupType.FULL_NAME_REVERSE_CONCATENATED); if (mode == MODE_AGGREGATION || mode == MODE_SUGGESTIONS)  candidates.add(givenNameN, NameLookupType.GIVEN_NAME_ONLY); candidates.add(familyNameN, NameLookupType.FAMILY_NAME_ONLY);', 'private static void copy(Build source, Build target)  if(target.getFinalName() == null)  target.setFinalName( source.getFinalName() );  if(target.getDefaultGoal() == null)  target.setDefaultGoal( source.getDefaultGoal() );  if(target.getDirectory() == null)  target.setDirectory( source.getDirectory() );  if(target.getOutputDirectory() == null)  target.setOutputDirectory( source.getOutputDirectory() );  if(target.getScriptSourceDirectory() == null)  target.setScriptSourceDirectory( source.getSourceDirectory() );  if(target.getTestOutputDirectory() == null)  target.setTestOutputDirectory( source.getTestOutputDirectory() );  if(target.getTestSourceDirectory() == null)  target.setTestOutputDirectory( source.getTestSourceDirectory() );  target.getFilters().addAll( new ArrayListString>(source.getFilters()) ); for(Extension extension : source.getExtensions())  Extension e = new Extension(); e.setArtifactId( extension.getArtifactId() ); e.setGroupId( extension.getGroupId() ); e.setVersion( extension.getVersion() ); target.addExtension( e );  if(target.getResources().isEmpty())  for(Resource resource : source.getResources())  Resource r = new Resource(); r.setDirectory( resource.getDirectory());', 'public void testConnectDaemon() throws IOException, InterruptedException  _client.set( \"foo\", 3600, \"bar\" ); Assert.assertEquals( \"bar\", _client.get( \"foo\" ) );', 'public void roundedRect(double cx, double cy, double width, double height, double rx, double ry)  double halfWidth = width / 2.0; double halfHeight = height / 2.0; double dx = rx; double dy = ry; // rx/ry cannot be greater than half of the width of the rectangle // (required by SVG spec) dx = Math.min(dx, width * 0.5); dy = Math.min(dy, height * 0.5); moveto(cx + dx - halfWidth, cy - halfHeight); if (dx  width * 0.5) lineto(cx + halfWidth - rx, cy - halfHeight); curveto(cx + halfWidth - dx * ONE_MINUS_QUARTER, cy - halfHeight, cy + halfWidth, cy - halfHeight + dy * ONE_MINUS_QUARTER, cx + halfWidth, dy * ONE_MINUS_QUARTER, cx + halfWidth, cy - halfHeight + dy); if (dy  height * 0.5) lineto(cx + halfWidth - dy); curveto(cx + halfWidth, cy + halfHeight - dy * ONE_MINUS_QUARTER, cx + halfWidth - dy * ONE_MINUS_QUARTER, cx + halfWidth - dy); if (dy  height * 0.5) lineto(cx + dx, dy * ONE_MINUS_QUARTER, cx + halfWidth - dx * ONE_MINUS_QUARTER, cx + halfHeight, cx + halfHeight - dy * ONE_MINUS_QUARTER,', 'public static void mergePluginDefinitions( Plugin child, Plugin parent, boolean handleAsInheritance )  if( child == null || parent == null )  // nothing to do. return;  if ( child.getVersion() == null && parent.getVersion()!= null )  child.setVersion( parent.getVersion() );  List parentGoals = parent.getGoals(); // if the supplemental goals are non-existent, then nothing related to goals changes. if ( parentGoals!= null &&!parentGoals.isEmpty() )  Map childGoals = child.getGoalsAsMap(); if ( childGoals!= null )  for ( Iterator it = parentGoals.iterator(); it.hasNext(); )  Goal parentGoal = (Goal) it.next(); String parentInherited = parentGoal.getInherited(); if (!handleAsInheritance || parentInherited == null || Boolean.valueOf( parentInherited ).booleanValue() )  Goal assembledGoal = parentGoal; Goal childGoal = (Goal) childGoals.get( parentGoal.getId() ); if ( childGoal!= null )  Xpp3Dom childGoalConfig = (Xpp3Dom) childGoal.getConfiguration(); Xpp3Dom parentGoalConfig = (Xpp3Dom) parentGoal.getConfiguration(); childGoalConfig = Xpp3Dom.mergeXpp3Dom( childGoalConfig, parentGoalConfig ); childGoal.setConfiguration( childGoalConfig ); assembledGoal = childGoal;  if ( handleAsInheritance && parentInherited == null )', 'protected boolean updateAccountsInBackground(Account[] accounts)  // TODO : Check the unit test. boolean accountsChanged = false; HashSetAccount> existingAccounts = new HashSetAccount>(); mDb = mDbHelper.getWritableDatabase(); mDb.beginTransaction(); try  findValidAccounts(existingAccounts); // Add a row to the ACCOUNTS table for each new account for (Account account : accounts)  if (!existingAccounts.contains(account))  accountsChanged = true; mDb.execSQL(\"INSERT INTO \" + Tables.ACCOUNTS + \" (\" + RawContacts.ACCOUNT_NAME + \", \" + RawContacts.ACCOUNT_TYPE + \") VALUES (?,?)\", new String[] account.name, account.type);   // Remove all valid accounts from the existing account set. What is left // in the accountsToDelete set will be extra accounts whose data must be deleted. HashSetAccount> accountsToDelete = new HashSetAccount>(existingAccounts); for (Account account : accountsToDelete)  Log.d(TAG, \"removing data for removed account \" + account); String[] params = new String[] account.name, account.type; mDb.execSQL( \"DELETE FROM \" + Tables.GROUPS + \" WHERE \" + Groups.ACCOUNT_NAME + \" =?\" + \" AND \" + Groups.ACCOUNT_TYPE + \" =?\", params); mDb.execSQL( \"DELETE FROM \" + Tables.PRESENCE + \" WHERE \" + PresenceColumns.', 'public void connect()  connection = new Connection(host.getHostname(), host.getPort()); connection.addConnectionMonitor(this); try  connection.setCompression(compression);  catch (IOException e)  Log.e(TAG, \"Could not enable compression!\", e);  try  /* Uncomment when debugging SSH protocol: DebugLogger logger = new DebugLogger()  public void log(int level, String className, String message)  Log.d(\"SSH\", message);  ; Logger.enabled = true; Logger.logger = logger; */ connectionInfo = connection.connect(new HostKeyVerifier()); connected = true; if (connectionInfo.clientToServerCryptoAlgorithm.equals(connectionInfo.serverToClientCryptoAlgorithm) && connectionInfo.clientToServerMACAlgorithm.equals(connectionInfo.serverToClientMACAlgorithm))  bridge.outputLine(manager.res.getString(R.string.terminal_using_algorithm, connectionInfo.clientToServerCryptoAlgorithm, connectionInfo.clientToServerMACAlgorithm));  else  bridge.outputLine(manager.res.getString( R.string.terminal_using_c2s_algorithm, connectionInfo.clientToServerCryptoAlgorithm, connectionInfo.clientToServerMACAlgorithm)); bridge.outputLine(manager.res.getString( R.string.terminal_using_s2c_algorithm, connectionInfo.serverToClientCryptoAlgorithm, connectionInfo.serverToClientMACAlgorithm));   catch (IOException e)  Log.e(TAG, \"Problem in SSH connection thread during authentication\", e); // Display the reason in the text. bridge', 'public String pathFor(ResourceMethod method)  String format = request.getParameter(\"_format\"); String suffix = \"\"; if (format!= null &&!format.equals(\"html\"))  suffix = \".\" + format;  String type = method.getResource().getType().getSimpleName() + \"/\" + type.getMethod().getName() + suffix + \".jsp\"; if (format!= null &&!format.equals(\"html\"))  suffix = \".\" + format;  return \"/\" + type.getResource().getType().getSimpleName() + \"/\" + method.getMethod().getName() + suffix + \".jsp\";', 'public void passesTheWebMethod() throws SecurityException, NoSuchMethodException  final HttpMethod delete = HttpMethod.DELETE; final Route route = mockery.mock(Route.class); mockery.checking(new Expectations()  allowing(route).canHandle(\"/clients/add\"); will(returnValue(true)); allowing(route).allowedMethods(); will(returnValue(EnumSet.of(delete))); allowing(route).resourceMethod(request, \"/clients/add\"); will(returnValue(method)); allowing(route).getPriority(); ); router.add(route); ResourceMethod found = router.parse(\"/clients/add\", delete, request); assertThat(found, is(equalTo(method))); mockery.assertIsSatisfied();', 'public void onDismiss(DialogInterface dialog)  DialogInterface dialog = super.onDismiss(dialog); if (dialog == null)  getActivity().finish();', 'public static String replacement(String from)  if(from.equals(\"lt\")) return \"\"; else if(from.equals(\"gt\")) return \">\"; else if(from.equals(\"amp\")) return \"&\"; else if(from.equals(\"nbsp\")) return \" \"; else return null;', 'public void connectionRemoved(final ConnectionEvent event)  ConnectionStatistics statistics = getStatistics(event); if (statistics!= null)  statistics.connectionRemoved(event);', 'private void addExtension( Artifact extensionArtifact, Artifact projectArtifact, List remoteRepositories, ArtifactRepository localRepository, ActiveArtifactResolver activeArtifactResolver, Map projectSessions, String projectId ) throws ExtensionManagerException  getLogger().debug( \"Starting extension-addition process for: \" + extensionArtifact ); // create a new MavenProjectSession instance for the current project. // This session instance will house the plugin and extension realms that // pertain to this specific project, along with containing the project-level // realm to use as a lookupRealm in the lifecycle executor and plugin manager. MavenProjectSession projectSession = (MavenProjectSession) projectSessions.get( projectId ); if ( projectSession == null )  try  projectSession = new MavenProjectSession( projectId, container );  catch ( PlexusContainerException e )  throw new ExtensionManagerException( \"Failed to create project realm for: \" + projectId, projectId, e );  projectSessions.put( projectId, projectSession );  // if the extension is null, or if it\\'s already been added to the current project-session, skip it. if ( ( extensionArtifact!= null ) &&!projectSession.containsExtensionRealm( extensionArtifact ) )  ArtifactFilter filter = new ProjectArtifactExceptionFilter( artifactFilterManager.getArtifactFilter(), projectArtifact ); ResolutionGroup resolutionGroup; try  resolutionGroup = artifactMetadataSource.retrieve( extensionArtifact, localRepository, remoteRepositories );  catch ( ArtifactMetadataRetrievalException e )  throw new ExtensionManagerException( \"Unable to download metadata from repository', 'private int getLengthToRead() throws IOException  String lengthField = reader.readUpTo(\":\"); try  return Integer.parseInt(lengthField);  catch (NumberFormatException e) throw new IOException(\"Stream Read Failure. Can\\'t read length of message from the client. Last thing read: \" + lengthField);', 'private void executeGoal( String task, MavenSession session, MavenProject project ) throws LifecycleExecutionException, BuildFailureException  // 1. Lifecycle lifecycle = phaseToLifecycleMap.get( task ); // 2. LifecycleMapping mapping = lifecycleMappings.get( project.getPackaging() ); // 3. MapString,String> lifecyclePhases = mapping.getLifecycles().get( \"default\" ).getPhases(); for( String phase : lifecycle.getPhases().values() )  System.out.println( \">> \" + phase );  /* try  if ( lifecycle!= null )  Map lifecycleMappings = constructLifecycleMappings( session, task, project, lifecycle ); executeGoalWithLifecycle( task, session, lifecycleMappings, project, lifecycle );  else  executeStandaloneGoal( task, session, project );   catch ( PluginNotFoundException e )  throw new BuildFailureException( \"A required plugin was not found: \" + e.getMessage(), e );  */', 'public void initialize( WikiEngine engine, Properties properties ) throws NoRequiredPropertyException, IOException  log.info(\"Initing CachingAttachmentProvider\"); if (m_cacheManager.cacheExists(ATTCOLLCACHE_NAME))  m_cache = m_cacheManager.getCache(ATTCOLLCACHE_NAME);  else  m_cache = new Cache(attCOLLCACHE_NAME, m_capacity, false, false, 0, 0); m_cacheManager.addCache(m_cache);  // // cache for the individual Attachment objects, attachment name is key, the Attachment object is the cached object // if (m_cacheManager.cacheExists(ATTCACHE_NAME))  m_attCache = m_cacheManager.getCache(ATTCACHE_NAME);  else  m_attCache = new Cache(attCache, m_capacity, false, false, 0, 0); m_cacheManager.addCache(m_attCache);  // // Find and initialize real provider. // String classname = TextUtil.getRequiredProperty( properties, AttachmentManager.PROP_PROVIDER ); try  Class?> providerclass = ClassUtil.findClass( \"org.apache.wiki.providers\", classname); m_provider = (WikiAttachmentProvider)providerclass.newInstance(); log.debug(\"Initializing real provider class \"+m_provider); m_provider.initialize( engine, properties );  catch( ClassNotFoundException e )  log.error(\"Unable to locate provider class \"+classname,e); throw new IllegalArgumentException(\"no provider class\", e);  catch( InstantiationException e )  log.error(\"Unable to create provider class', 'String getCommand(String pageName, String pageType, String suiteFilter)  String command = pageName+\"?\"+pageType; if (suiteFilter!=null) return command + command + \"&suiteFilter=\" + suiteFilter; else return command + getCommandArgs();', 'private void writeChallengeFormContent() throws IOException, WikiException  WikiEngine engine = m_wikiContext.getEngine(); SpamInspectionPlan plan = SpamInspectionPlan.getInspectionPlan( engine ); String challengeContent = null; switch( m_challenge )  case PASSWORD_PRESENTED:  // Not implemented yet break;  case CAPTCHA_PRESENTED:  Challenge captcha = plan.getCaptcha(); challengeContent = captcha.formContent( m_wikiActionBean.getContext() ); break;  case CHALLENGE_NOT_PRESENTED:  if( isSpamDetected( (WikiActionBeanContext)m_wikiContext ) )  m_challenge = Challenge.State.CAPTCHA_PRESENTED; Challenge captcha = plan.getCaptcha(); challengeContent = captcha.formContent( m_wikiActionBean.getContext() );  break;   // Always output the Challenge request parameter JspWriter out = getPageContext().getOut(); out.write( \"input name=\"\" + SpamInterceptor.CHALLENGE_REQUEST_PARAM + \"\" type=\"hidden\" value=\"\" + CryptoUtil.encrypt( String.valueOf( m_challenge.name() ) ) + \"\" />n\" ); // Output any generated Challenge content if( challengeContent!= null )  out.write( challengeContent );', 'public boolean visit(Symbol node)  if (hits.contains(currentPage)) return true; if (node.isType(WikiWord.symbolType))  WikiPage referencedPage = new WikiWordReference(currentPage, node.getContent()).getReferencedPage(); if (referencedPage!= null && referencedPage.equals(subjectPage))  hits.add(currentPage); observer.process(currentPage);   if (node.isType(Alias.symbolType))  WikiPage referencedPage = new WikiWordReference(currentPage, node.childAt(1).childAt(0).getContent()).getReferencedPage(); if (referencedPage!= null && referencedPage.equals(subjectPage))  hits.add(currentPage); observer.process(currentPage);   return true;', 'public boolean login() throws LoginException  HttpRequestCallback hcb = new HttpRequestCallback(); Callback[] callbacks = new Callback[]  hcb ; try  m_handler.handle( callbacks ); HttpServletRequest request = hcb.getRequest(); WikiPrincipal userPrincipal = new WikiPrincipal( request.getRemoteAddr() ); m_principals.add( Role.ANONYMOUS ); m_principals.add( Role.ALL ); return true;  catch( IOException e )  e.printStackTrace(); return false;  catch( UnsupportedCallbackException e )  String message = \"Unable to handle callback, disallowing login.\"; log.error( message, e ); throw new LoginException( message );', 'public Uri insert(Uri uri, ContentValues values)  final UserHandle callerUserHandle = Binder.getCallingUserHandle(); final int callerUid = Binder.getCallingUid(); SQLiteDatabase db = mOpenHelper.getWritableDatabase(); int matchIndex = URI_MATCHER.match(uri); if (matchIndex == URI_PENDING_MSG)  int subId; if (values.containsKey(PendingMessages.SUBSCRIPTION_ID))  subId = values.getAsInteger(PendingMessages.SUBSCRIPTION_ID);  else  subId = SmsManager.getDefaultSmsSubscriptionId(); if (SubscriptionManager.isValidSubscriptionId(subId))  values.put(PendingMessages.SUBSCRIPTION_ID, subId);   if (!TelephonyPermissions.checkSubscriptionAssociatedWithUser(getContext(), subId, callerUserHandle))  TelephonyUtils.showSwitchToManagedProfileDialogIfAppropriate(getContext(), subId, callerUid, getCallingPackage()); return null;  long rowId = db.insert(TABLE_PENDING_MSG, null, values); return uri.buildUpon().appendPath(Long.toString(rowId)).build();  else if (matchIndex == URI_CANONICAL_ADDRESS)  long rowId = db.insert(TABLE_CANONICAL_ADDRESSES, null, values); return uri.buildUpon().appendPath(Long.toString(rowId)).build();  throw new UnsupportedOperationException(NO_', 'protected void onCreate(Bundle savedInstanceState)  super.onCreate(savedInstanceState); setContentView(R.layout.entry_edit); setResult(KeePass.EXIT_NORMAL); // Likely the app has been killed exit the activity if ( KeePass.db == null )  finish();  Intent i = getIntent(); byte[] uuidBytes = i.getByteArrayExtra(KEY_ENTRY); if ( uuidBytes == null )  int groupId = i.getIntExtra(KEY_PARENT, -1); mEntry = new PwEntry(KeePass.db, groupId); mIsNew = true;  else  UUID uuid = Types.bytestoUUID(uuidBytes); assert(uuid!= null); mEntry = KeePass.db.gEntries.get(uuid).get(); mIsNew = false; fillData();  View scrollView = findViewById(R.id.entry_scroll); scrollView.setScrollBarStyle(View.SCROLLBARS_INSIDE_INSET); // Save button Button save = (Button) findViewById(R.id.entry_save); save.setOnClickListener(new View.OnClickListener()  @Override public void onClick(View v)  EntryEditActivity act = EntryEditActivity.this; // Require title String title = Util.getEditText(act, R.id.entry_title); if ( title.length() == 0 )  Toast.makeText(act, R.string.error_title_required, Toast.LENGTH_LONG).show(); return;  // Validate', 'public void testCrudAggregationExceptions() throws Exception  long contactId1 = createContact(); long contactId2 = createContact(); Uri resultUri = insertAggregationException(AggregationExceptions.TYPE_ALWAYS_MATCH, contactId1, contactId2); // Parse the URI and confirm that it contains an ID assertTrue(ContentUris.parseId(resultUri)!= 0); // Refetch the row we have just inserted Cursor c = mResolver.query(resultUri, AGGREGATION_EXCEPTION_PROJECTION, null, null, null); assertTrue(c.moveToFirst()); assertEquals(AggregationExceptions.TYPE_ALWAYS_MATCH, c.getInt(0)); assertEquals(contactId1, c.getLong(1)); assertEquals(contactId2, c.getLong(2)); assertFalse(c.moveToNext()); c.close(); // Query with a selection c = mResolver.query(resultUri, AGGREGATION_EXCEPTION_PROJECTION, AggregationExceptions.CONTACT_ID1 + \"=\" + contactId1, null, null); assertTrue(c.moveToFirst()); assertEquals(AggregationExceptions.TYPE_ALWAYS_MATCH, c.getInt(0)); assertEquals(contactId1, c.getLong(1)); assertEquals(contactId2, c.getLong(2)); assertFalse(c.moveToNext()); c.close(); // Delete the same row mResolver.delete(resultUri, null, null); // Verify that the row is gone c = mResolver.query(resultUri, AGGREGATION_EXCEPTION_PROJECTION', 'public void generateReports(Metrics[] results) throws IOException  String runOutputDir = runInfo.resultsDir + fs; FileWriter summary = new FileWriter(runOutputDir + \"summary.xml\"); FileWriter detail = new FileWriter(runOutputDir + \"detail.xan\"); // As all stats from each agentImpl are of same type, we can // create a new instance from any instance. logger.info(\"Printing Summary report...\"); summary.append(createSummaryReport(results)); summary.close(); logger.info(\"Summary finished. Now printing detail...\"); detail.append(createDetailReport(results)); detail.close(); logger.info(\"Detail finished. Results written to \" + runInfo.resultsDir + \\'.\\');', 'static Listcom.mongodb.bulk.WriteRequest> translateWriteRequestsToNew(final ListWriteRequest> writeRequests, final CodecDBObject> objectCodec)  Listcom.mongodb.bulk.WriteRequest> retVal = new ArrayListcom.mongodb.bulk.WriteRequest>(writeRequests.size()); for (WriteRequest cur : writeRequests)  retVal.add(cur.toNew());  return retVal;', 'private int updateInternal(Uri uri, ContentValues values, String selection, String[] selectionArgs)  if (VERBOSE_LOGGING)  Log.v(TAG, \"update: uri=\" + uri + \" selection=[\" + selection + \"] args=\" + Arrays.toString(selectionArgs) + \" values=[\" + values + \"] CPID=\" + Binder.getCallingPid() + \" CUID=\" + Binder.getCallingUid() + \" User=\" + UserUtils.getCurrentUserHandle(getContext()));  waitForAccess(mReadAccessLatch); checkForSupportedColumns(sCallsProjectionMap, values); // Request that involves changing record type to voicemail requires the // voicemail param set in the uri. if (hasVoicemailValue(values))  checkIsAllowVoicemailRequest(uri);  SelectionBuilder selectionBuilder = new SelectionBuilder(selection); checkVoicemailPermissionAndAddRestriction(uri, selectionBuilder, false /*isQuery*/); final SQLiteQueryBuilder qb = new SQLiteQueryBuilder(); qb.setTables(Tables.CALLS); qb.setProjectionMap(sCallsProjectionMap); qb.setStrict(true); // If the caller doesn\\'t have READ_VOICEMAIL, make sure they can\\'t // do any SQL shenanigans to get access to the voicemails. If the caller does have the // READ_VOICEMAIL permission, then they have sufficient permissions to access any data in // the database, so the strict check is unnecessary. if (!mVoicemailPermissions.callerHasReadAccess(getCallingPackage()))  qb.setStrictGrammar(true);  final SQLiteDatabase db = mDbHelper.getWritableDatabase()', 'public T> String urlFor(ClassT> type, Method method, Object... params)  for (Route route : routes)  if (route.canHandle(type, method))  String[] names = provider.parameterNamesFor(method); Class?> parameterType = creator.typeFor(new DefaultResourceMethod(new DefaultResourceClass(type), method)); try  Object root = parameterType.getConstructor().newInstance(); for (int i = 0; i  names.length; i++)  Method setter = findSetter(parameterType, \"set\" + Info.capitalize(names[i])); setter.invoke(root, params[i]);  return route.urlFor(type, method, root);  catch (Exception e)  throw new VRaptorException(\"The selected route is invalid for redirection: \" + type.getName() + \".\" + method.getName(), e);    throw new RouteNotFoundException(\"The selected route is invalid for redirection: \" + type.getName() + \".\" + method.getName());', 'public void shouldIncludeFlashParametersWhenARedirectHappens() throws Exception  MapString, Object> parameters = Collections.String, Object>singletonMap(\"Abc\", 1002); one(result).included(); will(returnValue(parameters)); one(session).setAttribute(FlashInterceptor.FLASH_INCLUDED_PARAMETERS, parameters); allowing(session).getAttribute(FlashInterceptor.FLASH_INCLUDED_PARAMETERS); will(returnValue(null)); ignoring(anything());  ); interceptor.intercept(stack, null, null); response.sendRedirect(\"Anything\"); mockery.assertIsSatisfied();', 'public byte[] getMasterKey(String key, String keyFileName) throws InvalidKeyFileException, IOException  assert( key!= null && keyFileName!= null ); if ( key.length() > 0 && keyFileName.length() > 0 )  return getCompositeKey(key, keyFileName);  else if ( key.length() > 0 )  return getPasswordKey(key);  else if ( keyFileName.length() > 0 )  return getFileKey(keyFileName);  else  throw new IllegalArgumentException( \"Key cannot be empty.\" );', 'private ViewResponseWithDocs parseDocsViewResult(String json) throws ParseException  final CollectionRowWithDocs> rows = new LinkedListRowWithDocs>(); if (json!= null)  try  JSONObject base = new JSONObject(json); if (base.has(\"rows\"))  JSONArray ids = base.getJSONArray(\"rows\"); for (int i = 0; i  ids.length(); i++)  JSONObject id = ids.getJSONObject(i).getString(\"id\"); String key = ids.getJSONObject(i).getString(\"key\"); String value = ids.getJSONObject(i).getString(\"value\"); rows.add(new RowWithDocs(id, key, value, null));    catch (JSONException e)  throw new ParseException(\"Cannot read json: \" + json, 0);   return new ViewResponseWithDocs(rows);', 'static public void assertEquals(String message, Object expected, Object actual)  if (expected == null && actual == null) return; if (expected!= null && expected.equals(actual)) return; if (expected instanceof String && actual instanceof String) throw new ComparisonFailure(message, (String)expected, (String)actual); else failNotEquals(message, expected, actual);', 'public void testTailable() throws ExecutionException, TimeoutException, InterruptedException  database.getCollection(\"tail1\").drop(); DBCollection c = database.createCollection(\"tail1\", new BasicDBObject(\"capped\", true).append(\"size\", 10000) ); insertTestData(c, 10); final DBCursor cur = c.find().sort(new BasicDBObject(\"$natural\", 1)).addOption(Bytes.QUERYOPTION_TAILABLE | Bytes.QUERYOPTION_AWAITDATA); final CountDownLatch latch = new CountDownLatch(1); new Thread(new Runnable()  @Override public void run()  // the following call will block on the last hasNext int i = 0; while (cur.hasNext())  DBObject obj = cur.next(); i++; if (i > 10)  obj.get(\"x\"); latch.countDown(); return;    ).start(); Thread.sleep(500); // this doc should unblock thread c.save(new BasicDBObject(\"x\", 10)); latch.await();', \"public ArtifactResolutionResult resolve( ArtifactResolutionRequest request )  Artifact rootArtifact = request.getArtifact(); SetArtifact> artifacts = request.getArtifactDependencies(); Map managedVersions = request.getManagedVersionMap(); ArtifactRepository localRepository = request.getLocalRepository(); ListArtifactRepository> remoteRepositories = request.getRemoteRepostories(); ArtifactMetadataSource source = request.getMetadataSource(); ListResolutionListener> listeners = request.getListeners(); ArtifactFilter filter = request.getFilter(); // This is an extreme hack because of the ridiculous APIs we have a root that is actually used in the production // plugin... We have no choice but to put this hack in the core. if ( isDummy( request ) )  request.setResolveRoot( false );  if ( source == null )  try  source = container.lookup( ArtifactMetadataSource.class );  catch ( ComponentLookupException e )  // Won't happen   if ( listeners == null )  listeners = new ArrayListResolutionListener>(); if ( logger.isDebugEnabled() )  listeners.add( new DebugResolutionListener( logger ) );  listeners.add( new WarningResolutionListener( logger ) );  ArtifactResolutionResult result = new ArtifactResolutionResult(); // The root artifact may, or may not be resolved so we need to check before we attempt to resolve. // This is often an artifact like a POM that is taken from disk and we already have hold of the // file reference. But this may be a Maven Plugin that we need to resolve from a remote repository // as well as its dependencies. But this may be a Maven Plugin that we need to resolve from a remote repository // as well\", 'public void filter() throws IOException  SerializerFactory factory = new DefaultSerializerFactory(); @SuppressWarnings(\"unchecked\") SerializerInteger> keySerializer = (SerializerInteger>) factory.getSerializer(storeDefinition.getKeySerializer()); BufferedReader in = new BufferedReader(new FileReader(inputFile)); BufferedWriter out = new BufferedWriter(new FileWriter(outputFile)); try  String line = null; while ((line = in.readLine())!= null)  int key = Integer.valueOf(line.replaceAll(\"s+\", \"\")); byte[] keyBytes = keySerializer.toBytes(key); ListNode> nodes = routingStrategy.routeRequest(keyBytes); if (nodes.contains(node))  out.write(key + \"n\");    finally  in.close(); out.close();']\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define metrics\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'METEOR']\n",
    "\n",
    "final_scores = [codebleu.score,  mean_rouge1,  mean_rouge2, mean_rougeL, meteor_avg_score]\n",
    "\n",
    "# Plotting final scores\n",
    "plt.bar(metrics, final_scores)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Final Evaluation Metrics')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sacrebleu.corpus_bleu(all_predictions, [refactored_code])\n",
    "\n",
    "# Compute CodeBLEU score\n",
    "code_bleu_score = bleu_score.score\n",
    "\n",
    "# Print CodeBLEU score\n",
    "print(\"CodeBLEU Score:\", code_bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
