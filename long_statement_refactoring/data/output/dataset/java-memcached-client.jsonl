{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.Logger;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.MultiGetOperationImpl;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\n/**\n * Main class for handling connections to a memcached cluster.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  /**\n   * The number of empty selects we'll allow before assuming we may have\n   * missed one and should check the current selectors. This generally\n   * indicates a bug, but we'll check it nonetheless.\n   */\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n\n  /**\n   * The number of empty selects we'll allow before blowing up. It's too\n   * easy to write a bug that causes it to loop uncontrollably. This helps\n   * find those bugs and often works around them.\n   */\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  /**\n   * The default wakeup delay if not overridden by a system property.\n   */\n  private static final int DEFAULT_WAKEUP_DELAY = 1000;\n\n  /**\n   * By default, do not bound the retry queue.\n   */\n  private static final int DEFAULT_RETRY_QUEUE_SIZE = -1;\n\n  /**\n   * If an operation gets cloned more than this ceiling, cancel it for\n   * safety reasons.\n   */\n  private static final int MAX_CLONE_COUNT = 100;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  /**\n   * If the connection is alread shut down or shutting down.\n   */\n  protected volatile boolean shutDown = false;\n\n  /**\n   * If true, optimization will collapse multiple sequential get ops.\n   */\n  private final boolean shouldOptimize;\n\n  /**\n   * Holds the current {@link Selector} to use.\n   */\n  protected Selector selector = null;\n\n  /**\n   * The {@link NodeLocator} to use for this connection.\n   */\n  protected final NodeLocator locator;\n\n  /**\n   * The configured {@link FailureMode}.\n   */\n  protected final FailureMode failureMode;\n\n  /**\n   * Maximum amount of time to wait between reconnect attempts.\n   */\n  private final long maxDelay;\n\n  /**\n   * Contains the current number of empty select() calls, which could indicate\n   * bugs.\n   */\n  private int emptySelects = 0;\n\n  /**\n   * The buffer size that will be used when reading from the server.\n   */\n  private final int bufSize;\n\n  /**\n   * The connection factory to create {@link MemcachedNode}s from.\n   */\n  private final ConnectionFactory connectionFactory;\n\n  /**\n   * AddedQueue is used to track the QueueAttachments for which operations\n   * have recently been queued.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\n  /**\n   * reconnectQueue contains the attachments that need to be reconnected.\n   * The key is the time at which they are eligible for reconnect.\n   */\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  /**\n   * True if not shutting down or shut down.\n   */\n  protected volatile boolean running = true;\n\n  /**\n   * Holds all connection observers that get notified on connection status\n   * changes.\n   */\n  private final Collection<ConnectionObserver> connObservers =\n    new ConcurrentLinkedQueue<ConnectionObserver>();\n\n  /**\n   * The {@link OperationFactory} to clone or create operations.\n   */\n  private final OperationFactory opFact;\n\n  /**\n   * The threshold for timeout exceptions.\n   */\n  private final int timeoutExceptionThreshold;\n\n  /**\n   * Holds operations that need to be retried.\n   */\n  private final List<Operation> retryOps;\n\n  /**\n   * Holds all nodes that are scheduled for shutdown.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n  /**\n   * If set to true, a proper check after finish connecting is done to see\n   * if the node is not responding but really alive.\n   */\n  private final boolean verifyAliveOnConnect;\n\n  /**\n   * The {@link ExecutorService} to use for callbacks.\n   */\n  private final ExecutorService listenerExecutorService;\n\n  /**\n   * The {@link MetricCollector} to accumulate metrics (or dummy).\n   */\n  protected final MetricCollector metrics;\n\n  /**\n   * The current type of metrics to collect.\n   */\n  protected final MetricType metricType;\n\n  /**\n   * The selector wakeup delay, defaults to 1000ms.\n   */\n  private final int wakeupDelay;\n\n  /**\n   * Optionally bound the retry queue if set via system property.\n   */\n  private final int retryQueueSize;\n\n  /**\n   * Construct a {@link MemcachedConnection}.\n   *\n   * @param bufSize the size of the buffer used for reading from the server.\n   * @param f the factory that will provide an operation queue.\n   * @param a the addresses of the servers to connect to.\n   * @param obs the initial observers to add.\n   * @param fm the failure mode to use.\n   * @param opfactory the operation factory.\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(final int bufSize, final ConnectionFactory f,\n      final List<InetSocketAddress> a, final Collection<ConnectionObserver> obs,\n      final FailureMode fm, final OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = TimeUnit.SECONDS.toMillis(f.getMaxReconnectDelay());\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = Collections.synchronizedList(new ArrayList<Operation>());\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    wakeupDelay = Integer.parseInt( System.getProperty(\"net.spy.wakeupDelay\",\n      Integer.toString(DEFAULT_WAKEUP_DELAY)));\n\n    retryQueueSize = Integer.parseInt(System.getProperty(\"net.spy.retryQueueSize\",\n        Integer.toString(DEFAULT_RETRY_QUEUE_SIZE)));\n    getLogger().info(\"Setting retryQueueSize to \" + retryQueueSize);\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG)\n      || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  /**\n   * Create connections for the given list of addresses.\n   *\n   * @param addrs the list of addresses to connect to.\n   * @return addrs list of {@link MemcachedNode}s.\n   * @throws IOException if connecting was not successful.\n   */\n  protected List<MemcachedNode> createConnections(\n    final Collection<InetSocketAddress> addrs) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(addrs.size());\n\n    for (SocketAddress sa : addrs) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa = connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      qa.setConnection(this);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n\n    return connections;\n  }\n\n  /**\n   * Make sure that the current selectors make sense.\n   *\n   * @return true if they do.\n   */\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n            + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * Handle all IO that flows through the connection.\n   *\n   * This method is called in an endless loop, listens on NIO selectors and\n   * dispatches the underlying read/write calls if needed.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      getLogger().debug(\"No IO while shut down.\");\n      return;\n    }\n\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = wakeupDelay;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n\n    if (shutDown) {\n      return;\n    } else if (selected == 0 && addedQueue.isEmpty()) {\n      handleWokenUpSelector();\n    } else if (selector.selectedKeys().isEmpty()) {\n      handleEmptySelects();\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n        selector.selectedKeys().size());\n      emptySelects = 0;\n\n      Iterator<SelectionKey> iterator = selector.selectedKeys().iterator();\n      while(iterator.hasNext()) {\n        SelectionKey sk = iterator.next();\n        handleIO(sk);\n        iterator.remove();\n      }\n    }\n\n    handleOperationalTasks();\n  }\n\n  /**\n   * Helper method which gets called if the selector is woken up because of the\n   * timeout setting, if has been interrupted or if happens during regular\n   * write operation phases.\n   *\n   * <p>This method can be overriden by child implementations to handle custom\n   * behavior on a manually woken selector, like sending pings through the\n   * channels to make sure they are alive.<\/p>\n   *\n   * <p>Note that there is no guarantee that this method is at all or in the\n   * regular interval called, so all overriding implementations need to take\n   * that into account. Also, it needs to take into account that it may be\n   * called very often under heavy workloads, so it should not perform extensive\n   * tasks in the same thread.<\/p>\n   */\n  protected void handleWokenUpSelector() { }\n\n  /**\n   * Helper method for {@link #handleIO()} to encapsulate everything that\n   * needs to be checked on a regular basis that has nothing to do directly\n   * with reading and writing data.\n   *\n   * @throws IOException if an error happens during shutdown queue handling.\n   */\n  private void handleOperationalTasks() throws IOException {\n    checkPotentiallyTimedOutConnection();\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n\n    if (!retryOps.isEmpty()) {\n      ArrayList<Operation> operations = new ArrayList<Operation>(retryOps);\n      retryOps.clear();\n      redistributeOperations(operations);\n    }\n\n    handleShutdownQueue();\n  }\n\n  /**\n   * Helper method for {@link #handleIO()} to handle empty select calls.\n   */\n  private void handleEmptySelects() {\n    getLogger().debug(\"No selectors ready, interrupted: %b\",\n      Thread.interrupted());\n\n    if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n      for (SelectionKey sk : selector.keys()) {\n        getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n          sk.interestOps());\n        if (sk.readyOps() != 0) {\n          getLogger().debug(\"%s has a ready op, handling IO\", sk);\n          handleIO(sk);\n        } else {\n          lostConnection((MemcachedNode) sk.attachment());\n        }\n      }\n      assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n    }\n  }\n\n  /**\n   * Check if nodes need to be shut down and do so if needed.\n   *\n   * @throws IOException if the channel could not be closed properly.\n   */\n  private void handleShutdownQueue() throws IOException {\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  /**\n   * Check if one or more nodes exceeded the timeout Threshold.\n   */\n  private void checkPotentiallyTimedOutConnection() {\n    boolean stillCheckingTimeouts = true;\n    while (stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n  }\n\n  /**\n   * Handle any requests that have been made against the client.\n   */\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n\n      MemcachedNode qaNode;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      for (MemcachedNode node : todo) {\n        boolean readyForIO = false;\n        if (node.isActive()) {\n          if (node.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", node);\n          }\n        } else {\n          toAdd.add(node);\n        }\n        node.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (node.getWbuf().hasRemaining()) {\n              handleWrites(node);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(node);\n          }\n        }\n        node.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added.\n   */\n  public boolean addObserver(final ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't.\n   */\n  public boolean removeObserver(final ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  /**\n   * Indicate a successful connect to the given node.\n   *\n   * @param node the node which was successfully connected.\n   */\n  private void connected(final MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  /**\n   * Indicate a lost connection to the given node.\n   *\n   * @param node the node where the connection was lost.\n   */\n  private void lostConnection(final MemcachedNode node) {\n    queueReconnect(node);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(node.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given node belongs to the current cluster.\n   *\n   * Before trying to connect to a node, make sure it actually belongs to the\n   * currently connected cluster.\n   */\n  boolean belongsToCluster(final MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector.\n   *\n   * Any IOException will cause a reconnect. Note that this code makes sure\n   * that the corresponding node is not only able to connect, but also able to\n   * respond in a correct fashion (if verifyAliveOnConnect is set to true\n   * through a property). This is handled by issuing a dummy\n   * version/noop call and making sure it returns in a correct and timely\n   * fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(final SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n        sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n        sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().debug(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n          finishConnect(sk, node);\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        handleReadsAndWrites(sk, node);\n      }\n    } catch (ClosedChannelException e) {\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n        + \"operation on %s. This may be due to an authentication failure.\",\n        node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  /**\n   * A helper method for {@link #handleIO(java.nio.channels.SelectionKey)} to\n   * handle reads and writes if appropriate.\n   *\n   * @param sk the selection key to use.\n   * @param node th enode to read write from.\n   * @throws IOException if an error occurs during read/write.\n   */\n  private void handleReadsAndWrites(final SelectionKey sk, final MemcachedNode node) throws IOException {\n      if (sk.isValid() && sk.isReadable()) {\n          handleReads(node);\n      }\n\n      if (sk.isValid() && sk.isWritable()) {\n          handleWrites(node);\n      }\n  }\n  /**\n   * Finish the connect phase and potentially verify its liveness.\n   *\n   * @param sk the selection key for the node.\n   * @param node the actual node.\n   * @throws IOException if something goes wrong during reading/writing.\n   */\n  private void finishConnect(final SelectionKey sk, final MemcachedNode node)\n    throws IOException {\n    if (verifyAliveOnConnect) {\n      final CountDownLatch latch = new CountDownLatch(1);\n      final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(\"noop\",\n        latch, 2500, listenerExecutorService);\n      NoopOperation testOp = opFact.noop(new OperationCallback() {\n        public void receivedStatus(OperationStatus status) {\n          rv.set(status.isSuccess(), status);\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      });\n\n      testOp.setHandlingNode(node);\n      testOp.initialize();\n      checkState();\n      insertOperation(node, testOp);\n      node.copyInputQueue();\n\n      boolean done = false;\n      if (sk.isValid()) {\n        long timeout = TimeUnit.MILLISECONDS.toNanos(\n          connectionFactory.getOperationTimeout());\n\n        long stop = System.nanoTime() + timeout;\n        while (stop > System.nanoTime()) {\n          handleWrites(node);\n          handleReads(node);\n          if(done = (latch.getCount() == 0)) {\n            break;\n          }\n        }\n      }\n\n      if (!done || testOp.isCancelled() || testOp.hasErrored()\n        || testOp.isTimedOut()) {\n        throw new ConnectException(\"Could not send noop upon connect! \"\n          + \"This may indicate a running, but not responding memcached \"\n          + \"instance.\");\n      }\n    }\n\n    connected(node);\n    addedQueue.offer(node);\n    if (node.getWbuf().hasRemaining()) {\n      handleWrites(node);\n    }\n  }\n\n  /**\n   * Handle pending writes for the given node.\n   *\n   * @param node the node to handle writes for.\n   * @throws IOException can be raised during writing failures.\n   */\n  private void handleWrites(final MemcachedNode node) throws IOException {\n    node.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = node.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = node.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      node.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && node.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  /**\n   * Handle pending reads for the given node.\n   *\n   * @param node the node to handle reads for.\n   * @throws IOException can be raised during reading failures.\n   */\n  private void handleReads(final MemcachedNode node) throws IOException {\n    Operation currentOp = node.getCurrentReadOp();\n    if (currentOp instanceof TapAckOperationImpl) {\n      node.removeCurrentReadOp();\n      return;\n    }\n\n    ByteBuffer rbuf = node.getRbuf();\n    final SocketChannel channel = node.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      currentOp = handleReadsWhenChannelEndOfStream(currentOp, node, rbuf);\n    }\n\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n\n        long timeOnWire =\n          System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n        metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC,\n          (int)(timeOnWire / 1000));\n        metrics.markMeter(OVERALL_RESPONSE_METRIC);\n        synchronized(currentOp) {\n          readBufferAndLogMetrics(currentOp, rbuf, node);\n        }\n\n        currentOp = node.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      node.completedRead();\n    }\n  }\n\n  /**\n   * Read from the buffer and add metrics information.\n   *\n   * @param currentOp the current operation to read.\n   * @param rbuf the read buffer to read from.\n   * @param node the node to read from.\n   * @throws IOException if reading was not successful.\n   */\n  private void readBufferAndLogMetrics(final Operation currentOp,\n    final ByteBuffer rbuf, final MemcachedNode node) throws IOException {\n    currentOp.readFromBuffer(rbuf);\n    if (currentOp.getState() == OperationState.COMPLETE) {\n      getLogger().debug(\"Completed read op: %s and giving the next %d \"\n        + \"bytes\", currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      if (op.hasErrored()) {\n        metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      } else {\n        metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n      }\n    } else if (currentOp.getState() == OperationState.RETRY) {\n      handleRetryInformation(currentOp.getErrorMsg());\n      getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n        + \"%s \", currentOp);\n      ((VBucketAware) currentOp).addNotMyVbucketNode(\n        currentOp.getHandlingNode());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      retryOperation(currentOp);\n      metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n    }\n  }\n\n  /**\n   * Deal with an operation where the channel reached the end of a stream.\n   *\n   * @param currentOp the current operation to read.\n   * @param node the node for that operation.\n   * @param rbuf the read buffer.\n   *\n   * @return the next operation on the node to read.\n   * @throws IOException if disconnect while reading.\n   */\n  private Operation handleReadsWhenChannelEndOfStream(final Operation currentOp,\n    final MemcachedNode node, final ByteBuffer rbuf) throws IOException {\n    if (currentOp instanceof TapOperation) {\n      currentOp.getCallback().complete();\n      ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n\n      getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n        currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n      return node.getCurrentReadOp();\n    } else {\n      throw new IOException(\"Disconnected unexpected, will reconnect.\");\n    }\n  }\n\n  /**\n   * Convert the {@link ByteBuffer} into a string for easier debugging.\n   *\n   * @param b the buffer to debug.\n   * @param size the size of the buffer.\n   * @return the stringified {@link ByteBuffer}.\n   */\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Optionally handle retry (NOT_MY_VBUKET) responses.\n   *\n   * This method can be overridden in subclasses to handle the content\n   * of the retry message appropriately.\n   *\n   * @param retryMessage the body of the retry message.\n   */\n  protected void handleRetryInformation(final byte[] retryMessage) {\n    getLogger().debug(\"Got RETRY message: \" + new String(retryMessage)\n      + \", but not handled.\");\n  }\n\n  /**\n   * Enqueue the given {@link MemcachedNode} for reconnect.\n   *\n   * @param node the node to reconnect.\n   */\n  protected void queueReconnect(final MemcachedNode node) {\n    if (shutDown) {\n      return;\n    }\n    getLogger().warn(\"Closing, and reopening %s, attempt %d.\", node,\n      node.getReconnectCount());\n\n    if (node.getSk() != null) {\n      node.getSk().cancel();\n      assert !node.getSk().isValid() : \"Cancelled selection key is valid\";\n    }\n    node.reconnecting();\n\n    try {\n      if (node.getChannel() != null && node.getChannel().socket() != null) {\n        node.getChannel().socket().close();\n      } else {\n        getLogger().info(\"The channel or socket was null for %s\", node);\n      }\n    } catch (IOException e) {\n      getLogger().warn(\"IOException trying to close a socket\", e);\n    }\n    node.setChannel(null);\n\n    long delay = (long) Math.min(maxDelay, Math.pow(2,\n        node.getReconnectCount()) * 1000);\n    long reconnectTime = System.currentTimeMillis() + delay;\n    while (reconnectQueue.containsKey(reconnectTime)) {\n      reconnectTime++;\n    }\n\n    reconnectQueue.put(reconnectTime, node);\n    metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n    node.setupResend();\n    if (failureMode == FailureMode.Redistribute) {\n      redistributeOperations(node.destroyInputQueue());\n    } else if (failureMode == FailureMode.Cancel) {\n      cancelOperations(node.destroyInputQueue());\n    }\n  }\n\n  /**\n   * Cancel the given collection of operations.\n   *\n   * @param ops the list of operations to cancel.\n   */\n  private void cancelOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Redistribute the given list of operations to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param ops the operations to redistribute.\n   */\n  public void redistributeOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      redistributeOperation(op);\n    }\n  }\n\n  /**\n   * Redistribute the given operation to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param op the operation to redistribute.\n   */\n  public void redistributeOperation(Operation op) {\n    if (op.isCancelled() || op.isTimedOut()) {\n      return;\n    }\n\n    if (op.getCloneCount() >= MAX_CLONE_COUNT) {\n      getLogger().warn(\"Cancelling operation \" + op + \"because it has been \"\n        + \"retried (cloned) more than \" + MAX_CLONE_COUNT + \"times.\");\n      op.cancel();\n      return;\n    }\n\n    // The operation gets redistributed but has never been actually written,\n    // it we just straight re-add it without cloning.\n    if (op.getState() == OperationState.WRITE_QUEUED && op.getHandlingNode() != null) {\n      addOperation(op.getHandlingNode(), op);\n      return;\n    }\n\n    if (op instanceof MultiGetOperationImpl) {\n      for (String key : ((MultiGetOperationImpl) op).getRetryKeys()) {\n        addOperation(key, opFact.get(key,\n          (GetOperation.Callback) op.getCallback()));\n      }\n    } else if (op instanceof KeyedOperation) {\n      KeyedOperation ko = (KeyedOperation) op;\n      int added = 0;\n      for (Operation newop : opFact.clone(ko)) {\n        if (newop instanceof KeyedOperation) {\n          KeyedOperation newKeyedOp = (KeyedOperation) newop;\n          for (String k : newKeyedOp.getKeys()) {\n            addOperation(k, newop);\n            op.addClone(newop);\n            newop.setCloneCount(op.getCloneCount()+1);\n          }\n        } else {\n          newop.cancel();\n          getLogger().warn(\"Could not redistribute cloned non-keyed \" +\n            \"operation\", newop);\n        }\n        added++;\n      }\n      assert added > 0 : \"Didn't add any new operations when redistributing\";\n    } else {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Attempt to reconnect {@link MemcachedNode}s in the reconnect queue.\n   *\n   * If the {@link MemcachedNode} does not belong to the cluster list anymore,\n   * the reconnect attempt is cancelled. If it does, the code tries to\n   * reconnect immediately and if this is not possible it waits until the\n   * connection information arrives.\n   *\n   * Note that if a socket error arises during reconnect, the node is scheduled\n   * for re-reconnect immediately.\n   */\n  private void attemptReconnects() {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n      new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n\n\n    Iterator<MemcachedNode> i = reconnectQueue.headMap(now).values().iterator();\n    while(i.hasNext()) {\n      final MemcachedNode node = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n\n      try {\n        if (!belongsToCluster(node)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", node);\n          continue;\n        }\n\n        if (!seen.containsKey(node)) {\n          seen.put(node, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", node);\n\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n          int ops = 0;\n          if (ch.connect(node.getSocketAddress())) {\n            connected(node);\n            addedQueue.offer(node);\n            getLogger().info(\"Immediately reconnected to %s\", node);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          node.registerChannel(ch, ch.register(selector, ops, node));\n          assert node.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\",\n            node);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(node);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", node, e);\n      } finally {\n        potentiallyCloseLeakingChannel(ch, node);\n      }\n    }\n\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Make sure channel connections are not leaked and properly close under\n   * faulty reconnect cirumstances.\n   *\n   * @param ch the channel to potentially close.\n   * @param node the node to which the channel should be bound to.\n   */\n  private void potentiallyCloseLeakingChannel(final SocketChannel ch,\n    final MemcachedNode node) {\n    if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n      try {\n        ch.close();\n      } catch (IOException e) {\n        getLogger().error(\"Exception closing channel: %s\", node, e);\n      }\n    }\n  }\n\n  /**\n   * Returns the {@link NodeLocator} in use for this connection.\n   *\n   * @return  the current {@link NodeLocator}.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  /**\n   * Enqueue the given {@link Operation} with the used key.\n   *\n   * @param key the key to use.\n   * @param o the {@link Operation} to enqueue.\n   */\n  public void enqueueOperation(final String key, final Operation o) {\n    checkState();\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to a connection identified by the given key.\n   *\n   * If the {@link MemcachedNode} is active or the {@link FailureMode} is set\n   * to retry, the primary node will be used for that key. If the primary\n   * node is not available and the {@link FailureMode} cancel is used, the\n   * operation will be cancelled without further retry.\n   *\n   * For any other {@link FailureMode} mechanisms (Redistribute), another\n   * possible node is used (only if its active as well). If no other active\n   * node could be identified, the original primary node is used and retried.\n   *\n   * @param key the key the operation is operating upon.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final String key, final Operation o) {\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      Iterator<MemcachedNode> i = locator.getSequence(key);\n      while (placeIn == null && i.hasNext()) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\"Could not redistribute to another node, \"\n          + \"retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key + \" (and not \"\n        + \"immediately cancelled)\";\n    }\n  }\n\n  /**\n   * Insert an operation on the given node to the beginning of the queue.\n   *\n   * @param node the node where to insert the {@link Operation}.\n   * @param o the operation to insert.\n   */\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue an operation on the given node.\n   *\n   * @param node the node where to enqueue the {@link Operation}.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    if (!node.isAuthenticated()) {\n      retryOperation(o);\n      return;\n    }\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue the given list of operations on each handling node.\n   *\n   * @param ops the operations for each node.\n   */\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      addOperation(me.getKey(), me.getValue());\n    }\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a collection of nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n    final Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all connections and do not accept further incoming ops.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n    try {\n      Selector s = selector.wakeup();\n      assert s == selector : \"Wakeup returned the wrong selector.\";\n      for (MemcachedNode node : locator.getAll()) {\n        if (node.getChannel() != null) {\n          node.getChannel().close();\n          node.setSk(null);\n          if (node.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              node.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", node.getChannel());\n        }\n      }\n\n      selector.close();\n      getLogger().debug(\"Shut down selector %s\", selector);\n    } finally {\n      running = false;\n    }\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \").append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Construct a String containing information about all nodes and their state.\n   *\n   * @return a stringified representation of the connection status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus\n        .append(\" \")\n        .append(node.getSocketAddress())\n        .append(\" active: \")\n        .append(node.isActive())\n        .append(\", authed: \")\n        .append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * Increase the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opTimedOut(final Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * Reset the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opSucceeded(final Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * Set the continuous timeout on an operation.\n   *\n   * Ignore operations which have no handling nodes set yet (which may happen before nodes are properly\n   * authenticated).\n   *\n   * @param op the operation to use.\n   * @param isTimeout is timed out or not.\n   */\n  private static void setTimeout(final Operation op, final boolean isTimeout) {\n    Logger logger = LoggerFactory.getLogger(MemcachedConnection.class);\n\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return;\n      }\n\n      MemcachedNode node = op.getHandlingNode();\n      if (node != null) {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      logger.error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   *\n   * @throws IllegalStateException when shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Handle IO as long as the application is running.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  /**\n   * Log a exception to different levels depending on the state.\n   *\n   * Exceptions get logged at debug level when happening during shutdown, but\n   * at warning level when operating normally.\n   *\n   * @param e the exception to log.\n   */\n  private void logRunException(final Exception e) {\n    if (shutDown) {\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n\n  /**\n   * Returns whether the connection is shut down or not.\n   *\n   * @return true if the connection is shut down, false otherwise.\n   */\n  public boolean isShutDown() {\n    return shutDown;\n  }\n\n  /**\n   * Add a operation to the retry queue.\n   *\n   * If the retry queue size is bounded and the size of the queue is reaching\n   * that boundary, the operation is cancelled rather than added to the\n   * retry queue.\n   *\n   * @param op the operation to retry.\n   */\n  public void retryOperation(Operation op) {\n    if (retryQueueSize >= 0 && retryOps.size() >= retryQueueSize) {\n      if (!op.isCancelled()) {\n        op.cancel();\n      }\n    }\n    retryOps.add(op);\n  }\n\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.Logger;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.MultiGetOperationImpl;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.Socket;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\n/**\n * Main class for handling connections to a memcached cluster.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  /**\n   * The number of empty selects we'll allow before assuming we may have\n   * missed one and should check the current selectors. This generally\n   * indicates a bug, but we'll check it nonetheless.\n   */\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n\n  /**\n   * The number of empty selects we'll allow before blowing up. It's too\n   * easy to write a bug that causes it to loop uncontrollably. This helps\n   * find those bugs and often works around them.\n   */\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  /**\n   * The default wakeup delay if not overridden by a system property.\n   */\n  private static final int DEFAULT_WAKEUP_DELAY = 1000;\n\n  /**\n   * By default, do not bound the retry queue.\n   */\n  private static final int DEFAULT_RETRY_QUEUE_SIZE = -1;\n\n  /**\n   * If an operation gets cloned more than this ceiling, cancel it for\n   * safety reasons.\n   */\n  private static final int MAX_CLONE_COUNT = 100;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  /**\n   * If the connection is alread shut down or shutting down.\n   */\n  protected volatile boolean shutDown = false;\n\n  /**\n   * If true, optimization will collapse multiple sequential get ops.\n   */\n  private final boolean shouldOptimize;\n\n  /**\n   * Holds the current {@link Selector} to use.\n   */\n  protected Selector selector = null;\n\n  /**\n   * The {@link NodeLocator} to use for this connection.\n   */\n  protected final NodeLocator locator;\n\n  /**\n   * The configured {@link FailureMode}.\n   */\n  protected final FailureMode failureMode;\n\n  /**\n   * Maximum amount of time to wait between reconnect attempts.\n   */\n  private final long maxDelay;\n\n  /**\n   * Contains the current number of empty select() calls, which could indicate\n   * bugs.\n   */\n  private int emptySelects = 0;\n\n  /**\n   * The buffer size that will be used when reading from the server.\n   */\n  private final int bufSize;\n\n  /**\n   * The connection factory to create {@link MemcachedNode}s from.\n   */\n  private final ConnectionFactory connectionFactory;\n\n  /**\n   * AddedQueue is used to track the QueueAttachments for which operations\n   * have recently been queued.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\n  /**\n   * reconnectQueue contains the attachments that need to be reconnected.\n   * The key is the time at which they are eligible for reconnect.\n   */\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  /**\n   * True if not shutting down or shut down.\n   */\n  protected volatile boolean running = true;\n\n  /**\n   * Holds all connection observers that get notified on connection status\n   * changes.\n   */\n  private final Collection<ConnectionObserver> connObservers =\n    new ConcurrentLinkedQueue<ConnectionObserver>();\n\n  /**\n   * The {@link OperationFactory} to clone or create operations.\n   */\n  private final OperationFactory opFact;\n\n  /**\n   * The threshold for timeout exceptions.\n   */\n  private final int timeoutExceptionThreshold;\n\n  /**\n   * Holds operations that need to be retried.\n   */\n  private final List<Operation> retryOps;\n\n  /**\n   * Holds all nodes that are scheduled for shutdown.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n  /**\n   * If set to true, a proper check after finish connecting is done to see\n   * if the node is not responding but really alive.\n   */\n  private final boolean verifyAliveOnConnect;\n\n  /**\n   * The {@link ExecutorService} to use for callbacks.\n   */\n  private final ExecutorService listenerExecutorService;\n\n  /**\n   * The {@link MetricCollector} to accumulate metrics (or dummy).\n   */\n  protected final MetricCollector metrics;\n\n  /**\n   * The current type of metrics to collect.\n   */\n  protected final MetricType metricType;\n\n  /**\n   * The selector wakeup delay, defaults to 1000ms.\n   */\n  private final int wakeupDelay;\n\n  /**\n   * Optionally bound the retry queue if set via system property.\n   */\n  private final int retryQueueSize;\n\n  /**\n   * Construct a {@link MemcachedConnection}.\n   *\n   * @param bufSize the size of the buffer used for reading from the server.\n   * @param f the factory that will provide an operation queue.\n   * @param a the addresses of the servers to connect to.\n   * @param obs the initial observers to add.\n   * @param fm the failure mode to use.\n   * @param opfactory the operation factory.\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(final int bufSize, final ConnectionFactory f,\n      final List<InetSocketAddress> a, final Collection<ConnectionObserver> obs,\n      final FailureMode fm, final OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = TimeUnit.SECONDS.toMillis(f.getMaxReconnectDelay());\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = Collections.synchronizedList(new ArrayList<Operation>());\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    wakeupDelay = Integer.parseInt( System.getProperty(\"net.spy.wakeupDelay\",\n      Integer.toString(DEFAULT_WAKEUP_DELAY)));\n\n    retryQueueSize = Integer.parseInt(System.getProperty(\"net.spy.retryQueueSize\",\n        Integer.toString(DEFAULT_RETRY_QUEUE_SIZE)));\n    getLogger().info(\"Setting retryQueueSize to \" + retryQueueSize);\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG)\n      || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  /**\n   * Create connections for the given list of addresses.\n   *\n   * @param addrs the list of addresses to connect to.\n   * @return addrs list of {@link MemcachedNode}s.\n   * @throws IOException if connecting was not successful.\n   */\n  protected List<MemcachedNode> createConnections(\n    final Collection<InetSocketAddress> addrs) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(addrs.size());\n\n    for (SocketAddress sa : addrs) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa = connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      qa.setConnection(this);\n      int ops = 0;\n      Socket socket = ch.socket();\n      socket.setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n      socket.setKeepAlive(connectionFactory.getKeepAlive());\n      \n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n\n    return connections;\n  }\n\n  /**\n   * Make sure that the current selectors make sense.\n   *\n   * @return true if they do.\n   */\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n            + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * Handle all IO that flows through the connection.\n   *\n   * This method is called in an endless loop, listens on NIO selectors and\n   * dispatches the underlying read/write calls if needed.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      getLogger().debug(\"No IO while shut down.\");\n      return;\n    }\n\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = wakeupDelay;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n\n    if (shutDown) {\n      return;\n    } else if (selected == 0 && addedQueue.isEmpty()) {\n      handleWokenUpSelector();\n    } else if (selector.selectedKeys().isEmpty()) {\n      handleEmptySelects();\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n        selector.selectedKeys().size());\n      emptySelects = 0;\n\n      Iterator<SelectionKey> iterator = selector.selectedKeys().iterator();\n      while(iterator.hasNext()) {\n        SelectionKey sk = iterator.next();\n        handleIO(sk);\n        iterator.remove();\n      }\n    }\n\n    handleOperationalTasks();\n  }\n\n  /**\n   * Helper method which gets called if the selector is woken up because of the\n   * timeout setting, if has been interrupted or if happens during regular\n   * write operation phases.\n   *\n   * <p>This method can be overriden by child implementations to handle custom\n   * behavior on a manually woken selector, like sending pings through the\n   * channels to make sure they are alive.<\/p>\n   *\n   * <p>Note that there is no guarantee that this method is at all or in the\n   * regular interval called, so all overriding implementations need to take\n   * that into account. Also, it needs to take into account that it may be\n   * called very often under heavy workloads, so it should not perform extensive\n   * tasks in the same thread.<\/p>\n   */\n  protected void handleWokenUpSelector() { }\n\n  /**\n   * Helper method for {@link #handleIO()} to encapsulate everything that\n   * needs to be checked on a regular basis that has nothing to do directly\n   * with reading and writing data.\n   *\n   * @throws IOException if an error happens during shutdown queue handling.\n   */\n  private void handleOperationalTasks() throws IOException {\n    checkPotentiallyTimedOutConnection();\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n\n    if (!retryOps.isEmpty()) {\n      ArrayList<Operation> operations = new ArrayList<Operation>(retryOps);\n      retryOps.clear();\n      redistributeOperations(operations);\n    }\n\n    handleShutdownQueue();\n  }\n\n  /**\n   * Helper method for {@link #handleIO()} to handle empty select calls.\n   */\n  private void handleEmptySelects() {\n    getLogger().debug(\"No selectors ready, interrupted: %b\",\n      Thread.interrupted());\n\n    if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n      for (SelectionKey sk : selector.keys()) {\n        getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n          sk.interestOps());\n        if (sk.readyOps() != 0) {\n          getLogger().debug(\"%s has a ready op, handling IO\", sk);\n          handleIO(sk);\n        } else {\n          lostConnection((MemcachedNode) sk.attachment());\n        }\n      }\n      assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n    }\n  }\n\n  /**\n   * Check if nodes need to be shut down and do so if needed.\n   *\n   * @throws IOException if the channel could not be closed properly.\n   */\n  private void handleShutdownQueue() throws IOException {\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  /**\n   * Check if one or more nodes exceeded the timeout Threshold.\n   */\n  private void checkPotentiallyTimedOutConnection() {\n    boolean stillCheckingTimeouts = true;\n    while (stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n  }\n\n  /**\n   * Handle any requests that have been made against the client.\n   */\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n\n      MemcachedNode qaNode;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      for (MemcachedNode node : todo) {\n        boolean readyForIO = false;\n        if (node.isActive()) {\n          if (node.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", node);\n          }\n        } else {\n          toAdd.add(node);\n        }\n        node.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (node.getWbuf().hasRemaining()) {\n              handleWrites(node);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(node);\n          }\n        }\n        node.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added.\n   */\n  public boolean addObserver(final ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't.\n   */\n  public boolean removeObserver(final ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  /**\n   * Indicate a successful connect to the given node.\n   *\n   * @param node the node which was successfully connected.\n   */\n  private void connected(final MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  /**\n   * Indicate a lost connection to the given node.\n   *\n   * @param node the node where the connection was lost.\n   */\n  private void lostConnection(final MemcachedNode node) {\n    queueReconnect(node);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(node.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given node belongs to the current cluster.\n   *\n   * Before trying to connect to a node, make sure it actually belongs to the\n   * currently connected cluster.\n   */\n  boolean belongsToCluster(final MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector.\n   *\n   * Any IOException will cause a reconnect. Note that this code makes sure\n   * that the corresponding node is not only able to connect, but also able to\n   * respond in a correct fashion (if verifyAliveOnConnect is set to true\n   * through a property). This is handled by issuing a dummy\n   * version/noop call and making sure it returns in a correct and timely\n   * fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(final SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n        sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n        sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().debug(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n          finishConnect(sk, node);\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        handleReadsAndWrites(sk, node);\n      }\n    } catch (ClosedChannelException e) {\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n        + \"operation on %s. This may be due to an authentication failure.\",\n        node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  /**\n   * A helper method for {@link #handleIO(java.nio.channels.SelectionKey)} to\n   * handle reads and writes if appropriate.\n   *\n   * @param sk the selection key to use.\n   * @param node th enode to read write from.\n   * @throws IOException if an error occurs during read/write.\n   */\n  private void handleReadsAndWrites(final SelectionKey sk, final MemcachedNode node) throws IOException {\n      if (sk.isValid() && sk.isReadable()) {\n          handleReads(node);\n      }\n\n      if (sk.isValid() && sk.isWritable()) {\n          handleWrites(node);\n      }\n  }\n  /**\n   * Finish the connect phase and potentially verify its liveness.\n   *\n   * @param sk the selection key for the node.\n   * @param node the actual node.\n   * @throws IOException if something goes wrong during reading/writing.\n   */\n  private void finishConnect(final SelectionKey sk, final MemcachedNode node)\n    throws IOException {\n    if (verifyAliveOnConnect) {\n      final CountDownLatch latch = new CountDownLatch(1);\n      final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(\"noop\",\n        latch, 2500, listenerExecutorService);\n      NoopOperation testOp = opFact.noop(new OperationCallback() {\n        public void receivedStatus(OperationStatus status) {\n          rv.set(status.isSuccess(), status);\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      });\n\n      testOp.setHandlingNode(node);\n      testOp.initialize();\n      checkState();\n      insertOperation(node, testOp);\n      node.copyInputQueue();\n\n      boolean done = false;\n      if (sk.isValid()) {\n        long timeout = TimeUnit.MILLISECONDS.toNanos(\n          connectionFactory.getOperationTimeout());\n\n        long stop = System.nanoTime() + timeout;\n        while (stop > System.nanoTime()) {\n          handleWrites(node);\n          handleReads(node);\n          if(done = (latch.getCount() == 0)) {\n            break;\n          }\n        }\n      }\n\n      if (!done || testOp.isCancelled() || testOp.hasErrored()\n        || testOp.isTimedOut()) {\n        throw new ConnectException(\"Could not send noop upon connect! \"\n          + \"This may indicate a running, but not responding memcached \"\n          + \"instance.\");\n      }\n    }\n\n    connected(node);\n    addedQueue.offer(node);\n    if (node.getWbuf().hasRemaining()) {\n      handleWrites(node);\n    }\n  }\n\n  /**\n   * Handle pending writes for the given node.\n   *\n   * @param node the node to handle writes for.\n   * @throws IOException can be raised during writing failures.\n   */\n  private void handleWrites(final MemcachedNode node) throws IOException {\n    node.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = node.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = node.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      node.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && node.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  /**\n   * Handle pending reads for the given node.\n   *\n   * @param node the node to handle reads for.\n   * @throws IOException can be raised during reading failures.\n   */\n  private void handleReads(final MemcachedNode node) throws IOException {\n    Operation currentOp = node.getCurrentReadOp();\n    if (currentOp instanceof TapAckOperationImpl) {\n      node.removeCurrentReadOp();\n      return;\n    }\n\n    ByteBuffer rbuf = node.getRbuf();\n    final SocketChannel channel = node.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      currentOp = handleReadsWhenChannelEndOfStream(currentOp, node, rbuf);\n    }\n\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n\n        long timeOnWire =\n          System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n        metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC,\n          (int)(timeOnWire / 1000));\n        metrics.markMeter(OVERALL_RESPONSE_METRIC);\n        synchronized(currentOp) {\n          readBufferAndLogMetrics(currentOp, rbuf, node);\n        }\n\n        currentOp = node.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      node.completedRead();\n    }\n  }\n\n  /**\n   * Read from the buffer and add metrics information.\n   *\n   * @param currentOp the current operation to read.\n   * @param rbuf the read buffer to read from.\n   * @param node the node to read from.\n   * @throws IOException if reading was not successful.\n   */\n  private void readBufferAndLogMetrics(final Operation currentOp,\n    final ByteBuffer rbuf, final MemcachedNode node) throws IOException {\n    currentOp.readFromBuffer(rbuf);\n    if (currentOp.getState() == OperationState.COMPLETE) {\n      getLogger().debug(\"Completed read op: %s and giving the next %d \"\n        + \"bytes\", currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      if (op.hasErrored()) {\n        metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      } else {\n        metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n      }\n    } else if (currentOp.getState() == OperationState.RETRY) {\n      handleRetryInformation(currentOp.getErrorMsg());\n      getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n        + \"%s \", currentOp);\n      ((VBucketAware) currentOp).addNotMyVbucketNode(\n        currentOp.getHandlingNode());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      retryOperation(currentOp);\n      metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n    }\n  }\n\n  /**\n   * Deal with an operation where the channel reached the end of a stream.\n   *\n   * @param currentOp the current operation to read.\n   * @param node the node for that operation.\n   * @param rbuf the read buffer.\n   *\n   * @return the next operation on the node to read.\n   * @throws IOException if disconnect while reading.\n   */\n  private Operation handleReadsWhenChannelEndOfStream(final Operation currentOp,\n    final MemcachedNode node, final ByteBuffer rbuf) throws IOException {\n    if (currentOp instanceof TapOperation) {\n      currentOp.getCallback().complete();\n      ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n\n      getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n        currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n      return node.getCurrentReadOp();\n    } else {\n      throw new IOException(\"Disconnected unexpected, will reconnect.\");\n    }\n  }\n\n  /**\n   * Convert the {@link ByteBuffer} into a string for easier debugging.\n   *\n   * @param b the buffer to debug.\n   * @param size the size of the buffer.\n   * @return the stringified {@link ByteBuffer}.\n   */\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Optionally handle retry (NOT_MY_VBUKET) responses.\n   *\n   * This method can be overridden in subclasses to handle the content\n   * of the retry message appropriately.\n   *\n   * @param retryMessage the body of the retry message.\n   */\n  protected void handleRetryInformation(final byte[] retryMessage) {\n    getLogger().debug(\"Got RETRY message: \" + new String(retryMessage)\n      + \", but not handled.\");\n  }\n\n  /**\n   * Enqueue the given {@link MemcachedNode} for reconnect.\n   *\n   * @param node the node to reconnect.\n   */\n  protected void queueReconnect(final MemcachedNode node) {\n    if (shutDown) {\n      return;\n    }\n    getLogger().warn(\"Closing, and reopening %s, attempt %d.\", node,\n      node.getReconnectCount());\n\n    if (node.getSk() != null) {\n      node.getSk().cancel();\n      assert !node.getSk().isValid() : \"Cancelled selection key is valid\";\n    }\n    node.reconnecting();\n\n    try {\n      if (node.getChannel() != null && node.getChannel().socket() != null) {\n        node.getChannel().socket().close();\n      } else {\n        getLogger().info(\"The channel or socket was null for %s\", node);\n      }\n    } catch (IOException e) {\n      getLogger().warn(\"IOException trying to close a socket\", e);\n    }\n    node.setChannel(null);\n\n    long delay = (long) Math.min(maxDelay, Math.pow(2,\n        node.getReconnectCount()) * 1000);\n    long reconnectTime = System.currentTimeMillis() + delay;\n    while (reconnectQueue.containsKey(reconnectTime)) {\n      reconnectTime++;\n    }\n\n    reconnectQueue.put(reconnectTime, node);\n    metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n    node.setupResend();\n    if (failureMode == FailureMode.Redistribute) {\n      redistributeOperations(node.destroyInputQueue());\n    } else if (failureMode == FailureMode.Cancel) {\n      cancelOperations(node.destroyInputQueue());\n    }\n  }\n\n  /**\n   * Cancel the given collection of operations.\n   *\n   * @param ops the list of operations to cancel.\n   */\n  private void cancelOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Redistribute the given list of operations to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param ops the operations to redistribute.\n   */\n  public void redistributeOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      redistributeOperation(op);\n    }\n  }\n\n  /**\n   * Redistribute the given operation to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param op the operation to redistribute.\n   */\n  public void redistributeOperation(Operation op) {\n    if (op.isCancelled() || op.isTimedOut()) {\n      return;\n    }\n\n    if (op.getCloneCount() >= MAX_CLONE_COUNT) {\n      getLogger().warn(\"Cancelling operation \" + op + \"because it has been \"\n        + \"retried (cloned) more than \" + MAX_CLONE_COUNT + \"times.\");\n      op.cancel();\n      return;\n    }\n\n    // The operation gets redistributed but has never been actually written,\n    // it we just straight re-add it without cloning.\n    if (op.getState() == OperationState.WRITE_QUEUED && op.getHandlingNode() != null) {\n      addOperation(op.getHandlingNode(), op);\n      return;\n    }\n\n    if (op instanceof MultiGetOperationImpl) {\n      for (String key : ((MultiGetOperationImpl) op).getRetryKeys()) {\n        addOperation(key, opFact.get(key,\n          (GetOperation.Callback) op.getCallback()));\n      }\n    } else if (op instanceof KeyedOperation) {\n      KeyedOperation ko = (KeyedOperation) op;\n      int added = 0;\n      for (Operation newop : opFact.clone(ko)) {\n        if (newop instanceof KeyedOperation) {\n          KeyedOperation newKeyedOp = (KeyedOperation) newop;\n          for (String k : newKeyedOp.getKeys()) {\n            addOperation(k, newop);\n            op.addClone(newop);\n            newop.setCloneCount(op.getCloneCount()+1);\n          }\n        } else {\n          newop.cancel();\n          getLogger().warn(\"Could not redistribute cloned non-keyed \" +\n            \"operation\", newop);\n        }\n        added++;\n      }\n      assert added > 0 : \"Didn't add any new operations when redistributing\";\n    } else {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Attempt to reconnect {@link MemcachedNode}s in the reconnect queue.\n   *\n   * If the {@link MemcachedNode} does not belong to the cluster list anymore,\n   * the reconnect attempt is cancelled. If it does, the code tries to\n   * reconnect immediately and if this is not possible it waits until the\n   * connection information arrives.\n   *\n   * Note that if a socket error arises during reconnect, the node is scheduled\n   * for re-reconnect immediately.\n   */\n  private void attemptReconnects() {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n      new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n\n\n    Iterator<MemcachedNode> i = reconnectQueue.headMap(now).values().iterator();\n    while(i.hasNext()) {\n      final MemcachedNode node = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n\n      try {\n        if (!belongsToCluster(node)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", node);\n          continue;\n        }\n\n        if (!seen.containsKey(node)) {\n          seen.put(node, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", node);\n\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n          int ops = 0;\n          if (ch.connect(node.getSocketAddress())) {\n            connected(node);\n            addedQueue.offer(node);\n            getLogger().info(\"Immediately reconnected to %s\", node);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          node.registerChannel(ch, ch.register(selector, ops, node));\n          assert node.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\",\n            node);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(node);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", node, e);\n      } finally {\n        potentiallyCloseLeakingChannel(ch, node);\n      }\n    }\n\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Make sure channel connections are not leaked and properly close under\n   * faulty reconnect cirumstances.\n   *\n   * @param ch the channel to potentially close.\n   * @param node the node to which the channel should be bound to.\n   */\n  private void potentiallyCloseLeakingChannel(final SocketChannel ch,\n    final MemcachedNode node) {\n    if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n      try {\n        ch.close();\n      } catch (IOException e) {\n        getLogger().error(\"Exception closing channel: %s\", node, e);\n      }\n    }\n  }\n\n  /**\n   * Returns the {@link NodeLocator} in use for this connection.\n   *\n   * @return  the current {@link NodeLocator}.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  /**\n   * Enqueue the given {@link Operation} with the used key.\n   *\n   * @param key the key to use.\n   * @param o the {@link Operation} to enqueue.\n   */\n  public void enqueueOperation(final String key, final Operation o) {\n    checkState();\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to a connection identified by the given key.\n   *\n   * If the {@link MemcachedNode} is active or the {@link FailureMode} is set\n   * to retry, the primary node will be used for that key. If the primary\n   * node is not available and the {@link FailureMode} cancel is used, the\n   * operation will be cancelled without further retry.\n   *\n   * For any other {@link FailureMode} mechanisms (Redistribute), another\n   * possible node is used (only if its active as well). If no other active\n   * node could be identified, the original primary node is used and retried.\n   *\n   * @param key the key the operation is operating upon.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final String key, final Operation o) {\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      Iterator<MemcachedNode> i = locator.getSequence(key);\n      while (placeIn == null && i.hasNext()) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\"Could not redistribute to another node, \"\n          + \"retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key + \" (and not \"\n        + \"immediately cancelled)\";\n    }\n  }\n\n  /**\n   * Insert an operation on the given node to the beginning of the queue.\n   *\n   * @param node the node where to insert the {@link Operation}.\n   * @param o the operation to insert.\n   */\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue an operation on the given node.\n   *\n   * @param node the node where to enqueue the {@link Operation}.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    if (!node.isAuthenticated()) {\n      retryOperation(o);\n      return;\n    }\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue the given list of operations on each handling node.\n   *\n   * @param ops the operations for each node.\n   */\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      addOperation(me.getKey(), me.getValue());\n    }\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a collection of nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n    final Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all connections and do not accept further incoming ops.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n    try {\n      Selector s = selector.wakeup();\n      assert s == selector : \"Wakeup returned the wrong selector.\";\n      for (MemcachedNode node : locator.getAll()) {\n        if (node.getChannel() != null) {\n          node.getChannel().close();\n          node.setSk(null);\n          if (node.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              node.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", node.getChannel());\n        }\n      }\n\n      selector.close();\n      getLogger().debug(\"Shut down selector %s\", selector);\n    } finally {\n      running = false;\n    }\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \").append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Construct a String containing information about all nodes and their state.\n   *\n   * @return a stringified representation of the connection status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus\n        .append(\" \")\n        .append(node.getSocketAddress())\n        .append(\" active: \")\n        .append(node.isActive())\n        .append(\", authed: \")\n        .append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * Increase the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opTimedOut(final Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * Reset the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opSucceeded(final Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * Set the continuous timeout on an operation.\n   *\n   * Ignore operations which have no handling nodes set yet (which may happen before nodes are properly\n   * authenticated).\n   *\n   * @param op the operation to use.\n   * @param isTimeout is timed out or not.\n   */\n  private static void setTimeout(final Operation op, final boolean isTimeout) {\n    Logger logger = LoggerFactory.getLogger(MemcachedConnection.class);\n\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return;\n      }\n\n      MemcachedNode node = op.getHandlingNode();\n      if (node != null) {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      logger.error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   *\n   * @throws IllegalStateException when shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Handle IO as long as the application is running.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  /**\n   * Log a exception to different levels depending on the state.\n   *\n   * Exceptions get logged at debug level when happening during shutdown, but\n   * at warning level when operating normally.\n   *\n   * @param e the exception to log.\n   */\n  private void logRunException(final Exception e) {\n    if (shutDown) {\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n\n  /**\n   * Returns whether the connection is shut down or not.\n   *\n   * @return true if the connection is shut down, false otherwise.\n   */\n  public boolean isShutDown() {\n    return shutDown;\n  }\n\n  /**\n   * Add a operation to the retry queue.\n   *\n   * If the retry queue size is bounded and the size of the queue is reaching\n   * that boundary, the operation is cancelled rather than added to the\n   * retry queue.\n   *\n   * @param op the operation to retry.\n   */\n  public void retryOperation(Operation op) {\n    if (retryQueueSize >= 0 && retryOps.size() >= retryQueueSize) {\n      if (!op.isCancelled()) {\n        op.cancel();\n      }\n    }\n    retryOps.add(op);\n  }\n\n}\n","lineNo":353}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.Logger;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.MultiGetOperationImpl;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\n/**\n * Main class for handling connections to a memcached cluster.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  /**\n   * The number of empty selects we'll allow before assuming we may have\n   * missed one and should check the current selectors. This generally\n   * indicates a bug, but we'll check it nonetheless.\n   */\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n\n  /**\n   * The number of empty selects we'll allow before blowing up. It's too\n   * easy to write a bug that causes it to loop uncontrollably. This helps\n   * find those bugs and often works around them.\n   */\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  /**\n   * The default wakeup delay if not overriden by a system property.\n   */\n  private static final int DEFAULT_WAKEUP_DELAY = 1000;\n\n  /**\n   * If an operation gets cloned more than this ceiling, cancel it for\n   * safety reasons.\n   */\n  private static final int MAX_CLONE_COUNT = 100;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  /**\n   * If the connection is alread shut down or shutting down.\n   */\n  protected volatile boolean shutDown = false;\n\n  /**\n   * If true, optimization will collapse multiple sequential get ops.\n   */\n  private final boolean shouldOptimize;\n\n  /**\n   * Holds the current {@link Selector} to use.\n   */\n  protected Selector selector = null;\n\n  /**\n   * The {@link NodeLocator} to use for this connection.\n   */\n  protected final NodeLocator locator;\n\n  /**\n   * The configured {@link FailureMode}.\n   */\n  protected final FailureMode failureMode;\n\n  /**\n   * Maximum amount of time to wait between reconnect attempts.\n   */\n  private final long maxDelay;\n\n  /**\n   * Contains the current number of empty select() calls, which could indicate\n   * bugs.\n   */\n  private int emptySelects = 0;\n\n  /**\n   * The buffer size that will be used when reading from the server.\n   */\n  private final int bufSize;\n\n  /**\n   * The connection factory to create {@link MemcachedNode}s from.\n   */\n  private final ConnectionFactory connectionFactory;\n\n  /**\n   * AddedQueue is used to track the QueueAttachments for which operations\n   * have recently been queued.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\n  /**\n   * reconnectQueue contains the attachments that need to be reconnected.\n   * The key is the time at which they are eligible for reconnect.\n   */\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  /**\n   * True if not shutting down or shut down.\n   */\n  protected volatile boolean running = true;\n\n  /**\n   * Holds all connection observers that get notified on connection status\n   * changes.\n   */\n  private final Collection<ConnectionObserver> connObservers =\n    new ConcurrentLinkedQueue<ConnectionObserver>();\n\n  /**\n   * The {@link OperationFactory} to clone or create operations.\n   */\n  private final OperationFactory opFact;\n\n  /**\n   * The threshold for timeout exceptions.\n   */\n  private final int timeoutExceptionThreshold;\n\n  /**\n   * Holds operations that need to be retried.\n   */\n  private final List<Operation> retryOps;\n\n  /**\n   * Holds all nodes that are scheduled for shutdown.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n  /**\n   * If set to true, a proper check after finish connecting is done to see\n   * if the node is not responding but really alive.\n   */\n  private final boolean verifyAliveOnConnect;\n\n  /**\n   * The {@link ExecutorService} to use for callbacks.\n   */\n  private final ExecutorService listenerExecutorService;\n\n  /**\n   * The {@link MetricCollector} to accumulate metrics (or dummy).\n   */\n  protected final MetricCollector metrics;\n\n  /**\n   * The current type of metrics to collect.\n   */\n  protected final MetricType metricType;\n\n  /**\n   * The selector wakeup delay, defaults to 1000ms.\n   */\n  private final int wakeupDelay;\n\n  /**\n   * Construct a {@link MemcachedConnection}.\n   *\n   * @param bufSize the size of the buffer used for reading from the server.\n   * @param f the factory that will provide an operation queue.\n   * @param a the addresses of the servers to connect to.\n   * @param obs the initial observers to add.\n   * @param fm the failure mode to use.\n   * @param opfactory the operation factory.\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(final int bufSize, final ConnectionFactory f,\n      final List<InetSocketAddress> a, final Collection<ConnectionObserver> obs,\n      final FailureMode fm, final OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = TimeUnit.SECONDS.toMillis(f.getMaxReconnectDelay());\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = Collections.synchronizedList(new ArrayList<Operation>());\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    wakeupDelay = Integer.parseInt( System.getProperty(\"net.spy.wakeupDelay\",\n      Integer.toString(DEFAULT_WAKEUP_DELAY)));\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG)\n      || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  /**\n   * Create connections for the given list of addresses.\n   *\n   * @param addrs the list of addresses to connect to.\n   * @return addrs list of {@link MemcachedNode}s.\n   * @throws IOException if connecting was not successful.\n   */\n  protected List<MemcachedNode> createConnections(\n    final Collection<InetSocketAddress> addrs) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(addrs.size());\n\n    for (SocketAddress sa : addrs) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa = connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      qa.setConnection(this);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n\n    return connections;\n  }\n\n  /**\n   * Make sure that the current selectors make sense.\n   *\n   * @return true if they do.\n   */\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n            + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * Handle all IO that flows through the connection.\n   *\n   * This method is called in an endless loop, listens on NIO selectors and\n   * dispatches the underlying read/write calls if needed.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      getLogger().debug(\"No IO while shut down.\");\n      return;\n    }\n\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = 1000;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n\n    if (shutDown) {\n      return;\n    } else if (selected == 0 && addedQueue.isEmpty()) {\n      handleWokenUpSelector();\n    } else if (selector.selectedKeys().isEmpty()) {\n      handleEmptySelects();\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n        selector.selectedKeys().size());\n      emptySelects = 0;\n\n      Iterator<SelectionKey> iterator = selector.selectedKeys().iterator();\n      while(iterator.hasNext()) {\n        SelectionKey sk = iterator.next();\n        handleIO(sk);\n        iterator.remove();\n      }\n    }\n\n    handleOperationalTasks();\n  }\n\n  /**\n   * Helper method which gets called if the selector is woken up because of the\n   * timeout setting, if has been interrupted or if happens during regular\n   * write operation phases.\n   *\n   * <p>This method can be overriden by child implementations to handle custom\n   * behavior on a manually woken selector, like sending pings through the\n   * channels to make sure they are alive.<\/p>\n   *\n   * <p>Note that there is no guarantee that this method is at all or in the\n   * regular interval called, so all overriding implementations need to take\n   * that into account. Also, it needs to take into account that it may be\n   * called very often under heavy workloads, so it should not perform extensive\n   * tasks in the same thread.<\/p>\n   */\n  protected void handleWokenUpSelector() { }\n\n  /**\n   * Helper method for {@link #handleIO()} to encapsulate everything that\n   * needs to be checked on a regular basis that has nothing to do directly\n   * with reading and writing data.\n   *\n   * @throws IOException if an error happens during shutdown queue handling.\n   */\n  private void handleOperationalTasks() throws IOException {\n    checkPotentiallyTimedOutConnection();\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n\n    if (!retryOps.isEmpty()) {\n      redistributeOperations(new ArrayList<Operation>(retryOps));\n      retryOps.clear();\n    }\n\n    handleShutdownQueue();\n  }\n\n  /**\n   * Helper method for {@link #handleIO()} to handle empty select calls.\n   */\n  private void handleEmptySelects() {\n    getLogger().debug(\"No selectors ready, interrupted: \"\n      + Thread.interrupted());\n\n    if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n      for (SelectionKey sk : selector.keys()) {\n        getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n          sk.interestOps());\n        if (sk.readyOps() != 0) {\n          getLogger().debug(\"%s has a ready op, handling IO\", sk);\n          handleIO(sk);\n        } else {\n          lostConnection((MemcachedNode) sk.attachment());\n        }\n      }\n      assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n    }\n  }\n\n  /**\n   * Check if nodes need to be shut down and do so if needed.\n   *\n   * @throws IOException if the channel could not be closed properly.\n   */\n  private void handleShutdownQueue() throws IOException {\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  /**\n   * Check if one or more nodes exceeded the timeout Threshold.\n   */\n  private void checkPotentiallyTimedOutConnection() {\n    boolean stillCheckingTimeouts = true;\n    while (stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n  }\n\n  /**\n   * Handle any requests that have been made against the client.\n   */\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n\n      MemcachedNode qaNode;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      for (MemcachedNode node : todo) {\n        boolean readyForIO = false;\n        if (node.isActive()) {\n          if (node.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", node);\n          }\n        } else {\n          toAdd.add(node);\n        }\n        node.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (node.getWbuf().hasRemaining()) {\n              handleWrites(node);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(node);\n          }\n        }\n        node.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added.\n   */\n  public boolean addObserver(final ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't.\n   */\n  public boolean removeObserver(final ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  /**\n   * Indicate a successful connect to the given node.\n   *\n   * @param node the node which was successfully connected.\n   */\n  private void connected(final MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  /**\n   * Indicate a lost connection to the given node.\n   *\n   * @param node the node where the connection was lost.\n   */\n  private void lostConnection(final MemcachedNode node) {\n    queueReconnect(node);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(node.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given node belongs to the current cluster.\n   *\n   * Before trying to connect to a node, make sure it actually belongs to the\n   * currently connected cluster.\n   */\n  boolean belongsToCluster(final MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector.\n   *\n   * Any IOException will cause a reconnect. Note that this code makes sure\n   * that the corresponding node is not only able to connect, but also able to\n   * respond in a correct fashion (if verifyAliveOnConnect is set to true\n   * through a property). This is handled by issuing a dummy\n   * version/noop call and making sure it returns in a correct and timely\n   * fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(final SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n        sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n        sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().debug(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n          finishConnect(sk, node);\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        handleReadsAndWrites(sk, node);\n      }\n    } catch (ClosedChannelException e) {\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n        + \"operation on %s. This may be due to an authentication failure.\",\n        node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  /**\n   * A helper method for {@link #handleIO(java.nio.channels.SelectionKey)} to\n   * handle reads and writes if appropriate.\n   *\n   * @param sk the selection key to use.\n   * @param node th enode to read write from.\n   * @throws IOException if an error occurs during read/write.\n   */\n  private void handleReadsAndWrites(final SelectionKey sk,\n    final MemcachedNode node) throws IOException {\n    if (sk.isValid()) {\n      if (sk.isReadable()) {\n        handleReads(node);\n      }\n      if (sk.isWritable()) {\n        handleWrites(node);\n      }\n    }\n  }\n\n  /**\n   * Finish the connect phase and potentially verify its liveness.\n   *\n   * @param sk the selection key for the node.\n   * @param node the actual node.\n   * @throws IOException if something goes wrong during reading/writing.\n   */\n  private void finishConnect(final SelectionKey sk, final MemcachedNode node)\n    throws IOException {\n    if (verifyAliveOnConnect) {\n      final CountDownLatch latch = new CountDownLatch(1);\n      final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(\"noop\",\n        latch, 2500, listenerExecutorService);\n      NoopOperation testOp = opFact.noop(new OperationCallback() {\n        public void receivedStatus(OperationStatus status) {\n          rv.set(status.isSuccess(), status);\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      });\n\n      testOp.setHandlingNode(node);\n      testOp.initialize();\n      checkState();\n      insertOperation(node, testOp);\n      node.copyInputQueue();\n\n      boolean done = false;\n      if (sk.isValid()) {\n        long timeout = TimeUnit.MILLISECONDS.toNanos(\n          connectionFactory.getOperationTimeout());\n\n        long stop = System.nanoTime() + timeout;\n        while (stop > System.nanoTime()) {\n          handleWrites(node);\n          handleReads(node);\n          if(done = (latch.getCount() == 0)) {\n            break;\n          }\n        }\n      }\n\n      if (!done || testOp.isCancelled() || testOp.hasErrored()\n        || testOp.isTimedOut()) {\n        throw new ConnectException(\"Could not send noop upon connect! \"\n          + \"This may indicate a running, but not responding memcached \"\n          + \"instance.\");\n      }\n    }\n\n    connected(node);\n    addedQueue.offer(node);\n    if (node.getWbuf().hasRemaining()) {\n      handleWrites(node);\n    }\n  }\n\n  /**\n   * Handle pending writes for the given node.\n   *\n   * @param node the node to handle writes for.\n   * @throws IOException can be raised during writing failures.\n   */\n  private void handleWrites(final MemcachedNode node) throws IOException {\n    node.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = node.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = node.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      node.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && node.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  /**\n   * Handle pending reads for the given node.\n   *\n   * @param node the node to handle reads for.\n   * @throws IOException can be raised during reading failures.\n   */\n  private void handleReads(final MemcachedNode node) throws IOException {\n    Operation currentOp = node.getCurrentReadOp();\n    if (currentOp instanceof TapAckOperationImpl) {\n      node.removeCurrentReadOp();\n      return;\n    }\n\n    ByteBuffer rbuf = node.getRbuf();\n    final SocketChannel channel = node.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      currentOp = handleReadsWhenChannelEndOfStream(currentOp, node, rbuf);\n    }\n\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n\n        long timeOnWire =\n          System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n        metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC,\n          (int)(timeOnWire / 1000));\n        metrics.markMeter(OVERALL_RESPONSE_METRIC);\n        synchronized(currentOp) {\n          readBufferAndLogMetrics(currentOp, rbuf, node);\n        }\n\n        currentOp = node.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      node.completedRead();\n    }\n  }\n\n  /**\n   * Read from the buffer and add metrics information.\n   *\n   * @param currentOp the current operation to read.\n   * @param rbuf the read buffer to read from.\n   * @param node the node to read from.\n   * @throws IOException if reading was not successful.\n   */\n  private void readBufferAndLogMetrics(final Operation currentOp,\n    final ByteBuffer rbuf, final MemcachedNode node) throws IOException {\n    currentOp.readFromBuffer(rbuf);\n    if (currentOp.getState() == OperationState.COMPLETE) {\n      getLogger().debug(\"Completed read op: %s and giving the next %d \"\n        + \"bytes\", currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      if (op.hasErrored()) {\n        metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      } else {\n        metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n      }\n    } else if (currentOp.getState() == OperationState.RETRY) {\n      handleRetryInformation(currentOp.getErrorMsg());\n      getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n        + \"%s \", currentOp);\n      ((VBucketAware) currentOp).addNotMyVbucketNode(\n        currentOp.getHandlingNode());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      retryOps.add(currentOp);\n      metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n    }\n  }\n\n  /**\n   * Deal with an operation where the channel reached the end of a stream.\n   *\n   * @param currentOp the current operation to read.\n   * @param node the node for that operation.\n   * @param rbuf the read buffer.\n   *\n   * @return the next operation on the node to read.\n   * @throws IOException if disconnect while reading.\n   */\n  private Operation handleReadsWhenChannelEndOfStream(final Operation currentOp,\n    final MemcachedNode node, final ByteBuffer rbuf) throws IOException {\n    if (currentOp instanceof TapOperation) {\n      currentOp.getCallback().complete();\n      ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n\n      getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n        currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n      return node.getCurrentReadOp();\n    } else {\n      throw new IOException(\"Disconnected unexpected, will reconnect.\");\n    }\n  }\n\n  /**\n   * Convert the {@link ByteBuffer} into a string for easier debugging.\n   *\n   * @param b the buffer to debug.\n   * @param size the size of the buffer.\n   * @return the stringified {@link ByteBuffer}.\n   */\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Optionally handle retry (NOT_MY_VBUKET) responses.\n   *\n   * This method can be overridden in subclasses to handle the content\n   * of the retry message appropriately.\n   *\n   * @param retryMessage the body of the retry message.\n   */\n  protected void handleRetryInformation(final byte[] retryMessage) {\n    getLogger().debug(\"Got RETRY message: \" + new String(retryMessage)\n      + \", but not handled.\");\n  }\n\n  /**\n   * Enqueue the given {@link MemcachedNode} for reconnect.\n   *\n   * @param node the node to reconnect.\n   */\n  protected void queueReconnect(final MemcachedNode node) {\n    if (shutDown) {\n      return;\n    }\n    getLogger().warn(\"Closing, and reopening %s, attempt %d.\", node,\n      node.getReconnectCount());\n\n    if (node.getSk() != null) {\n      node.getSk().cancel();\n      assert !node.getSk().isValid() : \"Cancelled selection key is valid\";\n    }\n    node.reconnecting();\n\n    try {\n      if (node.getChannel() != null && node.getChannel().socket() != null) {\n        node.getChannel().socket().close();\n      } else {\n        getLogger().info(\"The channel or socket was null for %s\", node);\n      }\n    } catch (IOException e) {\n      getLogger().warn(\"IOException trying to close a socket\", e);\n    }\n    node.setChannel(null);\n\n    long delay = (long) Math.min(maxDelay, Math.pow(2,\n        node.getReconnectCount())) * 1000;\n    long reconnectTime = System.currentTimeMillis() + delay;\n    while (reconnectQueue.containsKey(reconnectTime)) {\n      reconnectTime++;\n    }\n\n    reconnectQueue.put(reconnectTime, node);\n    metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n    node.setupResend();\n    if (failureMode == FailureMode.Redistribute) {\n      redistributeOperations(node.destroyInputQueue());\n    } else if (failureMode == FailureMode.Cancel) {\n      cancelOperations(node.destroyInputQueue());\n    }\n  }\n\n  /**\n   * Cancel the given collection of operations.\n   *\n   * @param ops the list of operations to cancel.\n   */\n  private void cancelOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Redistribute the given list of operations to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param ops the operations to redistribute.\n   */\n  public void redistributeOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      redistributeOperation(op);\n    }\n  }\n\n  /**\n   * Redistribute the given operation to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param op the operation to redistribute.\n   */\n  public void redistributeOperation(Operation op) {\n    if (op.isCancelled() || op.isTimedOut()) {\n      return;\n    }\n\n    if (op.getCloneCount() >= MAX_CLONE_COUNT) {\n      getLogger().warn(\"Cancelling operation \" + op + \"because it has been \"\n        + \"retried (cloned) more than \" + MAX_CLONE_COUNT + \"times.\");\n      op.cancel();\n      return;\n    }\n\n    // The operation gets redistributed but has never been actually written,\n    // it we just straight re-add it without cloning.\n    if (op.getState() == OperationState.WRITE_QUEUED) {\n      addOperation(op.getHandlingNode(), op);\n      return;\n    }\n\n    if (op instanceof MultiGetOperationImpl) {\n      for (String key : ((MultiGetOperationImpl) op).getRetryKeys()) {\n        addOperation(key, opFact.get(key,\n          (GetOperation.Callback) op.getCallback()));\n      }\n    } else if (op instanceof KeyedOperation) {\n      KeyedOperation ko = (KeyedOperation) op;\n      int added = 0;\n      for (Operation newop : opFact.clone(ko)) {\n        if (newop instanceof KeyedOperation) {\n          KeyedOperation newKeyedOp = (KeyedOperation) newop;\n          for (String k : newKeyedOp.getKeys()) {\n            addOperation(k, newop);\n            op.addClone(newop);\n            newop.setCloneCount(op.getCloneCount()+1);\n          }\n        } else {\n          newop.cancel();\n          getLogger().warn(\"Could not redistribute cloned non-keyed \" +\n            \"operation\", newop);\n        }\n        added++;\n      }\n      assert added > 0 : \"Didn't add any new operations when redistributing\";\n    } else {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Attempt to reconnect {@link MemcachedNode}s in the reconnect queue.\n   *\n   * If the {@link MemcachedNode} does not belong to the cluster list anymore,\n   * the reconnect attempt is cancelled. If it does, the code tries to\n   * reconnect immediately and if this is not possible it waits until the\n   * connection information arrives.\n   *\n   * Note that if a socket error arises during reconnect, the node is scheduled\n   * for re-reconnect immediately.\n   */\n  private void attemptReconnects() {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n      new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n\n\n    Iterator<MemcachedNode> i = reconnectQueue.headMap(now).values().iterator();\n    while(i.hasNext()) {\n      final MemcachedNode node = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n\n      try {\n        if (!belongsToCluster(node)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", node);\n          continue;\n        }\n\n        if (!seen.containsKey(node)) {\n          seen.put(node, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", node);\n\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n          int ops = 0;\n          if (ch.connect(node.getSocketAddress())) {\n            connected(node);\n            addedQueue.offer(node);\n            getLogger().info(\"Immediately reconnected to %s\", node);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          node.registerChannel(ch, ch.register(selector, ops, node));\n          assert node.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\",\n            node);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(node);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", node, e);\n      } finally {\n        potentiallyCloseLeakingChannel(ch, node);\n      }\n    }\n\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Make sure channel connections are not leaked and properly close under\n   * faulty reconnect cirumstances.\n   *\n   * @param ch the channel to potentially close.\n   * @param node the node to which the channel should be bound to.\n   */\n  private void potentiallyCloseLeakingChannel(final SocketChannel ch,\n    final MemcachedNode node) {\n    if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n      try {\n        ch.close();\n      } catch (IOException e) {\n        getLogger().error(\"Exception closing channel: %s\", node, e);\n      }\n    }\n  }\n\n  /**\n   * Returns the {@link NodeLocator} in use for this connection.\n   *\n   * @return  the current {@link NodeLocator}.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  /**\n   * Enqueue the given {@link Operation} with the used key.\n   *\n   * @param key the key to use.\n   * @param o the {@link Operation} to enqueue.\n   */\n  public void enqueueOperation(final String key, final Operation o) {\n    checkState();\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to a connection identified by the given key.\n   *\n   * If the {@link MemcachedNode} is active or the {@link FailureMode} is set\n   * to retry, the primary node will be used for that key. If the primary\n   * node is not available and the {@link FailureMode} cancel is used, the\n   * operation will be cancelled without further retry.\n   *\n   * For any other {@link FailureMode} mechanisms (Redistribute), another\n   * possible node is used (only if its active as well). If no other active\n   * node could be identified, the original primary node is used and retried.\n   *\n   * @param key the key the operation is operating upon.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final String key, final Operation o) {\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      Iterator<MemcachedNode> i = locator.getSequence(key);\n      while (placeIn == null && i.hasNext()) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\"Could not redistribute to another node, \"\n          + \"retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key + \" (and not \"\n        + \"immediately cancelled)\";\n    }\n  }\n\n  /**\n   * Insert an operation on the given node to the beginning of the queue.\n   *\n   * @param node the node where to insert the {@link Operation}.\n   * @param o the operation to insert.\n   */\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue an operation on the given node.\n   *\n   * @param node the node where to enqueue the {@link Operation}.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue the given list of operations on each handling node.\n   *\n   * @param ops the operations for each node.\n   */\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      addOperation(me.getKey(), me.getValue());\n    }\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a collection of nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n    final Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all connections and do not accept further incoming ops.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n    try {\n      Selector s = selector.wakeup();\n      assert s == selector : \"Wakeup returned the wrong selector.\";\n      for (MemcachedNode node : locator.getAll()) {\n        if (node.getChannel() != null) {\n          node.getChannel().close();\n          node.setSk(null);\n          if (node.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              node.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", node.getChannel());\n        }\n      }\n\n      selector.close();\n      getLogger().debug(\"Shut down selector %s\", selector);\n    } finally {\n      running = false;\n    }\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \").append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Construct a String containing information about all nodes and their state.\n   *\n   * @return a stringified representation of the connection status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus\n        .append(\" \")\n        .append(node.getSocketAddress())\n        .append(\" active: \")\n        .append(node.isActive())\n        .append(\", authed: \")\n        .append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * Increase the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opTimedOut(final Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * Reset the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opSucceeded(final Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * Set the continous timeout on an operation.\n   *\n   * @param op the operation to use.\n   * @param isTimeout is timed out or not.\n   */\n  private static void setTimeout(final Operation op, final boolean isTimeout) {\n    Logger logger = LoggerFactory.getLogger(MemcachedConnection.class);\n\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return;\n      }\n\n      MemcachedNode node = op.getHandlingNode();\n      if (node == null) {\n        logger.warn(\"handling node for operation is not set\");\n      } else {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      logger.error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   *\n   * @throws IllegalStateException when shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Handle IO as long as the application is running.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  /**\n   * Log a exception to different levels depending on the state.\n   *\n   * Exceptions get logged at debug level when happening during shutdown, but\n   * at warning level when operating normally.\n   *\n   * @param e the exception to log.\n   */\n  private void logRunException(final Exception e) {\n    if (shutDown) {\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n\n  /**\n   * Returns whether the connection is shut down or not.\n   *\n   * @return true if the connection is shut down, false otherwise.\n   */\n  public boolean isShutDown() {\n    return shutDown;\n  }\n\n  /**\n   * Add a operation to the retry queue.\n   *\n   * @param op the operation to retry.\n   */\n  public void retryOperation(Operation op) {\n    retryOps.add(op);\n  }\n\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.Logger;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.MultiGetOperationImpl;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\n/**\n * Main class for handling connections to a memcached cluster.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  /**\n   * The number of empty selects we'll allow before assuming we may have\n   * missed one and should check the current selectors. This generally\n   * indicates a bug, but we'll check it nonetheless.\n   */\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n\n  /**\n   * The number of empty selects we'll allow before blowing up. It's too\n   * easy to write a bug that causes it to loop uncontrollably. This helps\n   * find those bugs and often works around them.\n   */\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  /**\n   * The default wakeup delay if not overriden by a system property.\n   */\n  private static final int DEFAULT_WAKEUP_DELAY = 1000;\n\n  /**\n   * If an operation gets cloned more than this ceiling, cancel it for\n   * safety reasons.\n   */\n  private static final int MAX_CLONE_COUNT = 100;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  /**\n   * If the connection is alread shut down or shutting down.\n   */\n  protected volatile boolean shutDown = false;\n\n  /**\n   * If true, optimization will collapse multiple sequential get ops.\n   */\n  private final boolean shouldOptimize;\n\n  /**\n   * Holds the current {@link Selector} to use.\n   */\n  protected Selector selector = null;\n\n  /**\n   * The {@link NodeLocator} to use for this connection.\n   */\n  protected final NodeLocator locator;\n\n  /**\n   * The configured {@link FailureMode}.\n   */\n  protected final FailureMode failureMode;\n\n  /**\n   * Maximum amount of time to wait between reconnect attempts.\n   */\n  private final long maxDelay;\n\n  /**\n   * Contains the current number of empty select() calls, which could indicate\n   * bugs.\n   */\n  private int emptySelects = 0;\n\n  /**\n   * The buffer size that will be used when reading from the server.\n   */\n  private final int bufSize;\n\n  /**\n   * The connection factory to create {@link MemcachedNode}s from.\n   */\n  private final ConnectionFactory connectionFactory;\n\n  /**\n   * AddedQueue is used to track the QueueAttachments for which operations\n   * have recently been queued.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\n  /**\n   * reconnectQueue contains the attachments that need to be reconnected.\n   * The key is the time at which they are eligible for reconnect.\n   */\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  /**\n   * True if not shutting down or shut down.\n   */\n  protected volatile boolean running = true;\n\n  /**\n   * Holds all connection observers that get notified on connection status\n   * changes.\n   */\n  private final Collection<ConnectionObserver> connObservers =\n    new ConcurrentLinkedQueue<ConnectionObserver>();\n\n  /**\n   * The {@link OperationFactory} to clone or create operations.\n   */\n  private final OperationFactory opFact;\n\n  /**\n   * The threshold for timeout exceptions.\n   */\n  private final int timeoutExceptionThreshold;\n\n  /**\n   * Holds operations that need to be retried.\n   */\n  private final List<Operation> retryOps;\n\n  /**\n   * Holds all nodes that are scheduled for shutdown.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n  /**\n   * If set to true, a proper check after finish connecting is done to see\n   * if the node is not responding but really alive.\n   */\n  private final boolean verifyAliveOnConnect;\n\n  /**\n   * The {@link ExecutorService} to use for callbacks.\n   */\n  private final ExecutorService listenerExecutorService;\n\n  /**\n   * The {@link MetricCollector} to accumulate metrics (or dummy).\n   */\n  protected final MetricCollector metrics;\n\n  /**\n   * The current type of metrics to collect.\n   */\n  protected final MetricType metricType;\n\n  /**\n   * The selector wakeup delay, defaults to 1000ms.\n   */\n  private final int wakeupDelay;\n\n  /**\n   * Construct a {@link MemcachedConnection}.\n   *\n   * @param bufSize the size of the buffer used for reading from the server.\n   * @param f the factory that will provide an operation queue.\n   * @param a the addresses of the servers to connect to.\n   * @param obs the initial observers to add.\n   * @param fm the failure mode to use.\n   * @param opfactory the operation factory.\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(final int bufSize, final ConnectionFactory f,\n      final List<InetSocketAddress> a, final Collection<ConnectionObserver> obs,\n      final FailureMode fm, final OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = TimeUnit.SECONDS.toMillis(f.getMaxReconnectDelay());\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = Collections.synchronizedList(new ArrayList<Operation>());\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    wakeupDelay = Integer.parseInt( System.getProperty(\"net.spy.wakeupDelay\",\n      Integer.toString(DEFAULT_WAKEUP_DELAY)));\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG)\n      || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  /**\n   * Create connections for the given list of addresses.\n   *\n   * @param addrs the list of addresses to connect to.\n   * @return addrs list of {@link MemcachedNode}s.\n   * @throws IOException if connecting was not successful.\n   */\n  protected List<MemcachedNode> createConnections(\n    final Collection<InetSocketAddress> addrs) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(addrs.size());\n\n    for (SocketAddress sa : addrs) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa = connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      qa.setConnection(this);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n\n    return connections;\n  }\n\n  /**\n   * Make sure that the current selectors make sense.\n   *\n   * @return true if they do.\n   */\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n            + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * Handle all IO that flows through the connection.\n   *\n   * This method is called in an endless loop, listens on NIO selectors and\n   * dispatches the underlying read/write calls if needed.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      getLogger().debug(\"No IO while shut down.\");\n      return;\n    }\n\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = 1000;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n\n    if (shutDown) {\n      return;\n    } else if (selected == 0 && addedQueue.isEmpty()) {\n      handleWokenUpSelector();\n    } else if (selector.selectedKeys().isEmpty()) {\n      handleEmptySelects();\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n        selector.selectedKeys().size());\n      emptySelects = 0;\n\n      Iterator<SelectionKey> iterator = selector.selectedKeys().iterator();\n      while(iterator.hasNext()) {\n        SelectionKey sk = iterator.next();\n        handleIO(sk);\n        iterator.remove();\n      }\n    }\n\n    handleOperationalTasks();\n  }\n\n  /**\n   * Helper method which gets called if the selector is woken up because of the\n   * timeout setting, if has been interrupted or if happens during regular\n   * write operation phases.\n   *\n   * <p>This method can be overriden by child implementations to handle custom\n   * behavior on a manually woken selector, like sending pings through the\n   * channels to make sure they are alive.<\/p>\n   *\n   * <p>Note that there is no guarantee that this method is at all or in the\n   * regular interval called, so all overriding implementations need to take\n   * that into account. Also, it needs to take into account that it may be\n   * called very often under heavy workloads, so it should not perform extensive\n   * tasks in the same thread.<\/p>\n   */\n  protected void handleWokenUpSelector() { }\n\n  /**\n   * Helper method for {@link #handleIO()} to encapsulate everything that\n   * needs to be checked on a regular basis that has nothing to do directly\n   * with reading and writing data.\n   *\n   * @throws IOException if an error happens during shutdown queue handling.\n   */\n  private void handleOperationalTasks() throws IOException {\n    checkPotentiallyTimedOutConnection();\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n\n    if (!retryOps.isEmpty()) {\n      ArrayList<Operation> operations = new ArrayList<Operation>(retryOps);\n      retryOps.clear();\n      redistributeOperations(operations);\n    }\n\n    handleShutdownQueue();\n  }\n\n  /**\n   * Helper method for {@link #handleIO()} to handle empty select calls.\n   */\n  private void handleEmptySelects() {\n    getLogger().debug(\"No selectors ready, interrupted: \"\n      + Thread.interrupted());\n\n    if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n      for (SelectionKey sk : selector.keys()) {\n        getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n          sk.interestOps());\n        if (sk.readyOps() != 0) {\n          getLogger().debug(\"%s has a ready op, handling IO\", sk);\n          handleIO(sk);\n        } else {\n          lostConnection((MemcachedNode) sk.attachment());\n        }\n      }\n      assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n    }\n  }\n\n  /**\n   * Check if nodes need to be shut down and do so if needed.\n   *\n   * @throws IOException if the channel could not be closed properly.\n   */\n  private void handleShutdownQueue() throws IOException {\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  /**\n   * Check if one or more nodes exceeded the timeout Threshold.\n   */\n  private void checkPotentiallyTimedOutConnection() {\n    boolean stillCheckingTimeouts = true;\n    while (stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n  }\n\n  /**\n   * Handle any requests that have been made against the client.\n   */\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n\n      MemcachedNode qaNode;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      for (MemcachedNode node : todo) {\n        boolean readyForIO = false;\n        if (node.isActive()) {\n          if (node.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", node);\n          }\n        } else {\n          toAdd.add(node);\n        }\n        node.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (node.getWbuf().hasRemaining()) {\n              handleWrites(node);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(node);\n          }\n        }\n        node.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added.\n   */\n  public boolean addObserver(final ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't.\n   */\n  public boolean removeObserver(final ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  /**\n   * Indicate a successful connect to the given node.\n   *\n   * @param node the node which was successfully connected.\n   */\n  private void connected(final MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  /**\n   * Indicate a lost connection to the given node.\n   *\n   * @param node the node where the connection was lost.\n   */\n  private void lostConnection(final MemcachedNode node) {\n    queueReconnect(node);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(node.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given node belongs to the current cluster.\n   *\n   * Before trying to connect to a node, make sure it actually belongs to the\n   * currently connected cluster.\n   */\n  boolean belongsToCluster(final MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector.\n   *\n   * Any IOException will cause a reconnect. Note that this code makes sure\n   * that the corresponding node is not only able to connect, but also able to\n   * respond in a correct fashion (if verifyAliveOnConnect is set to true\n   * through a property). This is handled by issuing a dummy\n   * version/noop call and making sure it returns in a correct and timely\n   * fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(final SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n        sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n        sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().debug(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n          finishConnect(sk, node);\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        handleReadsAndWrites(sk, node);\n      }\n    } catch (ClosedChannelException e) {\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n        + \"operation on %s. This may be due to an authentication failure.\",\n        node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  /**\n   * A helper method for {@link #handleIO(java.nio.channels.SelectionKey)} to\n   * handle reads and writes if appropriate.\n   *\n   * @param sk the selection key to use.\n   * @param node th enode to read write from.\n   * @throws IOException if an error occurs during read/write.\n   */\n  private void handleReadsAndWrites(final SelectionKey sk,\n    final MemcachedNode node) throws IOException {\n    if (sk.isValid()) {\n      if (sk.isReadable()) {\n        handleReads(node);\n      }\n      if (sk.isWritable()) {\n        handleWrites(node);\n      }\n    }\n  }\n\n  /**\n   * Finish the connect phase and potentially verify its liveness.\n   *\n   * @param sk the selection key for the node.\n   * @param node the actual node.\n   * @throws IOException if something goes wrong during reading/writing.\n   */\n  private void finishConnect(final SelectionKey sk, final MemcachedNode node)\n    throws IOException {\n    if (verifyAliveOnConnect) {\n      final CountDownLatch latch = new CountDownLatch(1);\n      final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(\"noop\",\n        latch, 2500, listenerExecutorService);\n      NoopOperation testOp = opFact.noop(new OperationCallback() {\n        public void receivedStatus(OperationStatus status) {\n          rv.set(status.isSuccess(), status);\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      });\n\n      testOp.setHandlingNode(node);\n      testOp.initialize();\n      checkState();\n      insertOperation(node, testOp);\n      node.copyInputQueue();\n\n      boolean done = false;\n      if (sk.isValid()) {\n        long timeout = TimeUnit.MILLISECONDS.toNanos(\n          connectionFactory.getOperationTimeout());\n\n        long stop = System.nanoTime() + timeout;\n        while (stop > System.nanoTime()) {\n          handleWrites(node);\n          handleReads(node);\n          if(done = (latch.getCount() == 0)) {\n            break;\n          }\n        }\n      }\n\n      if (!done || testOp.isCancelled() || testOp.hasErrored()\n        || testOp.isTimedOut()) {\n        throw new ConnectException(\"Could not send noop upon connect! \"\n          + \"This may indicate a running, but not responding memcached \"\n          + \"instance.\");\n      }\n    }\n\n    connected(node);\n    addedQueue.offer(node);\n    if (node.getWbuf().hasRemaining()) {\n      handleWrites(node);\n    }\n  }\n\n  /**\n   * Handle pending writes for the given node.\n   *\n   * @param node the node to handle writes for.\n   * @throws IOException can be raised during writing failures.\n   */\n  private void handleWrites(final MemcachedNode node) throws IOException {\n    node.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = node.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = node.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      node.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && node.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  /**\n   * Handle pending reads for the given node.\n   *\n   * @param node the node to handle reads for.\n   * @throws IOException can be raised during reading failures.\n   */\n  private void handleReads(final MemcachedNode node) throws IOException {\n    Operation currentOp = node.getCurrentReadOp();\n    if (currentOp instanceof TapAckOperationImpl) {\n      node.removeCurrentReadOp();\n      return;\n    }\n\n    ByteBuffer rbuf = node.getRbuf();\n    final SocketChannel channel = node.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      currentOp = handleReadsWhenChannelEndOfStream(currentOp, node, rbuf);\n    }\n\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n\n        long timeOnWire =\n          System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n        metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC,\n          (int)(timeOnWire / 1000));\n        metrics.markMeter(OVERALL_RESPONSE_METRIC);\n        synchronized(currentOp) {\n          readBufferAndLogMetrics(currentOp, rbuf, node);\n        }\n\n        currentOp = node.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      node.completedRead();\n    }\n  }\n\n  /**\n   * Read from the buffer and add metrics information.\n   *\n   * @param currentOp the current operation to read.\n   * @param rbuf the read buffer to read from.\n   * @param node the node to read from.\n   * @throws IOException if reading was not successful.\n   */\n  private void readBufferAndLogMetrics(final Operation currentOp,\n    final ByteBuffer rbuf, final MemcachedNode node) throws IOException {\n    currentOp.readFromBuffer(rbuf);\n    if (currentOp.getState() == OperationState.COMPLETE) {\n      getLogger().debug(\"Completed read op: %s and giving the next %d \"\n        + \"bytes\", currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      if (op.hasErrored()) {\n        metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      } else {\n        metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n      }\n    } else if (currentOp.getState() == OperationState.RETRY) {\n      handleRetryInformation(currentOp.getErrorMsg());\n      getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n        + \"%s \", currentOp);\n      ((VBucketAware) currentOp).addNotMyVbucketNode(\n        currentOp.getHandlingNode());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n        + op;\n\n      retryOps.add(currentOp);\n      metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n    }\n  }\n\n  /**\n   * Deal with an operation where the channel reached the end of a stream.\n   *\n   * @param currentOp the current operation to read.\n   * @param node the node for that operation.\n   * @param rbuf the read buffer.\n   *\n   * @return the next operation on the node to read.\n   * @throws IOException if disconnect while reading.\n   */\n  private Operation handleReadsWhenChannelEndOfStream(final Operation currentOp,\n    final MemcachedNode node, final ByteBuffer rbuf) throws IOException {\n    if (currentOp instanceof TapOperation) {\n      currentOp.getCallback().complete();\n      ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n\n      getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n        currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n      return node.getCurrentReadOp();\n    } else {\n      throw new IOException(\"Disconnected unexpected, will reconnect.\");\n    }\n  }\n\n  /**\n   * Convert the {@link ByteBuffer} into a string for easier debugging.\n   *\n   * @param b the buffer to debug.\n   * @param size the size of the buffer.\n   * @return the stringified {@link ByteBuffer}.\n   */\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Optionally handle retry (NOT_MY_VBUKET) responses.\n   *\n   * This method can be overridden in subclasses to handle the content\n   * of the retry message appropriately.\n   *\n   * @param retryMessage the body of the retry message.\n   */\n  protected void handleRetryInformation(final byte[] retryMessage) {\n    getLogger().debug(\"Got RETRY message: \" + new String(retryMessage)\n      + \", but not handled.\");\n  }\n\n  /**\n   * Enqueue the given {@link MemcachedNode} for reconnect.\n   *\n   * @param node the node to reconnect.\n   */\n  protected void queueReconnect(final MemcachedNode node) {\n    if (shutDown) {\n      return;\n    }\n    getLogger().warn(\"Closing, and reopening %s, attempt %d.\", node,\n      node.getReconnectCount());\n\n    if (node.getSk() != null) {\n      node.getSk().cancel();\n      assert !node.getSk().isValid() : \"Cancelled selection key is valid\";\n    }\n    node.reconnecting();\n\n    try {\n      if (node.getChannel() != null && node.getChannel().socket() != null) {\n        node.getChannel().socket().close();\n      } else {\n        getLogger().info(\"The channel or socket was null for %s\", node);\n      }\n    } catch (IOException e) {\n      getLogger().warn(\"IOException trying to close a socket\", e);\n    }\n    node.setChannel(null);\n\n    long delay = (long) Math.min(maxDelay, Math.pow(2,\n        node.getReconnectCount())) * 1000;\n    long reconnectTime = System.currentTimeMillis() + delay;\n    while (reconnectQueue.containsKey(reconnectTime)) {\n      reconnectTime++;\n    }\n\n    reconnectQueue.put(reconnectTime, node);\n    metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n    node.setupResend();\n    if (failureMode == FailureMode.Redistribute) {\n      redistributeOperations(node.destroyInputQueue());\n    } else if (failureMode == FailureMode.Cancel) {\n      cancelOperations(node.destroyInputQueue());\n    }\n  }\n\n  /**\n   * Cancel the given collection of operations.\n   *\n   * @param ops the list of operations to cancel.\n   */\n  private void cancelOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Redistribute the given list of operations to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param ops the operations to redistribute.\n   */\n  public void redistributeOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      redistributeOperation(op);\n    }\n  }\n\n  /**\n   * Redistribute the given operation to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param op the operation to redistribute.\n   */\n  public void redistributeOperation(Operation op) {\n    if (op.isCancelled() || op.isTimedOut()) {\n      return;\n    }\n\n    if (op.getCloneCount() >= MAX_CLONE_COUNT) {\n      getLogger().warn(\"Cancelling operation \" + op + \"because it has been \"\n        + \"retried (cloned) more than \" + MAX_CLONE_COUNT + \"times.\");\n      op.cancel();\n      return;\n    }\n\n    // The operation gets redistributed but has never been actually written,\n    // it we just straight re-add it without cloning.\n    if (op.getState() == OperationState.WRITE_QUEUED && op.getHandlingNode() != null) {\n      addOperation(op.getHandlingNode(), op);\n      return;\n    }\n\n    if (op instanceof MultiGetOperationImpl) {\n      for (String key : ((MultiGetOperationImpl) op).getRetryKeys()) {\n        addOperation(key, opFact.get(key,\n          (GetOperation.Callback) op.getCallback()));\n      }\n    } else if (op instanceof KeyedOperation) {\n      KeyedOperation ko = (KeyedOperation) op;\n      int added = 0;\n      for (Operation newop : opFact.clone(ko)) {\n        if (newop instanceof KeyedOperation) {\n          KeyedOperation newKeyedOp = (KeyedOperation) newop;\n          for (String k : newKeyedOp.getKeys()) {\n            addOperation(k, newop);\n            op.addClone(newop);\n            newop.setCloneCount(op.getCloneCount()+1);\n          }\n        } else {\n          newop.cancel();\n          getLogger().warn(\"Could not redistribute cloned non-keyed \" +\n            \"operation\", newop);\n        }\n        added++;\n      }\n      assert added > 0 : \"Didn't add any new operations when redistributing\";\n    } else {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Attempt to reconnect {@link MemcachedNode}s in the reconnect queue.\n   *\n   * If the {@link MemcachedNode} does not belong to the cluster list anymore,\n   * the reconnect attempt is cancelled. If it does, the code tries to\n   * reconnect immediately and if this is not possible it waits until the\n   * connection information arrives.\n   *\n   * Note that if a socket error arises during reconnect, the node is scheduled\n   * for re-reconnect immediately.\n   */\n  private void attemptReconnects() {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n      new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n\n\n    Iterator<MemcachedNode> i = reconnectQueue.headMap(now).values().iterator();\n    while(i.hasNext()) {\n      final MemcachedNode node = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n\n      try {\n        if (!belongsToCluster(node)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", node);\n          continue;\n        }\n\n        if (!seen.containsKey(node)) {\n          seen.put(node, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", node);\n\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          ch.socket().setTcpNoDelay(!connectionFactory.useNagleAlgorithm());\n          int ops = 0;\n          if (ch.connect(node.getSocketAddress())) {\n            connected(node);\n            addedQueue.offer(node);\n            getLogger().info(\"Immediately reconnected to %s\", node);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          node.registerChannel(ch, ch.register(selector, ops, node));\n          assert node.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\",\n            node);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(node);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", node, e);\n      } finally {\n        potentiallyCloseLeakingChannel(ch, node);\n      }\n    }\n\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Make sure channel connections are not leaked and properly close under\n   * faulty reconnect cirumstances.\n   *\n   * @param ch the channel to potentially close.\n   * @param node the node to which the channel should be bound to.\n   */\n  private void potentiallyCloseLeakingChannel(final SocketChannel ch,\n    final MemcachedNode node) {\n    if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n      try {\n        ch.close();\n      } catch (IOException e) {\n        getLogger().error(\"Exception closing channel: %s\", node, e);\n      }\n    }\n  }\n\n  /**\n   * Returns the {@link NodeLocator} in use for this connection.\n   *\n   * @return  the current {@link NodeLocator}.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  /**\n   * Enqueue the given {@link Operation} with the used key.\n   *\n   * @param key the key to use.\n   * @param o the {@link Operation} to enqueue.\n   */\n  public void enqueueOperation(final String key, final Operation o) {\n    checkState();\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to a connection identified by the given key.\n   *\n   * If the {@link MemcachedNode} is active or the {@link FailureMode} is set\n   * to retry, the primary node will be used for that key. If the primary\n   * node is not available and the {@link FailureMode} cancel is used, the\n   * operation will be cancelled without further retry.\n   *\n   * For any other {@link FailureMode} mechanisms (Redistribute), another\n   * possible node is used (only if its active as well). If no other active\n   * node could be identified, the original primary node is used and retried.\n   *\n   * @param key the key the operation is operating upon.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final String key, final Operation o) {\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      Iterator<MemcachedNode> i = locator.getSequence(key);\n      while (placeIn == null && i.hasNext()) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\"Could not redistribute to another node, \"\n          + \"retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key + \" (and not \"\n        + \"immediately cancelled)\";\n    }\n  }\n\n  /**\n   * Insert an operation on the given node to the beginning of the queue.\n   *\n   * @param node the node where to insert the {@link Operation}.\n   * @param o the operation to insert.\n   */\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue an operation on the given node.\n   *\n   * @param node the node where to enqueue the {@link Operation}.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    if (!node.isAuthenticated()) {\n      retryOperation(o);\n      return;\n    }\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue the given list of operations on each handling node.\n   *\n   * @param ops the operations for each node.\n   */\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      addOperation(me.getKey(), me.getValue());\n    }\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a collection of nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n    final Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all connections and do not accept further incoming ops.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n    try {\n      Selector s = selector.wakeup();\n      assert s == selector : \"Wakeup returned the wrong selector.\";\n      for (MemcachedNode node : locator.getAll()) {\n        if (node.getChannel() != null) {\n          node.getChannel().close();\n          node.setSk(null);\n          if (node.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              node.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", node.getChannel());\n        }\n      }\n\n      selector.close();\n      getLogger().debug(\"Shut down selector %s\", selector);\n    } finally {\n      running = false;\n    }\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \").append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Construct a String containing information about all nodes and their state.\n   *\n   * @return a stringified representation of the connection status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus\n        .append(\" \")\n        .append(node.getSocketAddress())\n        .append(\" active: \")\n        .append(node.isActive())\n        .append(\", authed: \")\n        .append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * Increase the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opTimedOut(final Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * Reset the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opSucceeded(final Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * Set the continous timeout on an operation.\n   *\n   * @param op the operation to use.\n   * @param isTimeout is timed out or not.\n   */\n  private static void setTimeout(final Operation op, final boolean isTimeout) {\n    Logger logger = LoggerFactory.getLogger(MemcachedConnection.class);\n\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return;\n      }\n\n      MemcachedNode node = op.getHandlingNode();\n      if (node == null) {\n        logger.warn(\"handling node for operation is not set\");\n      } else {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      logger.error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   *\n   * @throws IllegalStateException when shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Handle IO as long as the application is running.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  /**\n   * Log a exception to different levels depending on the state.\n   *\n   * Exceptions get logged at debug level when happening during shutdown, but\n   * at warning level when operating normally.\n   *\n   * @param e the exception to log.\n   */\n  private void logRunException(final Exception e) {\n    if (shutDown) {\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n\n  /**\n   * Returns whether the connection is shut down or not.\n   *\n   * @return true if the connection is shut down, false otherwise.\n   */\n  public boolean isShutDown() {\n    return shutDown;\n  }\n\n  /**\n   * Add a operation to the retry queue.\n   *\n   * @param op the operation to retry.\n   */\n  public void retryOperation(Operation op) {\n    retryOps.add(op);\n  }\n\n}\n","lineNo":476}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CancellationException;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.auth.AuthDescriptor;\nimport net.spy.memcached.auth.AuthThreadMonitor;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.internal.SingleElementInfiniteIterator;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetAndTouchOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StatusCode;\nimport net.spy.memcached.ops.StoreOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.ops.TimedOutOperationStatus;\nimport net.spy.memcached.protocol.ascii.AsciiOperationFactory;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n * MemcachedClient c = new MemcachedClient(\n *    new InetSocketAddress(&quot;hostname&quot;, portNum));\n *\n * // Store a value (async) for one hour\n * c.set(&quot;someKey&quot;, 3600, someObject);\n * // Retrieve a value.\n * Object myObject = c.get(&quot;someKey&quot;);\n * <\/pre>\n *\n * <h2>Advanced Usage<\/h2>\n *\n * <p>\n * MemcachedClient may be processing a great deal of asynchronous messages or\n * possibly dealing with an unreachable memcached, which may delay processing.\n * If a memcached is disabled, for example, MemcachedConnection will continue to\n * attempt to reconnect and replay pending operations until it comes back up. To\n * prevent this from causing your application to hang, you can use one of the\n * asynchronous mechanisms to time out a request and cancel the operation to the\n * server.\n * <\/p>\n *\n * <pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n *\n * <p>Optionally, it is possible to activate a check that makes sure that\n * the node is alive and responding before running actual operations (even\n * before authentication. Only enable this if you are sure that you do not\n * run into issues during connection (some memcached services have problems\n * with it). You can enable it by setting the net.spy.verifyAliveOnConnect\n * System Property to \"true\".<\/p>\n */\npublic class MemcachedClient extends SpyObject implements MemcachedClientIF,\n    ConnectionObserver {\n\n  protected volatile boolean shuttingDown;\n\n  protected final long operationTimeout;\n\n  protected final MemcachedConnection mconn;\n\n  protected final OperationFactory opFact;\n\n  protected final Transcoder<Object> transcoder;\n\n  protected final TranscodeService tcService;\n\n  protected final AuthDescriptor authDescriptor;\n\n  protected final ConnectionFactory connFactory;\n\n  protected final AuthThreadMonitor authMonitor = new AuthThreadMonitor();\n\n  protected final ExecutorService executorService;\n\n  /**\n   * Get a memcache client operating on the specified memcached locations.\n   *\n   * @param ia the memcached locations\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(InetSocketAddress... ia) throws IOException {\n    this(new DefaultConnectionFactory(), Arrays.asList(ia));\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param addrs the socket addrs\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(List<InetSocketAddress> addrs) throws IOException {\n    this(new DefaultConnectionFactory(), addrs);\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param cf the connection factory to configure connections for this client\n   * @param addrs the socket addresses\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n    throws IOException {\n    if (cf == null) {\n      throw new NullPointerException(\"Connection factory required\");\n    }\n    if (addrs == null) {\n      throw new NullPointerException(\"Server list required\");\n    }\n    if (addrs.isEmpty()) {\n      throw new IllegalArgumentException(\"You must have at least one server to\"\n          + \" connect to\");\n    }\n    if (cf.getOperationTimeout() <= 0) {\n      throw new IllegalArgumentException(\"Operation timeout must be positive.\");\n    }\n    connFactory = cf;\n    tcService = new TranscodeService(cf.isDaemon());\n    transcoder = cf.getDefaultTranscoder();\n    opFact = cf.getOperationFactory();\n    assert opFact != null : \"Connection factory failed to make op factory\";\n    mconn = cf.createConnection(addrs);\n    assert mconn != null : \"Connection factory failed to make a connection\";\n    operationTimeout = cf.getOperationTimeout();\n    authDescriptor = cf.getAuthDescriptor();\n    executorService = cf.getListenerExecutorService();\n    if (authDescriptor != null) {\n      addObserver(this);\n    }\n  }\n\n  /**\n   * Get the addresses of available servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  @Override\n  public Collection<SocketAddress> getAvailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get the addresses of unavailable servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  @Override\n  public Collection<SocketAddress> getUnavailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (!node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get a read-only wrapper around the node locator wrapping this instance.\n   *\n   * @return this instance's NodeLocator\n   */\n  @Override\n  public NodeLocator getNodeLocator() {\n    return mconn.getLocator().getReadonlyCopy();\n  }\n\n  /**\n   * Get the default transcoder that's in use.\n   *\n   * @return this instance's Transcoder\n   */\n  @Override\n  public Transcoder<Object> getTranscoder() {\n    return transcoder;\n  }\n\n  @Override\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of) {\n    return broadcastOp(of, mconn.getLocator().getAll(), true);\n  }\n\n  @Override\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    return broadcastOp(of, nodes, true);\n  }\n\n  private CountDownLatch broadcastOp(BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes, boolean checkShuttingDown) {\n    if (checkShuttingDown && shuttingDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    return mconn.broadcastOperation(of, nodes);\n  }\n\n  private <T> OperationFuture<Boolean> asyncStore(StoreType storeType,\n      String key, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n      new OperationFuture<Boolean>(key, latch, operationTimeout,\n      executorService);\n    Operation op = opFact.store(storeType, key, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            @Override\n            public void receivedStatus(OperationStatus val) {\n              rv.set(val.isSuccess(), val);\n            }\n            @Override\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n\n            @Override\n            public void complete() {\n              latch.countDown();\n              rv.signalComplete();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  private OperationFuture<Boolean> asyncStore(StoreType storeType, String key,\n      int exp, Object value) {\n    return asyncStore(storeType, key, exp, value, transcoder);\n  }\n\n  private <T> OperationFuture<Boolean> asyncCat(ConcatenationType catType,\n      long cas, String key, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout, executorService);\n    Operation op = opFact.cat(catType, cas, key, co.getData(),\n        new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus val) {\n            rv.set(val.isSuccess(), val);\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n            rv.signalComplete();\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Touch the given key to reset its expiration time with the default\n   * transcoder.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp) {\n    return touch(key, exp, transcoder);\n  }\n\n  /**\n   * Touch the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp,\n      final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n      new OperationFuture<Boolean>(key, latch, operationTimeout,\n      executorService);\n\n    Operation op = opFact.touch(key, exp, new OperationCallback() {\n      @Override\n      public void receivedStatus(OperationStatus status) {\n        rv.set(status.isSuccess(), status);\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> append(long cas, String key, Object val) {\n    return append(cas, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> append(String key, Object val) {\n    return append(0, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> append(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, cas, key, val, tc);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> append(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, 0, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> prepend(long cas, String key, Object val) {\n    return prepend(cas, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> prepend(String key, Object val) {\n    return prepend(0, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> prepend(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> prepend(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, 0, key, val, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, T value, Transcoder<T> tc) {\n    return asyncCAS(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASResponse> rv =\n      new OperationFuture<CASResponse>(key, latch, operationTimeout,\n      executorService);\n    Operation op = opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            @Override\n            public void receivedStatus(OperationStatus val) {\n              if (val instanceof CASOperationStatus) {\n                rv.set(((CASOperationStatus) val).getCASResponse(), val);\n              } else if (val instanceof CancelledOperationStatus) {\n                getLogger().debug(\"CAS operation cancelled\");\n              } else if (val instanceof TimedOutOperationStatus) {\n                getLogger().debug(\"CAS operation timed out\");\n              } else {\n                throw new RuntimeException(\"Unhandled state: \" + val);\n              }\n            }\n            @Override\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n            @Override\n            public void complete() {\n              latch.countDown();\n              rv.signalComplete();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, Object value) {\n    return asyncCAS(key, casId, value, transcoder);\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder with expiration.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, Object value) {\n    return asyncCAS(key, casId, exp, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASResponse cas(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return cas(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASResponse cas(String key, long casId, int exp, T value,\n      Transcoder<T> tc) {\n    CASResponse casr;\n    try {\n      OperationFuture<CASResponse> casOp = asyncCAS(key,\n              casId, exp, value, tc);\n      casr = casOp.get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n      return casr;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value: \"\n        + buildTimeoutMessage(operationTimeout, TimeUnit.MILLISECONDS), e);\n    }\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASResponse cas(String key, long casId, Object value) {\n    return cas(key, casId, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASResponse cas(String key, long casId, int exp, Object value) {\n    return cas(key, casId, exp, value, transcoder);\n  }\n\n  /**\n   * Add an object to the cache iff it does not exist already.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> add(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.add, key, exp, o, tc);\n  }\n\n  /**\n   * Add an object to the cache (using the default transcoder) iff it does not\n   * exist already.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> add(String key, int exp, Object o) {\n    return asyncStore(StoreType.add, key, exp, o, transcoder);\n  }\n\n  /**\n   * Set an object in the cache regardless of any existing value.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> set(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.set, key, exp, o, tc);\n  }\n\n  /**\n   * Set an object in the cache (using the default transcoder) regardless of any\n   * existing value.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> set(String key, int exp, Object o) {\n    return asyncStore(StoreType.set, key, exp, o, transcoder);\n  }\n\n  /**\n   * Replace an object with the given value iff there is already a value for the\n   * given key.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> replace(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.replace, key, exp, o, tc);\n  }\n\n  /**\n   * Replace an object with the given value (transcoded with the default\n   * transcoder) iff there is already a value for the given key.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> replace(String key, int exp, Object o) {\n    return asyncStore(StoreType.replace, key, exp, o, transcoder);\n  }\n\n  /**\n   * Get the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> GetFuture<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final GetFuture<T> rv = new GetFuture<T>(latch, operationTimeout, key,\n      executorService);\n    Operation op = opFact.get(key, new GetOperation.Callback() {\n      private Future<T> val;\n\n      @Override\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      @Override\n      public void gotData(String k, int flags, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize()));\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the given key asynchronously and decode with the default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public GetFuture<Object> asyncGet(final String key) {\n    return asyncGet(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASValue<T>> asyncGets(final String key,\n      final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv =\n      new OperationFuture<CASValue<T>>(key, latch, operationTimeout,\n      executorService);\n\n    Operation op = opFact.gets(key, new GetsOperation.Callback() {\n      private CASValue<T> val;\n\n      @Override\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      @Override\n      public void gotData(String k, int flags, long cas, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                tc.getMaxSize())));\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously and decode using the\n   * default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASValue<Object>> asyncGets(final String key) {\n    return asyncGets(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n    try {\n      return asyncGets(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and reset its expiration.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASValue<T> getAndTouch(String key, int exp, Transcoder<T> tc) {\n    try {\n      return asyncGetAndTouch(key, exp, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get a single key and reset its expiration using the default transcoder.\n   *\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASValue<Object> getAndTouch(String key, int exp) {\n    return getAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASValue<Object> gets(String key) {\n    return gets(key, transcoder);\n  }\n\n  /**\n   * Get with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> T get(String key, Transcoder<T> tc) {\n    try {\n      return asyncGet(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value: \"\n        + buildTimeoutMessage(operationTimeout, TimeUnit.MILLISECONDS), e);\n    }\n  }\n\n  /**\n   * Get with a single key and decode using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Object get(String key) {\n    return get(key, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces keys.\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Iterator<Transcoder<T>> tcIter) {\n    final Map<String, Future<T>> m = new ConcurrentHashMap<String, Future<T>>();\n\n    // This map does not need to be a ConcurrentHashMap\n    // because it is fully populated when it is used and\n    // used only to read the transcoder for a key.\n    final Map<String, Transcoder<T>> tcMap =\n        new HashMap<String, Transcoder<T>>();\n\n    // Break the gets down into groups by key\n    final Map<MemcachedNode, Collection<String>> chunks =\n        new HashMap<MemcachedNode, Collection<String>>();\n    final NodeLocator locator = mconn.getLocator();\n\n    while (keyIter.hasNext() && tcIter.hasNext()) {\n      String key = keyIter.next();\n      tcMap.put(key, tcIter.next());\n      StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n      final MemcachedNode primaryNode = locator.getPrimary(key);\n      MemcachedNode node = null;\n      if (primaryNode.isActive()) {\n        node = primaryNode;\n      } else {\n        for (Iterator<MemcachedNode> i = locator.getSequence(key); node == null\n            && i.hasNext();) {\n          MemcachedNode n = i.next();\n          if (n.isActive()) {\n            node = n;\n          }\n        }\n        if (node == null) {\n          node = primaryNode;\n        }\n      }\n      assert node != null : \"Didn't find a node for \" + key;\n      Collection<String> ks = chunks.get(node);\n      if (ks == null) {\n        ks = new ArrayList<String>();\n        chunks.put(node, ks);\n      }\n      ks.add(key);\n    }\n\n    final AtomicInteger pendingChunks = new AtomicInteger(chunks.size());\n    final CountDownLatch latch = new CountDownLatch(1);\n    final Collection<Operation> ops = new ArrayList<Operation>(chunks.size());\n    final BulkGetFuture<T> rv = new BulkGetFuture<T>(m, ops, latch, executorService);\n\n    GetOperation.Callback cb = new GetOperation.Callback() {\n      @Override\n      @SuppressWarnings(\"synthetic-access\")\n      public void receivedStatus(OperationStatus status) {\n        if (status.getStatusCode() == StatusCode.ERR_NOT_MY_VBUCKET) {\n          pendingChunks.addAndGet(Integer.parseInt(status.getMessage()));\n        }\n        rv.setStatus(status);\n      }\n\n      @Override\n      public void gotData(String k, int flags, byte[] data) {\n        Transcoder<T> tc = tcMap.get(k);\n        m.put(k,\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize())));\n      }\n\n      @Override\n      public void complete() {\n        if (pendingChunks.decrementAndGet() <= 0) {\n          latch.countDown();\n          rv.signalComplete();\n        }\n      }\n    };\n\n    // Now that we know how many servers it breaks down into, and the latch\n    // is all set up, convert all of these strings collections to operations\n    final Map<MemcachedNode, Operation> mops =\n        new HashMap<MemcachedNode, Operation>();\n\n    for (Map.Entry<MemcachedNode, Collection<String>> me : chunks.entrySet()) {\n      Operation op = opFact.get(me.getValue(), cb);\n      mops.put(me.getKey(), op);\n      ops.add(op);\n    }\n    assert mops.size() == chunks.size();\n    mconn.checkState();\n    mconn.addOperations(mops);\n    return rv;\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n          Iterator<Transcoder<T>> tcIter) {\n    return asyncGetBulk(keys.iterator(), tcIter);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator for the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keyIter,\n            new SingleElementInfiniteIterator<Transcoder<T>>(tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keys, new SingleElementInfiniteIterator<Transcoder<T>>(\n        tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keyIter Iterator that produces the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public BulkFuture<Map<String, Object>> asyncGetBulk(\n         Iterator<String> keyIter) {\n    return asyncGetBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keys the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public BulkFuture<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n    return asyncGetBulk(keys, transcoder);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n      String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n   *\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public BulkFuture<Map<String, Object>> asyncGetBulk(String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASValue<Object>> asyncGetAndTouch(final String key,\n      final int exp) {\n    return asyncGetAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASValue<T>> asyncGetAndTouch(final String key,\n      final int exp, final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv = new OperationFuture<CASValue<T>>(\n        key, latch, operationTimeout, executorService);\n\n    Operation op = opFact.getAndTouch(key, exp,\n        new GetAndTouchOperation.Callback() {\n          private CASValue<T> val;\n\n          @Override\n          public void receivedStatus(OperationStatus status) {\n            rv.set(val, status);\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n            rv.signalComplete();\n          }\n\n          @Override\n          public void gotData(String k, int flags, long cas, byte[] data) {\n            assert k.equals(key) : \"Wrong key returned\";\n            val =\n                new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                    tc.getMaxSize())));\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> Map<String, T> getBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    try {\n      return asyncGetBulk(keyIter, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted getting bulk values\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for bulk values\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for bulk values: \"\n        + buildTimeoutMessage(operationTimeout, TimeUnit.MILLISECONDS), e);\n    }\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keyIter Iterator that produces the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<String, Object> getBulk(Iterator<String> keyIter) {\n    return getBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keys the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> Map<String, T> getBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return getBulk(keys.iterator(), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<String, Object> getBulk(Collection<String> keys) {\n    return getBulk(keys, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n    return getBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<String, Object> getBulk(String... keys) {\n    return getBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the versions of all of the connected memcacheds.\n   *\n   * @return a Map of SocketAddress to String for connected servers\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<SocketAddress, String> getVersions() {\n    final Map<SocketAddress, String> rv =\n        new ConcurrentHashMap<SocketAddress, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        return opFact.version(new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            rv.put(sa, s.getMessage());\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for versions\", e);\n    }\n    return rv;\n  }\n\n  /**\n   * Get all of the stats from all of the connections.\n   *\n   * @return a Map of a Map of stats replies by SocketAddress\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<SocketAddress, Map<String, String>> getStats() {\n    return getStats(null);\n  }\n\n  /**\n   * Get a set of stats from all connections.\n   *\n   * @param arg which stats to get\n   * @return a Map of the server SocketAddress to a map of String stat keys to\n   *         String stat values.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n    final Map<SocketAddress, Map<String, String>> rv =\n        new HashMap<SocketAddress, Map<String, String>>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        rv.put(sa, new HashMap<String, String>());\n        return opFact.stats(arg, new StatsOperation.Callback() {\n          @Override\n          public void gotStat(String name, String val) {\n            rv.get(sa).put(name, val);\n          }\n\n          @Override\n          @SuppressWarnings(\"synthetic-access\")\n          public void receivedStatus(OperationStatus status) {\n            if (!status.isSuccess()) {\n              getLogger().warn(\"Unsuccessful stat fetch: %s\", status);\n            }\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for stats\", e);\n    }\n    return rv;\n  }\n\n  private long mutate(Mutator m, String key, long by, long def, int exp) {\n    final AtomicLong rv = new AtomicLong();\n    final CountDownLatch latch = new CountDownLatch(1);\n    mconn.enqueueOperation(key, opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n        @Override\n        public void receivedStatus(OperationStatus s) {\n          // XXX: Potential abstraction leak.\n          // The handling of incr/decr in the binary protocol\n          // Allows us to avoid string processing.\n          rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      }));\n    try {\n      if (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n        throw new OperationTimeoutException(\"Mutate operation timed out,\"\n            + \"unable to modify counter [\" + key + ']');\n      }\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted\", e);\n    }\n    getLogger().debug(\"Mutation returned %s\", rv);\n    return rv.get();\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, long by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, int by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, long by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, int by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  private long mutateWithDefault(Mutator t, String key, long by, long def,\n      int exp) {\n    long rv = mutate(t, key, by, def, exp);\n    // The ascii protocol doesn't support defaults, so I added them\n    // manually here.\n    if (rv == -1) {\n      Future<Boolean> f = asyncStore(StoreType.add, key, exp,\n          String.valueOf(def));\n      try {\n        if (f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n          rv = def;\n        } else {\n          rv = mutate(t, key, by, 0, exp);\n          assert rv != -1 : \"Failed to mutate or init value\";\n        }\n      } catch (InterruptedException e) {\n        throw new RuntimeException(\"Interrupted waiting for store\", e);\n      } catch (ExecutionException e) {\n        if(e.getCause() instanceof CancellationException) {\n          throw (CancellationException) e.getCause();\n        } else {\n          throw new RuntimeException(\"Failed waiting for store\", e);\n        }\n      } catch (TimeoutException e) {\n        throw new OperationTimeoutException(\"Timeout waiting to mutate or init\"\n          + \" value\" + buildTimeoutMessage(operationTimeout,\n            TimeUnit.MILLISECONDS), e);\n      }\n    }\n    return rv;\n  }\n\n  private OperationFuture<Long> asyncMutate(Mutator m, String key, long by,\n      long def, int exp) {\n    if (!(opFact instanceof BinaryOperationFactory) && (def != 0 || exp != -1)) {\n      throw new UnsupportedOperationException(\"Default value or expiration \"\n        + \"time are not supported on the async mutate methods. Use either the \"\n        + \"binary protocol or the sync variant.\");\n    }\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Long> rv =\n        new OperationFuture<Long>(key, latch, operationTimeout, executorService);\n    Operation op = opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"), s);\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n            rv.signalComplete();\n          }\n        });\n    mconn.enqueueOperation(key, op);\n    rv.setOperation(op);\n    return rv;\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, long by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, int by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, long by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, int by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, long by, long def,\n    int exp) {\n    return asyncMutate(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, int by, long def,\n    int exp) {\n    return asyncMutate(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, long by, long def,\n    int exp) {\n    return asyncMutate(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, int by, long def,\n    int exp) {\n    return asyncMutate(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, long by, long def) {\n    return asyncMutate(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, int by, long def) {\n    return asyncMutate(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, long by, long def) {\n    return asyncMutate(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, int by, long def) {\n    return asyncMutate(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * <p>\n   * The hold argument specifies the amount of time in seconds (or Unix time\n   * until which) the client wishes the server to refuse \"add\" and \"replace\"\n   * commands with this key. For this amount of item, the item is put into a\n   * delete queue, which means that it won't possible to retrieve it by the\n   * \"get\" command, but \"add\" and \"replace\" command with this key will also fail\n   * (the \"set\" command will succeed, however). After the time passes, the item\n   * is finally deleted from server memory.\n   * <\/p>\n   *\n   * @param key the key to delete\n   * @param hold how long the key should be unavailable to add commands\n   *\n   * @return whether or not the operation was performed\n   * @deprecated Hold values are no longer honored.\n   */\n  @Deprecated\n  public OperationFuture<Boolean> delete(String key, int hold) {\n    return delete(key);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * @param key the key to delete\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> delete(String key) {\n    return delete(key, 0L);\n  }\n\n  /**\n   * Delete the given key from the cache of the given CAS value applies.\n   *\n   * @param key the key to delete\n   * @param cas the CAS value to apply.\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> delete(String key, long cas) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout, executorService);\n\n    DeleteOperation.Callback callback = new DeleteOperation.Callback() {\n      @Override\n      public void receivedStatus(OperationStatus s) {\n        rv.set(s.isSuccess(), s);\n      }\n\n      @Override\n      public void gotData(long cas) {\n        rv.setCas(cas);\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    };\n\n    DeleteOperation op;\n    if(cas == 0) {\n      op = opFact.delete(key, callback);\n    } else {\n      op = opFact.delete(key, cas, callback);\n    }\n\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Flush all caches from all servers with a delay of application.\n   *\n   * @param delay the period of time to delay, in seconds\n   * @return whether or not the operation was accepted\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> flush(final int delay) {\n    final AtomicReference<Boolean> flushResult =\n        new AtomicReference<Boolean>(null);\n    final ConcurrentLinkedQueue<Operation> ops =\n        new ConcurrentLinkedQueue<Operation>();\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        Operation op = opFact.flush(delay, new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            flushResult.set(s.isSuccess());\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n        ops.add(op);\n        return op;\n      }\n    });\n\n    return new OperationFuture<Boolean>(null, blatch, flushResult,\n        operationTimeout, executorService) {\n\n      @Override\n      public void set(Boolean o, OperationStatus s) {\n        super.set(o, s);\n        notifyListeners();\n      }\n\n      @Override\n      public boolean cancel(boolean ign) {\n        boolean rv = false;\n        for (Operation op : ops) {\n          op.cancel();\n          rv |= op.getState() == OperationState.WRITE_QUEUED;\n        }\n        notifyListeners();\n        return rv;\n      }\n\n      @Override\n      public Boolean get(long duration, TimeUnit units)\n        throws InterruptedException, TimeoutException, ExecutionException {\n        status = new OperationStatus(true, \"OK\", StatusCode.SUCCESS);\n        return super.get(duration, units);\n      }\n\n      @Override\n      public boolean isCancelled() {\n        boolean rv = false;\n        for (Operation op : ops) {\n          rv |= op.isCancelled();\n        }\n        return rv;\n      }\n\n      @Override\n      public boolean isDone() {\n        boolean rv = true;\n        for (Operation op : ops) {\n          rv &= op.getState() == OperationState.COMPLETE;\n        }\n        return rv || isCancelled();\n      }\n    };\n  }\n\n  /**\n   * Flush all caches from all servers immediately.\n   *\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> flush() {\n    return flush(-1);\n  }\n\n  @Override\n  public Set<String> listSaslMechanisms() {\n    final ConcurrentMap<String, String> rv =\n        new ConcurrentHashMap<String, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(MemcachedNode n, final CountDownLatch latch) {\n        return opFact.saslMechs(new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus status) {\n            for (String s : status.getMessage().split(\" \")) {\n              rv.put(s, s);\n            }\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n\n    try {\n      blatch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    return rv.keySet();\n  }\n\n  /**\n   * Shut down immediately.\n   */\n  @Override\n  public void shutdown() {\n    shutdown(-1, TimeUnit.MILLISECONDS);\n  }\n\n  /**\n   * Shut down this client gracefully.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the shutdown request\n   */\n  @Override\n  public boolean shutdown(long timeout, TimeUnit unit) {\n    // Guard against double shutdowns (bug 8).\n    if (shuttingDown) {\n      getLogger().info(\"Suppressing duplicate attempt to shut down\");\n      return false;\n    }\n    shuttingDown = true;\n    String baseName = mconn.getName();\n    mconn.setName(baseName + \" - SHUTTING DOWN\");\n    boolean rv = true;\n    if (connFactory.isDefaultExecutorService()) {\n      try {\n        executorService.shutdown();\n      } catch (Exception ex) {\n        getLogger().warn(\"Failed shutting down the ExecutorService: \", ex);\n      }\n    }\n    try {\n      // Conditionally wait\n      if (timeout > 0) {\n        mconn.setName(baseName + \" - SHUTTING DOWN (waiting)\");\n        rv = waitForQueues(timeout, unit);\n      }\n    } finally {\n      // But always begin the shutdown sequence\n      try {\n        mconn.setName(baseName + \" - SHUTTING DOWN (telling client)\");\n        mconn.shutdown();\n        mconn.setName(baseName + \" - SHUTTING DOWN (informed client)\");\n        tcService.shutdown();\n        //terminate all pending Auth Threads\n        authMonitor.interruptAllPendingAuth();\n      } catch (IOException e) {\n        getLogger().warn(\"exception while shutting down\", e);\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Wait for the queues to die down.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the request for the wait\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public boolean waitForQueues(long timeout, TimeUnit unit) {\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        return opFact.noop(new OperationCallback() {\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            // Nothing special when receiving status, only\n            // necessary to complete the interface\n          }\n        });\n      }\n    }, mconn.getLocator().getAll(), false);\n    try {\n      // XXX: Perhaps IllegalStateException should be caught here\n      // and the check retried.\n      return blatch.await(timeout, unit);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for queues\", e);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * If connections are already established, your observer will be called with\n   * the address and -1.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer was added.\n   */\n  @Override\n  public boolean addObserver(ConnectionObserver obs) {\n    boolean rv = mconn.addObserver(obs);\n    if (rv) {\n      for (MemcachedNode node : mconn.getLocator().getAll()) {\n        if (node.isActive()) {\n          obs.connectionEstablished(node.getSocketAddress(), -1);\n        }\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer existed, but no longer does\n   */\n  @Override\n  public boolean removeObserver(ConnectionObserver obs) {\n    return mconn.removeObserver(obs);\n  }\n\n  @Override\n  public void connectionEstablished(SocketAddress sa, int reconnectCount) {\n    if (authDescriptor != null) {\n      if (authDescriptor.authThresholdReached()) {\n        shutdown();\n      }\n      authMonitor.authConnection(mconn, opFact, authDescriptor, findNode(sa));\n    }\n  }\n\n  private MemcachedNode findNode(SocketAddress sa) {\n    MemcachedNode node = null;\n    for (MemcachedNode n : mconn.getLocator().getAll()) {\n      if (n.getSocketAddress().equals(sa)) {\n        node = n;\n      }\n    }\n    assert node != null : \"Couldn't find node connected to \" + sa;\n    return node;\n  }\n\n  private String buildTimeoutMessage(long timeWaited, TimeUnit unit) {\n    StringBuilder message = new StringBuilder();\n\n    message.append(MessageFormat.format(\"waited {0} ms.\",\n      unit.convert(timeWaited, TimeUnit.MILLISECONDS)));\n    message.append(\" Node status: \").append(mconn.connectionsStatus());\n    return message.toString();\n  }\n\n  @Override\n  public void connectionLost(SocketAddress sa) {\n    // Don't care.\n  }\n\n  @Override\n  public String toString() {\n    return connFactory.toString();\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CancellationException;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.auth.AuthDescriptor;\nimport net.spy.memcached.auth.AuthThreadMonitor;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.internal.SingleElementInfiniteIterator;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetAndTouchOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StatusCode;\nimport net.spy.memcached.ops.StoreOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.ops.TimedOutOperationStatus;\nimport net.spy.memcached.protocol.ascii.AsciiOperationFactory;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n * MemcachedClient c = new MemcachedClient(\n *    new InetSocketAddress(&quot;hostname&quot;, portNum));\n *\n * // Store a value (async) for one hour\n * c.set(&quot;someKey&quot;, 3600, someObject);\n * // Retrieve a value.\n * Object myObject = c.get(&quot;someKey&quot;);\n * <\/pre>\n *\n * <h2>Advanced Usage<\/h2>\n *\n * <p>\n * MemcachedClient may be processing a great deal of asynchronous messages or\n * possibly dealing with an unreachable memcached, which may delay processing.\n * If a memcached is disabled, for example, MemcachedConnection will continue to\n * attempt to reconnect and replay pending operations until it comes back up. To\n * prevent this from causing your application to hang, you can use one of the\n * asynchronous mechanisms to time out a request and cancel the operation to the\n * server.\n * <\/p>\n *\n * <pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n *\n * <p>Optionally, it is possible to activate a check that makes sure that\n * the node is alive and responding before running actual operations (even\n * before authentication. Only enable this if you are sure that you do not\n * run into issues during connection (some memcached services have problems\n * with it). You can enable it by setting the net.spy.verifyAliveOnConnect\n * System Property to \"true\".<\/p>\n */\npublic class MemcachedClient extends SpyObject implements MemcachedClientIF,\n    ConnectionObserver {\n\n  protected volatile boolean shuttingDown;\n\n  protected final long operationTimeout;\n\n  protected final MemcachedConnection mconn;\n\n  protected final OperationFactory opFact;\n\n  protected final Transcoder<Object> transcoder;\n\n  protected final TranscodeService tcService;\n\n  protected final AuthDescriptor authDescriptor;\n\n  protected final ConnectionFactory connFactory;\n\n  protected final AuthThreadMonitor authMonitor = new AuthThreadMonitor();\n\n  protected final ExecutorService executorService;\n\n  /**\n   * Get a memcache client operating on the specified memcached locations.\n   *\n   * @param ia the memcached locations\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(InetSocketAddress... ia) throws IOException {\n    this(new DefaultConnectionFactory(), Arrays.asList(ia));\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param addrs the socket addrs\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(List<InetSocketAddress> addrs) throws IOException {\n    this(new DefaultConnectionFactory(), addrs);\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param cf the connection factory to configure connections for this client\n   * @param addrs the socket addresses\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n    throws IOException {\n    if (cf == null) {\n      throw new NullPointerException(\"Connection factory required\");\n    }\n    if (addrs == null) {\n      throw new NullPointerException(\"Server list required\");\n    }\n    if (addrs.isEmpty()) {\n      throw new IllegalArgumentException(\"You must have at least one server to\"\n          + \" connect to\");\n    }\n    if (cf.getOperationTimeout() <= 0) {\n      throw new IllegalArgumentException(\"Operation timeout must be positive.\");\n    }\n    connFactory = cf;\n    tcService = new TranscodeService(cf.isDaemon());\n    transcoder = cf.getDefaultTranscoder();\n    opFact = cf.getOperationFactory();\n    assert opFact != null : \"Connection factory failed to make op factory\";\n    mconn = cf.createConnection(addrs);\n    assert mconn != null : \"Connection factory failed to make a connection\";\n    operationTimeout = cf.getOperationTimeout();\n    authDescriptor = cf.getAuthDescriptor();\n    executorService = cf.getListenerExecutorService();\n    if (authDescriptor != null) {\n      addObserver(this);\n    }\n  }\n\n  /**\n   * Get the addresses of available servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  @Override\n  public Collection<SocketAddress> getAvailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get the addresses of unavailable servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  @Override\n  public Collection<SocketAddress> getUnavailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (!node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get a read-only wrapper around the node locator wrapping this instance.\n   *\n   * @return this instance's NodeLocator\n   */\n  @Override\n  public NodeLocator getNodeLocator() {\n    return mconn.getLocator().getReadonlyCopy();\n  }\n\n  /**\n   * Get the default transcoder that's in use.\n   *\n   * @return this instance's Transcoder\n   */\n  @Override\n  public Transcoder<Object> getTranscoder() {\n    return transcoder;\n  }\n\n  @Override\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of) {\n    return broadcastOp(of, mconn.getLocator().getAll(), true);\n  }\n\n  @Override\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    return broadcastOp(of, nodes, true);\n  }\n\n  private CountDownLatch broadcastOp(BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes, boolean checkShuttingDown) {\n    if (checkShuttingDown && shuttingDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    return mconn.broadcastOperation(of, nodes);\n  }\n\n  private <T> OperationFuture<Boolean> asyncStore(StoreType storeType,\n      String key, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n      new OperationFuture<Boolean>(key, latch, operationTimeout,\n      executorService);\n    Operation op = opFact.store(storeType, key, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            @Override\n            public void receivedStatus(OperationStatus val) {\n              rv.set(val.isSuccess(), val);\n            }\n            @Override\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n\n            @Override\n            public void complete() {\n              latch.countDown();\n              rv.signalComplete();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  private OperationFuture<Boolean> asyncStore(StoreType storeType, String key,\n      int exp, Object value) {\n    return asyncStore(storeType, key, exp, value, transcoder);\n  }\n\n  private <T> OperationFuture<Boolean> asyncCat(ConcatenationType catType,\n      long cas, String key, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout, executorService);\n    Operation op = opFact.cat(catType, cas, key, co.getData(),\n        new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus val) {\n            rv.set(val.isSuccess(), val);\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n            rv.signalComplete();\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Touch the given key to reset its expiration time with the default\n   * transcoder.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp) {\n    return touch(key, exp, transcoder);\n  }\n\n  /**\n   * Touch the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp,\n      final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n      new OperationFuture<Boolean>(key, latch, operationTimeout,\n      executorService);\n\n    Operation op = opFact.touch(key, exp, new OperationCallback() {\n      @Override\n      public void receivedStatus(OperationStatus status) {\n        rv.set(status.isSuccess(), status);\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> append(long cas, String key, Object val) {\n    return append(cas, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> append(String key, Object val) {\n    return append(0, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> append(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, cas, key, val, tc);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> append(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, 0, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> prepend(long cas, String key, Object val) {\n    return prepend(cas, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> prepend(String key, Object val) {\n    return prepend(0, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> prepend(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> prepend(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, 0, key, val, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, T value, Transcoder<T> tc) {\n    return asyncCAS(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASResponse> rv =\n      new OperationFuture<CASResponse>(key, latch, operationTimeout,\n      executorService);\n    Operation op = opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            @Override\n            public void receivedStatus(OperationStatus val) {\n              if (val instanceof CASOperationStatus) {\n                rv.set(((CASOperationStatus) val).getCASResponse(), val);\n              } else if (val instanceof CancelledOperationStatus) {\n                getLogger().debug(\"CAS operation cancelled\");\n              } else if (val instanceof TimedOutOperationStatus) {\n                getLogger().debug(\"CAS operation timed out\");\n              } else {\n                throw new RuntimeException(\"Unhandled state: \" + val);\n              }\n            }\n            @Override\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n            @Override\n            public void complete() {\n              latch.countDown();\n              rv.signalComplete();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, Object value) {\n    return asyncCAS(key, casId, value, transcoder);\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder with expiration.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, Object value) {\n    return asyncCAS(key, casId, exp, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASResponse cas(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return cas(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASResponse cas(String key, long casId, int exp, T value,\n      Transcoder<T> tc) {\n    CASResponse casr;\n    try {\n      OperationFuture<CASResponse> casOp = asyncCAS(key,\n              casId, exp, value, tc);\n      casr = casOp.get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n      return casr;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value: \"\n        + buildTimeoutMessage(operationTimeout, TimeUnit.MILLISECONDS), e);\n    }\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASResponse cas(String key, long casId, Object value) {\n    return cas(key, casId, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASResponse cas(String key, long casId, int exp, Object value) {\n    return cas(key, casId, exp, value, transcoder);\n  }\n\n  /**\n   * Add an object to the cache iff it does not exist already.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> add(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.add, key, exp, o, tc);\n  }\n\n  /**\n   * Add an object to the cache (using the default transcoder) iff it does not\n   * exist already.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> add(String key, int exp, Object o) {\n    return asyncStore(StoreType.add, key, exp, o, transcoder);\n  }\n\n  /**\n   * Set an object in the cache regardless of any existing value.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> set(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.set, key, exp, o, tc);\n  }\n\n  /**\n   * Set an object in the cache (using the default transcoder) regardless of any\n   * existing value.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> set(String key, int exp, Object o) {\n    return asyncStore(StoreType.set, key, exp, o, transcoder);\n  }\n\n  /**\n   * Replace an object with the given value iff there is already a value for the\n   * given key.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<Boolean> replace(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.replace, key, exp, o, tc);\n  }\n\n  /**\n   * Replace an object with the given value (transcoded with the default\n   * transcoder) iff there is already a value for the given key.\n   *\n   * <p>\n   * The {@code exp} value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> replace(String key, int exp, Object o) {\n    return asyncStore(StoreType.replace, key, exp, o, transcoder);\n  }\n\n  /**\n   * Get the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> GetFuture<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final GetFuture<T> rv = new GetFuture<T>(latch, operationTimeout, key,\n      executorService);\n    Operation op = opFact.get(key, new GetOperation.Callback() {\n      private Future<T> val;\n\n      @Override\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      @Override\n      public void gotData(String k, int flags, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize()));\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the given key asynchronously and decode with the default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public GetFuture<Object> asyncGet(final String key) {\n    return asyncGet(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASValue<T>> asyncGets(final String key,\n      final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv =\n      new OperationFuture<CASValue<T>>(key, latch, operationTimeout,\n      executorService);\n\n    Operation op = opFact.gets(key, new GetsOperation.Callback() {\n      private CASValue<T> val;\n\n      @Override\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      @Override\n      public void gotData(String k, int flags, long cas, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                tc.getMaxSize())));\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously and decode using the\n   * default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASValue<Object>> asyncGets(final String key) {\n    return asyncGets(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n    try {\n      return asyncGets(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and reset its expiration.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> CASValue<T> getAndTouch(String key, int exp, Transcoder<T> tc) {\n    try {\n      return asyncGetAndTouch(key, exp, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get a single key and reset its expiration using the default transcoder.\n   *\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASValue<Object> getAndTouch(String key, int exp) {\n    return getAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public CASValue<Object> gets(String key) {\n    return gets(key, transcoder);\n  }\n\n  /**\n   * Get with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> T get(String key, Transcoder<T> tc) {\n    try {\n      return asyncGet(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value: \"\n        + buildTimeoutMessage(operationTimeout, TimeUnit.MILLISECONDS), e);\n    }\n  }\n\n  /**\n   * Get with a single key and decode using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Object get(String key) {\n    return get(key, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces keys.\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Iterator<Transcoder<T>> tcIter) {\n    final Map<String, Future<T>> m = new ConcurrentHashMap<String, Future<T>>();\n\n    // This map does not need to be a ConcurrentHashMap\n    // because it is fully populated when it is used and\n    // used only to read the transcoder for a key.\n    final Map<String, Transcoder<T>> tcMap =\n        new HashMap<String, Transcoder<T>>();\n\n    // Break the gets down into groups by key\n    final Map<MemcachedNode, Collection<String>> chunks =\n        new HashMap<MemcachedNode, Collection<String>>();\n    final NodeLocator locator = mconn.getLocator();\n\n    while (keyIter.hasNext() && tcIter.hasNext()) {\n      String key = keyIter.next();\n      tcMap.put(key, tcIter.next());\n      StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n      final MemcachedNode primaryNode = locator.getPrimary(key);\n      MemcachedNode node = null;\n      if (primaryNode.isActive()) {\n        node = primaryNode;\n      } else {\n        for (Iterator<MemcachedNode> i = locator.getSequence(key); node == null\n            && i.hasNext();) {\n          MemcachedNode n = i.next();\n          if (n.isActive()) {\n            node = n;\n          }\n        }\n        if (node == null) {\n          node = primaryNode;\n        }\n      }\n      assert node != null : \"Didn't find a node for \" + key;\n      Collection<String> ks = chunks.get(node);\n      if (ks == null) {\n        ks = new ArrayList<String>();\n        chunks.put(node, ks);\n      }\n      ks.add(key);\n    }\n\n    final AtomicInteger pendingChunks = new AtomicInteger(chunks.size());\n    int initialLatchCount = chunks.isEmpty() ? 0 : 1;\n    final CountDownLatch latch = new CountDownLatch(initialLatchCount);\n    final Collection<Operation> ops = new ArrayList<Operation>(chunks.size());\n    final BulkGetFuture<T> rv = new BulkGetFuture<T>(m, ops, latch, executorService);\n\n    GetOperation.Callback cb = new GetOperation.Callback() {\n      @Override\n      @SuppressWarnings(\"synthetic-access\")\n      public void receivedStatus(OperationStatus status) {\n        if (status.getStatusCode() == StatusCode.ERR_NOT_MY_VBUCKET) {\n          pendingChunks.addAndGet(Integer.parseInt(status.getMessage()));\n        }\n        rv.setStatus(status);\n      }\n\n      @Override\n      public void gotData(String k, int flags, byte[] data) {\n        Transcoder<T> tc = tcMap.get(k);\n        m.put(k,\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize())));\n      }\n\n      @Override\n      public void complete() {\n        if (pendingChunks.decrementAndGet() <= 0) {\n          latch.countDown();\n          rv.signalComplete();\n        }\n      }\n    };\n\n    // Now that we know how many servers it breaks down into, and the latch\n    // is all set up, convert all of these strings collections to operations\n    final Map<MemcachedNode, Operation> mops =\n        new HashMap<MemcachedNode, Operation>();\n\n    for (Map.Entry<MemcachedNode, Collection<String>> me : chunks.entrySet()) {\n      Operation op = opFact.get(me.getValue(), cb);\n      mops.put(me.getKey(), op);\n      ops.add(op);\n    }\n    assert mops.size() == chunks.size();\n    mconn.checkState();\n    mconn.addOperations(mops);\n    return rv;\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n          Iterator<Transcoder<T>> tcIter) {\n    return asyncGetBulk(keys.iterator(), tcIter);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator for the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keyIter,\n            new SingleElementInfiniteIterator<Transcoder<T>>(tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keys, new SingleElementInfiniteIterator<Transcoder<T>>(\n        tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keyIter Iterator that produces the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public BulkFuture<Map<String, Object>> asyncGetBulk(\n         Iterator<String> keyIter) {\n    return asyncGetBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keys the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public BulkFuture<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n    return asyncGetBulk(keys, transcoder);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n      String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n   *\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public BulkFuture<Map<String, Object>> asyncGetBulk(String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<CASValue<Object>> asyncGetAndTouch(final String key,\n      final int exp) {\n    return asyncGetAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> OperationFuture<CASValue<T>> asyncGetAndTouch(final String key,\n      final int exp, final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv = new OperationFuture<CASValue<T>>(\n        key, latch, operationTimeout, executorService);\n\n    Operation op = opFact.getAndTouch(key, exp,\n        new GetAndTouchOperation.Callback() {\n          private CASValue<T> val;\n\n          @Override\n          public void receivedStatus(OperationStatus status) {\n            rv.set(val, status);\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n            rv.signalComplete();\n          }\n\n          @Override\n          public void gotData(String k, int flags, long cas, byte[] data) {\n            assert k.equals(key) : \"Wrong key returned\";\n            val =\n                new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                    tc.getMaxSize())));\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> Map<String, T> getBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    try {\n      return asyncGetBulk(keyIter, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted getting bulk values\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for bulk values\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for bulk values: \"\n        + buildTimeoutMessage(operationTimeout, TimeUnit.MILLISECONDS), e);\n    }\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keyIter Iterator that produces the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<String, Object> getBulk(Iterator<String> keyIter) {\n    return getBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keys the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> Map<String, T> getBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return getBulk(keys.iterator(), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<String, Object> getBulk(Collection<String> keys) {\n    return getBulk(keys, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n    return getBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<String, Object> getBulk(String... keys) {\n    return getBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the versions of all of the connected memcacheds.\n   *\n   * @return a Map of SocketAddress to String for connected servers\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<SocketAddress, String> getVersions() {\n    final Map<SocketAddress, String> rv =\n        new ConcurrentHashMap<SocketAddress, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        return opFact.version(new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            rv.put(sa, s.getMessage());\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for versions\", e);\n    }\n    return rv;\n  }\n\n  /**\n   * Get all of the stats from all of the connections.\n   *\n   * @return a Map of a Map of stats replies by SocketAddress\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<SocketAddress, Map<String, String>> getStats() {\n    return getStats(null);\n  }\n\n  /**\n   * Get a set of stats from all connections.\n   *\n   * @param arg which stats to get\n   * @return a Map of the server SocketAddress to a map of String stat keys to\n   *         String stat values.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n    final Map<SocketAddress, Map<String, String>> rv =\n        new HashMap<SocketAddress, Map<String, String>>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        rv.put(sa, new HashMap<String, String>());\n        return opFact.stats(arg, new StatsOperation.Callback() {\n          @Override\n          public void gotStat(String name, String val) {\n            rv.get(sa).put(name, val);\n          }\n\n          @Override\n          @SuppressWarnings(\"synthetic-access\")\n          public void receivedStatus(OperationStatus status) {\n            if (!status.isSuccess()) {\n              getLogger().warn(\"Unsuccessful stat fetch: %s\", status);\n            }\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for stats\", e);\n    }\n    return rv;\n  }\n\n  private long mutate(Mutator m, String key, long by, long def, int exp) {\n    final AtomicLong rv = new AtomicLong();\n    final CountDownLatch latch = new CountDownLatch(1);\n    mconn.enqueueOperation(key, opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n        @Override\n        public void receivedStatus(OperationStatus s) {\n          // XXX: Potential abstraction leak.\n          // The handling of incr/decr in the binary protocol\n          // Allows us to avoid string processing.\n          rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      }));\n    try {\n      if (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n        throw new OperationTimeoutException(\"Mutate operation timed out,\"\n            + \"unable to modify counter [\" + key + ']');\n      }\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted\", e);\n    }\n    getLogger().debug(\"Mutation returned %s\", rv);\n    return rv.get();\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, long by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, int by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, long by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, int by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  private long mutateWithDefault(Mutator t, String key, long by, long def,\n      int exp) {\n    long rv = mutate(t, key, by, def, exp);\n    // The ascii protocol doesn't support defaults, so I added them\n    // manually here.\n    if (rv == -1) {\n      Future<Boolean> f = asyncStore(StoreType.add, key, exp,\n          String.valueOf(def));\n      try {\n        if (f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n          rv = def;\n        } else {\n          rv = mutate(t, key, by, 0, exp);\n          assert rv != -1 : \"Failed to mutate or init value\";\n        }\n      } catch (InterruptedException e) {\n        throw new RuntimeException(\"Interrupted waiting for store\", e);\n      } catch (ExecutionException e) {\n        if(e.getCause() instanceof CancellationException) {\n          throw (CancellationException) e.getCause();\n        } else {\n          throw new RuntimeException(\"Failed waiting for store\", e);\n        }\n      } catch (TimeoutException e) {\n        throw new OperationTimeoutException(\"Timeout waiting to mutate or init\"\n          + \" value\" + buildTimeoutMessage(operationTimeout,\n            TimeUnit.MILLISECONDS), e);\n      }\n    }\n    return rv;\n  }\n\n  private OperationFuture<Long> asyncMutate(Mutator m, String key, long by,\n      long def, int exp) {\n    if (!(opFact instanceof BinaryOperationFactory) && (def != 0 || exp != -1)) {\n      throw new UnsupportedOperationException(\"Default value or expiration \"\n        + \"time are not supported on the async mutate methods. Use either the \"\n        + \"binary protocol or the sync variant.\");\n    }\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Long> rv =\n        new OperationFuture<Long>(key, latch, operationTimeout, executorService);\n    Operation op = opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"), s);\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n            rv.signalComplete();\n          }\n        });\n    mconn.enqueueOperation(key, op);\n    rv.setOperation(op);\n    return rv;\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, long by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, int by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, long by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, int by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, long by, long def,\n    int exp) {\n    return asyncMutate(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, int by, long def,\n    int exp) {\n    return asyncMutate(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, long by, long def,\n    int exp) {\n    return asyncMutate(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, int by, long def,\n    int exp) {\n    return asyncMutate(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, long by, long def) {\n    return asyncMutate(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncIncr(String key, int by, long def) {\n    return asyncMutate(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, long by, long def) {\n    return asyncMutate(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to decrement\n   * @param by the amount to decrement the value by\n   * @param def the default value (if the counter does not exist)\n   * @return a future with the decremented value, or -1 if the decrement failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Long> asyncDecr(String key, int by, long def) {\n    return asyncMutate(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long incr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public long decr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * <p>\n   * The hold argument specifies the amount of time in seconds (or Unix time\n   * until which) the client wishes the server to refuse \"add\" and \"replace\"\n   * commands with this key. For this amount of item, the item is put into a\n   * delete queue, which means that it won't possible to retrieve it by the\n   * \"get\" command, but \"add\" and \"replace\" command with this key will also fail\n   * (the \"set\" command will succeed, however). After the time passes, the item\n   * is finally deleted from server memory.\n   * <\/p>\n   *\n   * @param key the key to delete\n   * @param hold how long the key should be unavailable to add commands\n   *\n   * @return whether or not the operation was performed\n   * @deprecated Hold values are no longer honored.\n   */\n  @Deprecated\n  public OperationFuture<Boolean> delete(String key, int hold) {\n    return delete(key);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * @param key the key to delete\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> delete(String key) {\n    return delete(key, 0L);\n  }\n\n  /**\n   * Delete the given key from the cache of the given CAS value applies.\n   *\n   * @param key the key to delete\n   * @param cas the CAS value to apply.\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> delete(String key, long cas) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout, executorService);\n\n    DeleteOperation.Callback callback = new DeleteOperation.Callback() {\n      @Override\n      public void receivedStatus(OperationStatus s) {\n        rv.set(s.isSuccess(), s);\n      }\n\n      @Override\n      public void gotData(long cas) {\n        rv.setCas(cas);\n      }\n\n      @Override\n      public void complete() {\n        latch.countDown();\n        rv.signalComplete();\n      }\n    };\n\n    DeleteOperation op;\n    if(cas == 0) {\n      op = opFact.delete(key, callback);\n    } else {\n      op = opFact.delete(key, cas, callback);\n    }\n\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Flush all caches from all servers with a delay of application.\n   *\n   * @param delay the period of time to delay, in seconds\n   * @return whether or not the operation was accepted\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> flush(final int delay) {\n    final AtomicReference<Boolean> flushResult =\n        new AtomicReference<Boolean>(null);\n    final ConcurrentLinkedQueue<Operation> ops =\n        new ConcurrentLinkedQueue<Operation>();\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        Operation op = opFact.flush(delay, new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            flushResult.set(s.isSuccess());\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n        ops.add(op);\n        return op;\n      }\n    });\n\n    return new OperationFuture<Boolean>(null, blatch, flushResult,\n        operationTimeout, executorService) {\n\n      @Override\n      public void set(Boolean o, OperationStatus s) {\n        super.set(o, s);\n        notifyListeners();\n      }\n\n      @Override\n      public boolean cancel(boolean ign) {\n        boolean rv = false;\n        for (Operation op : ops) {\n          op.cancel();\n          rv |= op.getState() == OperationState.WRITE_QUEUED;\n        }\n        notifyListeners();\n        return rv;\n      }\n\n      @Override\n      public Boolean get(long duration, TimeUnit units)\n        throws InterruptedException, TimeoutException, ExecutionException {\n        status = new OperationStatus(true, \"OK\", StatusCode.SUCCESS);\n        return super.get(duration, units);\n      }\n\n      @Override\n      public boolean isCancelled() {\n        boolean rv = false;\n        for (Operation op : ops) {\n          rv |= op.isCancelled();\n        }\n        return rv;\n      }\n\n      @Override\n      public boolean isDone() {\n        boolean rv = true;\n        for (Operation op : ops) {\n          rv &= op.getState() == OperationState.COMPLETE;\n        }\n        return rv || isCancelled();\n      }\n    };\n  }\n\n  /**\n   * Flush all caches from all servers immediately.\n   *\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public OperationFuture<Boolean> flush() {\n    return flush(-1);\n  }\n\n  @Override\n  public Set<String> listSaslMechanisms() {\n    final ConcurrentMap<String, String> rv =\n        new ConcurrentHashMap<String, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(MemcachedNode n, final CountDownLatch latch) {\n        return opFact.saslMechs(new OperationCallback() {\n          @Override\n          public void receivedStatus(OperationStatus status) {\n            for (String s : status.getMessage().split(\" \")) {\n              rv.put(s, s);\n            }\n          }\n\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n\n    try {\n      blatch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    return rv.keySet();\n  }\n\n  /**\n   * Shut down immediately.\n   */\n  @Override\n  public void shutdown() {\n    shutdown(-1, TimeUnit.MILLISECONDS);\n  }\n\n  /**\n   * Shut down this client gracefully.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the shutdown request\n   */\n  @Override\n  public boolean shutdown(long timeout, TimeUnit unit) {\n    // Guard against double shutdowns (bug 8).\n    if (shuttingDown) {\n      getLogger().info(\"Suppressing duplicate attempt to shut down\");\n      return false;\n    }\n    shuttingDown = true;\n    String baseName = mconn.getName();\n    mconn.setName(baseName + \" - SHUTTING DOWN\");\n    boolean rv = true;\n    if (connFactory.isDefaultExecutorService()) {\n      try {\n        executorService.shutdown();\n      } catch (Exception ex) {\n        getLogger().warn(\"Failed shutting down the ExecutorService: \", ex);\n      }\n    }\n    try {\n      // Conditionally wait\n      if (timeout > 0) {\n        mconn.setName(baseName + \" - SHUTTING DOWN (waiting)\");\n        rv = waitForQueues(timeout, unit);\n      }\n    } finally {\n      // But always begin the shutdown sequence\n      try {\n        mconn.setName(baseName + \" - SHUTTING DOWN (telling client)\");\n        mconn.shutdown();\n        mconn.setName(baseName + \" - SHUTTING DOWN (informed client)\");\n        tcService.shutdown();\n        //terminate all pending Auth Threads\n        authMonitor.interruptAllPendingAuth();\n      } catch (IOException e) {\n        getLogger().warn(\"exception while shutting down\", e);\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Wait for the queues to die down.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the request for the wait\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  @Override\n  public boolean waitForQueues(long timeout, TimeUnit unit) {\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      @Override\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        return opFact.noop(new OperationCallback() {\n          @Override\n          public void complete() {\n            latch.countDown();\n          }\n\n          @Override\n          public void receivedStatus(OperationStatus s) {\n            // Nothing special when receiving status, only\n            // necessary to complete the interface\n          }\n        });\n      }\n    }, mconn.getLocator().getAll(), false);\n    try {\n      // XXX: Perhaps IllegalStateException should be caught here\n      // and the check retried.\n      return blatch.await(timeout, unit);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for queues\", e);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * If connections are already established, your observer will be called with\n   * the address and -1.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer was added.\n   */\n  @Override\n  public boolean addObserver(ConnectionObserver obs) {\n    boolean rv = mconn.addObserver(obs);\n    if (rv) {\n      for (MemcachedNode node : mconn.getLocator().getAll()) {\n        if (node.isActive()) {\n          obs.connectionEstablished(node.getSocketAddress(), -1);\n        }\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer existed, but no longer does\n   */\n  @Override\n  public boolean removeObserver(ConnectionObserver obs) {\n    return mconn.removeObserver(obs);\n  }\n\n  @Override\n  public void connectionEstablished(SocketAddress sa, int reconnectCount) {\n    if (authDescriptor != null) {\n      if (authDescriptor.authThresholdReached()) {\n        shutdown();\n      }\n      authMonitor.authConnection(mconn, opFact, authDescriptor, findNode(sa));\n    }\n  }\n\n  private MemcachedNode findNode(SocketAddress sa) {\n    MemcachedNode node = null;\n    for (MemcachedNode n : mconn.getLocator().getAll()) {\n      if (n.getSocketAddress().equals(sa)) {\n        node = n;\n      }\n    }\n    assert node != null : \"Couldn't find node connected to \" + sa;\n    return node;\n  }\n\n  private String buildTimeoutMessage(long timeWaited, TimeUnit unit) {\n    StringBuilder message = new StringBuilder();\n\n    message.append(MessageFormat.format(\"waited {0} ms.\",\n      unit.convert(timeWaited, TimeUnit.MILLISECONDS)));\n    message.append(\" Node status: \").append(mconn.connectionsStatus());\n    return message.toString();\n  }\n\n  @Override\n  public void connectionLost(SocketAddress sa) {\n    // Don't care.\n  }\n\n  @Override\n  public String toString() {\n    return connFactory.toString();\n  }\n}\n","lineNo":1320}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  // The number of empty selects we'll allow before assuming we may have\n  // missed one and should check the current selectors. This generally\n  // indicates a bug, but we'll check it nonetheless.\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n  // The number of empty selects we'll allow before blowing up. It's too\n  // easy to write a bug that causes it to loop uncontrollably. This helps\n  // find those bugs and often works around them.\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  protected volatile boolean shutDown = false;\n  // If true, optimization will collapse multiple sequential get ops\n  private final boolean shouldOptimize;\n  protected Selector selector = null;\n  protected final NodeLocator locator;\n  protected final FailureMode failureMode;\n  // maximum amount of time to wait between reconnect attempts\n  private final long maxDelay;\n  private int emptySelects = 0;\n  private final int bufSize;\n  private final ConnectionFactory connectionFactory;\n  // AddedQueue is used to track the QueueAttachments for which operations\n  // have recently been queued.\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n  // reconnectQueue contains the attachments that need to be reconnected\n  // The key is the time at which they are eligible for reconnect\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  protected volatile boolean running = true;\n\n  private final Collection<ConnectionObserver> connObservers =\n      new ConcurrentLinkedQueue<ConnectionObserver>();\n  private final OperationFactory opFact;\n  private final int timeoutExceptionThreshold;\n  private final Collection<Operation> retryOps;\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n  private final boolean verifyAliveOnConnect;\n  private final ExecutorService listenerExecutorService;\n\n  protected final MetricCollector metrics;\n  protected final MetricType metricType;\n\n  /**\n   * Construct a memcached connection.\n   *\n   * @param bufSize the size of the buffer used for reading from the server\n   * @param f the factory that will provide an operation queue\n   * @param a the addresses of the servers to connect to\n   *\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(int bufSize, ConnectionFactory f,\n      List<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n      FailureMode fm, OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = f.getMaxReconnectDelay();\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = new ArrayList<Operation>();\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG) || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  protected List<MemcachedNode> createConnections(\n      final Collection<InetSocketAddress> a) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(a.size());\n    for (SocketAddress sa : a) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa =\n          this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n      // Initially I had attempted to skirt this by queueing every\n      // connect, but it considerably slowed down start time.\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n    return connections;\n  }\n\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n              + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * MemcachedClient calls this method to handle IO over the connections.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      throw new IOException(\"No IO while shut down\");\n    }\n\n    // Deal with all of the stuff that's been added, but may not be marked\n    // writable.\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = 0;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n    Set<SelectionKey> selectedKeys = selector.selectedKeys();\n\n    if (selectedKeys.isEmpty() && !shutDown) {\n      getLogger().debug(\"No selectors ready, interrupted: \"\n          + Thread.interrupted());\n      if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n        for (SelectionKey sk : selector.keys()) {\n          getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n              sk.interestOps());\n          if (sk.readyOps() != 0) {\n            getLogger().debug(\"%s has a ready op, handling IO\", sk);\n            handleIO(sk);\n          } else {\n            lostConnection((MemcachedNode) sk.attachment());\n          }\n        }\n        assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n      }\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n          selectedKeys.size());\n      emptySelects = 0;\n\n      for (SelectionKey sk : selectedKeys) {\n        handleIO(sk);\n      }\n      selectedKeys.clear();\n    }\n\n    // see if any connections blew up with large number of timeouts\n    boolean stillCheckingTimeouts = true;\n    while(stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n    // rehash any operations that are in retry state\n    redistributeOperations(retryOps);\n    retryOps.clear();\n\n    // try to shutdown odd nodes\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n                qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  // Handle any requests that have been made against the client.\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      // If there's stuff in the added queue. Try to process it.\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      // Transfer the queue into a hashset. There are very likely more\n      // additions than there are nodes.\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n      MemcachedNode qaNode = null;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      // Now process the queue.\n      for (MemcachedNode qa : todo) {\n        boolean readyForIO = false;\n        if (qa.isActive()) {\n          if (qa.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", qa);\n          }\n        } else {\n          toAdd.add(qa);\n        }\n        qa.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (qa.getWbuf().hasRemaining()) {\n              handleWrites(qa.getSk(), qa);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(qa);\n          }\n        }\n        qa.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added\n   */\n  public boolean addObserver(ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't\n   */\n  public boolean removeObserver(ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  private void connected(MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  private void lostConnection(MemcachedNode qa) {\n    queueReconnect(qa);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(qa.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given SelectionKey belongs to the current\n   * cluster.\n   *\n   * Before trying to connect to a node, make sure it actually\n   * belongs to the currently connected cluster.\n   */\n  boolean belongsToCluster(MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector. Any IOException will cause a\n   * reconnect.\n   *\n   * Note that this code makes sure that the corresponding node is not only\n   * able to connect, but also able to respond in a correct fashion. This is\n   * handled by issuing a dummy version/noop call and making sure it returns in\n   * a correct and timely fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n              sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n              sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().info(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n\n          if(verifyAliveOnConnect) {\n            // Test to see if it's truly alive, could be a hung process, OS\n            final CountDownLatch latch = new CountDownLatch(1);\n            final OperationFuture<Boolean> rv =\n              new OperationFuture<Boolean>(\"noop\", latch, 2500,\n                listenerExecutorService);\n            NoopOperation testOp = opFact.noop(new OperationCallback() {\n              public void receivedStatus(OperationStatus status) {\n                rv.set(status.isSuccess(), status);\n              }\n\n              @Override\n              public void complete() {\n                latch.countDown();\n              }\n            });\n\n            testOp.setHandlingNode(node);\n            testOp.initialize();\n\n            checkState();\n            insertOperation(node, testOp);\n            node.copyInputQueue();\n\n            boolean done = false;\n            if(sk.isValid()) {\n              long timeout = TimeUnit.MILLISECONDS.toNanos(\n                connectionFactory.getOperationTimeout());\n              for(long stop = System.nanoTime() + timeout;\n                stop > System.nanoTime();) {\n                handleWrites(sk, node);\n                handleReads(sk, node);\n                if(done = (latch.getCount() == 0)) {\n                  break;\n                }\n              }\n            }\n\n            if (!done || testOp.isCancelled() || testOp.hasErrored()\n              || testOp.isTimedOut()) {\n              throw new ConnectException(\"Could not send noop upon connect! \"\n                + \"This may indicate a running, but not responding memcached \"\n                + \"instance.\");\n            }\n          }\n\n          connected(node);\n          addedQueue.offer(node);\n          if (node.getWbuf().hasRemaining()) {\n            handleWrites(sk, node);\n          }\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        if (sk.isValid() && sk.isReadable()) {\n          handleReads(sk, node);\n        }\n        if (sk.isValid() && sk.isWritable()) {\n          handleWrites(sk, node);\n        }\n      }\n    } catch (ClosedChannelException e) {\n      // Note, not all channel closes end up here\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      // Failures to establish a connection should attempt a reconnect\n      // without signaling the observers.\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth(); // noop if !shouldAuth\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n          + \"operation on %s. This may be due to an authentication failure.\",\n          node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      // Any particular error processing an item should simply\n      // cause us to reconnect to the server.\n      //\n      // One cause is just network oddness or servers\n      // restarting, which lead here with IOException\n      node.setupForAuth(); // noop if !shouldAuth\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  private void handleWrites(SelectionKey sk, MemcachedNode qa)\n    throws IOException {\n    qa.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = qa.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = qa.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      qa.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  private void handleReads(SelectionKey sk, MemcachedNode qa)\n    throws IOException {\n    Operation currentOp = qa.getCurrentReadOp();\n    // If it's a tap ack there is no response\n    if (currentOp instanceof TapAckOperationImpl) {\n      qa.removeCurrentReadOp();\n      return;\n    }\n    ByteBuffer rbuf = qa.getRbuf();\n    final SocketChannel channel = qa.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      if (currentOp instanceof TapOperation) {\n        // If were doing tap then we won't throw an exception\n        currentOp.getCallback().complete();\n        ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n        getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n            currentOp, rbuf.remaining());\n        Operation op = qa.removeCurrentReadOp();\n        assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n        currentOp = qa.getCurrentReadOp();\n      } else {\n        // our model is to keep the connection alive for future ops\n        // so we'll queue a reconnect if disconnected via an IOException\n        throw new IOException(\"Disconnected unexpected, will reconnect.\");\n      }\n    }\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n        synchronized(currentOp) {\n          currentOp.readFromBuffer(rbuf);\n          if (currentOp.getState() == OperationState.COMPLETE) {\n            long timeOnWire = System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n            metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC, (int)(timeOnWire / 1000));\n            getLogger().debug(\"Completed read op: %s and giving the next %d \"\n                + \"bytes\", currentOp, rbuf.remaining());\n            Operation op = qa.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n                + op;\n            metrics.markMeter(OVERALL_RESPONSE_METRIC);\n            if (op.hasErrored()) {\n              metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n            } else {\n              metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n            }\n          } else if (currentOp.getState() == OperationState.RETRY) {\n            long timeOnWire = System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n            metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC, (int)(timeOnWire / 1000));\n            getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n                + \"%s \", currentOp);\n            ((VBucketAware) currentOp).addNotMyVbucketNode(\n                currentOp.getHandlingNode());\n            Operation op = qa.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n                + op;\n            retryOps.add(currentOp);\n            metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n            metrics.markMeter(OVERALL_RESPONSE_METRIC);\n          }\n        }\n        currentOp=qa.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      qa.completedRead();\n    }\n  }\n\n  // Make a debug string out of the given buffer's values\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  protected void queueReconnect(MemcachedNode qa) {\n    if (!shutDown) {\n      getLogger().warn(\"Closing, and reopening %s, attempt %d.\", qa,\n          qa.getReconnectCount());\n      if (qa.getSk() != null) {\n        qa.getSk().cancel();\n        assert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n      }\n      qa.reconnecting();\n      try {\n        if (qa.getChannel() != null && qa.getChannel().socket() != null) {\n          qa.getChannel().socket().close();\n        } else {\n          getLogger().info(\"The channel or socket was null for %s\", qa);\n        }\n      } catch (IOException e) {\n        getLogger().warn(\"IOException trying to close a socket\", e);\n      }\n      qa.setChannel(null);\n\n      long delay = (long) Math.min(maxDelay, Math.pow(2,\n          qa.getReconnectCount())) * 1000;\n      long reconTime = System.currentTimeMillis() + delay;\n\n      // Avoid potential condition where two connections are scheduled\n      // for reconnect at the exact same time. This is expected to be\n      // a rare situation.\n      while (reconnectQueue.containsKey(reconTime)) {\n        reconTime++;\n      }\n\n      reconnectQueue.put(reconTime, qa);\n      metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n      // Need to do a little queue management.\n      qa.setupResend();\n\n      if (failureMode == FailureMode.Redistribute) {\n        redistributeOperations(qa.destroyInputQueue());\n      } else if (failureMode == FailureMode.Cancel) {\n        cancelOperations(qa.destroyInputQueue());\n      }\n    }\n  }\n\n  private void cancelOperations(Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  private void redistributeOperations(Collection<Operation> ops) {\n    for (Operation op : ops) {\n      if (op.isCancelled() || op.isTimedOut()) {\n        continue;\n      }\n      if (op instanceof KeyedOperation) {\n        KeyedOperation ko = (KeyedOperation) op;\n        int added = 0;\n        for (String k : ko.getKeys()) {\n          for (Operation newop : opFact.clone(ko)) {\n            addOperation(k, newop);\n            added++;\n          }\n        }\n        assert added > 0 : \"Didn't add any new operations when redistributing\";\n      } else {\n        // Cancel things that don't have definite targets.\n        op.cancel();\n      }\n    }\n  }\n\n  private void attemptReconnects() throws IOException {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n        new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n    for (Iterator<MemcachedNode> i =\n        reconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n      final MemcachedNode qa = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n      try {\n        if(!belongsToCluster(qa)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", qa);\n          continue;\n        }\n        if (!seen.containsKey(qa)) {\n          seen.put(qa, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", qa);\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          int ops = 0;\n          if (ch.connect(qa.getSocketAddress())) {\n            connected(qa);\n            addedQueue.offer(qa);\n            getLogger().info(\"Immediately reconnected to %s\", qa);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          qa.registerChannel(ch, ch.register(selector, ops, qa));\n          assert qa.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\", qa);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(qa);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", qa, e);\n      } finally {\n        // it's possible that above code will leak file descriptors under\n        // abnormal\n        // conditions (when ch.open() fails and throws IOException.\n        // always close non connected channel\n        if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n          try {\n            ch.close();\n          } catch (IOException x) {\n            getLogger().error(\"Exception closing channel: %s\", qa, x);\n          }\n        }\n      }\n    }\n    // Requeue any fast-failed connects.\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Get the node locator used by this connection.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  public void enqueueOperation(String key, Operation o) {\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    checkState();\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to the given connection.\n   *\n   * @param key the key the operation is operating upon\n   * @param o the operation\n   */\n  protected void addOperation(final String key, final Operation o) {\n\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      // Look for another node in sequence that is ready.\n      for (Iterator<MemcachedNode> i = locator.getSequence(key); placeIn == null\n          && i.hasNext();) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n      // If we didn't find an active node, queue it in the primary node\n      // and wait for it to come back online.\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\n            \"Could not redistribute \"\n                + \"to another node, retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key\n          + \" (and not immediately cancelled)\";\n    }\n  }\n\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      final MemcachedNode node = me.getKey();\n      Operation o = me.getValue();\n      o.setHandlingNode(node);\n      o.initialize();\n      node.addOp(o);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   */\n  public CountDownLatch broadcastOperation(BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a specific collection of nodes.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all of the connections.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getChannel() != null) {\n        qa.getChannel().close();\n        qa.setSk(null);\n        if (qa.getBytesRemainingToWrite() > 0) {\n          getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n        }\n        getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n      }\n    }\n    running = false;\n    selector.close();\n    getLogger().debug(\"Shut down selector %s\", selector);\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \");\n      sb.append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Get information about connections and their active status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus.append(\" \");\n      connStatus.append(node.getSocketAddress())\n        .append(\" active: \").append(node.isActive())\n        .append(\", authed: \").append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * helper method: increase timeout count on node attached to this op.\n   *\n   * @param op\n   */\n  public static void opTimedOut(Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * helper method: reset timeout counter.\n   *\n   * @param op\n   */\n  public static void opSucceeded(Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * helper method: do some error checking and set timeout boolean.\n   *\n   * @param op\n   * @param isTimeout\n   */\n  private static void setTimeout(Operation op, boolean isTimeout) {\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return; // op may be null in some cases, e.g. flush\n      }\n      MemcachedNode node = op.getHandlingNode();\n      if (node == null) {\n        LoggerFactory.getLogger(MemcachedConnection.class).warn(\n            \"handling node for operation is not set\");\n      } else {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      LoggerFactory.getLogger(MemcachedConnection.class).error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Infinitely loop processing IO.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  private void logRunException(Exception e) {\n    if (shutDown) {\n      // There are a couple types of errors that occur during the\n      // shutdown sequence that are considered OK. Log at debug.\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.Logger;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Main class for handling connections to a memcached cluster.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  /**\n   * The number of empty selects we'll allow before assuming we may have\n   * missed one and should check the current selectors. This generally\n   * indicates a bug, but we'll check it nonetheless.\n   */\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n\n  /**\n   * The number of empty selects we'll allow before blowing up. It's too\n   * easy to write a bug that causes it to loop uncontrollably. This helps\n   * find those bugs and often works around them.\n   */\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  /**\n   * If the connection is alread shut down or shutting down.\n   */\n  protected volatile boolean shutDown = false;\n\n  /**\n   * If true, optimization will collapse multiple sequential get ops.\n   */\n  private final boolean shouldOptimize;\n\n  /**\n   * Holds the current {@link Selector} to use.\n   */\n  protected Selector selector = null;\n\n  /**\n   * The {@link NodeLocator} to use for this connection.\n   */\n  protected final NodeLocator locator;\n\n  /**\n   * The configured {@link FailureMode}.\n   */\n  protected final FailureMode failureMode;\n\n  /**\n   * Maximum amount of time to wait between reconnect attempts.\n   */\n  private final long maxDelay;\n\n  /**\n   * Contains the current number of empty select() calls, which could indicate\n   * bugs.\n   */\n  private int emptySelects = 0;\n\n  /**\n   * The buffer size that will be used when reading from the server.\n   */\n  private final int bufSize;\n\n  /**\n   * The connection factory to create {@link MemcachedNode}s from.\n   */\n  private final ConnectionFactory connectionFactory;\n\n  /**\n   * AddedQueue is used to track the QueueAttachments for which operations\n   * have recently been queued.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\n  /**\n   * reconnectQueue contains the attachments that need to be reconnected.\n   * The key is the time at which they are eligible for reconnect.\n   */\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  /**\n   * True if not shutting down or shut down.\n   */\n  protected volatile boolean running = true;\n\n  /**\n   * Holds all connection observers that get notified on connection status\n   * changes.\n   */\n  private final Collection<ConnectionObserver> connObservers =\n    new ConcurrentLinkedQueue<ConnectionObserver>();\n\n  /**\n   * The {@link OperationFactory} to clone or create operations.\n   */\n  private final OperationFactory opFact;\n\n  /**\n   * The threshold for timeout exceptions.\n   */\n  private final int timeoutExceptionThreshold;\n\n  /**\n   * Holds operations that need to be retried.\n   */\n  private final Collection<Operation> retryOps;\n\n  /**\n   * Holds all nodes that are scheduled for shutdown.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n  /**\n   * If set to true, a proper check after finish connecting is done to see\n   * if the node is not responding but really alive.\n   */\n  private final boolean verifyAliveOnConnect;\n\n  /**\n   * The {@link ExecutorService} to use for callbacks.\n   */\n  private final ExecutorService listenerExecutorService;\n\n  /**\n   * The {@link MetricCollector} to accumulate metrics (or dummy).\n   */\n  protected final MetricCollector metrics;\n\n  /**\n   * The current type of metrics to collect.\n   */\n  protected final MetricType metricType;\n\n  /**\n   * Construct a {@link MemcachedConnection}.\n   *\n   * @param bufSize the size of the buffer used for reading from the server.\n   * @param f the factory that will provide an operation queue.\n   * @param a the addresses of the servers to connect to.\n   * @param obs the initial observers to add.\n   * @param fm the failure mode to use.\n   * @param opfactory the operation factory.\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(final int bufSize, final ConnectionFactory f,\n      final List<InetSocketAddress> a, final Collection<ConnectionObserver> obs,\n      final FailureMode fm, final OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = f.getMaxReconnectDelay();\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = new ArrayList<Operation>();\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG)\n      || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  /**\n   * Create connections for the given list of addresses.\n   *\n   * @param addrs the list of addresses to connect to.\n   * @return addrs list of {@link MemcachedNode}s.\n   * @throws IOException if connecting was not successful.\n   */\n  protected List<MemcachedNode> createConnections(\n    final Collection<InetSocketAddress> addrs) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(addrs.size());\n\n    for (SocketAddress sa : addrs) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa =\n          this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n\n    return connections;\n  }\n\n  /**\n   * Make sure that the current selectors make sense.\n   *\n   * @return true if they do.\n   */\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n            + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * Handle all IO that flows through the connection.\n   *\n   * This method is called in an endless loop, listens on NIO selectors and\n   * dispatches the underlying read/write calls if needed.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      throw new IOException(\"No IO while shut down\");\n    }\n\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = 0;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n    Set<SelectionKey> selectedKeys = selector.selectedKeys();\n\n    if (selectedKeys.isEmpty() && !shutDown) {\n      getLogger().debug(\"No selectors ready, interrupted: \"\n        + Thread.interrupted());\n      if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n        for (SelectionKey sk : selector.keys()) {\n          getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n            sk.interestOps());\n          if (sk.readyOps() != 0) {\n            getLogger().debug(\"%s has a ready op, handling IO\", sk);\n            handleIO(sk);\n          } else {\n            lostConnection((MemcachedNode) sk.attachment());\n          }\n        }\n        assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n      }\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n        selectedKeys.size());\n      emptySelects = 0;\n\n      for (SelectionKey sk : selectedKeys) {\n        handleIO(sk);\n      }\n\n      selectedKeys.clear();\n    }\n\n    checkPotentiallyTimedOutConnection();\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n    redistributeOperations(retryOps);\n    retryOps.clear();\n\n    handleShutdownQueue();\n  }\n\n  /**\n   * Check if nodes need to be shut down and do so if needed.\n   *\n   * @throws IOException if the channel could not be closed properly.\n   */\n  private void handleShutdownQueue() throws IOException {\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  /**\n   * Check if one or more nodes exceeded the timeout Threshold.\n   */\n  private void checkPotentiallyTimedOutConnection() {\n    boolean stillCheckingTimeouts = true;\n    while (stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n  }\n\n  /**\n   * Handle any requests that have been made against the client.\n   */\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n\n      MemcachedNode qaNode;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      for (MemcachedNode node : todo) {\n        boolean readyForIO = false;\n        if (node.isActive()) {\n          if (node.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", node);\n          }\n        } else {\n          toAdd.add(node);\n        }\n        node.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (node.getWbuf().hasRemaining()) {\n              handleWrites(node);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(node);\n          }\n        }\n        node.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added.\n   */\n  public boolean addObserver(final ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't.\n   */\n  public boolean removeObserver(final ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  /**\n   * Indicate a successful connect to the given node.\n   *\n   * @param node the node which was successfully connected.\n   */\n  private void connected(final MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  /**\n   * Indicate a lost connection to the given node.\n   *\n   * @param node the node where the connection was lost.\n   */\n  private void lostConnection(final MemcachedNode node) {\n    queueReconnect(node);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(node.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given node belongs to the current cluster.\n   *\n   * Before trying to connect to a node, make sure it actually belongs to the\n   * currently connected cluster.\n   */\n  boolean belongsToCluster(final MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector.\n   *\n   * Any IOException will cause a reconnect. Note that this code makes sure\n   * that the corresponding node is not only able to connect, but also able to\n   * respond in a correct fashion (if verifyAliveOnConnect is set to true\n   * through a property). This is handled by issuing a dummy\n   * version/noop call and making sure it returns in a correct and timely\n   * fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(final SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n        sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n        sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().info(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n          finishConnect(sk, node);\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        if (sk.isValid() && sk.isReadable()) {\n          handleReads(node);\n        }\n        if (sk.isValid() && sk.isWritable()) {\n          handleWrites(node);\n        }\n      }\n    } catch (ClosedChannelException e) {\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n        + \"operation on %s. This may be due to an authentication failure.\",\n        node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  /**\n   * Finish the connect phase and potentially verify its liveness.\n   *\n   * @param sk the selection key for the node.\n   * @param node the actual node.\n   * @throws IOException if something goes wrong during reading/writing.\n   */\n  private void finishConnect(final SelectionKey sk, final MemcachedNode node)\n    throws IOException {\n    if (verifyAliveOnConnect) {\n      final CountDownLatch latch = new CountDownLatch(1);\n      final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(\"noop\",\n        latch, 2500, listenerExecutorService);\n      NoopOperation testOp = opFact.noop(new OperationCallback() {\n        public void receivedStatus(OperationStatus status) {\n          rv.set(status.isSuccess(), status);\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      });\n\n      testOp.setHandlingNode(node);\n      testOp.initialize();\n      checkState();\n      insertOperation(node, testOp);\n      node.copyInputQueue();\n\n      boolean done = false;\n      if (sk.isValid()) {\n        long timeout = TimeUnit.MILLISECONDS.toNanos(\n          connectionFactory.getOperationTimeout());\n\n        long stop = System.nanoTime() + timeout;\n        while (stop > System.nanoTime()) {\n          handleWrites(node);\n          handleReads(node);\n          if(done = (latch.getCount() == 0)) {\n            break;\n          }\n        }\n      }\n\n      if (!done || testOp.isCancelled() || testOp.hasErrored()\n        || testOp.isTimedOut()) {\n        throw new ConnectException(\"Could not send noop upon connect! \"\n          + \"This may indicate a running, but not responding memcached \"\n          + \"instance.\");\n      }\n    }\n\n    connected(node);\n    addedQueue.offer(node);\n    if (node.getWbuf().hasRemaining()) {\n      handleWrites(node);\n    }\n  }\n\n  /**\n   * Handle pending writes for the given node.\n   *\n   * @param node the node to handle writes for.\n   * @throws IOException can be raised during writing failures.\n   */\n  private void handleWrites(final MemcachedNode node) throws IOException {\n    node.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = node.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = node.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      node.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && node.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  /**\n   * Handle pending reads for the given node.\n   *\n   * @param node the node to handle reads for.\n   * @throws IOException can be raised during reading failures.\n   */\n  private void handleReads(final MemcachedNode node) throws IOException {\n    Operation currentOp = node.getCurrentReadOp();\n    if (currentOp instanceof TapAckOperationImpl) {\n      node.removeCurrentReadOp();\n      return;\n    }\n\n    ByteBuffer rbuf = node.getRbuf();\n    final SocketChannel channel = node.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      currentOp = handleReadsWhenChannelEndOfStream(currentOp, node, rbuf);\n    }\n\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n\n        long timeOnWire =\n          System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n        metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC,\n          (int)(timeOnWire / 1000));\n        metrics.markMeter(OVERALL_RESPONSE_METRIC);\n        synchronized(currentOp) {\n          currentOp.readFromBuffer(rbuf);\n\n          if (currentOp.getState() == OperationState.COMPLETE) {\n            getLogger().debug(\"Completed read op: %s and giving the next %d \"\n              + \"bytes\", currentOp, rbuf.remaining());\n            Operation op = node.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n              + op;\n\n            if (op.hasErrored()) {\n              metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n            } else {\n              metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n            }\n          } else if (currentOp.getState() == OperationState.RETRY) {\n            getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n              + \"%s \", currentOp);\n            ((VBucketAware) currentOp).addNotMyVbucketNode(\n              currentOp.getHandlingNode());\n            Operation op = node.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n              + op;\n\n            retryOps.add(currentOp);\n            metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n          }\n        }\n\n        currentOp = node.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      node.completedRead();\n    }\n  }\n\n  /**\n   * Deal with an operation where the channel reached the end of a stream.\n   *\n   * @param currentOp the current operation to read.\n   * @param node the node for that operation.\n   * @param rbuf the read buffer.\n   *\n   * @return the next operation on the node to read.\n   * @throws IOException if disconnect while reading.\n   */\n  private Operation handleReadsWhenChannelEndOfStream(final Operation currentOp,\n    final MemcachedNode node, final ByteBuffer rbuf) throws IOException {\n    if (currentOp instanceof TapOperation) {\n      currentOp.getCallback().complete();\n      ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n\n      getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n        currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n      return node.getCurrentReadOp();\n    } else {\n      throw new IOException(\"Disconnected unexpected, will reconnect.\");\n    }\n  }\n\n  /**\n   * Convert the {@link ByteBuffer} into a string for easier debugging.\n   *\n   * @param b the buffer to debug.\n   * @param size the size of the buffer.\n   * @return the stringified {@link ByteBuffer}.\n   */\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Enqueue the given {@link MemcachedNode} for reconnect.\n   *\n   * @param node the node to reconnect.\n   */\n  protected void queueReconnect(final MemcachedNode node) {\n    if (shutDown) {\n      return;\n    }\n    getLogger().warn(\"Closing, and reopening %s, attempt %d.\", node,\n      node.getReconnectCount());\n\n    if (node.getSk() != null) {\n      node.getSk().cancel();\n      assert !node.getSk().isValid() : \"Cancelled selection key is valid\";\n    }\n    node.reconnecting();\n\n    try {\n      if (node.getChannel() != null && node.getChannel().socket() != null) {\n        node.getChannel().socket().close();\n      } else {\n        getLogger().info(\"The channel or socket was null for %s\", node);\n      }\n    } catch (IOException e) {\n      getLogger().warn(\"IOException trying to close a socket\", e);\n    }\n    node.setChannel(null);\n\n    long delay = (long) Math.min(maxDelay, Math.pow(2,\n        node.getReconnectCount())) * 1000;\n    long reconnectTime = System.currentTimeMillis() + delay;\n    while (reconnectQueue.containsKey(reconnectTime)) {\n      reconnectTime++;\n    }\n\n    reconnectQueue.put(reconnectTime, node);\n    metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n    node.setupResend();\n    if (failureMode == FailureMode.Redistribute) {\n      redistributeOperations(node.destroyInputQueue());\n    } else if (failureMode == FailureMode.Cancel) {\n      cancelOperations(node.destroyInputQueue());\n    }\n  }\n\n  /**\n   * Cancel the given collection of operations.\n   *\n   * @param ops the list of operations to cancel.\n   */\n  private void cancelOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Redistribute the given list of operations to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param ops the operations to redistribute.\n   */\n  private void redistributeOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      if (op.isCancelled() || op.isTimedOut()) {\n        continue;\n      }\n\n      if (op instanceof KeyedOperation) {\n        KeyedOperation ko = (KeyedOperation) op;\n        int added = 0;\n        for (String k : ko.getKeys()) {\n          for (Operation newop : opFact.clone(ko)) {\n            addOperation(k, newop);\n            added++;\n          }\n        }\n        assert added > 0 : \"Didn't add any new operations when redistributing\";\n      } else {\n        op.cancel();\n      }\n    }\n  }\n\n  /**\n   * Attempt to reconnect {@link MemcachedNode}s in the reconnect queue.\n   *\n   * If the {@link MemcachedNode} does not belong to the cluster list anymore,\n   * the reconnect attempt is cancelled. If it does, the code tries to\n   * reconnect immediately and if this is not possible it waits until the\n   * connection information arrives.\n   *\n   * Note that if a socket error arises during reconnect, the node is scheduled\n   * for re-reconnect immediately.\n   */\n  private void attemptReconnects() {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n      new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n\n\n    Iterator<MemcachedNode> i = reconnectQueue.headMap(now).values().iterator();\n    while(i.hasNext()) {\n      final MemcachedNode node = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n\n      try {\n        if (!belongsToCluster(node)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", node);\n          continue;\n        }\n\n        if (!seen.containsKey(node)) {\n          seen.put(node, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", node);\n\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          int ops = 0;\n          if (ch.connect(node.getSocketAddress())) {\n            connected(node);\n            addedQueue.offer(node);\n            getLogger().info(\"Immediately reconnected to %s\", node);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          node.registerChannel(ch, ch.register(selector, ops, node));\n          assert node.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\",\n            node);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(node);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", node, e);\n      } finally {\n        potentiallyCloseLeakingChannel(ch, node);\n      }\n    }\n\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Make sure channel connections are not leaked and properly close under\n   * faulty reconnect cirumstances.\n   *\n   * @param ch the channel to potentially close.\n   * @param node the node to which the channel should be bound to.\n   */\n  private void potentiallyCloseLeakingChannel(final SocketChannel ch,\n    final MemcachedNode node) {\n    if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n      try {\n        ch.close();\n      } catch (IOException e) {\n        getLogger().error(\"Exception closing channel: %s\", node, e);\n      }\n    }\n  }\n\n  /**\n   * Returns the {@link NodeLocator} in use for this connection.\n   *\n   * @return  the current {@link NodeLocator}.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  /**\n   * Enqueue the given {@link Operation} with the used key.\n   *\n   * @param key the key to use.\n   * @param o the {@link Operation} to enqueue.\n   */\n  public void enqueueOperation(final String key, final Operation o) {\n    checkState();\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to a connection identified by the given key.\n   *\n   * If the {@link MemcachedNode} is active or the {@link FailureMode} is set\n   * to retry, the primary node will be used for that key. If the primary\n   * node is not available and the {@link FailureMode} cancel is used, the\n   * operation will be cancelled without further retry.\n   *\n   * For any other {@link FailureMode} mechanisms (Redistribute), another\n   * possible node is used (only if its active as well). If no other active\n   * node could be identified, the original primary node is used and retried.\n   *\n   * @param key the key the operation is operating upon.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final String key, final Operation o) {\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      Iterator<MemcachedNode> i = locator.getSequence(key);\n      while (placeIn == null && i.hasNext()) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\"Could not redistribute to another node, \"\n          + \"retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key + \" (and not \"\n        + \"immediately cancelled)\";\n    }\n  }\n\n  /**\n   * Insert an operation on the given node to the beginning of the queue.\n   *\n   * @param node the node where to insert the {@link Operation}.\n   * @param o the operation to insert.\n   */\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue an operation on the given node.\n   *\n   * @param node the node where to enqueue the {@link Operation}.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue the given list of operations on each handling node.\n   *\n   * @param ops the operations for each node.\n   */\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      addOperation(me.getKey(), me.getValue());\n    }\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a collection of nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n    final Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all connections and do not accept further incoming ops.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    for (MemcachedNode node : locator.getAll()) {\n      if (node.getChannel() != null) {\n        node.getChannel().close();\n        node.setSk(null);\n        if (node.getBytesRemainingToWrite() > 0) {\n          getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              node.getBytesRemainingToWrite());\n        }\n        getLogger().debug(\"Shut down channel %s\", node.getChannel());\n      }\n    }\n    running = false;\n    selector.close();\n    getLogger().debug(\"Shut down selector %s\", selector);\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \").append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Construct a String containing information about all nodes and their state.\n   *\n   * @return a stringified representation of the connection status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus\n        .append(\" \")\n        .append(node.getSocketAddress())\n        .append(\" active: \")\n        .append(node.isActive())\n        .append(\", authed: \")\n        .append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * Increase the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opTimedOut(final Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * Reset the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opSucceeded(final Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * Set the continous timeout on an operation.\n   *\n   * @param op the operation to use.\n   * @param isTimeout is timed out or not.\n   */\n  private static void setTimeout(final Operation op, final boolean isTimeout) {\n    Logger logger = LoggerFactory.getLogger(MemcachedConnection.class);\n\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return;\n      }\n\n      MemcachedNode node = op.getHandlingNode();\n      if (node == null) {\n        logger.warn(\"handling node for operation is not set\");\n      } else {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      logger.error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   *\n   * @throws IllegalStateException when shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Handle IO as long as the application is running.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  /**\n   * Log a exception to different levels depending on the state.\n   *\n   * Exceptions get logged at debug level when happening during shutdown, but\n   * at warning level when operating normally.\n   *\n   * @param e the exception to log.\n   */\n  private void logRunException(final Exception e) {\n    if (shutDown) {\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n\n}\n","lineNo":1063}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  // The number of empty selects we'll allow before assuming we may have\n  // missed one and should check the current selectors. This generally\n  // indicates a bug, but we'll check it nonetheless.\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n  // The number of empty selects we'll allow before blowing up. It's too\n  // easy to write a bug that causes it to loop uncontrollably. This helps\n  // find those bugs and often works around them.\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  protected volatile boolean shutDown = false;\n  // If true, optimization will collapse multiple sequential get ops\n  private final boolean shouldOptimize;\n  protected Selector selector = null;\n  protected final NodeLocator locator;\n  protected final FailureMode failureMode;\n  // maximum amount of time to wait between reconnect attempts\n  private final long maxDelay;\n  private int emptySelects = 0;\n  private final int bufSize;\n  private final ConnectionFactory connectionFactory;\n  // AddedQueue is used to track the QueueAttachments for which operations\n  // have recently been queued.\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n  // reconnectQueue contains the attachments that need to be reconnected\n  // The key is the time at which they are eligible for reconnect\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  protected volatile boolean running = true;\n\n  private final Collection<ConnectionObserver> connObservers =\n      new ConcurrentLinkedQueue<ConnectionObserver>();\n  private final OperationFactory opFact;\n  private final int timeoutExceptionThreshold;\n  private final Collection<Operation> retryOps;\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n  private final boolean verifyAliveOnConnect;\n  private final ExecutorService listenerExecutorService;\n\n  protected final MetricCollector metrics;\n  protected final MetricType metricType;\n\n  /**\n   * Construct a memcached connection.\n   *\n   * @param bufSize the size of the buffer used for reading from the server\n   * @param f the factory that will provide an operation queue\n   * @param a the addresses of the servers to connect to\n   *\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(int bufSize, ConnectionFactory f,\n      List<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n      FailureMode fm, OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = f.getMaxReconnectDelay();\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = new ArrayList<Operation>();\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG) || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  protected List<MemcachedNode> createConnections(\n      final Collection<InetSocketAddress> a) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(a.size());\n    for (SocketAddress sa : a) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa =\n          this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n      // Initially I had attempted to skirt this by queueing every\n      // connect, but it considerably slowed down start time.\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n    return connections;\n  }\n\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n              + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * MemcachedClient calls this method to handle IO over the connections.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      throw new IOException(\"No IO while shut down\");\n    }\n\n    // Deal with all of the stuff that's been added, but may not be marked\n    // writable.\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = 0;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n    Set<SelectionKey> selectedKeys = selector.selectedKeys();\n\n    if (selectedKeys.isEmpty() && !shutDown) {\n      getLogger().debug(\"No selectors ready, interrupted: \"\n          + Thread.interrupted());\n      if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n        for (SelectionKey sk : selector.keys()) {\n          getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n              sk.interestOps());\n          if (sk.readyOps() != 0) {\n            getLogger().debug(\"%s has a ready op, handling IO\", sk);\n            handleIO(sk);\n          } else {\n            lostConnection((MemcachedNode) sk.attachment());\n          }\n        }\n        assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n      }\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n          selectedKeys.size());\n      emptySelects = 0;\n\n      for (SelectionKey sk : selectedKeys) {\n        handleIO(sk);\n      }\n      selectedKeys.clear();\n    }\n\n    // see if any connections blew up with large number of timeouts\n    boolean stillCheckingTimeouts = true;\n    while(stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n    // rehash any operations that are in retry state\n    redistributeOperations(retryOps);\n    retryOps.clear();\n\n    // try to shutdown odd nodes\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n                qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  // Handle any requests that have been made against the client.\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      // If there's stuff in the added queue. Try to process it.\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      // Transfer the queue into a hashset. There are very likely more\n      // additions than there are nodes.\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n      MemcachedNode qaNode = null;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      // Now process the queue.\n      for (MemcachedNode qa : todo) {\n        boolean readyForIO = false;\n        if (qa.isActive()) {\n          if (qa.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", qa);\n          }\n        } else {\n          toAdd.add(qa);\n        }\n        qa.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (qa.getWbuf().hasRemaining()) {\n              handleWrites(qa.getSk(), qa);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(qa);\n          }\n        }\n        qa.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added\n   */\n  public boolean addObserver(ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't\n   */\n  public boolean removeObserver(ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  private void connected(MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  private void lostConnection(MemcachedNode qa) {\n    queueReconnect(qa);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(qa.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given SelectionKey belongs to the current\n   * cluster.\n   *\n   * Before trying to connect to a node, make sure it actually\n   * belongs to the currently connected cluster.\n   */\n  boolean belongsToCluster(MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector. Any IOException will cause a\n   * reconnect.\n   *\n   * Note that this code makes sure that the corresponding node is not only\n   * able to connect, but also able to respond in a correct fashion. This is\n   * handled by issuing a dummy version/noop call and making sure it returns in\n   * a correct and timely fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n              sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n              sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().info(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n\n          if(verifyAliveOnConnect) {\n            // Test to see if it's truly alive, could be a hung process, OS\n            final CountDownLatch latch = new CountDownLatch(1);\n            final OperationFuture<Boolean> rv =\n              new OperationFuture<Boolean>(\"noop\", latch, 2500,\n                listenerExecutorService);\n            NoopOperation testOp = opFact.noop(new OperationCallback() {\n              public void receivedStatus(OperationStatus status) {\n                rv.set(status.isSuccess(), status);\n              }\n\n              @Override\n              public void complete() {\n                latch.countDown();\n              }\n            });\n\n            testOp.setHandlingNode(node);\n            testOp.initialize();\n\n            checkState();\n            insertOperation(node, testOp);\n            node.copyInputQueue();\n\n            boolean done = false;\n            if(sk.isValid()) {\n              long timeout = TimeUnit.MILLISECONDS.toNanos(\n                connectionFactory.getOperationTimeout());\n              for(long stop = System.nanoTime() + timeout;\n                stop > System.nanoTime();) {\n                handleWrites(sk, node);\n                handleReads(sk, node);\n                if(done = (latch.getCount() == 0)) {\n                  break;\n                }\n              }\n            }\n\n            if (!done || testOp.isCancelled() || testOp.hasErrored()\n              || testOp.isTimedOut()) {\n              throw new ConnectException(\"Could not send noop upon connect! \"\n                + \"This may indicate a running, but not responding memcached \"\n                + \"instance.\");\n            }\n          }\n\n          connected(node);\n          addedQueue.offer(node);\n          if (node.getWbuf().hasRemaining()) {\n            handleWrites(sk, node);\n          }\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        if (sk.isValid() && sk.isReadable()) {\n          handleReads(sk, node);\n        }\n        if (sk.isValid() && sk.isWritable()) {\n          handleWrites(sk, node);\n        }\n      }\n    } catch (ClosedChannelException e) {\n      // Note, not all channel closes end up here\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      // Failures to establish a connection should attempt a reconnect\n      // without signaling the observers.\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth(); // noop if !shouldAuth\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n          + \"operation on %s. This may be due to an authentication failure.\",\n          node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      // Any particular error processing an item should simply\n      // cause us to reconnect to the server.\n      //\n      // One cause is just network oddness or servers\n      // restarting, which lead here with IOException\n      node.setupForAuth(); // noop if !shouldAuth\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  private void handleWrites(SelectionKey sk, MemcachedNode qa)\n    throws IOException {\n    qa.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = qa.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = qa.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      qa.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  private void handleReads(SelectionKey sk, MemcachedNode qa)\n    throws IOException {\n    Operation currentOp = qa.getCurrentReadOp();\n    // If it's a tap ack there is no response\n    if (currentOp instanceof TapAckOperationImpl) {\n      qa.removeCurrentReadOp();\n      return;\n    }\n    ByteBuffer rbuf = qa.getRbuf();\n    final SocketChannel channel = qa.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      if (currentOp instanceof TapOperation) {\n        // If were doing tap then we won't throw an exception\n        currentOp.getCallback().complete();\n        ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n        getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n            currentOp, rbuf.remaining());\n        Operation op = qa.removeCurrentReadOp();\n        assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n        currentOp = qa.getCurrentReadOp();\n      } else {\n        // our model is to keep the connection alive for future ops\n        // so we'll queue a reconnect if disconnected via an IOException\n        throw new IOException(\"Disconnected unexpected, will reconnect.\");\n      }\n    }\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n        synchronized(currentOp) {\n          currentOp.readFromBuffer(rbuf);\n          if (currentOp.getState() == OperationState.COMPLETE) {\n            long timeOnWire = System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n            metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC, (int)(timeOnWire / 1000));\n            getLogger().debug(\"Completed read op: %s and giving the next %d \"\n                + \"bytes\", currentOp, rbuf.remaining());\n            Operation op = qa.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n                + op;\n            metrics.markMeter(OVERALL_RESPONSE_METRIC);\n            if (op.hasErrored()) {\n              metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n            } else {\n              metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n            }\n          } else if (currentOp.getState() == OperationState.RETRY) {\n            long timeOnWire = System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n            metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC, (int)(timeOnWire / 1000));\n            getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n                + \"%s \", currentOp);\n            ((VBucketAware) currentOp).addNotMyVbucketNode(\n                currentOp.getHandlingNode());\n            Operation op = qa.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n                + op;\n            retryOps.add(currentOp);\n            metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n            metrics.markMeter(OVERALL_RESPONSE_METRIC);\n          }\n        }\n        currentOp=qa.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      qa.completedRead();\n    }\n  }\n\n  // Make a debug string out of the given buffer's values\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  protected void queueReconnect(MemcachedNode qa) {\n    if (!shutDown) {\n      getLogger().warn(\"Closing, and reopening %s, attempt %d.\", qa,\n          qa.getReconnectCount());\n      if (qa.getSk() != null) {\n        qa.getSk().cancel();\n        assert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n      }\n      qa.reconnecting();\n      try {\n        if (qa.getChannel() != null && qa.getChannel().socket() != null) {\n          qa.getChannel().socket().close();\n        } else {\n          getLogger().info(\"The channel or socket was null for %s\", qa);\n        }\n      } catch (IOException e) {\n        getLogger().warn(\"IOException trying to close a socket\", e);\n      }\n      qa.setChannel(null);\n\n      long delay = (long) Math.min(maxDelay, Math.pow(2,\n          qa.getReconnectCount())) * 1000;\n      long reconTime = System.currentTimeMillis() + delay;\n\n      // Avoid potential condition where two connections are scheduled\n      // for reconnect at the exact same time. This is expected to be\n      // a rare situation.\n      while (reconnectQueue.containsKey(reconTime)) {\n        reconTime++;\n      }\n\n      reconnectQueue.put(reconTime, qa);\n      metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n      // Need to do a little queue management.\n      qa.setupResend();\n\n      if (failureMode == FailureMode.Redistribute) {\n        redistributeOperations(qa.destroyInputQueue());\n      } else if (failureMode == FailureMode.Cancel) {\n        cancelOperations(qa.destroyInputQueue());\n      }\n    }\n  }\n\n  private void cancelOperations(Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  private void redistributeOperations(Collection<Operation> ops) {\n    for (Operation op : ops) {\n      if (op.isCancelled() || op.isTimedOut()) {\n        continue;\n      }\n      if (op instanceof KeyedOperation) {\n        KeyedOperation ko = (KeyedOperation) op;\n        int added = 0;\n        for (String k : ko.getKeys()) {\n          for (Operation newop : opFact.clone(ko)) {\n            addOperation(k, newop);\n            added++;\n          }\n        }\n        assert added > 0 : \"Didn't add any new operations when redistributing\";\n      } else {\n        // Cancel things that don't have definite targets.\n        op.cancel();\n      }\n    }\n  }\n\n  private void attemptReconnects() throws IOException {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n        new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n    for (Iterator<MemcachedNode> i =\n        reconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n      final MemcachedNode qa = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n      try {\n        if(!belongsToCluster(qa)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", qa);\n          continue;\n        }\n        if (!seen.containsKey(qa)) {\n          seen.put(qa, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", qa);\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          int ops = 0;\n          if (ch.connect(qa.getSocketAddress())) {\n            connected(qa);\n            addedQueue.offer(qa);\n            getLogger().info(\"Immediately reconnected to %s\", qa);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          qa.registerChannel(ch, ch.register(selector, ops, qa));\n          assert qa.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\", qa);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(qa);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", qa, e);\n      } finally {\n        // it's possible that above code will leak file descriptors under\n        // abnormal\n        // conditions (when ch.open() fails and throws IOException.\n        // always close non connected channel\n        if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n          try {\n            ch.close();\n          } catch (IOException x) {\n            getLogger().error(\"Exception closing channel: %s\", qa, x);\n          }\n        }\n      }\n    }\n    // Requeue any fast-failed connects.\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Get the node locator used by this connection.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  public void enqueueOperation(String key, Operation o) {\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    checkState();\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to the given connection.\n   *\n   * @param key the key the operation is operating upon\n   * @param o the operation\n   */\n  protected void addOperation(final String key, final Operation o) {\n\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      // Look for another node in sequence that is ready.\n      for (Iterator<MemcachedNode> i = locator.getSequence(key); placeIn == null\n          && i.hasNext();) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n      // If we didn't find an active node, queue it in the primary node\n      // and wait for it to come back online.\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\n            \"Could not redistribute \"\n                + \"to another node, retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key\n          + \" (and not immediately cancelled)\";\n    }\n  }\n\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      final MemcachedNode node = me.getKey();\n      Operation o = me.getValue();\n      o.setHandlingNode(node);\n      o.initialize();\n      node.addOp(o);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   */\n  public CountDownLatch broadcastOperation(BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a specific collection of nodes.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all of the connections.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getChannel() != null) {\n        qa.getChannel().close();\n        qa.setSk(null);\n        if (qa.getBytesRemainingToWrite() > 0) {\n          getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n        }\n        getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n      }\n    }\n    running = false;\n    selector.close();\n    getLogger().debug(\"Shut down selector %s\", selector);\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \");\n      sb.append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Get information about connections and their active status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus.append(\" \");\n      connStatus.append(node.getSocketAddress())\n        .append(\" active: \").append(node.isActive())\n        .append(\", authed: \").append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * helper method: increase timeout count on node attached to this op.\n   *\n   * @param op\n   */\n  public static void opTimedOut(Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * helper method: reset timeout counter.\n   *\n   * @param op\n   */\n  public static void opSucceeded(Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * helper method: do some error checking and set timeout boolean.\n   *\n   * @param op\n   * @param isTimeout\n   */\n  private static void setTimeout(Operation op, boolean isTimeout) {\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return; // op may be null in some cases, e.g. flush\n      }\n      MemcachedNode node = op.getHandlingNode();\n      if (node == null) {\n        LoggerFactory.getLogger(MemcachedConnection.class).warn(\n            \"handling node for operation is not set\");\n      } else {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      LoggerFactory.getLogger(MemcachedConnection.class).error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Infinitely loop processing IO.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  private void logRunException(Exception e) {\n    if (shutDown) {\n      // There are a couple types of errors that occur during the\n      // shutdown sequence that are considered OK. Log at debug.\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2013 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.text.MessageFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.ConcurrentModificationException;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.compat.log.Logger;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.metrics.MetricCollector;\nimport net.spy.memcached.metrics.MetricType;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.NoopOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.TapOperation;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.protocol.binary.TapAckOperationImpl;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Main class for handling connections to a memcached cluster.\n */\npublic class MemcachedConnection extends SpyThread {\n\n  /**\n   * The number of empty selects we'll allow before assuming we may have\n   * missed one and should check the current selectors. This generally\n   * indicates a bug, but we'll check it nonetheless.\n   */\n  private static final int DOUBLE_CHECK_EMPTY = 256;\n\n  /**\n   * The number of empty selects we'll allow before blowing up. It's too\n   * easy to write a bug that causes it to loop uncontrollably. This helps\n   * find those bugs and often works around them.\n   */\n  private static final int EXCESSIVE_EMPTY = 0x1000000;\n\n  private static final String RECON_QUEUE_METRIC =\n    \"[MEM] Reconnecting Nodes (ReconnectQueue)\";\n  private static final String SHUTD_QUEUE_METRIC =\n    \"[MEM] Shutting Down Nodes (NodesToShutdown)\";\n  private static final String OVERALL_REQUEST_METRIC =\n    \"[MEM] Request Rate: All\";\n  private static final String OVERALL_AVG_BYTES_WRITE_METRIC =\n    \"[MEM] Average Bytes written to OS per write\";\n  private static final String OVERALL_AVG_BYTES_READ_METRIC =\n    \"[MEM] Average Bytes read from OS per read\";\n  private static final String OVERALL_AVG_TIME_ON_WIRE_METRIC =\n    \"[MEM] Average Time on wire for operations (Âµs)\";\n  private static final String OVERALL_RESPONSE_METRIC =\n    \"[MEM] Response Rate: All (Failure + Success + Retry)\";\n  private static final String OVERALL_RESPONSE_RETRY_METRIC =\n    \"[MEM] Response Rate: Retry\";\n  private static final String OVERALL_RESPONSE_FAIL_METRIC =\n    \"[MEM] Response Rate: Failure\";\n  private static final String OVERALL_RESPONSE_SUCC_METRIC =\n    \"[MEM] Response Rate: Success\";\n\n  /**\n   * If the connection is alread shut down or shutting down.\n   */\n  protected volatile boolean shutDown = false;\n\n  /**\n   * If true, optimization will collapse multiple sequential get ops.\n   */\n  private final boolean shouldOptimize;\n\n  /**\n   * Holds the current {@link Selector} to use.\n   */\n  protected Selector selector = null;\n\n  /**\n   * The {@link NodeLocator} to use for this connection.\n   */\n  protected final NodeLocator locator;\n\n  /**\n   * The configured {@link FailureMode}.\n   */\n  protected final FailureMode failureMode;\n\n  /**\n   * Maximum amount of time to wait between reconnect attempts.\n   */\n  private final long maxDelay;\n\n  /**\n   * Contains the current number of empty select() calls, which could indicate\n   * bugs.\n   */\n  private int emptySelects = 0;\n\n  /**\n   * The buffer size that will be used when reading from the server.\n   */\n  private final int bufSize;\n\n  /**\n   * The connection factory to create {@link MemcachedNode}s from.\n   */\n  private final ConnectionFactory connectionFactory;\n\n  /**\n   * AddedQueue is used to track the QueueAttachments for which operations\n   * have recently been queued.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\n  /**\n   * reconnectQueue contains the attachments that need to be reconnected.\n   * The key is the time at which they are eligible for reconnect.\n   */\n  private final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n  /**\n   * True if not shutting down or shut down.\n   */\n  protected volatile boolean running = true;\n\n  /**\n   * Holds all connection observers that get notified on connection status\n   * changes.\n   */\n  private final Collection<ConnectionObserver> connObservers =\n    new ConcurrentLinkedQueue<ConnectionObserver>();\n\n  /**\n   * The {@link OperationFactory} to clone or create operations.\n   */\n  private final OperationFactory opFact;\n\n  /**\n   * The threshold for timeout exceptions.\n   */\n  private final int timeoutExceptionThreshold;\n\n  /**\n   * Holds operations that need to be retried.\n   */\n  private final Collection<Operation> retryOps;\n\n  /**\n   * Holds all nodes that are scheduled for shutdown.\n   */\n  protected final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n  /**\n   * If set to true, a proper check after finish connecting is done to see\n   * if the node is not responding but really alive.\n   */\n  private final boolean verifyAliveOnConnect;\n\n  /**\n   * The {@link ExecutorService} to use for callbacks.\n   */\n  private final ExecutorService listenerExecutorService;\n\n  /**\n   * The {@link MetricCollector} to accumulate metrics (or dummy).\n   */\n  protected final MetricCollector metrics;\n\n  /**\n   * The current type of metrics to collect.\n   */\n  protected final MetricType metricType;\n\n  /**\n   * Construct a {@link MemcachedConnection}.\n   *\n   * @param bufSize the size of the buffer used for reading from the server.\n   * @param f the factory that will provide an operation queue.\n   * @param a the addresses of the servers to connect to.\n   * @param obs the initial observers to add.\n   * @param fm the failure mode to use.\n   * @param opfactory the operation factory.\n   * @throws IOException if a connection attempt fails early\n   */\n  public MemcachedConnection(final int bufSize, final ConnectionFactory f,\n      final List<InetSocketAddress> a, final Collection<ConnectionObserver> obs,\n      final FailureMode fm, final OperationFactory opfactory) throws IOException {\n    connObservers.addAll(obs);\n    reconnectQueue = new TreeMap<Long, MemcachedNode>();\n    addedQueue = new ConcurrentLinkedQueue<MemcachedNode>();\n    failureMode = fm;\n    shouldOptimize = f.shouldOptimize();\n    maxDelay = f.getMaxReconnectDelay();\n    opFact = opfactory;\n    timeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n    selector = Selector.open();\n    retryOps = new ArrayList<Operation>();\n    nodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n    listenerExecutorService = f.getListenerExecutorService();\n    this.bufSize = bufSize;\n    this.connectionFactory = f;\n\n    String verifyAlive = System.getProperty(\"net.spy.verifyAliveOnConnect\");\n    if(verifyAlive != null && verifyAlive.equals(\"true\")) {\n      verifyAliveOnConnect = true;\n    } else {\n      verifyAliveOnConnect = false;\n    }\n\n    List<MemcachedNode> connections = createConnections(a);\n    locator = f.createLocator(connections);\n\n    metrics = f.getMetricCollector();\n    metricType = f.enableMetrics();\n\n    registerMetrics();\n\n    setName(\"Memcached IO over \" + this);\n    setDaemon(f.isDaemon());\n    start();\n  }\n\n  /**\n   * Register Metrics for collection.\n   *\n   * Note that these Metrics may or may not take effect, depending on the\n   * {@link MetricCollector} implementation. This can be controlled from\n   * the {@link DefaultConnectionFactory}.\n   */\n  protected void registerMetrics() {\n    if (metricType.equals(MetricType.DEBUG)\n      || metricType.equals(MetricType.PERFORMANCE)) {\n      metrics.addHistogram(OVERALL_AVG_BYTES_READ_METRIC);\n      metrics.addHistogram(OVERALL_AVG_BYTES_WRITE_METRIC);\n      metrics.addHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC);\n      metrics.addMeter(OVERALL_RESPONSE_METRIC);\n      metrics.addMeter(OVERALL_REQUEST_METRIC);\n\n      if (metricType.equals(MetricType.DEBUG)) {\n        metrics.addCounter(RECON_QUEUE_METRIC);\n        metrics.addCounter(SHUTD_QUEUE_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_RETRY_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_SUCC_METRIC);\n        metrics.addMeter(OVERALL_RESPONSE_FAIL_METRIC);\n      }\n    }\n  }\n\n  /**\n   * Create connections for the given list of addresses.\n   *\n   * @param addrs the list of addresses to connect to.\n   * @return addrs list of {@link MemcachedNode}s.\n   * @throws IOException if connecting was not successful.\n   */\n  protected List<MemcachedNode> createConnections(\n    final Collection<InetSocketAddress> addrs) throws IOException {\n    List<MemcachedNode> connections = new ArrayList<MemcachedNode>(addrs.size());\n\n    for (SocketAddress sa : addrs) {\n      SocketChannel ch = SocketChannel.open();\n      ch.configureBlocking(false);\n      MemcachedNode qa =\n          this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n      int ops = 0;\n      ch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n\n      try {\n        if (ch.connect(sa)) {\n          getLogger().info(\"Connected to %s immediately\", qa);\n          connected(qa);\n        } else {\n          getLogger().info(\"Added %s to connect queue\", qa);\n          ops = SelectionKey.OP_CONNECT;\n        }\n\n        selector.wakeup();\n        qa.setSk(ch.register(selector, ops, qa));\n        assert ch.isConnected()\n            || qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n            : \"Not connected, and not wanting to connect\";\n      } catch (SocketException e) {\n        getLogger().warn(\"Socket error on initial connect\", e);\n        queueReconnect(qa);\n      }\n      connections.add(qa);\n    }\n\n    return connections;\n  }\n\n  /**\n   * Make sure that the current selectors make sense.\n   *\n   * @return true if they do.\n   */\n  private boolean selectorsMakeSense() {\n    for (MemcachedNode qa : locator.getAll()) {\n      if (qa.getSk() != null && qa.getSk().isValid()) {\n        if (qa.getChannel().isConnected()) {\n          int sops = qa.getSk().interestOps();\n          int expected = 0;\n          if (qa.hasReadOp()) {\n            expected |= SelectionKey.OP_READ;\n          }\n          if (qa.hasWriteOp()) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          if (qa.getBytesRemainingToWrite() > 0) {\n            expected |= SelectionKey.OP_WRITE;\n          }\n          assert sops == expected : \"Invalid ops:  \" + qa + \", expected \"\n            + expected + \", got \" + sops;\n        } else {\n          int sops = qa.getSk().interestOps();\n          assert sops == SelectionKey.OP_CONNECT\n            : \"Not connected, and not watching for connect: \" + sops;\n        }\n      }\n    }\n    getLogger().debug(\"Checked the selectors.\");\n    return true;\n  }\n\n  /**\n   * Handle all IO that flows through the connection.\n   *\n   * This method is called in an endless loop, listens on NIO selectors and\n   * dispatches the underlying read/write calls if needed.\n   */\n  public void handleIO() throws IOException {\n    if (shutDown) {\n      throw new IOException(\"No IO while shut down\");\n    }\n\n    handleInputQueue();\n    getLogger().debug(\"Done dealing with queue.\");\n\n    long delay = 0;\n    if (!reconnectQueue.isEmpty()) {\n      long now = System.currentTimeMillis();\n      long then = reconnectQueue.firstKey();\n      delay = Math.max(then - now, 1);\n    }\n    getLogger().debug(\"Selecting with delay of %sms\", delay);\n    assert selectorsMakeSense() : \"Selectors don't make sense.\";\n    int selected = selector.select(delay);\n    Set<SelectionKey> selectedKeys = selector.selectedKeys();\n\n    if (selectedKeys.isEmpty() && !shutDown) {\n      getLogger().debug(\"No selectors ready, interrupted: \"\n        + Thread.interrupted());\n      if (++emptySelects > DOUBLE_CHECK_EMPTY) {\n        for (SelectionKey sk : selector.keys()) {\n          getLogger().debug(\"%s has %s, interested in %s\", sk, sk.readyOps(),\n            sk.interestOps());\n          if (sk.readyOps() != 0) {\n            getLogger().debug(\"%s has a ready op, handling IO\", sk);\n            handleIO(sk);\n          } else {\n            lostConnection((MemcachedNode) sk.attachment());\n          }\n        }\n        assert emptySelects < EXCESSIVE_EMPTY : \"Too many empty selects\";\n      }\n    } else {\n      getLogger().debug(\"Selected %d, selected %d keys\", selected,\n        selectedKeys.size());\n      emptySelects = 0;\n\n      for (SelectionKey sk : selectedKeys) {\n        handleIO(sk);\n      }\n\n      selectedKeys.clear();\n    }\n\n    checkPotentiallyTimedOutConnection();\n\n    if (!shutDown && !reconnectQueue.isEmpty()) {\n      attemptReconnects();\n    }\n    redistributeOperations(retryOps);\n    retryOps.clear();\n\n    handleShutdownQueue();\n  }\n\n  /**\n   * Check if nodes need to be shut down and do so if needed.\n   *\n   * @throws IOException if the channel could not be closed properly.\n   */\n  private void handleShutdownQueue() throws IOException {\n    for (MemcachedNode qa : nodesToShutdown) {\n      if (!addedQueue.contains(qa)) {\n        nodesToShutdown.remove(qa);\n        metrics.decrementCounter(SHUTD_QUEUE_METRIC);\n        Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n        if (qa.getChannel() != null) {\n          qa.getChannel().close();\n          qa.setSk(null);\n          if (qa.getBytesRemainingToWrite() > 0) {\n            getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              qa.getBytesRemainingToWrite());\n          }\n          getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n        }\n        redistributeOperations(notCompletedOperations);\n      }\n    }\n  }\n\n  /**\n   * Check if one or more nodes exceeded the timeout Threshold.\n   */\n  private void checkPotentiallyTimedOutConnection() {\n    boolean stillCheckingTimeouts = true;\n    while (stillCheckingTimeouts) {\n      try {\n        for (SelectionKey sk : selector.keys()) {\n          MemcachedNode mn = (MemcachedNode) sk.attachment();\n          if (mn.getContinuousTimeout() > timeoutExceptionThreshold) {\n            getLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n            lostConnection(mn);\n          }\n        }\n        stillCheckingTimeouts = false;\n      } catch(ConcurrentModificationException e) {\n        getLogger().warn(\"Retrying selector keys after \"\n          + \"ConcurrentModificationException caught\", e);\n        continue;\n      }\n    }\n  }\n\n  /**\n   * Handle any requests that have been made against the client.\n   */\n  private void handleInputQueue() {\n    if (!addedQueue.isEmpty()) {\n      getLogger().debug(\"Handling queue\");\n      Collection<MemcachedNode> toAdd = new HashSet<MemcachedNode>();\n      Collection<MemcachedNode> todo = new HashSet<MemcachedNode>();\n\n      MemcachedNode qaNode;\n      while ((qaNode = addedQueue.poll()) != null) {\n        todo.add(qaNode);\n      }\n\n      for (MemcachedNode node : todo) {\n        boolean readyForIO = false;\n        if (node.isActive()) {\n          if (node.getCurrentWriteOp() != null) {\n            readyForIO = true;\n            getLogger().debug(\"Handling queued write %s\", node);\n          }\n        } else {\n          toAdd.add(node);\n        }\n        node.copyInputQueue();\n        if (readyForIO) {\n          try {\n            if (node.getWbuf().hasRemaining()) {\n              handleWrites(node);\n            }\n          } catch (IOException e) {\n            getLogger().warn(\"Exception handling write\", e);\n            lostConnection(node);\n          }\n        }\n        node.fixupOps();\n      }\n      addedQueue.addAll(toAdd);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * @return whether the observer was successfully added.\n   */\n  public boolean addObserver(final ConnectionObserver obs) {\n    return connObservers.add(obs);\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @return true if the observer existed and now doesn't.\n   */\n  public boolean removeObserver(final ConnectionObserver obs) {\n    return connObservers.remove(obs);\n  }\n\n  /**\n   * Indicate a successful connect to the given node.\n   *\n   * @param node the node which was successfully connected.\n   */\n  private void connected(final MemcachedNode node) {\n    assert node.getChannel().isConnected() : \"Not connected.\";\n    int rt = node.getReconnectCount();\n    node.connected();\n\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionEstablished(node.getSocketAddress(), rt);\n    }\n  }\n\n  /**\n   * Indicate a lost connection to the given node.\n   *\n   * @param node the node where the connection was lost.\n   */\n  private void lostConnection(final MemcachedNode node) {\n    queueReconnect(node);\n    for (ConnectionObserver observer : connObservers) {\n      observer.connectionLost(node.getSocketAddress());\n    }\n  }\n\n  /**\n   * Makes sure that the given node belongs to the current cluster.\n   *\n   * Before trying to connect to a node, make sure it actually belongs to the\n   * currently connected cluster.\n   */\n  boolean belongsToCluster(final MemcachedNode node) {\n    for (MemcachedNode n : locator.getAll()) {\n      if (n.getSocketAddress().equals(node.getSocketAddress())) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Handle IO for a specific selector.\n   *\n   * Any IOException will cause a reconnect. Note that this code makes sure\n   * that the corresponding node is not only able to connect, but also able to\n   * respond in a correct fashion (if verifyAliveOnConnect is set to true\n   * through a property). This is handled by issuing a dummy\n   * version/noop call and making sure it returns in a correct and timely\n   * fashion.\n   *\n   * @param sk the selector to handle IO against.\n   */\n  private void handleIO(final SelectionKey sk) {\n    MemcachedNode node = (MemcachedNode) sk.attachment();\n\n    try {\n      getLogger().debug(\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\", sk,\n        sk.isReadable(), sk.isWritable(), sk.isConnectable(),\n        sk.attachment());\n      if (sk.isConnectable() && belongsToCluster(node)) {\n        getLogger().info(\"Connection state changed for %s\", sk);\n        final SocketChannel channel = node.getChannel();\n        if (channel.finishConnect()) {\n          finishConnect(sk, node);\n        } else {\n          assert !channel.isConnected() : \"connected\";\n        }\n      } else {\n        if (sk.isValid() && sk.isReadable()) {\n          handleReads(node);\n        }\n        if (sk.isValid() && sk.isWritable()) {\n          handleWrites(node);\n        }\n      }\n    } catch (ClosedChannelException e) {\n      if (!shutDown) {\n        getLogger().info(\"Closed channel and not shutting down. Queueing\"\n            + \" reconnect on %s\", node, e);\n        lostConnection(node);\n      }\n    } catch (ConnectException e) {\n      getLogger().info(\"Reconnecting due to failure to connect to %s\", node, e);\n      queueReconnect(node);\n    } catch (OperationException e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnection due to exception handling a memcached \"\n        + \"operation on %s. This may be due to an authentication failure.\",\n        node, e);\n      lostConnection(node);\n    } catch (Exception e) {\n      node.setupForAuth();\n      getLogger().info(\"Reconnecting due to exception on %s\", node, e);\n      lostConnection(node);\n    }\n    node.fixupOps();\n  }\n\n  /**\n   * Finish the connect phase and potentially verify its liveness.\n   *\n   * @param sk the selection key for the node.\n   * @param node the actual node.\n   * @throws IOException if something goes wrong during reading/writing.\n   */\n  private void finishConnect(final SelectionKey sk, final MemcachedNode node)\n    throws IOException {\n    if (verifyAliveOnConnect) {\n      final CountDownLatch latch = new CountDownLatch(1);\n      final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(\"noop\",\n        latch, 2500, listenerExecutorService);\n      NoopOperation testOp = opFact.noop(new OperationCallback() {\n        public void receivedStatus(OperationStatus status) {\n          rv.set(status.isSuccess(), status);\n        }\n\n        @Override\n        public void complete() {\n          latch.countDown();\n        }\n      });\n\n      testOp.setHandlingNode(node);\n      testOp.initialize();\n      checkState();\n      insertOperation(node, testOp);\n      node.copyInputQueue();\n\n      boolean done = false;\n      if (sk.isValid()) {\n        long timeout = TimeUnit.MILLISECONDS.toNanos(\n          connectionFactory.getOperationTimeout());\n\n        long stop = System.nanoTime() + timeout;\n        while (stop > System.nanoTime()) {\n          handleWrites(node);\n          handleReads(node);\n          if(done = (latch.getCount() == 0)) {\n            break;\n          }\n        }\n      }\n\n      if (!done || testOp.isCancelled() || testOp.hasErrored()\n        || testOp.isTimedOut()) {\n        throw new ConnectException(\"Could not send noop upon connect! \"\n          + \"This may indicate a running, but not responding memcached \"\n          + \"instance.\");\n      }\n    }\n\n    connected(node);\n    addedQueue.offer(node);\n    if (node.getWbuf().hasRemaining()) {\n      handleWrites(node);\n    }\n  }\n\n  /**\n   * Handle pending writes for the given node.\n   *\n   * @param node the node to handle writes for.\n   * @throws IOException can be raised during writing failures.\n   */\n  private void handleWrites(final MemcachedNode node) throws IOException {\n    node.fillWriteBuffer(shouldOptimize);\n    boolean canWriteMore = node.getBytesRemainingToWrite() > 0;\n    while (canWriteMore) {\n      int wrote = node.writeSome();\n      metrics.updateHistogram(OVERALL_AVG_BYTES_WRITE_METRIC, wrote);\n      node.fillWriteBuffer(shouldOptimize);\n      canWriteMore = wrote > 0 && node.getBytesRemainingToWrite() > 0;\n    }\n  }\n\n  /**\n   * Handle pending reads for the given node.\n   *\n   * @param node the node to handle reads for.\n   * @throws IOException can be raised during reading failures.\n   */\n  private void handleReads(final MemcachedNode node) throws IOException {\n    Operation currentOp = node.getCurrentReadOp();\n    if (currentOp instanceof TapAckOperationImpl) {\n      node.removeCurrentReadOp();\n      return;\n    }\n\n    ByteBuffer rbuf = node.getRbuf();\n    final SocketChannel channel = node.getChannel();\n    int read = channel.read(rbuf);\n    metrics.updateHistogram(OVERALL_AVG_BYTES_READ_METRIC, read);\n    if (read < 0) {\n      currentOp = handleReadsWhenChannelEndOfStream(currentOp, node, rbuf);\n    }\n\n    while (read > 0) {\n      getLogger().debug(\"Read %d bytes\", read);\n      rbuf.flip();\n      while (rbuf.remaining() > 0) {\n        if (currentOp == null) {\n          throw new IllegalStateException(\"No read operation.\");\n        }\n\n        long timeOnWire =\n          System.nanoTime() - currentOp.getWriteCompleteTimestamp();\n        metrics.updateHistogram(OVERALL_AVG_TIME_ON_WIRE_METRIC,\n          (int)(timeOnWire / 1000));\n        metrics.markMeter(OVERALL_RESPONSE_METRIC);\n        synchronized(currentOp) {\n          currentOp.readFromBuffer(rbuf);\n\n          if (currentOp.getState() == OperationState.COMPLETE) {\n            getLogger().debug(\"Completed read op: %s and giving the next %d \"\n              + \"bytes\", currentOp, rbuf.remaining());\n            Operation op = node.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n              + op;\n\n            if (op.hasErrored()) {\n              metrics.markMeter(OVERALL_RESPONSE_FAIL_METRIC);\n            } else {\n              metrics.markMeter(OVERALL_RESPONSE_SUCC_METRIC);\n            }\n          } else if (currentOp.getState() == OperationState.RETRY) {\n            getLogger().debug(\"Reschedule read op due to NOT_MY_VBUCKET error: \"\n              + \"%s \", currentOp);\n            ((VBucketAware) currentOp).addNotMyVbucketNode(\n              currentOp.getHandlingNode());\n            Operation op = node.removeCurrentReadOp();\n            assert op == currentOp : \"Expected to pop \" + currentOp + \" got \"\n              + op;\n\n            retryOps.add(currentOp);\n            metrics.markMeter(OVERALL_RESPONSE_RETRY_METRIC);\n          }\n        }\n\n        currentOp = node.getCurrentReadOp();\n      }\n      rbuf.clear();\n      read = channel.read(rbuf);\n      node.completedRead();\n    }\n  }\n\n  /**\n   * Deal with an operation where the channel reached the end of a stream.\n   *\n   * @param currentOp the current operation to read.\n   * @param node the node for that operation.\n   * @param rbuf the read buffer.\n   *\n   * @return the next operation on the node to read.\n   * @throws IOException if disconnect while reading.\n   */\n  private Operation handleReadsWhenChannelEndOfStream(final Operation currentOp,\n    final MemcachedNode node, final ByteBuffer rbuf) throws IOException {\n    if (currentOp instanceof TapOperation) {\n      currentOp.getCallback().complete();\n      ((TapOperation) currentOp).streamClosed(OperationState.COMPLETE);\n\n      getLogger().debug(\"Completed read op: %s and giving the next %d bytes\",\n        currentOp, rbuf.remaining());\n      Operation op = node.removeCurrentReadOp();\n      assert op == currentOp : \"Expected to pop \" + currentOp + \" got \" + op;\n      return node.getCurrentReadOp();\n    } else {\n      throw new IOException(\"Disconnected unexpected, will reconnect.\");\n    }\n  }\n\n  /**\n   * Convert the {@link ByteBuffer} into a string for easier debugging.\n   *\n   * @param b the buffer to debug.\n   * @param size the size of the buffer.\n   * @return the stringified {@link ByteBuffer}.\n   */\n  static String dbgBuffer(ByteBuffer b, int size) {\n    StringBuilder sb = new StringBuilder();\n    byte[] bytes = b.array();\n    for (int i = 0; i < size; i++) {\n      char ch = (char) bytes[i];\n      if (Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n        sb.append(ch);\n      } else {\n        sb.append(\"\\\\x\");\n        sb.append(Integer.toHexString(bytes[i] & 0xff));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Enqueue the given {@link MemcachedNode} for reconnect.\n   *\n   * @param node the node to reconnect.\n   */\n  protected void queueReconnect(final MemcachedNode node) {\n    if (shutDown) {\n      return;\n    }\n    getLogger().warn(\"Closing, and reopening %s, attempt %d.\", node,\n      node.getReconnectCount());\n\n    if (node.getSk() != null) {\n      node.getSk().cancel();\n      assert !node.getSk().isValid() : \"Cancelled selection key is valid\";\n    }\n    node.reconnecting();\n\n    try {\n      if (node.getChannel() != null && node.getChannel().socket() != null) {\n        node.getChannel().socket().close();\n      } else {\n        getLogger().info(\"The channel or socket was null for %s\", node);\n      }\n    } catch (IOException e) {\n      getLogger().warn(\"IOException trying to close a socket\", e);\n    }\n    node.setChannel(null);\n\n    long delay = (long) Math.min(maxDelay, Math.pow(2,\n        node.getReconnectCount())) * 1000;\n    long reconnectTime = System.currentTimeMillis() + delay;\n    while (reconnectQueue.containsKey(reconnectTime)) {\n      reconnectTime++;\n    }\n\n    reconnectQueue.put(reconnectTime, node);\n    metrics.incrementCounter(RECON_QUEUE_METRIC);\n\n    node.setupResend();\n    if (failureMode == FailureMode.Redistribute) {\n      redistributeOperations(node.destroyInputQueue());\n    } else if (failureMode == FailureMode.Cancel) {\n      cancelOperations(node.destroyInputQueue());\n    }\n  }\n\n  /**\n   * Cancel the given collection of operations.\n   *\n   * @param ops the list of operations to cancel.\n   */\n  private void cancelOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      op.cancel();\n    }\n  }\n\n  /**\n   * Redistribute the given list of operations to (potentially) other nodes.\n   *\n   * Note that operations can only be redistributed if they have not been\n   * cancelled already, timed out already or do not have definite targets\n   * (a key).\n   *\n   * @param ops the operations to redistribute.\n   */\n  private void redistributeOperations(final Collection<Operation> ops) {\n    for (Operation op : ops) {\n      if (op.isCancelled() || op.isTimedOut()) {\n        continue;\n      }\n\n      if (op instanceof KeyedOperation) {\n        KeyedOperation ko = (KeyedOperation) op;\n        int added = 0;\n        for (String k : ko.getKeys()) {\n          for (Operation newop : opFact.clone(ko)) {\n            addOperation(k, newop);\n            added++;\n          }\n        }\n        assert added > 0 : \"Didn't add any new operations when redistributing\";\n      } else {\n        op.cancel();\n      }\n    }\n  }\n\n  /**\n   * Attempt to reconnect {@link MemcachedNode}s in the reconnect queue.\n   *\n   * If the {@link MemcachedNode} does not belong to the cluster list anymore,\n   * the reconnect attempt is cancelled. If it does, the code tries to\n   * reconnect immediately and if this is not possible it waits until the\n   * connection information arrives.\n   *\n   * Note that if a socket error arises during reconnect, the node is scheduled\n   * for re-reconnect immediately.\n   */\n  private void attemptReconnects() {\n    final long now = System.currentTimeMillis();\n    final Map<MemcachedNode, Boolean> seen =\n      new IdentityHashMap<MemcachedNode, Boolean>();\n    final List<MemcachedNode> rereQueue = new ArrayList<MemcachedNode>();\n    SocketChannel ch = null;\n\n\n    Iterator<MemcachedNode> i = reconnectQueue.headMap(now).values().iterator();\n    while(i.hasNext()) {\n      final MemcachedNode node = i.next();\n      i.remove();\n      metrics.decrementCounter(RECON_QUEUE_METRIC);\n\n      try {\n        if (!belongsToCluster(node)) {\n          getLogger().debug(\"Node does not belong to cluster anymore, \"\n            + \"skipping reconnect: %s\", node);\n          continue;\n        }\n\n        if (!seen.containsKey(node)) {\n          seen.put(node, Boolean.TRUE);\n          getLogger().info(\"Reconnecting %s\", node);\n\n          ch = SocketChannel.open();\n          ch.configureBlocking(false);\n          int ops = 0;\n          if (ch.connect(node.getSocketAddress())) {\n            connected(node);\n            addedQueue.offer(node);\n            getLogger().info(\"Immediately reconnected to %s\", node);\n            assert ch.isConnected();\n          } else {\n            ops = SelectionKey.OP_CONNECT;\n          }\n          node.registerChannel(ch, ch.register(selector, ops, node));\n          assert node.getChannel() == ch : \"Channel was lost.\";\n        } else {\n          getLogger().debug(\"Skipping duplicate reconnect request for %s\",\n            node);\n        }\n      } catch (SocketException e) {\n        getLogger().warn(\"Error on reconnect\", e);\n        rereQueue.add(node);\n      } catch (Exception e) {\n        getLogger().error(\"Exception on reconnect, lost node %s\", node, e);\n      } finally {\n        potentiallyCloseLeakingChannel(ch, node);\n      }\n    }\n\n    for (MemcachedNode n : rereQueue) {\n      queueReconnect(n);\n    }\n  }\n\n  /**\n   * Make sure channel connections are not leaked and properly close under\n   * faulty reconnect cirumstances.\n   *\n   * @param ch the channel to potentially close.\n   * @param node the node to which the channel should be bound to.\n   */\n  private void potentiallyCloseLeakingChannel(final SocketChannel ch,\n    final MemcachedNode node) {\n    if (ch != null && !ch.isConnected() && !ch.isConnectionPending()) {\n      try {\n        ch.close();\n      } catch (IOException e) {\n        getLogger().error(\"Exception closing channel: %s\", node, e);\n      }\n    }\n  }\n\n  /**\n   * Returns the {@link NodeLocator} in use for this connection.\n   *\n   * @return  the current {@link NodeLocator}.\n   */\n  public NodeLocator getLocator() {\n    return locator;\n  }\n\n  /**\n   * Enqueue the given {@link Operation} with the used key.\n   *\n   * @param key the key to use.\n   * @param o the {@link Operation} to enqueue.\n   */\n  public void enqueueOperation(final String key, final Operation o) {\n    checkState();\n    StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n    addOperation(key, o);\n  }\n\n  /**\n   * Add an operation to a connection identified by the given key.\n   *\n   * If the {@link MemcachedNode} is active or the {@link FailureMode} is set\n   * to retry, the primary node will be used for that key. If the primary\n   * node is not available and the {@link FailureMode} cancel is used, the\n   * operation will be cancelled without further retry.\n   *\n   * For any other {@link FailureMode} mechanisms (Redistribute), another\n   * possible node is used (only if its active as well). If no other active\n   * node could be identified, the original primary node is used and retried.\n   *\n   * @param key the key the operation is operating upon.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final String key, final Operation o) {\n    MemcachedNode placeIn = null;\n    MemcachedNode primary = locator.getPrimary(key);\n\n    if (primary.isActive() || failureMode == FailureMode.Retry) {\n      placeIn = primary;\n    } else if (failureMode == FailureMode.Cancel) {\n      o.cancel();\n    } else {\n      Iterator<MemcachedNode> i = locator.getSequence(key);\n      while (placeIn == null && i.hasNext()) {\n        MemcachedNode n = i.next();\n        if (n.isActive()) {\n          placeIn = n;\n        }\n      }\n\n      if (placeIn == null) {\n        placeIn = primary;\n        this.getLogger().warn(\"Could not redistribute to another node, \"\n          + \"retrying primary node for %s.\", key);\n      }\n    }\n\n    assert o.isCancelled() || placeIn != null : \"No node found for key \" + key;\n    if (placeIn != null) {\n      addOperation(placeIn, o);\n    } else {\n      assert o.isCancelled() : \"No node found for \" + key + \" (and not \"\n        + \"immediately cancelled)\";\n    }\n  }\n\n  /**\n   * Insert an operation on the given node to the beginning of the queue.\n   *\n   * @param node the node where to insert the {@link Operation}.\n   * @param o the operation to insert.\n   */\n  public void insertOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.insertOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue an operation on the given node.\n   *\n   * @param node the node where to enqueue the {@link Operation}.\n   * @param o the operation to add.\n   */\n  protected void addOperation(final MemcachedNode node, final Operation o) {\n    o.setHandlingNode(node);\n    o.initialize();\n    node.addOp(o);\n    addedQueue.offer(node);\n    metrics.markMeter(OVERALL_REQUEST_METRIC);\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    getLogger().debug(\"Added %s to %s\", o, node);\n  }\n\n  /**\n   * Enqueue the given list of operations on each handling node.\n   *\n   * @param ops the operations for each node.\n   */\n  public void addOperations(final Map<MemcachedNode, Operation> ops) {\n    for (Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n      addOperation(me.getKey(), me.getValue());\n    }\n  }\n\n  /**\n   * Broadcast an operation to all nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n    return broadcastOperation(of, locator.getAll());\n  }\n\n  /**\n   * Broadcast an operation to a collection of nodes.\n   *\n   * @return a {@link CountDownLatch} that will be counted down when the\n   *         operations are complete.\n   */\n  public CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n    final Collection<MemcachedNode> nodes) {\n    final CountDownLatch latch = new CountDownLatch(nodes.size());\n\n    for (MemcachedNode node : nodes) {\n      getLogger().debug(\"broadcast Operation: node = \" + node);\n      Operation op = of.newOp(node, latch);\n      op.initialize();\n      node.addOp(op);\n      op.setHandlingNode(node);\n      addedQueue.offer(node);\n      metrics.markMeter(OVERALL_REQUEST_METRIC);\n    }\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    return latch;\n  }\n\n  /**\n   * Shut down all connections and do not accept further incoming ops.\n   */\n  public void shutdown() throws IOException {\n    shutDown = true;\n\n    Selector s = selector.wakeup();\n    assert s == selector : \"Wakeup returned the wrong selector.\";\n    for (MemcachedNode node : locator.getAll()) {\n      if (node.getChannel() != null) {\n        node.getChannel().close();\n        node.setSk(null);\n        if (node.getBytesRemainingToWrite() > 0) {\n          getLogger().warn(\"Shut down with %d bytes remaining to write\",\n              node.getBytesRemainingToWrite());\n        }\n        getLogger().debug(\"Shut down channel %s\", node.getChannel());\n      }\n    }\n    running = false;\n    selector.close();\n    getLogger().debug(\"Shut down selector %s\", selector);\n  }\n\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"{MemcachedConnection to\");\n    for (MemcachedNode qa : locator.getAll()) {\n      sb.append(\" \").append(qa.getSocketAddress());\n    }\n    sb.append(\"}\");\n    return sb.toString();\n  }\n\n  /**\n   * Construct a String containing information about all nodes and their state.\n   *\n   * @return a stringified representation of the connection status.\n   */\n  public String connectionsStatus() {\n    StringBuilder connStatus = new StringBuilder();\n    connStatus.append(\"Connection Status {\");\n    for (MemcachedNode node : locator.getAll()) {\n      connStatus\n        .append(\" \")\n        .append(node.getSocketAddress())\n        .append(\" active: \")\n        .append(node.isActive())\n        .append(\", authed: \")\n        .append(node.isAuthenticated())\n        .append(MessageFormat.format(\", last read: {0} ms ago\",\n          node.lastReadDelta()));\n    }\n    connStatus.append(\" }\");\n    return connStatus.toString();\n  }\n\n  /**\n   * Increase the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opTimedOut(final Operation op) {\n    MemcachedConnection.setTimeout(op, true);\n  }\n\n  /**\n   * Reset the timeout counter for the given handling node.\n   *\n   * @param op the operation to grab the node from.\n   */\n  public static void opSucceeded(final Operation op) {\n    MemcachedConnection.setTimeout(op, false);\n  }\n\n  /**\n   * Set the continous timeout on an operation.\n   *\n   * @param op the operation to use.\n   * @param isTimeout is timed out or not.\n   */\n  private static void setTimeout(final Operation op, final boolean isTimeout) {\n    Logger logger = LoggerFactory.getLogger(MemcachedConnection.class);\n\n    try {\n      if (op == null || op.isTimedOutUnsent()) {\n        return;\n      }\n\n      MemcachedNode node = op.getHandlingNode();\n      if (node == null) {\n        logger.warn(\"handling node for operation is not set\");\n      } else {\n        node.setContinuousTimeout(isTimeout);\n      }\n    } catch (Exception e) {\n      logger.error(e.getMessage());\n    }\n  }\n\n  /**\n   * Check to see if this connection is shutting down.\n   *\n   * @throws IllegalStateException when shutting down.\n   */\n  protected void checkState() {\n    if (shutDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    assert isAlive() : \"IO Thread is not running.\";\n  }\n\n  /**\n   * Handle IO as long as the application is running.\n   */\n  @Override\n  public void run() {\n    while (running) {\n      try {\n        handleIO();\n      } catch (IOException e) {\n        logRunException(e);\n      } catch (CancelledKeyException e) {\n        logRunException(e);\n      } catch (ClosedSelectorException e) {\n        logRunException(e);\n      } catch (IllegalStateException e) {\n        logRunException(e);\n      } catch (ConcurrentModificationException e) {\n        logRunException(e);\n      }\n    }\n    getLogger().info(\"Shut down memcached client\");\n  }\n\n  /**\n   * Log a exception to different levels depending on the state.\n   *\n   * Exceptions get logged at debug level when happening during shutdown, but\n   * at warning level when operating normally.\n   *\n   * @param e the exception to log.\n   */\n  private void logRunException(final Exception e) {\n    if (shutDown) {\n      getLogger().debug(\"Exception occurred during shutdown\", e);\n    } else {\n      getLogger().warn(\"Problem handling memcached IO\", e);\n    }\n  }\n\n}\n","lineNo":1252}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CancellationException;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.auth.AuthDescriptor;\nimport net.spy.memcached.auth.AuthThreadMonitor;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.internal.SingleElementInfiniteIterator;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetAndTouchOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.ops.TimedOutOperationStatus;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n * MemcachedClient c = new MemcachedClient(\n *    new InetSocketAddress(&quot;hostname&quot;, portNum));\n *\n * // Store a value (async) for one hour\n * c.set(&quot;someKey&quot;, 3600, someObject);\n * // Retrieve a value.\n * Object myObject = c.get(&quot;someKey&quot;);\n * <\/pre>\n *\n * <h2>Advanced Usage<\/h2>\n *\n * <p>\n * MemcachedClient may be processing a great deal of asynchronous messages or\n * possibly dealing with an unreachable memcached, which may delay processing.\n * If a memcached is disabled, for example, MemcachedConnection will continue to\n * attempt to reconnect and replay pending operations until it comes back up. To\n * prevent this from causing your application to hang, you can use one of the\n * asynchronous mechanisms to time out a request and cancel the operation to the\n * server.\n * <\/p>\n *\n * <pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n *\n * <p>Optionally, it is possible to activate a check that makes sure that\n * the node is alive and responding before running actual operations (even\n * before authentication. Only enable this if you are sure that you do not\n * run into issues during connection (some memcached services have problems\n * with it). You can enable it by setting the net.spy.verifyAliveOnConnect\n * System Property to \"true\".<\/p>\n */\npublic class MemcachedClient extends SpyObject implements MemcachedClientIF,\n    ConnectionObserver {\n\n  protected volatile boolean shuttingDown = false;\n\n  protected final long operationTimeout;\n\n  protected final MemcachedConnection mconn;\n\n  protected final OperationFactory opFact;\n\n  protected final Transcoder<Object> transcoder;\n\n  protected final TranscodeService tcService;\n\n  protected final AuthDescriptor authDescriptor;\n\n  protected final ConnectionFactory connFactory;\n\n  protected final AuthThreadMonitor authMonitor = new AuthThreadMonitor();\n\n  /**\n   * Get a memcache client operating on the specified memcached locations.\n   *\n   * @param ia the memcached locations\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(InetSocketAddress... ia) throws IOException {\n    this(new DefaultConnectionFactory(), Arrays.asList(ia));\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param addrs the socket addrs\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(List<InetSocketAddress> addrs) throws IOException {\n    this(new DefaultConnectionFactory(), addrs);\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param cf the connection factory to configure connections for this client\n   * @param addrs the socket addresses\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n    throws IOException {\n    if (cf == null) {\n      throw new NullPointerException(\"Connection factory required\");\n    }\n    if (addrs == null) {\n      throw new NullPointerException(\"Server list required\");\n    }\n    if (addrs.isEmpty()) {\n      throw new IllegalArgumentException(\"You must have at least one server to\"\n          + \" connect to\");\n    }\n    if (cf.getOperationTimeout() <= 0) {\n      throw new IllegalArgumentException(\"Operation timeout must be positive.\");\n    }\n    connFactory = cf;\n    tcService = new TranscodeService(cf.isDaemon());\n    transcoder = cf.getDefaultTranscoder();\n    opFact = cf.getOperationFactory();\n    assert opFact != null : \"Connection factory failed to make op factory\";\n    mconn = cf.createConnection(addrs);\n    assert mconn != null : \"Connection factory failed to make a connection\";\n    operationTimeout = cf.getOperationTimeout();\n    authDescriptor = cf.getAuthDescriptor();\n    if (authDescriptor != null) {\n      addObserver(this);\n    }\n  }\n\n  /**\n   * Get the addresses of available servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getAvailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get the addresses of unavailable servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getUnavailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (!node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get a read-only wrapper around the node locator wrapping this instance.\n   *\n   * @return this instance's NodeLocator\n   */\n  public NodeLocator getNodeLocator() {\n    return mconn.getLocator().getReadonlyCopy();\n  }\n\n  /**\n   * Get the default transcoder that's in use.\n   *\n   * @return this instance's Transcoder\n   */\n  public Transcoder<Object> getTranscoder() {\n    return transcoder;\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of) {\n    return broadcastOp(of, mconn.getLocator().getAll(), true);\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    return broadcastOp(of, nodes, true);\n  }\n\n  private CountDownLatch broadcastOp(BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes, boolean checkShuttingDown) {\n    if (checkShuttingDown && shuttingDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    return mconn.broadcastOperation(of, nodes);\n  }\n\n  private <T> OperationFuture<Boolean> asyncStore(StoreType storeType,\n      String key, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n    Operation op = opFact.store(storeType, key, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              rv.set(val.isSuccess(), val);\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  private OperationFuture<Boolean> asyncStore(StoreType storeType, String key,\n      int exp, Object value) {\n    return asyncStore(storeType, key, exp, value, transcoder);\n  }\n\n  private <T> OperationFuture<Boolean> asyncCat(ConcatenationType catType,\n      long cas, String key, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    Operation op = opFact.cat(catType, cas, key, co.getData(),\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus val) {\n            rv.set(val.isSuccess(), val);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Touch the given key to reset its expiration time with the default\n   * transcoder.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp) {\n    return touch(key, exp, transcoder);\n  }\n\n  /**\n   * Touch the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp,\n      final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n\n    Operation op = opFact.touch(key, exp, new OperationCallback() {\n      public void receivedStatus(OperationStatus status) {\n        rv.set(status.isSuccess(), status);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> append(long cas, String key, Object val) {\n    return append(cas, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> append(String key, Object val) {\n    return append(0, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> append(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, cas, key, val, tc);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> append(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, 0, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> prepend(long cas, String key, Object val) {\n    return prepend(cas, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> prepend(String key, Object val) {\n    return prepend(0, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> prepend(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> prepend(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, 0, key, val, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, T value, Transcoder<T> tc) {\n    return asyncCAS(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASResponse> rv =\n      new OperationFuture<CASResponse>(key, latch, operationTimeout);\n    Operation op = opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              if (val instanceof CASOperationStatus) {\n                rv.set(((CASOperationStatus) val).getCASResponse(), val);\n              } else if (val instanceof CancelledOperationStatus) {\n                getLogger().debug(\"CAS operation cancelled\");\n              } else if (val instanceof TimedOutOperationStatus) {\n                getLogger().debug(\"CAS operation timed out\");\n              } else {\n                throw new RuntimeException(\"Unhandled state: \" + val);\n              }\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, Object value) {\n    return asyncCAS(key, casId, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return cas(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, int exp, T value,\n      Transcoder<T> tc) {\n    CASResponse casr = null;\n    try {\n      OperationFuture<CASResponse> casOp = asyncCAS(key,\n              casId, exp, value, tc);\n      casr = casOp.get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n      return casr;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASResponse cas(String key, long casId, Object value) {\n    return cas(key, casId, value, transcoder);\n  }\n\n  /**\n   * Add an object to the cache iff it does not exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> add(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.add, key, exp, o, tc);\n  }\n\n  /**\n   * Add an object to the cache (using the default transcoder) iff it does not\n   * exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> add(String key, int exp, Object o) {\n    return asyncStore(StoreType.add, key, exp, o, transcoder);\n  }\n\n  /**\n   * Set an object in the cache regardless of any existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> set(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.set, key, exp, o, tc);\n  }\n\n  /**\n   * Set an object in the cache (using the default transcoder) regardless of any\n   * existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> set(String key, int exp, Object o) {\n    return asyncStore(StoreType.set, key, exp, o, transcoder);\n  }\n\n  /**\n   * Replace an object with the given value iff there is already a value for the\n   * given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> replace(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.replace, key, exp, o, tc);\n  }\n\n  /**\n   * Replace an object with the given value (transcoded with the default\n   * transcoder) iff there is already a value for the given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> replace(String key, int exp, Object o) {\n    return asyncStore(StoreType.replace, key, exp, o, transcoder);\n  }\n\n  /**\n   * Get the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> GetFuture<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final GetFuture<T> rv = new GetFuture<T>(latch, operationTimeout, key);\n    Operation op = opFact.get(key, new GetOperation.Callback() {\n      private Future<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize()));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the given key asynchronously and decode with the default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public GetFuture<Object> asyncGet(final String key) {\n    return asyncGet(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGets(final String key,\n      final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv =\n        new OperationFuture<CASValue<T>>(key, latch, operationTimeout);\n\n    Operation op = opFact.gets(key, new GetsOperation.Callback() {\n      private CASValue<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, long cas, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        assert cas > 0 : \"CAS was less than zero:  \" + cas;\n        val =\n            new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously and decode using the\n   * default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGets(final String key) {\n    return asyncGets(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n    try {\n      return asyncGets(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and reset its expiration.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> getAndTouch(String key, int exp, Transcoder<T> tc) {\n    try {\n      return asyncGetAndTouch(key, exp, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get a single key and reset its expiration using the default transcoder.\n   *\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> getAndTouch(String key, int exp) {\n    return getAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> gets(String key) {\n    return gets(key, transcoder);\n  }\n\n  /**\n   * Get with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> T get(String key, Transcoder<T> tc) {\n    try {\n      return asyncGet(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and decode using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Object get(String key) {\n    return get(key, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces keys.\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Iterator<Transcoder<T>> tcIter) {\n    final Map<String, Future<T>> m = new ConcurrentHashMap<String, Future<T>>();\n\n    // This map does not need to be a ConcurrentHashMap\n    // because it is fully populated when it is used and\n    // used only to read the transcoder for a key.\n    final Map<String, Transcoder<T>> tcMap =\n        new HashMap<String, Transcoder<T>>();\n\n    // Break the gets down into groups by key\n    final Map<MemcachedNode, Collection<String>> chunks =\n        new HashMap<MemcachedNode, Collection<String>>();\n    final NodeLocator locator = mconn.getLocator();\n\n    while (keyIter.hasNext() && tcIter.hasNext()) {\n      String key = keyIter.next();\n      tcMap.put(key, tcIter.next());\n      StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n      final MemcachedNode primaryNode = locator.getPrimary(key);\n      MemcachedNode node = null;\n      if (primaryNode.isActive()) {\n        node = primaryNode;\n      } else {\n        for (Iterator<MemcachedNode> i = locator.getSequence(key); node == null\n            && i.hasNext();) {\n          MemcachedNode n = i.next();\n          if (n.isActive()) {\n            node = n;\n          }\n        }\n        if (node == null) {\n          node = primaryNode;\n        }\n      }\n      assert node != null : \"Didn't find a node for \" + key;\n      Collection<String> ks = chunks.get(node);\n      if (ks == null) {\n        ks = new ArrayList<String>();\n        chunks.put(node, ks);\n      }\n      ks.add(key);\n    }\n\n    final CountDownLatch latch = new CountDownLatch(chunks.size());\n    final Collection<Operation> ops = new ArrayList<Operation>(chunks.size());\n    final BulkGetFuture<T> rv = new BulkGetFuture<T>(m, ops, latch);\n\n    GetOperation.Callback cb = new GetOperation.Callback() {\n      @SuppressWarnings(\"synthetic-access\")\n      public void receivedStatus(OperationStatus status) {\n        rv.setStatus(status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        Transcoder<T> tc = tcMap.get(k);\n        m.put(k,\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    };\n\n    // Now that we know how many servers it breaks down into, and the latch\n    // is all set up, convert all of these strings collections to operations\n    final Map<MemcachedNode, Operation> mops =\n        new HashMap<MemcachedNode, Operation>();\n\n    for (Map.Entry<MemcachedNode, Collection<String>> me : chunks.entrySet()) {\n      Operation op = opFact.get(me.getValue(), cb);\n      mops.put(me.getKey(), op);\n      ops.add(op);\n    }\n    assert mops.size() == chunks.size();\n    mconn.checkState();\n    mconn.addOperations(mops);\n    return rv;\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n          Iterator<Transcoder<T>> tcIter) {\n    return asyncGetBulk(keys.iterator(), tcIter);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator for the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keyIter,\n            new SingleElementInfiniteIterator<Transcoder<T>>(tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keys, new SingleElementInfiniteIterator<Transcoder<T>>(\n        tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keyIter Iterator that produces the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(\n         Iterator<String> keyIter) {\n    return asyncGetBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keys the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n    return asyncGetBulk(keys, transcoder);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n      String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n   *\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGetAndTouch(final String key,\n      final int exp) {\n    return asyncGetAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGetAndTouch(final String key,\n      final int exp, final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv = new OperationFuture<CASValue<T>>(\n        key, latch, operationTimeout);\n\n    Operation op = opFact.getAndTouch(key, exp,\n        new GetAndTouchOperation.Callback() {\n          private CASValue<T> val = null;\n\n          public void receivedStatus(OperationStatus status) {\n            rv.set(val, status);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void gotData(String k, int flags, long cas, byte[] data) {\n            assert k.equals(key) : \"Wrong key returned\";\n            assert cas > 0 : \"CAS was less than zero:  \" + cas;\n            val =\n                new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                    tc.getMaxSize())));\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    try {\n      return asyncGetBulk(keyIter, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted getting bulk values\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for bulk values\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for bulk values\", e);\n    }\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keyIter Iterator that produces the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Iterator<String> keyIter) {\n    return getBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keys the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return getBulk(keys.iterator(), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Collection<String> keys) {\n    return getBulk(keys, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n    return getBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(String... keys) {\n    return getBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the versions of all of the connected memcacheds.\n   *\n   * @return a Map of SocketAddress to String for connected servers\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, String> getVersions() {\n    final Map<SocketAddress, String> rv =\n        new ConcurrentHashMap<SocketAddress, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        return opFact.version(new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.put(sa, s.getMessage());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for versions\", e);\n    }\n    return rv;\n  }\n\n  /**\n   * Get all of the stats from all of the connections.\n   *\n   * @return a Map of a Map of stats replies by SocketAddress\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats() {\n    return getStats(null);\n  }\n\n  /**\n   * Get a set of stats from all connections.\n   *\n   * @param arg which stats to get\n   * @return a Map of the server SocketAddress to a map of String stat keys to\n   *         String stat values.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n    final Map<SocketAddress, Map<String, String>> rv =\n        new HashMap<SocketAddress, Map<String, String>>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        rv.put(sa, new HashMap<String, String>());\n        return opFact.stats(arg, new StatsOperation.Callback() {\n          public void gotStat(String name, String val) {\n            rv.get(sa).put(name, val);\n          }\n\n          @SuppressWarnings(\"synthetic-access\")\n          public void receivedStatus(OperationStatus status) {\n            if (!status.isSuccess()) {\n              getLogger().warn(\"Unsuccessful stat fetch: %s\", status);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for stats\", e);\n    }\n    return rv;\n  }\n\n  private long mutate(Mutator m, String key, long by, long def, int exp) {\n    final AtomicLong rv = new AtomicLong();\n    final CountDownLatch latch = new CountDownLatch(1);\n    mconn.enqueueOperation(key, opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n        public void receivedStatus(OperationStatus s) {\n          // XXX: Potential abstraction leak.\n          // The handling of incr/decr in the binary protocol\n          // Allows us to avoid string processing.\n          rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n        }\n\n        public void complete() {\n          latch.countDown();\n        }\n      }));\n    try {\n      if (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n        throw new OperationTimeoutException(\"Mutate operation timed out,\"\n            + \"unable to modify counter [\" + key + \"]\");\n      }\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted\", e);\n    }\n    getLogger().debug(\"Mutation returned %s\", rv);\n    return rv.get();\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by) {\n    return mutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by) {\n    return mutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, exp);\n  }\n\n  private long mutateWithDefault(Mutator t, String key, long by, long def,\n      int exp) {\n    long rv = mutate(t, key, by, def, exp);\n    // The ascii protocol doesn't support defaults, so I added them\n    // manually here.\n    if (rv == -1) {\n      Future<Boolean> f = asyncStore(StoreType.add, key, exp,\n          String.valueOf(def));\n      try {\n        if (f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n          rv = def;\n        } else {\n          rv = mutate(t, key, by, 0, exp);\n          assert rv != -1 : \"Failed to mutate or init value\";\n        }\n      } catch (InterruptedException e) {\n        throw new RuntimeException(\"Interrupted waiting for store\", e);\n      } catch (ExecutionException e) {\n        if(e.getCause() instanceof CancellationException) {\n          throw (CancellationException) e.getCause();\n        } else {\n          throw new RuntimeException(\"Failed waiting for store\", e);\n        }\n      } catch (TimeoutException e) {\n        throw new OperationTimeoutException(\"Timeout waiting to mutate or init\"\n            + \" value\", e);\n      }\n    }\n    return rv;\n  }\n\n  private OperationFuture<Long> asyncMutate(Mutator m, String key, long by,\n      long def, int exp) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Long> rv =\n        new OperationFuture<Long>(key, latch, operationTimeout);\n    Operation op = opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"), s);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    mconn.enqueueOperation(key, op);\n    rv.setOperation(op);\n    return rv;\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, long by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, int by) {\n    return asyncMutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, long by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, int by) {\n    return asyncMutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * <p>\n   * The hold argument specifies the amount of time in seconds (or Unix time\n   * until which) the client wishes the server to refuse \"add\" and \"replace\"\n   * commands with this key. For this amount of item, the item is put into a\n   * delete queue, which means that it won't possible to retrieve it by the\n   * \"get\" command, but \"add\" and \"replace\" command with this key will also fail\n   * (the \"set\" command will succeed, however). After the time passes, the item\n   * is finally deleted from server memory.\n   * <\/p>\n   *\n   * @param key the key to delete\n   * @param hold how long the key should be unavailable to add commands\n   *\n   * @return whether or not the operation was performed\n   * @deprecated Hold values are no longer honored.\n   */\n  @Deprecated\n  public OperationFuture<Boolean> delete(String key, int hold) {\n    return delete(key);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * @param key the key to delete\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> delete(String key) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    DeleteOperation op = opFact.delete(key, new DeleteOperation.Callback() {\n      public void receivedStatus(OperationStatus s) {\n        rv.set(s.isSuccess(), s);\n      }\n\n      public void gotData(long cas) {\n        rv.setCas(cas);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Flush all caches from all servers with a delay of application.\n   *\n   * @param delay the period of time to delay, in seconds\n   * @return whether or not the operation was accepted\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush(final int delay) {\n    final AtomicReference<Boolean> flushResult =\n        new AtomicReference<Boolean>(null);\n    final ConcurrentLinkedQueue<Operation> ops =\n        new ConcurrentLinkedQueue<Operation>();\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        Operation op = opFact.flush(delay, new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            flushResult.set(s.isSuccess());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n        ops.add(op);\n        return op;\n      }\n    });\n\n    return new OperationFuture<Boolean>(null, blatch, flushResult,\n        operationTimeout) {\n      @Override\n      public boolean cancel(boolean ign) {\n        boolean rv = false;\n        for (Operation op : ops) {\n          op.cancel();\n          rv |= op.getState() == OperationState.WRITE_QUEUED;\n        }\n        return rv;\n      }\n\n      @Override\n      public Boolean get(long duration, TimeUnit units)\n        throws InterruptedException, TimeoutException, ExecutionException {\n        status = new OperationStatus(true, \"OK\");\n        return super.get(duration, units);\n      }\n\n      @Override\n      public boolean isCancelled() {\n        boolean rv = false;\n        for (Operation op : ops) {\n          rv |= op.isCancelled();\n        }\n        return rv;\n      }\n\n      @Override\n      public boolean isDone() {\n        boolean rv = true;\n        for (Operation op : ops) {\n          rv &= op.getState() == OperationState.COMPLETE;\n        }\n        return rv || isCancelled();\n      }\n    };\n  }\n\n  /**\n   * Flush all caches from all servers immediately.\n   *\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush() {\n    return flush(-1);\n  }\n\n  public Set<String> listSaslMechanisms() {\n    final ConcurrentMap<String, String> rv =\n        new ConcurrentHashMap<String, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(MemcachedNode n, final CountDownLatch latch) {\n        return opFact.saslMechs(new OperationCallback() {\n          public void receivedStatus(OperationStatus status) {\n            for (String s : status.getMessage().split(\" \")) {\n              rv.put(s, s);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n\n    try {\n      blatch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    return rv.keySet();\n  }\n\n  /**\n   * Shut down immediately.\n   */\n  public void shutdown() {\n    shutdown(-1, TimeUnit.MILLISECONDS);\n  }\n\n  /**\n   * Shut down this client gracefully.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the shutdown request\n   */\n  public boolean shutdown(long timeout, TimeUnit unit) {\n    // Guard against double shutdowns (bug 8).\n    if (shuttingDown) {\n      getLogger().info(\"Suppressing duplicate attempt to shut down\");\n      return false;\n    }\n    shuttingDown = true;\n    String baseName = mconn.getName();\n    mconn.setName(baseName + \" - SHUTTING DOWN\");\n    boolean rv = true;\n    try {\n      // Conditionally wait\n      if (timeout > 0) {\n        mconn.setName(baseName + \" - SHUTTING DOWN (waiting)\");\n        rv = waitForQueues(timeout, unit);\n      }\n    } finally {\n      // But always begin the shutdown sequence\n      try {\n        mconn.setName(baseName + \" - SHUTTING DOWN (telling client)\");\n        mconn.shutdown();\n        mconn.setName(baseName + \" - SHUTTING DOWN (informed client)\");\n        tcService.shutdown();\n      } catch (IOException e) {\n        getLogger().warn(\"exception while shutting down\", e);\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Wait for the queues to die down.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the request for the wait\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public boolean waitForQueues(long timeout, TimeUnit unit) {\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        return opFact.noop(new OperationCallback() {\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void receivedStatus(OperationStatus s) {\n            // Nothing special when receiving status, only\n            // necessary to complete the interface\n          }\n        });\n      }\n    }, mconn.getLocator().getAll(), false);\n    try {\n      // XXX: Perhaps IllegalStateException should be caught here\n      // and the check retried.\n      return blatch.await(timeout, unit);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for queues\", e);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * If connections are already established, your observer will be called with\n   * the address and -1.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer was added.\n   */\n  public boolean addObserver(ConnectionObserver obs) {\n    boolean rv = mconn.addObserver(obs);\n    if (rv) {\n      for (MemcachedNode node : mconn.getLocator().getAll()) {\n        if (node.isActive()) {\n          obs.connectionEstablished(node.getSocketAddress(), -1);\n        }\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer existed, but no longer does\n   */\n  public boolean removeObserver(ConnectionObserver obs) {\n    return mconn.removeObserver(obs);\n  }\n\n  public void connectionEstablished(SocketAddress sa, int reconnectCount) {\n    if (authDescriptor != null) {\n      if (authDescriptor.authThresholdReached()) {\n        this.shutdown();\n      }\n      authMonitor.authConnection(mconn, opFact, authDescriptor, findNode(sa));\n    }\n  }\n\n  private MemcachedNode findNode(SocketAddress sa) {\n    MemcachedNode node = null;\n    for (MemcachedNode n : mconn.getLocator().getAll()) {\n      if (n.getSocketAddress().equals(sa)) {\n        node = n;\n      }\n    }\n    assert node != null : \"Couldn't find node connected to \" + sa;\n    return node;\n  }\n\n  public void connectionLost(SocketAddress sa) {\n    // Don't care.\n  }\n\n  @Override\n  public String toString() {\n    return connFactory.toString();\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CancellationException;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.auth.AuthDescriptor;\nimport net.spy.memcached.auth.AuthThreadMonitor;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.internal.SingleElementInfiniteIterator;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetAndTouchOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.ops.TimedOutOperationStatus;\nimport net.spy.memcached.protocol.binary.BinaryOperationFactory;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n * MemcachedClient c = new MemcachedClient(\n *    new InetSocketAddress(&quot;hostname&quot;, portNum));\n *\n * // Store a value (async) for one hour\n * c.set(&quot;someKey&quot;, 3600, someObject);\n * // Retrieve a value.\n * Object myObject = c.get(&quot;someKey&quot;);\n * <\/pre>\n *\n * <h2>Advanced Usage<\/h2>\n *\n * <p>\n * MemcachedClient may be processing a great deal of asynchronous messages or\n * possibly dealing with an unreachable memcached, which may delay processing.\n * If a memcached is disabled, for example, MemcachedConnection will continue to\n * attempt to reconnect and replay pending operations until it comes back up. To\n * prevent this from causing your application to hang, you can use one of the\n * asynchronous mechanisms to time out a request and cancel the operation to the\n * server.\n * <\/p>\n *\n * <pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n *\n * <p>Optionally, it is possible to activate a check that makes sure that\n * the node is alive and responding before running actual operations (even\n * before authentication. Only enable this if you are sure that you do not\n * run into issues during connection (some memcached services have problems\n * with it). You can enable it by setting the net.spy.verifyAliveOnConnect\n * System Property to \"true\".<\/p>\n */\npublic class MemcachedClient extends SpyObject implements MemcachedClientIF,\n    ConnectionObserver {\n\n  protected volatile boolean shuttingDown = false;\n\n  protected final long operationTimeout;\n\n  protected final MemcachedConnection mconn;\n\n  protected final OperationFactory opFact;\n\n  protected final Transcoder<Object> transcoder;\n\n  protected final TranscodeService tcService;\n\n  protected final AuthDescriptor authDescriptor;\n\n  protected final ConnectionFactory connFactory;\n\n  protected final AuthThreadMonitor authMonitor = new AuthThreadMonitor();\n\n  /**\n   * Get a memcache client operating on the specified memcached locations.\n   *\n   * @param ia the memcached locations\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(InetSocketAddress... ia) throws IOException {\n    this(new DefaultConnectionFactory(), Arrays.asList(ia));\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param addrs the socket addrs\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(List<InetSocketAddress> addrs) throws IOException {\n    this(new DefaultConnectionFactory(), addrs);\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param cf the connection factory to configure connections for this client\n   * @param addrs the socket addresses\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n    throws IOException {\n    if (cf == null) {\n      throw new NullPointerException(\"Connection factory required\");\n    }\n    if (addrs == null) {\n      throw new NullPointerException(\"Server list required\");\n    }\n    if (addrs.isEmpty()) {\n      throw new IllegalArgumentException(\"You must have at least one server to\"\n          + \" connect to\");\n    }\n    if (cf.getOperationTimeout() <= 0) {\n      throw new IllegalArgumentException(\"Operation timeout must be positive.\");\n    }\n    connFactory = cf;\n    tcService = new TranscodeService(cf.isDaemon());\n    transcoder = cf.getDefaultTranscoder();\n    opFact = cf.getOperationFactory();\n    assert opFact != null : \"Connection factory failed to make op factory\";\n    mconn = cf.createConnection(addrs);\n    assert mconn != null : \"Connection factory failed to make a connection\";\n    operationTimeout = cf.getOperationTimeout();\n    authDescriptor = cf.getAuthDescriptor();\n    if (authDescriptor != null) {\n      addObserver(this);\n    }\n  }\n\n  /**\n   * Get the addresses of available servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getAvailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get the addresses of unavailable servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getUnavailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (!node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get a read-only wrapper around the node locator wrapping this instance.\n   *\n   * @return this instance's NodeLocator\n   */\n  public NodeLocator getNodeLocator() {\n    return mconn.getLocator().getReadonlyCopy();\n  }\n\n  /**\n   * Get the default transcoder that's in use.\n   *\n   * @return this instance's Transcoder\n   */\n  public Transcoder<Object> getTranscoder() {\n    return transcoder;\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of) {\n    return broadcastOp(of, mconn.getLocator().getAll(), true);\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    return broadcastOp(of, nodes, true);\n  }\n\n  private CountDownLatch broadcastOp(BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes, boolean checkShuttingDown) {\n    if (checkShuttingDown && shuttingDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    return mconn.broadcastOperation(of, nodes);\n  }\n\n  private <T> OperationFuture<Boolean> asyncStore(StoreType storeType,\n      String key, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n    Operation op = opFact.store(storeType, key, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              rv.set(val.isSuccess(), val);\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  private OperationFuture<Boolean> asyncStore(StoreType storeType, String key,\n      int exp, Object value) {\n    return asyncStore(storeType, key, exp, value, transcoder);\n  }\n\n  private <T> OperationFuture<Boolean> asyncCat(ConcatenationType catType,\n      long cas, String key, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    Operation op = opFact.cat(catType, cas, key, co.getData(),\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus val) {\n            rv.set(val.isSuccess(), val);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Touch the given key to reset its expiration time with the default\n   * transcoder.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp) {\n    return touch(key, exp, transcoder);\n  }\n\n  /**\n   * Touch the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp,\n      final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n\n    Operation op = opFact.touch(key, exp, new OperationCallback() {\n      public void receivedStatus(OperationStatus status) {\n        rv.set(status.isSuccess(), status);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> append(long cas, String key, Object val) {\n    return append(cas, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> append(String key, Object val) {\n    return append(0, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> append(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, cas, key, val, tc);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> append(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, 0, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> prepend(long cas, String key, Object val) {\n    return prepend(cas, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> prepend(String key, Object val) {\n    return prepend(0, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * If 0 is passed in as the CAS identifier, it will override the value\n   * on the server without performing the CAS check.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> prepend(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> prepend(String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, 0, key, val, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, T value, Transcoder<T> tc) {\n    return asyncCAS(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASResponse> rv =\n      new OperationFuture<CASResponse>(key, latch, operationTimeout);\n    Operation op = opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              if (val instanceof CASOperationStatus) {\n                rv.set(((CASOperationStatus) val).getCASResponse(), val);\n              } else if (val instanceof CancelledOperationStatus) {\n                getLogger().debug(\"CAS operation cancelled\");\n              } else if (val instanceof TimedOutOperationStatus) {\n                getLogger().debug(\"CAS operation timed out\");\n              } else {\n                throw new RuntimeException(\"Unhandled state: \" + val);\n              }\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, Object value) {\n    return asyncCAS(key, casId, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return cas(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, int exp, T value,\n      Transcoder<T> tc) {\n    CASResponse casr = null;\n    try {\n      OperationFuture<CASResponse> casOp = asyncCAS(key,\n              casId, exp, value, tc);\n      casr = casOp.get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n      return casr;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASResponse cas(String key, long casId, Object value) {\n    return cas(key, casId, value, transcoder);\n  }\n\n  /**\n   * Add an object to the cache iff it does not exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> add(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.add, key, exp, o, tc);\n  }\n\n  /**\n   * Add an object to the cache (using the default transcoder) iff it does not\n   * exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> add(String key, int exp, Object o) {\n    return asyncStore(StoreType.add, key, exp, o, transcoder);\n  }\n\n  /**\n   * Set an object in the cache regardless of any existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> set(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.set, key, exp, o, tc);\n  }\n\n  /**\n   * Set an object in the cache (using the default transcoder) regardless of any\n   * existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> set(String key, int exp, Object o) {\n    return asyncStore(StoreType.set, key, exp, o, transcoder);\n  }\n\n  /**\n   * Replace an object with the given value iff there is already a value for the\n   * given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> replace(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.replace, key, exp, o, tc);\n  }\n\n  /**\n   * Replace an object with the given value (transcoded with the default\n   * transcoder) iff there is already a value for the given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> replace(String key, int exp, Object o) {\n    return asyncStore(StoreType.replace, key, exp, o, transcoder);\n  }\n\n  /**\n   * Get the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> GetFuture<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final GetFuture<T> rv = new GetFuture<T>(latch, operationTimeout, key);\n    Operation op = opFact.get(key, new GetOperation.Callback() {\n      private Future<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize()));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the given key asynchronously and decode with the default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public GetFuture<Object> asyncGet(final String key) {\n    return asyncGet(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGets(final String key,\n      final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv =\n        new OperationFuture<CASValue<T>>(key, latch, operationTimeout);\n\n    Operation op = opFact.gets(key, new GetsOperation.Callback() {\n      private CASValue<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, long cas, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        assert cas > 0 : \"CAS was less than zero:  \" + cas;\n        val =\n            new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously and decode using the\n   * default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGets(final String key) {\n    return asyncGets(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n    try {\n      return asyncGets(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and reset its expiration.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> getAndTouch(String key, int exp, Transcoder<T> tc) {\n    try {\n      return asyncGetAndTouch(key, exp, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get a single key and reset its expiration using the default transcoder.\n   *\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> getAndTouch(String key, int exp) {\n    return getAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> gets(String key) {\n    return gets(key, transcoder);\n  }\n\n  /**\n   * Get with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> T get(String key, Transcoder<T> tc) {\n    try {\n      return asyncGet(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for value\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and decode using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Object get(String key) {\n    return get(key, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces keys.\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Iterator<Transcoder<T>> tcIter) {\n    final Map<String, Future<T>> m = new ConcurrentHashMap<String, Future<T>>();\n\n    // This map does not need to be a ConcurrentHashMap\n    // because it is fully populated when it is used and\n    // used only to read the transcoder for a key.\n    final Map<String, Transcoder<T>> tcMap =\n        new HashMap<String, Transcoder<T>>();\n\n    // Break the gets down into groups by key\n    final Map<MemcachedNode, Collection<String>> chunks =\n        new HashMap<MemcachedNode, Collection<String>>();\n    final NodeLocator locator = mconn.getLocator();\n\n    while (keyIter.hasNext() && tcIter.hasNext()) {\n      String key = keyIter.next();\n      tcMap.put(key, tcIter.next());\n      StringUtils.validateKey(key, opFact instanceof BinaryOperationFactory);\n      final MemcachedNode primaryNode = locator.getPrimary(key);\n      MemcachedNode node = null;\n      if (primaryNode.isActive()) {\n        node = primaryNode;\n      } else {\n        for (Iterator<MemcachedNode> i = locator.getSequence(key); node == null\n            && i.hasNext();) {\n          MemcachedNode n = i.next();\n          if (n.isActive()) {\n            node = n;\n          }\n        }\n        if (node == null) {\n          node = primaryNode;\n        }\n      }\n      assert node != null : \"Didn't find a node for \" + key;\n      Collection<String> ks = chunks.get(node);\n      if (ks == null) {\n        ks = new ArrayList<String>();\n        chunks.put(node, ks);\n      }\n      ks.add(key);\n    }\n\n    final CountDownLatch latch = new CountDownLatch(chunks.size());\n    final Collection<Operation> ops = new ArrayList<Operation>(chunks.size());\n    final BulkGetFuture<T> rv = new BulkGetFuture<T>(m, ops, latch);\n\n    GetOperation.Callback cb = new GetOperation.Callback() {\n      @SuppressWarnings(\"synthetic-access\")\n      public void receivedStatus(OperationStatus status) {\n        rv.setStatus(status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        Transcoder<T> tc = tcMap.get(k);\n        m.put(k,\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    };\n\n    // Now that we know how many servers it breaks down into, and the latch\n    // is all set up, convert all of these strings collections to operations\n    final Map<MemcachedNode, Operation> mops =\n        new HashMap<MemcachedNode, Operation>();\n\n    for (Map.Entry<MemcachedNode, Collection<String>> me : chunks.entrySet()) {\n      Operation op = opFact.get(me.getValue(), cb);\n      mops.put(me.getKey(), op);\n      ops.add(op);\n    }\n    assert mops.size() == chunks.size();\n    mconn.checkState();\n    mconn.addOperations(mops);\n    return rv;\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n          Iterator<Transcoder<T>> tcIter) {\n    return asyncGetBulk(keys.iterator(), tcIter);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator for the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keyIter,\n            new SingleElementInfiniteIterator<Transcoder<T>>(tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keys, new SingleElementInfiniteIterator<Transcoder<T>>(\n        tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keyIter Iterator that produces the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(\n         Iterator<String> keyIter) {\n    return asyncGetBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keys the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n    return asyncGetBulk(keys, transcoder);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n      String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n   *\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGetAndTouch(final String key,\n      final int exp) {\n    return asyncGetAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGetAndTouch(final String key,\n      final int exp, final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv = new OperationFuture<CASValue<T>>(\n        key, latch, operationTimeout);\n\n    Operation op = opFact.getAndTouch(key, exp,\n        new GetAndTouchOperation.Callback() {\n          private CASValue<T> val = null;\n\n          public void receivedStatus(OperationStatus status) {\n            rv.set(val, status);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void gotData(String k, int flags, long cas, byte[] data) {\n            assert k.equals(key) : \"Wrong key returned\";\n            assert cas > 0 : \"CAS was less than zero:  \" + cas;\n            val =\n                new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                    tc.getMaxSize())));\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws CancellationException if operation was canceled\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    try {\n      return asyncGetBulk(keyIter, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted getting bulk values\", e);\n    } catch (ExecutionException e) {\n      if(e.getCause() instanceof CancellationException) {\n        throw (CancellationException) e.getCause();\n      } else {\n        throw new RuntimeException(\"Exception waiting for bulk values\", e);\n      }\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for bulk values\", e);\n    }\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keyIter Iterator that produces the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Iterator<String> keyIter) {\n    return getBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keys the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return getBulk(keys.iterator(), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Collection<String> keys) {\n    return getBulk(keys, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n    return getBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(String... keys) {\n    return getBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the versions of all of the connected memcacheds.\n   *\n   * @return a Map of SocketAddress to String for connected servers\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, String> getVersions() {\n    final Map<SocketAddress, String> rv =\n        new ConcurrentHashMap<SocketAddress, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        return opFact.version(new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.put(sa, s.getMessage());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for versions\", e);\n    }\n    return rv;\n  }\n\n  /**\n   * Get all of the stats from all of the connections.\n   *\n   * @return a Map of a Map of stats replies by SocketAddress\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats() {\n    return getStats(null);\n  }\n\n  /**\n   * Get a set of stats from all connections.\n   *\n   * @param arg which stats to get\n   * @return a Map of the server SocketAddress to a map of String stat keys to\n   *         String stat values.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n    final Map<SocketAddress, Map<String, String>> rv =\n        new HashMap<SocketAddress, Map<String, String>>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        rv.put(sa, new HashMap<String, String>());\n        return opFact.stats(arg, new StatsOperation.Callback() {\n          public void gotStat(String name, String val) {\n            rv.get(sa).put(name, val);\n          }\n\n          @SuppressWarnings(\"synthetic-access\")\n          public void receivedStatus(OperationStatus status) {\n            if (!status.isSuccess()) {\n              getLogger().warn(\"Unsuccessful stat fetch: %s\", status);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for stats\", e);\n    }\n    return rv;\n  }\n\n  private long mutate(Mutator m, String key, long by, long def, int exp) {\n    final AtomicLong rv = new AtomicLong();\n    final CountDownLatch latch = new CountDownLatch(1);\n    mconn.enqueueOperation(key, opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n        public void receivedStatus(OperationStatus s) {\n          // XXX: Potential abstraction leak.\n          // The handling of incr/decr in the binary protocol\n          // Allows us to avoid string processing.\n          rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n        }\n\n        public void complete() {\n          latch.countDown();\n        }\n      }));\n    try {\n      if (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n        throw new OperationTimeoutException(\"Mutate operation timed out,\"\n            + \"unable to modify counter [\" + key + \"]\");\n      }\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted\", e);\n    }\n    getLogger().debug(\"Mutation returned %s\", rv);\n    return rv.get();\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by) {\n    return mutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by) {\n    return mutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, exp);\n  }\n\n  private long mutateWithDefault(Mutator t, String key, long by, long def,\n      int exp) {\n    long rv = mutate(t, key, by, def, exp);\n    // The ascii protocol doesn't support defaults, so I added them\n    // manually here.\n    if (rv == -1) {\n      Future<Boolean> f = asyncStore(StoreType.add, key, exp,\n          String.valueOf(def));\n      try {\n        if (f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n          rv = def;\n        } else {\n          rv = mutate(t, key, by, 0, exp);\n          assert rv != -1 : \"Failed to mutate or init value\";\n        }\n      } catch (InterruptedException e) {\n        throw new RuntimeException(\"Interrupted waiting for store\", e);\n      } catch (ExecutionException e) {\n        if(e.getCause() instanceof CancellationException) {\n          throw (CancellationException) e.getCause();\n        } else {\n          throw new RuntimeException(\"Failed waiting for store\", e);\n        }\n      } catch (TimeoutException e) {\n        throw new OperationTimeoutException(\"Timeout waiting to mutate or init\"\n            + \" value\", e);\n      }\n    }\n    return rv;\n  }\n\n  private OperationFuture<Long> asyncMutate(Mutator m, String key, long by,\n      long def, int exp) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Long> rv =\n        new OperationFuture<Long>(key, latch, operationTimeout);\n    Operation op = opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"), s);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    mconn.enqueueOperation(key, op);\n    rv.setOperation(op);\n    return rv;\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, long by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, int by) {\n    return asyncMutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, long by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, int by) {\n    return asyncMutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * <p>\n   * The hold argument specifies the amount of time in seconds (or Unix time\n   * until which) the client wishes the server to refuse \"add\" and \"replace\"\n   * commands with this key. For this amount of item, the item is put into a\n   * delete queue, which means that it won't possible to retrieve it by the\n   * \"get\" command, but \"add\" and \"replace\" command with this key will also fail\n   * (the \"set\" command will succeed, however). After the time passes, the item\n   * is finally deleted from server memory.\n   * <\/p>\n   *\n   * @param key the key to delete\n   * @param hold how long the key should be unavailable to add commands\n   *\n   * @return whether or not the operation was performed\n   * @deprecated Hold values are no longer honored.\n   */\n  @Deprecated\n  public OperationFuture<Boolean> delete(String key, int hold) {\n    return delete(key);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * @param key the key to delete\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> delete(String key) {\n    return delete(key, (long) 0);\n  }\n\n  /**\n   * Delete the given key from the cache of the given CAS value applies.\n   *\n   * @param key the key to delete\n   * @param cas the CAS value to apply.\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> delete(String key, long cas) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n\n    DeleteOperation.Callback callback = new DeleteOperation.Callback() {\n      public void receivedStatus(OperationStatus s) {\n        rv.set(s.isSuccess(), s);\n      }\n\n      public void gotData(long cas) {\n        rv.setCas(cas);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    };\n\n    DeleteOperation op = null;\n    if(cas == 0) {\n      op = opFact.delete(key, callback);\n    } else {\n      op = opFact.delete(key, cas, callback);\n    }\n\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Flush all caches from all servers with a delay of application.\n   *\n   * @param delay the period of time to delay, in seconds\n   * @return whether or not the operation was accepted\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush(final int delay) {\n    final AtomicReference<Boolean> flushResult =\n        new AtomicReference<Boolean>(null);\n    final ConcurrentLinkedQueue<Operation> ops =\n        new ConcurrentLinkedQueue<Operation>();\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        Operation op = opFact.flush(delay, new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            flushResult.set(s.isSuccess());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n        ops.add(op);\n        return op;\n      }\n    });\n\n    return new OperationFuture<Boolean>(null, blatch, flushResult,\n        operationTimeout) {\n      @Override\n      public boolean cancel(boolean ign) {\n        boolean rv = false;\n        for (Operation op : ops) {\n          op.cancel();\n          rv |= op.getState() == OperationState.WRITE_QUEUED;\n        }\n        return rv;\n      }\n\n      @Override\n      public Boolean get(long duration, TimeUnit units)\n        throws InterruptedException, TimeoutException, ExecutionException {\n        status = new OperationStatus(true, \"OK\");\n        return super.get(duration, units);\n      }\n\n      @Override\n      public boolean isCancelled() {\n        boolean rv = false;\n        for (Operation op : ops) {\n          rv |= op.isCancelled();\n        }\n        return rv;\n      }\n\n      @Override\n      public boolean isDone() {\n        boolean rv = true;\n        for (Operation op : ops) {\n          rv &= op.getState() == OperationState.COMPLETE;\n        }\n        return rv || isCancelled();\n      }\n    };\n  }\n\n  /**\n   * Flush all caches from all servers immediately.\n   *\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush() {\n    return flush(-1);\n  }\n\n  public Set<String> listSaslMechanisms() {\n    final ConcurrentMap<String, String> rv =\n        new ConcurrentHashMap<String, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(MemcachedNode n, final CountDownLatch latch) {\n        return opFact.saslMechs(new OperationCallback() {\n          public void receivedStatus(OperationStatus status) {\n            for (String s : status.getMessage().split(\" \")) {\n              rv.put(s, s);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n\n    try {\n      blatch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    return rv.keySet();\n  }\n\n  /**\n   * Shut down immediately.\n   */\n  public void shutdown() {\n    shutdown(-1, TimeUnit.MILLISECONDS);\n  }\n\n  /**\n   * Shut down this client gracefully.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the shutdown request\n   */\n  public boolean shutdown(long timeout, TimeUnit unit) {\n    // Guard against double shutdowns (bug 8).\n    if (shuttingDown) {\n      getLogger().info(\"Suppressing duplicate attempt to shut down\");\n      return false;\n    }\n    shuttingDown = true;\n    String baseName = mconn.getName();\n    mconn.setName(baseName + \" - SHUTTING DOWN\");\n    boolean rv = true;\n    try {\n      // Conditionally wait\n      if (timeout > 0) {\n        mconn.setName(baseName + \" - SHUTTING DOWN (waiting)\");\n        rv = waitForQueues(timeout, unit);\n      }\n    } finally {\n      // But always begin the shutdown sequence\n      try {\n        mconn.setName(baseName + \" - SHUTTING DOWN (telling client)\");\n        mconn.shutdown();\n        mconn.setName(baseName + \" - SHUTTING DOWN (informed client)\");\n        tcService.shutdown();\n      } catch (IOException e) {\n        getLogger().warn(\"exception while shutting down\", e);\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Wait for the queues to die down.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the request for the wait\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public boolean waitForQueues(long timeout, TimeUnit unit) {\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        return opFact.noop(new OperationCallback() {\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void receivedStatus(OperationStatus s) {\n            // Nothing special when receiving status, only\n            // necessary to complete the interface\n          }\n        });\n      }\n    }, mconn.getLocator().getAll(), false);\n    try {\n      // XXX: Perhaps IllegalStateException should be caught here\n      // and the check retried.\n      return blatch.await(timeout, unit);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for queues\", e);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * If connections are already established, your observer will be called with\n   * the address and -1.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer was added.\n   */\n  public boolean addObserver(ConnectionObserver obs) {\n    boolean rv = mconn.addObserver(obs);\n    if (rv) {\n      for (MemcachedNode node : mconn.getLocator().getAll()) {\n        if (node.isActive()) {\n          obs.connectionEstablished(node.getSocketAddress(), -1);\n        }\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer existed, but no longer does\n   */\n  public boolean removeObserver(ConnectionObserver obs) {\n    return mconn.removeObserver(obs);\n  }\n\n  public void connectionEstablished(SocketAddress sa, int reconnectCount) {\n    if (authDescriptor != null) {\n      if (authDescriptor.authThresholdReached()) {\n        this.shutdown();\n      }\n      authMonitor.authConnection(mconn, opFact, authDescriptor, findNode(sa));\n    }\n  }\n\n  private MemcachedNode findNode(SocketAddress sa) {\n    MemcachedNode node = null;\n    for (MemcachedNode n : mconn.getLocator().getAll()) {\n      if (n.getSocketAddress().equals(sa)) {\n        node = n;\n      }\n    }\n    assert node != null : \"Couldn't find node connected to \" + sa;\n    return node;\n  }\n\n  public void connectionLost(SocketAddress sa) {\n    // Don't care.\n  }\n\n  @Override\n  public String toString() {\n    return connFactory.toString();\n  }\n}\n","lineNo":2006}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.auth.AuthDescriptor;\nimport net.spy.memcached.auth.AuthThreadMonitor;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.internal.SingleElementInfiniteIterator;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetAndTouchOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.ops.TimedOutOperationStatus;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n * MemcachedClient c = new MemcachedClient(\n *    new InetSocketAddress(&quot;hostname&quot;, portNum));\n *\n * // Store a value (async) for one hour\n * c.set(&quot;someKey&quot;, 3600, someObject);\n * // Retrieve a value.\n * Object myObject = c.get(&quot;someKey&quot;);\n * <\/pre>\n *\n * <h2>Advanced Usage<\/h2>\n *\n * <p>\n * MemcachedClient may be processing a great deal of asynchronous messages or\n * possibly dealing with an unreachable memcached, which may delay processing.\n * If a memcached is disabled, for example, MemcachedConnection will continue to\n * attempt to reconnect and replay pending operations until it comes back up. To\n * prevent this from causing your application to hang, you can use one of the\n * asynchronous mechanisms to time out a request and cancel the operation to the\n * server.\n * <\/p>\n *\n * <pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n */\npublic class MemcachedClient extends SpyObject implements MemcachedClientIF,\n    ConnectionObserver {\n\n  protected volatile boolean shuttingDown = false;\n\n  protected final long operationTimeout;\n\n  protected final MemcachedConnection mconn;\n\n  protected final OperationFactory opFact;\n\n  protected final Transcoder<Object> transcoder;\n\n  protected final TranscodeService tcService;\n\n  protected final AuthDescriptor authDescriptor;\n\n  protected final ConnectionFactory connFactory;\n\n  protected final AuthThreadMonitor authMonitor = new AuthThreadMonitor();\n\n  /**\n   * Get a memcache client operating on the specified memcached locations.\n   *\n   * @param ia the memcached locations\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(InetSocketAddress... ia) throws IOException {\n    this(new DefaultConnectionFactory(), Arrays.asList(ia));\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param addrs the socket addrs\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(List<InetSocketAddress> addrs) throws IOException {\n    this(new DefaultConnectionFactory(), addrs);\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param cf the connection factory to configure connections for this client\n   * @param addrs the socket addresses\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n    throws IOException {\n    if (cf == null) {\n      throw new NullPointerException(\"Connection factory required\");\n    }\n    if (addrs == null) {\n      throw new NullPointerException(\"Server list required\");\n    }\n    if (addrs.isEmpty()) {\n      throw new IllegalArgumentException(\"You must have at least one server to\"\n          + \" connect to\");\n    }\n    if (cf.getOperationTimeout() <= 0) {\n      throw new IllegalArgumentException(\"Operation timeout must be positive.\");\n    }\n    connFactory = cf;\n    tcService = new TranscodeService(cf.isDaemon());\n    transcoder = cf.getDefaultTranscoder();\n    opFact = cf.getOperationFactory();\n    assert opFact != null : \"Connection factory failed to make op factory\";\n    mconn = cf.createConnection(addrs);\n    assert mconn != null : \"Connection factory failed to make a connection\";\n    operationTimeout = cf.getOperationTimeout();\n    authDescriptor = cf.getAuthDescriptor();\n    if (authDescriptor != null) {\n      addObserver(this);\n    }\n  }\n\n  /**\n   * Get the addresses of available servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getAvailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get the addresses of unavailable servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getUnavailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (!node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get a read-only wrapper around the node locator wrapping this instance.\n   *\n   * @return this instance's NodeLocator\n   */\n  public NodeLocator getNodeLocator() {\n    return mconn.getLocator().getReadonlyCopy();\n  }\n\n  /**\n   * Get the default transcoder that's in use.\n   *\n   * @return this instance's Transcoder\n   */\n  public Transcoder<Object> getTranscoder() {\n    return transcoder;\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of) {\n    return broadcastOp(of, mconn.getLocator().getAll(), true);\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    return broadcastOp(of, nodes, true);\n  }\n\n  private CountDownLatch broadcastOp(BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes, boolean checkShuttingDown) {\n    if (checkShuttingDown && shuttingDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    return mconn.broadcastOperation(of, nodes);\n  }\n\n  private <T> OperationFuture<Boolean> asyncStore(StoreType storeType,\n      String key, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n    Operation op = opFact.store(storeType, key, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              rv.set(val.isSuccess(), val);\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  private OperationFuture<Boolean> asyncStore(StoreType storeType, String key,\n      int exp, Object value) {\n    return asyncStore(storeType, key, exp, value, transcoder);\n  }\n\n  private <T> OperationFuture<Boolean> asyncCat(ConcatenationType catType,\n      long cas, String key, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    Operation op = opFact.cat(catType, cas, key, co.getData(),\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus val) {\n            rv.set(val.isSuccess(), val);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Touch the given key to reset its expiration time with the default\n   * transcoder.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp) {\n    return touch(key, exp, transcoder);\n  }\n\n  /**\n   * Touch the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp,\n      final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n\n    Operation op = opFact.touch(key, exp, new OperationCallback() {\n      public void receivedStatus(OperationStatus status) {\n        rv.set(status.isSuccess(), status);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> append(long cas, String key, Object val) {\n    return append(cas, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> append(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, cas, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> prepend(long cas, String key, Object val) {\n    return prepend(cas, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> prepend(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return asyncCAS(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Future<CASResponse> asyncCAS(String key, long casId, int exp,\n      T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASResponse> rv =\n      new OperationFuture<CASResponse>(key, latch, operationTimeout);\n    Operation op = opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              if (val instanceof CASOperationStatus) {\n                rv.set(((CASOperationStatus) val).getCASResponse(), val);\n              } else if (val instanceof CancelledOperationStatus) {\n                getLogger().debug(\"CAS operation cancelled\");\n              } else if (val instanceof TimedOutOperationStatus) {\n                getLogger().debug(\"CAS operation timed out\");\n              } else {\n                throw new RuntimeException(\"Unhandled state: \" + val);\n              }\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n    return asyncCAS(key, casId, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return cas(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, int exp, T value,\n      Transcoder<T> tc) {\n    try {\n      return asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASResponse cas(String key, long casId, Object value) {\n    return cas(key, casId, value, transcoder);\n  }\n\n  /**\n   * Add an object to the cache iff it does not exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> add(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.add, key, exp, o, tc);\n  }\n\n  /**\n   * Add an object to the cache (using the default transcoder) iff it does not\n   * exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> add(String key, int exp, Object o) {\n    return asyncStore(StoreType.add, key, exp, o, transcoder);\n  }\n\n  /**\n   * Set an object in the cache regardless of any existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> set(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.set, key, exp, o, tc);\n  }\n\n  /**\n   * Set an object in the cache (using the default transcoder) regardless of any\n   * existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> set(String key, int exp, Object o) {\n    return asyncStore(StoreType.set, key, exp, o, transcoder);\n  }\n\n  /**\n   * Replace an object with the given value iff there is already a value for the\n   * given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> replace(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.replace, key, exp, o, tc);\n  }\n\n  /**\n   * Replace an object with the given value (transcoded with the default\n   * transcoder) iff there is already a value for the given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> replace(String key, int exp, Object o) {\n    return asyncStore(StoreType.replace, key, exp, o, transcoder);\n  }\n\n  /**\n   * Get the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> GetFuture<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final GetFuture<T> rv = new GetFuture<T>(latch, operationTimeout, key);\n    Operation op = opFact.get(key, new GetOperation.Callback() {\n      private Future<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize()));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the given key asynchronously and decode with the default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public GetFuture<Object> asyncGet(final String key) {\n    return asyncGet(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGets(final String key,\n      final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv =\n        new OperationFuture<CASValue<T>>(key, latch, operationTimeout);\n\n    Operation op = opFact.gets(key, new GetsOperation.Callback() {\n      private CASValue<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, long cas, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        assert cas > 0 : \"CAS was less than zero:  \" + cas;\n        val =\n            new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously and decode using the\n   * default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGets(final String key) {\n    return asyncGets(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n    try {\n      return asyncGets(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and reset its expiration.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> getAndTouch(String key, int exp, Transcoder<T> tc) {\n    try {\n      return asyncGetAndTouch(key, exp, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get a single key and reset its expiration using the default transcoder.\n   *\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> getAndTouch(String key, int exp) {\n    return getAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> gets(String key) {\n    return gets(key, transcoder);\n  }\n\n  /**\n   * Get with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> T get(String key, Transcoder<T> tc) {\n    try {\n      return asyncGet(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and decode using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Object get(String key) {\n    return get(key, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces keys.\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Iterator<Transcoder<T>> tcIter) {\n    final Map<String, Future<T>> m = new ConcurrentHashMap<String, Future<T>>();\n\n    // This map does not need to be a ConcurrentHashMap\n    // because it is fully populated when it is used and\n    // used only to read the transcoder for a key.\n    final Map<String, Transcoder<T>> tcMap =\n        new HashMap<String, Transcoder<T>>();\n\n    // Break the gets down into groups by key\n    final Map<MemcachedNode, Collection<String>> chunks =\n        new HashMap<MemcachedNode, Collection<String>>();\n    final NodeLocator locator = mconn.getLocator();\n\n    while (keyIter.hasNext() && tcIter.hasNext()) {\n      String key = keyIter.next();\n      tcMap.put(key, tcIter.next());\n      StringUtils.validateKey(key);\n      final MemcachedNode primaryNode = locator.getPrimary(key);\n      MemcachedNode node = null;\n      if (primaryNode.isActive()) {\n        node = primaryNode;\n      } else {\n        for (Iterator<MemcachedNode> i = locator.getSequence(key); node == null\n            && i.hasNext();) {\n          MemcachedNode n = i.next();\n          if (n.isActive()) {\n            node = n;\n          }\n        }\n        if (node == null) {\n          node = primaryNode;\n        }\n      }\n      assert node != null : \"Didn't find a node for \" + key;\n      Collection<String> ks = chunks.get(node);\n      if (ks == null) {\n        ks = new ArrayList<String>();\n        chunks.put(node, ks);\n      }\n      ks.add(key);\n    }\n\n    final CountDownLatch latch = new CountDownLatch(chunks.size());\n    final Collection<Operation> ops = new ArrayList<Operation>(chunks.size());\n    final BulkGetFuture<T> rv = new BulkGetFuture<T>(m, ops, latch);\n\n    GetOperation.Callback cb = new GetOperation.Callback() {\n      @SuppressWarnings(\"synthetic-access\")\n      public void receivedStatus(OperationStatus status) {\n        rv.setStatus(status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        Transcoder<T> tc = tcMap.get(k);\n        m.put(k,\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    };\n\n    // Now that we know how many servers it breaks down into, and the latch\n    // is all set up, convert all of these strings collections to operations\n    final Map<MemcachedNode, Operation> mops =\n        new HashMap<MemcachedNode, Operation>();\n\n    for (Map.Entry<MemcachedNode, Collection<String>> me : chunks.entrySet()) {\n      Operation op = opFact.get(me.getValue(), cb);\n      mops.put(me.getKey(), op);\n      ops.add(op);\n    }\n    assert mops.size() == chunks.size();\n    mconn.checkState();\n    mconn.addOperations(mops);\n    return rv;\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n          Iterator<Transcoder<T>> tcIter) {\n    return asyncGetBulk(keys.iterator(), tcIter);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator for the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keyIter,\n            new SingleElementInfiniteIterator<Transcoder<T>>(tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keys, new SingleElementInfiniteIterator<Transcoder<T>>(\n        tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keyIter Iterator that produces the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(\n         Iterator<String> keyIter) {\n    return asyncGetBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keys the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n    return asyncGetBulk(keys, transcoder);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n      String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n   *\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGetAndTouch(final String key,\n      final int exp) {\n    return asyncGetAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGetAndTouch(final String key,\n      final int exp, final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv = new OperationFuture<CASValue<T>>(\n        key, latch, operationTimeout);\n\n    Operation op = opFact.getAndTouch(key, exp,\n        new GetAndTouchOperation.Callback() {\n          private CASValue<T> val = null;\n\n          public void receivedStatus(OperationStatus status) {\n            rv.set(val, status);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void gotData(String k, int flags, long cas, byte[] data) {\n            assert k.equals(key) : \"Wrong key returned\";\n            assert cas > 0 : \"CAS was less than zero:  \" + cas;\n            val =\n                new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                    tc.getMaxSize())));\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    try {\n      return asyncGetBulk(keyIter, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted getting bulk values\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Failed getting bulk values\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for bulkvalues\", e);\n    }\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keyIter Iterator that produces the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Iterator<String> keyIter) {\n    return getBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keys the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return getBulk(keys.iterator(), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Collection<String> keys) {\n    return getBulk(keys, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n    return getBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(String... keys) {\n    return getBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the versions of all of the connected memcacheds.\n   *\n   * @return a Map of SocketAddress to String for connected servers\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, String> getVersions() {\n    final Map<SocketAddress, String> rv =\n        new ConcurrentHashMap<SocketAddress, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        return opFact.version(new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.put(sa, s.getMessage());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for versions\", e);\n    }\n    return rv;\n  }\n\n  /**\n   * Get all of the stats from all of the connections.\n   *\n   * @return a Map of a Map of stats replies by SocketAddress\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats() {\n    return getStats(null);\n  }\n\n  /**\n   * Get a set of stats from all connections.\n   *\n   * @param arg which stats to get\n   * @return a Map of the server SocketAddress to a map of String stat keys to\n   *         String stat values.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n    final Map<SocketAddress, Map<String, String>> rv =\n        new HashMap<SocketAddress, Map<String, String>>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        rv.put(sa, new HashMap<String, String>());\n        return opFact.stats(arg, new StatsOperation.Callback() {\n          public void gotStat(String name, String val) {\n            rv.get(sa).put(name, val);\n          }\n\n          @SuppressWarnings(\"synthetic-access\")\n          public void receivedStatus(OperationStatus status) {\n            if (!status.isSuccess()) {\n              getLogger().warn(\"Unsuccessful stat fetch: %s\", status);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for stats\", e);\n    }\n    return rv;\n  }\n\n  private long mutate(Mutator m, String key, long by, long def, int exp) {\n    final AtomicLong rv = new AtomicLong();\n    final CountDownLatch latch = new CountDownLatch(1);\n    mconn.enqueueOperation(key, opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n        public void receivedStatus(OperationStatus s) {\n          // XXX: Potential abstraction leak.\n          // The handling of incr/decr in the binary protocol\n          // Allows us to avoid string processing.\n          rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n        }\n\n        public void complete() {\n          latch.countDown();\n        }\n      }));\n    try {\n      if (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n        throw new OperationTimeoutException(\"Mutate operation timed out,\"\n            + \"unable to modify counter [\" + key + \"]\");\n      }\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted\", e);\n    }\n    getLogger().debug(\"Mutation returned %s\", rv);\n    return rv.get();\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by) {\n    return mutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by) {\n    return mutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, exp);\n  }\n\n  private long mutateWithDefault(Mutator t, String key, long by, long def,\n      int exp) {\n    long rv = mutate(t, key, by, def, exp);\n    // The ascii protocol doesn't support defaults, so I added them\n    // manually here.\n    if (rv == -1) {\n      Future<Boolean> f = asyncStore(StoreType.add, key, exp,\n          String.valueOf(def));\n      try {\n        if (f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n          rv = def;\n        } else {\n          rv = mutate(t, key, by, 0, exp);\n          assert rv != -1 : \"Failed to mutate or init value\";\n        }\n      } catch (InterruptedException e) {\n        throw new RuntimeException(\"Interrupted waiting for store\", e);\n      } catch (ExecutionException e) {\n        throw new RuntimeException(\"Failed waiting for store\", e);\n      } catch (TimeoutException e) {\n        throw new OperationTimeoutException(\"Timeout waiting to mutate or init\"\n            + \" value\", e);\n      }\n    }\n    return rv;\n  }\n\n  private OperationFuture<Long> asyncMutate(Mutator m, String key, long by,\n      long def, int exp) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Long> rv =\n        new OperationFuture<Long>(key, latch, operationTimeout);\n    Operation op = opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"), s);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    mconn.enqueueOperation(key, op);\n    rv.setOperation(op);\n    return rv;\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, long by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, int by) {\n    return asyncMutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, long by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, int by) {\n    return asyncMutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * <p>\n   * The hold argument specifies the amount of time in seconds (or Unix time\n   * until which) the client wishes the server to refuse \"add\" and \"replace\"\n   * commands with this key. For this amount of item, the item is put into a\n   * delete queue, which means that it won't possible to retrieve it by the\n   * \"get\" command, but \"add\" and \"replace\" command with this key will also fail\n   * (the \"set\" command will succeed, however). After the time passes, the item\n   * is finally deleted from server memory.\n   * <\/p>\n   *\n   * @param key the key to delete\n   * @param hold how long the key should be unavailable to add commands\n   *\n   * @return whether or not the operation was performed\n   * @deprecated Hold values are no longer honored.\n   */\n  @Deprecated\n  public OperationFuture<Boolean> delete(String key, int hold) {\n    return delete(key);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * @param key the key to delete\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> delete(String key) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    DeleteOperation op = opFact.delete(key, new OperationCallback() {\n      public void receivedStatus(OperationStatus s) {\n        rv.set(s.isSuccess(), s);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Flush all caches from all servers with a delay of application.\n   *\n   * @param delay the period of time to delay, in seconds\n   * @return whether or not the operation was accepted\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush(final int delay) {\n    final AtomicReference<Boolean> flushResult =\n        new AtomicReference<Boolean>(null);\n    final ConcurrentLinkedQueue<Operation> ops =\n        new ConcurrentLinkedQueue<Operation>();\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        Operation op = opFact.flush(delay, new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            flushResult.set(s.isSuccess());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n        ops.add(op);\n        return op;\n      }\n    });\n\n    return new OperationFuture<Boolean>(null, blatch, flushResult,\n        operationTimeout) {\n      @Override\n      public boolean cancel(boolean ign) {\n        boolean rv = false;\n        for (Operation op : ops) {\n          op.cancel();\n          rv |= op.getState() == OperationState.WRITE_QUEUED;\n        }\n        return rv;\n      }\n\n      @Override\n      public Boolean get(long duration, TimeUnit units)\n        throws InterruptedException, TimeoutException, ExecutionException {\n        status = new OperationStatus(true, \"OK\");\n        return super.get(duration, units);\n      }\n\n      @Override\n      public boolean isCancelled() {\n        boolean rv = false;\n        for (Operation op : ops) {\n          rv |= op.isCancelled();\n        }\n        return rv;\n      }\n\n      @Override\n      public boolean isDone() {\n        boolean rv = true;\n        for (Operation op : ops) {\n          rv &= op.getState() == OperationState.COMPLETE;\n        }\n        return rv || isCancelled();\n      }\n    };\n  }\n\n  /**\n   * Flush all caches from all servers immediately.\n   *\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush() {\n    return flush(-1);\n  }\n\n  public Set<String> listSaslMechanisms() {\n    final ConcurrentMap<String, String> rv =\n        new ConcurrentHashMap<String, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(MemcachedNode n, final CountDownLatch latch) {\n        return opFact.saslMechs(new OperationCallback() {\n          public void receivedStatus(OperationStatus status) {\n            for (String s : status.getMessage().split(\" \")) {\n              rv.put(s, s);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n\n    try {\n      blatch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    return rv.keySet();\n  }\n\n  /**\n   * Shut down immediately.\n   */\n  public void shutdown() {\n    shutdown(-1, TimeUnit.MILLISECONDS);\n  }\n\n  /**\n   * Shut down this client gracefully.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the shutdown request\n   */\n  public boolean shutdown(long timeout, TimeUnit unit) {\n    // Guard against double shutdowns (bug 8).\n    if (shuttingDown) {\n      getLogger().info(\"Suppressing duplicate attempt to shut down\");\n      return false;\n    }\n    shuttingDown = true;\n    String baseName = mconn.getName();\n    mconn.setName(baseName + \" - SHUTTING DOWN\");\n    boolean rv = true;\n    try {\n      // Conditionally wait\n      if (timeout > 0) {\n        mconn.setName(baseName + \" - SHUTTING DOWN (waiting)\");\n        rv = waitForQueues(timeout, unit);\n      }\n    } finally {\n      // But always begin the shutdown sequence\n      try {\n        mconn.setName(baseName + \" - SHUTTING DOWN (telling client)\");\n        mconn.shutdown();\n        mconn.setName(baseName + \" - SHUTTING DOWN (informed client)\");\n        tcService.shutdown();\n      } catch (IOException e) {\n        getLogger().warn(\"exception while shutting down\", e);\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Wait for the queues to die down.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the request for the wait\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public boolean waitForQueues(long timeout, TimeUnit unit) {\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        return opFact.noop(new OperationCallback() {\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void receivedStatus(OperationStatus s) {\n            // Nothing special when receiving status, only\n            // necessary to complete the interface\n          }\n        });\n      }\n    }, mconn.getLocator().getAll(), false);\n    try {\n      // XXX: Perhaps IllegalStateException should be caught here\n      // and the check retried.\n      return blatch.await(timeout, unit);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for queues\", e);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * If connections are already established, your observer will be called with\n   * the address and -1.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer was added.\n   */\n  public boolean addObserver(ConnectionObserver obs) {\n    boolean rv = mconn.addObserver(obs);\n    if (rv) {\n      for (MemcachedNode node : mconn.getLocator().getAll()) {\n        if (node.isActive()) {\n          obs.connectionEstablished(node.getSocketAddress(), -1);\n        }\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer existed, but no longer does\n   */\n  public boolean removeObserver(ConnectionObserver obs) {\n    return mconn.removeObserver(obs);\n  }\n\n  public void connectionEstablished(SocketAddress sa, int reconnectCount) {\n    if (authDescriptor != null) {\n      if (authDescriptor.authThresholdReached()) {\n        this.shutdown();\n      }\n      authMonitor.authConnection(mconn, opFact, authDescriptor, findNode(sa));\n    }\n  }\n\n  private MemcachedNode findNode(SocketAddress sa) {\n    MemcachedNode node = null;\n    for (MemcachedNode n : mconn.getLocator().getAll()) {\n      if (n.getSocketAddress().equals(sa)) {\n        node = n;\n      }\n    }\n    assert node != null : \"Couldn't find node connected to \" + sa;\n    return node;\n  }\n\n  public void connectionLost(SocketAddress sa) {\n    // Don't care.\n  }\n\n  @Override\n  public String toString() {\n    return connFactory.toString();\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.auth.AuthDescriptor;\nimport net.spy.memcached.auth.AuthThreadMonitor;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.internal.SingleElementInfiniteIterator;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetAndTouchOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.ops.TimedOutOperationStatus;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\nimport net.spy.memcached.util.StringUtils;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n * MemcachedClient c = new MemcachedClient(\n *    new InetSocketAddress(&quot;hostname&quot;, portNum));\n *\n * // Store a value (async) for one hour\n * c.set(&quot;someKey&quot;, 3600, someObject);\n * // Retrieve a value.\n * Object myObject = c.get(&quot;someKey&quot;);\n * <\/pre>\n *\n * <h2>Advanced Usage<\/h2>\n *\n * <p>\n * MemcachedClient may be processing a great deal of asynchronous messages or\n * possibly dealing with an unreachable memcached, which may delay processing.\n * If a memcached is disabled, for example, MemcachedConnection will continue to\n * attempt to reconnect and replay pending operations until it comes back up. To\n * prevent this from causing your application to hang, you can use one of the\n * asynchronous mechanisms to time out a request and cancel the operation to the\n * server.\n * <\/p>\n *\n * <pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n */\npublic class MemcachedClient extends SpyObject implements MemcachedClientIF,\n    ConnectionObserver {\n\n  protected volatile boolean shuttingDown = false;\n\n  protected final long operationTimeout;\n\n  protected final MemcachedConnection mconn;\n\n  protected final OperationFactory opFact;\n\n  protected final Transcoder<Object> transcoder;\n\n  protected final TranscodeService tcService;\n\n  protected final AuthDescriptor authDescriptor;\n\n  protected final ConnectionFactory connFactory;\n\n  protected final AuthThreadMonitor authMonitor = new AuthThreadMonitor();\n\n  /**\n   * Get a memcache client operating on the specified memcached locations.\n   *\n   * @param ia the memcached locations\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(InetSocketAddress... ia) throws IOException {\n    this(new DefaultConnectionFactory(), Arrays.asList(ia));\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param addrs the socket addrs\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(List<InetSocketAddress> addrs) throws IOException {\n    this(new DefaultConnectionFactory(), addrs);\n  }\n\n  /**\n   * Get a memcache client over the specified memcached locations.\n   *\n   * @param cf the connection factory to configure connections for this client\n   * @param addrs the socket addresses\n   * @throws IOException if connections cannot be established\n   */\n  public MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n    throws IOException {\n    if (cf == null) {\n      throw new NullPointerException(\"Connection factory required\");\n    }\n    if (addrs == null) {\n      throw new NullPointerException(\"Server list required\");\n    }\n    if (addrs.isEmpty()) {\n      throw new IllegalArgumentException(\"You must have at least one server to\"\n          + \" connect to\");\n    }\n    if (cf.getOperationTimeout() <= 0) {\n      throw new IllegalArgumentException(\"Operation timeout must be positive.\");\n    }\n    connFactory = cf;\n    tcService = new TranscodeService(cf.isDaemon());\n    transcoder = cf.getDefaultTranscoder();\n    opFact = cf.getOperationFactory();\n    assert opFact != null : \"Connection factory failed to make op factory\";\n    mconn = cf.createConnection(addrs);\n    assert mconn != null : \"Connection factory failed to make a connection\";\n    operationTimeout = cf.getOperationTimeout();\n    authDescriptor = cf.getAuthDescriptor();\n    if (authDescriptor != null) {\n      addObserver(this);\n    }\n  }\n\n  /**\n   * Get the addresses of available servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getAvailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get the addresses of unavailable servers.\n   *\n   * <p>\n   * This is based on a snapshot in time so shouldn't be considered completely\n   * accurate, but is a useful for getting a feel for what's working and what's\n   * not working.\n   * <\/p>\n   *\n   * @return point-in-time view of currently available servers\n   */\n  public Collection<SocketAddress> getUnavailableServers() {\n    ArrayList<SocketAddress> rv = new ArrayList<SocketAddress>();\n    for (MemcachedNode node : mconn.getLocator().getAll()) {\n      if (!node.isActive()) {\n        rv.add(node.getSocketAddress());\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Get a read-only wrapper around the node locator wrapping this instance.\n   *\n   * @return this instance's NodeLocator\n   */\n  public NodeLocator getNodeLocator() {\n    return mconn.getLocator().getReadonlyCopy();\n  }\n\n  /**\n   * Get the default transcoder that's in use.\n   *\n   * @return this instance's Transcoder\n   */\n  public Transcoder<Object> getTranscoder() {\n    return transcoder;\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of) {\n    return broadcastOp(of, mconn.getLocator().getAll(), true);\n  }\n\n  public CountDownLatch broadcastOp(final BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes) {\n    return broadcastOp(of, nodes, true);\n  }\n\n  private CountDownLatch broadcastOp(BroadcastOpFactory of,\n      Collection<MemcachedNode> nodes, boolean checkShuttingDown) {\n    if (checkShuttingDown && shuttingDown) {\n      throw new IllegalStateException(\"Shutting down\");\n    }\n    return mconn.broadcastOperation(of, nodes);\n  }\n\n  private <T> OperationFuture<Boolean> asyncStore(StoreType storeType,\n      String key, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n    Operation op = opFact.store(storeType, key, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              rv.set(val.isSuccess(), val);\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  private OperationFuture<Boolean> asyncStore(StoreType storeType, String key,\n      int exp, Object value) {\n    return asyncStore(storeType, key, exp, value, transcoder);\n  }\n\n  private <T> OperationFuture<Boolean> asyncCat(ConcatenationType catType,\n      long cas, String key, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    Operation op = opFact.cat(catType, cas, key, co.getData(),\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus val) {\n            rv.set(val.isSuccess(), val);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Touch the given key to reset its expiration time with the default\n   * transcoder.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp) {\n    return touch(key, exp, transcoder);\n  }\n\n  /**\n   * Touch the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of whether or not the\n   *         fetch succeeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> touch(final String key, final int exp,\n      final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv =\n        new OperationFuture<Boolean>(key, latch, operationTimeout);\n\n    Operation op = opFact.touch(key, exp, new OperationCallback() {\n      public void receivedStatus(OperationStatus status) {\n        rv.set(status.isSuccess(), status);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @return a future indicating success, false if there was no change to the\n   *         value\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> append(long cas, String key, Object val) {\n    return append(cas, key, val, transcoder);\n  }\n\n  /**\n   * Append to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be appended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> append(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.append, cas, key, val, tc);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> prepend(long cas, String key, Object val) {\n    return prepend(cas, key, val, transcoder);\n  }\n\n  /**\n   * Prepend to an existing value in the cache.\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * @param <T>\n   * @param cas cas identifier (ignored in the ascii protocol)\n   * @param key the key to whose value will be prepended\n   * @param val the value to append\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future indicating success\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> prepend(long cas, String key, T val,\n      Transcoder<T> tc) {\n    return asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, T value, Transcoder<T> tc) {\n    return asyncCAS(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Asynchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, int exp, T value, Transcoder<T> tc) {\n    CachedData co = tc.encode(value);\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASResponse> rv =\n      new OperationFuture<CASResponse>(key, latch, operationTimeout);\n    Operation op = opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n        co.getData(), new StoreOperation.Callback() {\n            public void receivedStatus(OperationStatus val) {\n              if (val instanceof CASOperationStatus) {\n                rv.set(((CASOperationStatus) val).getCASResponse(), val);\n              } else if (val instanceof CancelledOperationStatus) {\n                getLogger().debug(\"CAS operation cancelled\");\n              } else if (val instanceof TimedOutOperationStatus) {\n                getLogger().debug(\"CAS operation timed out\");\n              } else {\n                throw new RuntimeException(\"Unhandled state: \" + val);\n              }\n            }\n            public void gotData(String key, long cas) {\n              rv.setCas(cas);\n            }\n            public void complete() {\n              latch.countDown();\n            }\n          });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Asynchronous CAS operation using the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a future that will indicate the status of the CAS\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASResponse>\n  asyncCAS(String key, long casId, Object value) {\n    return asyncCAS(key, casId, value, transcoder);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, T value,\n      Transcoder<T> tc) {\n    return cas(key, casId, 0, value, tc);\n  }\n\n  /**\n   * Perform a synchronous CAS operation.\n   *\n   * @param <T>\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param exp the expiration of this object\n   * @param value the new value\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASResponse cas(String key, long casId, int exp, T value,\n      Transcoder<T> tc) {\n    CASResponse casr = null;\n    try {\n      OperationFuture<CASResponse> casOp = asyncCAS(key,\n              casId, exp, value, tc);\n      casr = casOp.get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n      return casr;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Perform a synchronous CAS operation with the default transcoder.\n   *\n   * @param key the key\n   * @param casId the CAS identifier (from a gets operation)\n   * @param value the new value\n   * @return a CASResponse\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASResponse cas(String key, long casId, Object value) {\n    return cas(key, casId, value, transcoder);\n  }\n\n  /**\n   * Add an object to the cache iff it does not exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> add(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.add, key, exp, o, tc);\n  }\n\n  /**\n   * Add an object to the cache (using the default transcoder) iff it does not\n   * exist already.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> add(String key, int exp, Object o) {\n    return asyncStore(StoreType.add, key, exp, o, transcoder);\n  }\n\n  /**\n   * Set an object in the cache regardless of any existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> set(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.set, key, exp, o, tc);\n  }\n\n  /**\n   * Set an object in the cache (using the default transcoder) regardless of any\n   * existing value.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> set(String key, int exp, Object o) {\n    return asyncStore(StoreType.set, key, exp, o, transcoder);\n  }\n\n  /**\n   * Replace an object with the given value iff there is already a value for the\n   * given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param <T>\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @param tc the transcoder to serialize and unserialize the value\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<Boolean> replace(String key, int exp, T o,\n      Transcoder<T> tc) {\n    return asyncStore(StoreType.replace, key, exp, o, tc);\n  }\n\n  /**\n   * Replace an object with the given value (transcoded with the default\n   * transcoder) iff there is already a value for the given key.\n   *\n   * <p>\n   * The <code>exp<\/code> value is passed along to memcached exactly as given,\n   * and will be processed per the memcached protocol specification:\n   * <\/p>\n   *\n   * <p>\n   * Note that the return will be false any time a mutation has not occurred.\n   * <\/p>\n   *\n   * <blockquote>\n   * <p>\n   * The actual value sent may either be Unix time (number of seconds since\n   * January 1, 1970, as a 32-bit value), or a number of seconds starting from\n   * current time. In the latter case, this number of seconds may not exceed\n   * 60*60*24*30 (number of seconds in 30 days); if the number sent by a client\n   * is larger than that, the server will consider it to be real Unix time value\n   * rather than an offset from current time.\n   * <\/p>\n   * <\/blockquote>\n   *\n   * @param key the key under which this object should be added.\n   * @param exp the expiration of this object\n   * @param o the object to store\n   * @return a future representing the processing of this operation\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> replace(String key, int exp, Object o) {\n    return asyncStore(StoreType.replace, key, exp, o, transcoder);\n  }\n\n  /**\n   * Get the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> GetFuture<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final GetFuture<T> rv = new GetFuture<T>(latch, operationTimeout, key);\n    Operation op = opFact.get(key, new GetOperation.Callback() {\n      private Future<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        val =\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize()));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the given key asynchronously and decode with the default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public GetFuture<Object> asyncGet(final String key) {\n    return asyncGet(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously.\n   *\n   * @param <T>\n   * @param key the key to fetch\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGets(final String key,\n      final Transcoder<T> tc) {\n\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv =\n        new OperationFuture<CASValue<T>>(key, latch, operationTimeout);\n\n    Operation op = opFact.gets(key, new GetsOperation.Callback() {\n      private CASValue<T> val = null;\n\n      public void receivedStatus(OperationStatus status) {\n        rv.set(val, status);\n      }\n\n      public void gotData(String k, int flags, long cas, byte[] data) {\n        assert key.equals(k) : \"Wrong key returned\";\n        assert cas > 0 : \"CAS was less than zero:  \" + cas;\n        val =\n            new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Gets (with CAS support) the given key asynchronously and decode using the\n   * default transcoder.\n   *\n   * @param key the key to fetch\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGets(final String key) {\n    return asyncGets(key, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if global operation timeout is exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n    try {\n      return asyncGets(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and reset its expiration.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> CASValue<T> getAndTouch(String key, int exp, Transcoder<T> tc) {\n    try {\n      return asyncGetAndTouch(key, exp, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get a single key and reset its expiration using the default transcoder.\n   *\n   * @param key the key to get\n   * @param exp the new expiration for the key\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> getAndTouch(String key, int exp) {\n    return getAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Gets (with CAS support) with a single key using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache and CAS id (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public CASValue<Object> gets(String key) {\n    return gets(key, transcoder);\n  }\n\n  /**\n   * Get with a single key.\n   *\n   * @param <T>\n   * @param key the key to get\n   * @param tc the transcoder to serialize and unserialize value\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> T get(String key, Transcoder<T> tc) {\n    try {\n      return asyncGet(key, tc).get(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for value\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Exception waiting for value\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for value\", e);\n    }\n  }\n\n  /**\n   * Get with a single key and decode using the default transcoder.\n   *\n   * @param key the key to get\n   * @return the result from the cache (null if there is none)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Object get(String key) {\n    return get(key, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces keys.\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Iterator<Transcoder<T>> tcIter) {\n    final Map<String, Future<T>> m = new ConcurrentHashMap<String, Future<T>>();\n\n    // This map does not need to be a ConcurrentHashMap\n    // because it is fully populated when it is used and\n    // used only to read the transcoder for a key.\n    final Map<String, Transcoder<T>> tcMap =\n        new HashMap<String, Transcoder<T>>();\n\n    // Break the gets down into groups by key\n    final Map<MemcachedNode, Collection<String>> chunks =\n        new HashMap<MemcachedNode, Collection<String>>();\n    final NodeLocator locator = mconn.getLocator();\n\n    while (keyIter.hasNext() && tcIter.hasNext()) {\n      String key = keyIter.next();\n      tcMap.put(key, tcIter.next());\n      StringUtils.validateKey(key);\n      final MemcachedNode primaryNode = locator.getPrimary(key);\n      MemcachedNode node = null;\n      if (primaryNode.isActive()) {\n        node = primaryNode;\n      } else {\n        for (Iterator<MemcachedNode> i = locator.getSequence(key); node == null\n            && i.hasNext();) {\n          MemcachedNode n = i.next();\n          if (n.isActive()) {\n            node = n;\n          }\n        }\n        if (node == null) {\n          node = primaryNode;\n        }\n      }\n      assert node != null : \"Didn't find a node for \" + key;\n      Collection<String> ks = chunks.get(node);\n      if (ks == null) {\n        ks = new ArrayList<String>();\n        chunks.put(node, ks);\n      }\n      ks.add(key);\n    }\n\n    final CountDownLatch latch = new CountDownLatch(chunks.size());\n    final Collection<Operation> ops = new ArrayList<Operation>(chunks.size());\n    final BulkGetFuture<T> rv = new BulkGetFuture<T>(m, ops, latch);\n\n    GetOperation.Callback cb = new GetOperation.Callback() {\n      @SuppressWarnings(\"synthetic-access\")\n      public void receivedStatus(OperationStatus status) {\n        rv.setStatus(status);\n      }\n\n      public void gotData(String k, int flags, byte[] data) {\n        Transcoder<T> tc = tcMap.get(k);\n        m.put(k,\n            tcService.decode(tc, new CachedData(flags, data, tc.getMaxSize())));\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    };\n\n    // Now that we know how many servers it breaks down into, and the latch\n    // is all set up, convert all of these strings collections to operations\n    final Map<MemcachedNode, Operation> mops =\n        new HashMap<MemcachedNode, Operation>();\n\n    for (Map.Entry<MemcachedNode, Collection<String>> me : chunks.entrySet()) {\n      Operation op = opFact.get(me.getValue(), cb);\n      mops.put(me.getKey(), op);\n      ops.add(op);\n    }\n    assert mops.size() == chunks.size();\n    mconn.checkState();\n    mconn.addOperations(mops);\n    return rv;\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tcIter an iterator of transcoders to serialize and unserialize\n   *          values; the transcoders are matched with the keys in the same\n   *          order. The minimum of the key collection length and number of\n   *          transcoders is used and no exception is thrown if they do not\n   *          match\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n          Iterator<Transcoder<T>> tcIter) {\n    return asyncGetBulk(keys.iterator(), tcIter);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator for the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keyIter,\n            new SingleElementInfiniteIterator<Transcoder<T>>(tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache.\n   *\n   * @param <T>\n   * @param keys the keys to request\n   * @param tc the transcoder to serialize and unserialize values\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return asyncGetBulk(keys, new SingleElementInfiniteIterator<Transcoder<T>>(\n        tc));\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keyIter Iterator that produces the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(\n         Iterator<String> keyIter) {\n    return asyncGetBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Asynchronously get a bunch of objects from the cache and decode them with\n   * the given transcoder.\n   *\n   * @param keys the keys to request\n   * @return a Future result of that fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n    return asyncGetBulk(keys, transcoder);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> BulkFuture<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n      String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n   *\n   * @param keys one more more keys to get\n   * @return the future values of those keys\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public BulkFuture<Map<String, Object>> asyncGetBulk(String... keys) {\n    return asyncGetBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<CASValue<Object>> asyncGetAndTouch(final String key,\n      final int exp) {\n    return asyncGetAndTouch(key, exp, transcoder);\n  }\n\n  /**\n   * Get the given key to reset its expiration time.\n   *\n   * @param key the key to fetch\n   * @param exp the new expiration to set for the given key\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a future that will hold the return value of the fetch\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> OperationFuture<CASValue<T>> asyncGetAndTouch(final String key,\n      final int exp, final Transcoder<T> tc) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<CASValue<T>> rv = new OperationFuture<CASValue<T>>(\n        key, latch, operationTimeout);\n\n    Operation op = opFact.getAndTouch(key, exp,\n        new GetAndTouchOperation.Callback() {\n          private CASValue<T> val = null;\n\n          public void receivedStatus(OperationStatus status) {\n            rv.set(val, status);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void gotData(String k, int flags, long cas, byte[] data) {\n            assert k.equals(key) : \"Wrong key returned\";\n            assert cas > 0 : \"CAS was less than zero:  \" + cas;\n            val =\n                new CASValue<T>(cas, tc.decode(new CachedData(flags, data,\n                    tc.getMaxSize())));\n          }\n        });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keyIter Iterator that produces the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Iterator<String> keyIter,\n      Transcoder<T> tc) {\n    try {\n      return asyncGetBulk(keyIter, tc).get(operationTimeout,\n          TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted getting bulk values\", e);\n    } catch (ExecutionException e) {\n      throw new RuntimeException(\"Failed getting bulk values\", e);\n    } catch (TimeoutException e) {\n      throw new OperationTimeoutException(\"Timeout waiting for bulkvalues\", e);\n    }\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keyIter Iterator that produces the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Iterator<String> keyIter) {\n    return getBulk(keyIter, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param keys the keys\n   * @param tc the transcoder to serialize and unserialize value\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Collection<String> keys,\n      Transcoder<T> tc) {\n    return getBulk(keys.iterator(), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(Collection<String> keys) {\n    return getBulk(keys, transcoder);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param <T>\n   * @param tc the transcoder to serialize and unserialize value\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n    return getBulk(Arrays.asList(keys), tc);\n  }\n\n  /**\n   * Get the values for multiple keys from the cache.\n   *\n   * @param keys the keys\n   * @return a map of the values (for each value that exists)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<String, Object> getBulk(String... keys) {\n    return getBulk(Arrays.asList(keys), transcoder);\n  }\n\n  /**\n   * Get the versions of all of the connected memcacheds.\n   *\n   * @return a Map of SocketAddress to String for connected servers\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, String> getVersions() {\n    final Map<SocketAddress, String> rv =\n        new ConcurrentHashMap<SocketAddress, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        return opFact.version(new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.put(sa, s.getMessage());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for versions\", e);\n    }\n    return rv;\n  }\n\n  /**\n   * Get all of the stats from all of the connections.\n   *\n   * @return a Map of a Map of stats replies by SocketAddress\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats() {\n    return getStats(null);\n  }\n\n  /**\n   * Get a set of stats from all connections.\n   *\n   * @param arg which stats to get\n   * @return a Map of the server SocketAddress to a map of String stat keys to\n   *         String stat values.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n    final Map<SocketAddress, Map<String, String>> rv =\n        new HashMap<SocketAddress, Map<String, String>>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        final SocketAddress sa = n.getSocketAddress();\n        rv.put(sa, new HashMap<String, String>());\n        return opFact.stats(arg, new StatsOperation.Callback() {\n          public void gotStat(String name, String val) {\n            rv.get(sa).put(name, val);\n          }\n\n          @SuppressWarnings(\"synthetic-access\")\n          public void receivedStatus(OperationStatus status) {\n            if (!status.isSuccess()) {\n              getLogger().warn(\"Unsuccessful stat fetch: %s\", status);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n    try {\n      blatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for stats\", e);\n    }\n    return rv;\n  }\n\n  private long mutate(Mutator m, String key, long by, long def, int exp) {\n    final AtomicLong rv = new AtomicLong();\n    final CountDownLatch latch = new CountDownLatch(1);\n    mconn.enqueueOperation(key, opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n        public void receivedStatus(OperationStatus s) {\n          // XXX: Potential abstraction leak.\n          // The handling of incr/decr in the binary protocol\n          // Allows us to avoid string processing.\n          rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n        }\n\n        public void complete() {\n          latch.countDown();\n        }\n      }));\n    try {\n      if (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n        throw new OperationTimeoutException(\"Mutate operation timed out,\"\n            + \"unable to modify counter [\" + key + \"]\");\n      }\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted\", e);\n    }\n    getLogger().debug(\"Mutation returned %s\", rv);\n    return rv.get();\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by) {\n    return mutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Increment the given key by the given amount.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by) {\n    return mutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by) {\n    return mutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Decrement the given key by the given value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the value\n   * @return the new value (-1 if the key doesn't exist)\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by) {\n    return mutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, by, def, exp);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, by, def, exp);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * Due to the way the memcached server operates on items, incremented and\n   * decremented items will be returned as Strings with any operations that\n   * return a value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @param exp the expiration of this object\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def, int exp) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, exp);\n  }\n\n  private long mutateWithDefault(Mutator t, String key, long by, long def,\n      int exp) {\n    long rv = mutate(t, key, by, def, exp);\n    // The ascii protocol doesn't support defaults, so I added them\n    // manually here.\n    if (rv == -1) {\n      Future<Boolean> f = asyncStore(StoreType.add, key, exp,\n          String.valueOf(def));\n      try {\n        if (f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n          rv = def;\n        } else {\n          rv = mutate(t, key, by, 0, exp);\n          assert rv != -1 : \"Failed to mutate or init value\";\n        }\n      } catch (InterruptedException e) {\n        throw new RuntimeException(\"Interrupted waiting for store\", e);\n      } catch (ExecutionException e) {\n        throw new RuntimeException(\"Failed waiting for store\", e);\n      } catch (TimeoutException e) {\n        throw new OperationTimeoutException(\"Timeout waiting to mutate or init\"\n            + \" value\", e);\n      }\n    }\n    return rv;\n  }\n\n  private OperationFuture<Long> asyncMutate(Mutator m, String key, long by,\n      long def, int exp) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Long> rv =\n        new OperationFuture<Long>(key, latch, operationTimeout);\n    Operation op = opFact.mutate(m, key, by, def, exp,\n        new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            rv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"), s);\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n    mconn.enqueueOperation(key, op);\n    rv.setOperation(op);\n    return rv;\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, long by) {\n    return asyncMutate(Mutator.incr, key, by, 0, -1);\n  }\n\n  /**\n   * Asychronous increment.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the incremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncIncr(String key, int by) {\n    return asyncMutate(Mutator.incr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, long by) {\n    return asyncMutate(Mutator.decr, key, by, 0, -1);\n  }\n\n  /**\n   * Asynchronous decrement.\n   *\n   * @param key key to increment\n   * @param by the amount to increment the value by\n   * @return a future with the decremented value, or -1 if the increment failed.\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Long> asyncDecr(String key, int by) {\n    return asyncMutate(Mutator.decr, key, (long)by, 0, -1);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.incr, key, by, def, 0);\n  }\n\n  /**\n   * Increment the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to increment\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to increment or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long incr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.incr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, long by, long def) {\n    return mutateWithDefault(Mutator.decr, key, by, def, 0);\n  }\n\n  /**\n   * Decrement the given counter, returning the new value.\n   *\n   * @param key the key\n   * @param by the amount to decrement\n   * @param def the default value (if the counter does not exist)\n   * @return the new value, or -1 if we were unable to decrement or add\n   * @throws OperationTimeoutException if the global operation timeout is\n   *           exceeded\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public long decr(String key, int by, long def) {\n    return mutateWithDefault(Mutator.decr, key, (long)by, def, 0);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * <p>\n   * The hold argument specifies the amount of time in seconds (or Unix time\n   * until which) the client wishes the server to refuse \"add\" and \"replace\"\n   * commands with this key. For this amount of item, the item is put into a\n   * delete queue, which means that it won't possible to retrieve it by the\n   * \"get\" command, but \"add\" and \"replace\" command with this key will also fail\n   * (the \"set\" command will succeed, however). After the time passes, the item\n   * is finally deleted from server memory.\n   * <\/p>\n   *\n   * @param key the key to delete\n   * @param hold how long the key should be unavailable to add commands\n   *\n   * @return whether or not the operation was performed\n   * @deprecated Hold values are no longer honored.\n   */\n  @Deprecated\n  public OperationFuture<Boolean> delete(String key, int hold) {\n    return delete(key);\n  }\n\n  /**\n   * Delete the given key from the cache.\n   *\n   * @param key the key to delete\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> delete(String key) {\n    final CountDownLatch latch = new CountDownLatch(1);\n    final OperationFuture<Boolean> rv = new OperationFuture<Boolean>(key,\n        latch, operationTimeout);\n    DeleteOperation op = opFact.delete(key, new OperationCallback() {\n      public void receivedStatus(OperationStatus s) {\n        rv.set(s.isSuccess(), s);\n      }\n\n      public void complete() {\n        latch.countDown();\n      }\n    });\n    rv.setOperation(op);\n    mconn.enqueueOperation(key, op);\n    return rv;\n  }\n\n  /**\n   * Flush all caches from all servers with a delay of application.\n   *\n   * @param delay the period of time to delay, in seconds\n   * @return whether or not the operation was accepted\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush(final int delay) {\n    final AtomicReference<Boolean> flushResult =\n        new AtomicReference<Boolean>(null);\n    final ConcurrentLinkedQueue<Operation> ops =\n        new ConcurrentLinkedQueue<Operation>();\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        Operation op = opFact.flush(delay, new OperationCallback() {\n          public void receivedStatus(OperationStatus s) {\n            flushResult.set(s.isSuccess());\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n        ops.add(op);\n        return op;\n      }\n    });\n\n    return new OperationFuture<Boolean>(null, blatch, flushResult,\n        operationTimeout) {\n      @Override\n      public boolean cancel(boolean ign) {\n        boolean rv = false;\n        for (Operation op : ops) {\n          op.cancel();\n          rv |= op.getState() == OperationState.WRITE_QUEUED;\n        }\n        return rv;\n      }\n\n      @Override\n      public Boolean get(long duration, TimeUnit units)\n        throws InterruptedException, TimeoutException, ExecutionException {\n        status = new OperationStatus(true, \"OK\");\n        return super.get(duration, units);\n      }\n\n      @Override\n      public boolean isCancelled() {\n        boolean rv = false;\n        for (Operation op : ops) {\n          rv |= op.isCancelled();\n        }\n        return rv;\n      }\n\n      @Override\n      public boolean isDone() {\n        boolean rv = true;\n        for (Operation op : ops) {\n          rv &= op.getState() == OperationState.COMPLETE;\n        }\n        return rv || isCancelled();\n      }\n    };\n  }\n\n  /**\n   * Flush all caches from all servers immediately.\n   *\n   * @return whether or not the operation was performed\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public OperationFuture<Boolean> flush() {\n    return flush(-1);\n  }\n\n  public Set<String> listSaslMechanisms() {\n    final ConcurrentMap<String, String> rv =\n        new ConcurrentHashMap<String, String>();\n\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(MemcachedNode n, final CountDownLatch latch) {\n        return opFact.saslMechs(new OperationCallback() {\n          public void receivedStatus(OperationStatus status) {\n            for (String s : status.getMessage().split(\" \")) {\n              rv.put(s, s);\n            }\n          }\n\n          public void complete() {\n            latch.countDown();\n          }\n        });\n      }\n    });\n\n    try {\n      blatch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    return rv.keySet();\n  }\n\n  /**\n   * Shut down immediately.\n   */\n  public void shutdown() {\n    shutdown(-1, TimeUnit.MILLISECONDS);\n  }\n\n  /**\n   * Shut down this client gracefully.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the shutdown request\n   */\n  public boolean shutdown(long timeout, TimeUnit unit) {\n    // Guard against double shutdowns (bug 8).\n    if (shuttingDown) {\n      getLogger().info(\"Suppressing duplicate attempt to shut down\");\n      return false;\n    }\n    shuttingDown = true;\n    String baseName = mconn.getName();\n    mconn.setName(baseName + \" - SHUTTING DOWN\");\n    boolean rv = true;\n    try {\n      // Conditionally wait\n      if (timeout > 0) {\n        mconn.setName(baseName + \" - SHUTTING DOWN (waiting)\");\n        rv = waitForQueues(timeout, unit);\n      }\n    } finally {\n      // But always begin the shutdown sequence\n      try {\n        mconn.setName(baseName + \" - SHUTTING DOWN (telling client)\");\n        mconn.shutdown();\n        mconn.setName(baseName + \" - SHUTTING DOWN (informed client)\");\n        tcService.shutdown();\n      } catch (IOException e) {\n        getLogger().warn(\"exception while shutting down\", e);\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Wait for the queues to die down.\n   *\n   * @param timeout the amount of time time for shutdown\n   * @param unit the TimeUnit for the timeout\n   * @return result of the request for the wait\n   * @throws IllegalStateException in the rare circumstance where queue is too\n   *           full to accept any more requests\n   */\n  public boolean waitForQueues(long timeout, TimeUnit unit) {\n    CountDownLatch blatch = broadcastOp(new BroadcastOpFactory() {\n      public Operation newOp(final MemcachedNode n,\n          final CountDownLatch latch) {\n        return opFact.noop(new OperationCallback() {\n          public void complete() {\n            latch.countDown();\n          }\n\n          public void receivedStatus(OperationStatus s) {\n            // Nothing special when receiving status, only\n            // necessary to complete the interface\n          }\n        });\n      }\n    }, mconn.getLocator().getAll(), false);\n    try {\n      // XXX: Perhaps IllegalStateException should be caught here\n      // and the check retried.\n      return blatch.await(timeout, unit);\n    } catch (InterruptedException e) {\n      throw new RuntimeException(\"Interrupted waiting for queues\", e);\n    }\n  }\n\n  /**\n   * Add a connection observer.\n   *\n   * If connections are already established, your observer will be called with\n   * the address and -1.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer was added.\n   */\n  public boolean addObserver(ConnectionObserver obs) {\n    boolean rv = mconn.addObserver(obs);\n    if (rv) {\n      for (MemcachedNode node : mconn.getLocator().getAll()) {\n        if (node.isActive()) {\n          obs.connectionEstablished(node.getSocketAddress(), -1);\n        }\n      }\n    }\n    return rv;\n  }\n\n  /**\n   * Remove a connection observer.\n   *\n   * @param obs the ConnectionObserver you wish to add\n   * @return true if the observer existed, but no longer does\n   */\n  public boolean removeObserver(ConnectionObserver obs) {\n    return mconn.removeObserver(obs);\n  }\n\n  public void connectionEstablished(SocketAddress sa, int reconnectCount) {\n    if (authDescriptor != null) {\n      if (authDescriptor.authThresholdReached()) {\n        this.shutdown();\n      }\n      authMonitor.authConnection(mconn, opFact, authDescriptor, findNode(sa));\n    }\n  }\n\n  private MemcachedNode findNode(SocketAddress sa) {\n    MemcachedNode node = null;\n    for (MemcachedNode n : mconn.getLocator().getAll()) {\n      if (n.getSocketAddress().equals(sa)) {\n        node = n;\n      }\n    }\n    assert node != null : \"Couldn't find node connected to \" + sa;\n    return node;\n  }\n\n  public void connectionLost(SocketAddress sa) {\n    // Don't care.\n  }\n\n  @Override\n  public String toString() {\n    return connFactory.toString();\n  }\n}\n","lineNo":567}
{"Smelly Sample":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached.protocol.binary;\n\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\nimport java.nio.ByteOrder;\nimport java.util.concurrent.atomic.AtomicInteger;\n\nimport net.spy.memcached.CASResponse;\nimport net.spy.memcached.KeyUtil;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.ErrorCode;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.protocol.BaseOperationImpl;\n\n/**\n * Base class for binary operations.\n */\nabstract class OperationImpl extends BaseOperationImpl implements Operation {\n\n  protected static final byte REQ_MAGIC = (byte) 0x80;\n  protected static final byte RES_MAGIC = (byte) 0x81;\n  protected static final byte DUMMY_OPCODE = (byte)0xff;\n  protected static final int MIN_RECV_PACKET = 24;\n\n  protected static final byte[] EMPTY_BYTES = new byte[0];\n\n  protected static final OperationStatus STATUS_OK = new CASOperationStatus(\n      true, \"OK\", CASResponse.OK);\n\n  private static final AtomicInteger SEQ_NUMBER = new AtomicInteger(0);\n\n  // request header fields\n  private final byte cmd;\n  protected short vbucket = 0;\n  protected final int opaque;\n\n  private final byte[] header = new byte[MIN_RECV_PACKET];\n  private int headerOffset = 0;\n  private byte[] payload = null;\n\n  // Response header fields\n  protected int keyLen;\n  protected byte responseCmd;\n  protected short errorCode;\n  protected int responseOpaque;\n  protected long responseCas;\n\n  private int payloadOffset = 0;\n\n  /**\n   * Construct with opaque.\n   *\n   * @param o the opaque value.\n   * @param cb\n   */\n  protected OperationImpl(byte c, int o, OperationCallback cb) {\n    super();\n    cmd = c;\n    opaque = o;\n    setCallback(cb);\n  }\n\n  protected void resetInput() {\n    payload = null;\n    payloadOffset = 0;\n    headerOffset = 0;\n  }\n\n  // Base response packet format:\n  // 0 1 2 3 4 5 6 7 8 9 10 11\n  // # magic, opcode, keylen, extralen, datatype, status, bodylen,\n  // 12,3,4,5 16\n  // opaque, cas\n  // RES_PKT_FMT=\">BBHBBHIIQ\"\n\n  @Override\n  public void readFromBuffer(ByteBuffer b) throws IOException {\n    // First process headers if we haven't completed them yet\n    if (headerOffset < MIN_RECV_PACKET) {\n      int toRead = MIN_RECV_PACKET - headerOffset;\n      int available = b.remaining();\n      toRead = Math.min(toRead, available);\n      getLogger().debug(\"Reading %d header bytes\", toRead);\n      b.get(header, headerOffset, toRead);\n      headerOffset += toRead;\n\n      // We've completed reading the header. Prepare body read.\n      if (headerOffset == MIN_RECV_PACKET) {\n        int magic = header[0];\n        assert magic == RES_MAGIC : \"Invalid magic:  \" + magic;\n        responseCmd = header[1];\n        assert cmd == DUMMY_OPCODE || responseCmd == cmd\n          : \"Unexpected response command value\";\n        keyLen = decodeShort(header, 2);\n        // TODO: Examine extralen and datatype\n        errorCode = (short) decodeShort(header, 6);\n        int bytesToRead = decodeInt(header, 8);\n        payload = new byte[bytesToRead];\n        responseOpaque = decodeInt(header, 12);\n        responseCas = decodeLong(header, 16);\n        assert opaqueIsValid() : \"Opaque is not valid\";\n      }\n    }\n\n    // Now process the payload if we can.\n    if (headerOffset >= MIN_RECV_PACKET && payload == null) {\n      finishedPayload(EMPTY_BYTES);\n    } else if (payload != null) {\n      int toRead = payload.length - payloadOffset;\n      int available = b.remaining();\n      toRead = Math.min(toRead, available);\n      getLogger().debug(\"Reading %d payload bytes\", toRead);\n      b.get(payload, payloadOffset, toRead);\n      payloadOffset += toRead;\n\n      // Have we read it all?\n      if (payloadOffset == payload.length) {\n        finishedPayload(payload);\n      }\n    } else {\n      // Haven't read enough to make up a payload. Must read more.\n      getLogger().debug(\"Only read %d of the %d needed to fill a header\",\n          headerOffset, MIN_RECV_PACKET);\n    }\n\n  }\n\n  protected void finishedPayload(byte[] pl) throws IOException {\n    OperationStatus status = getStatusForErrorCode(errorCode, pl);\n    ErrorCode ec = ErrorCode.getErrorCode(errorCode);\n\n    if (status == null) {\n      handleError(OperationErrorType.SERVER, new String(pl));\n    } else if (ec == ErrorCode.SUCCESS) {\n      decodePayload(pl);\n      transitionState(OperationState.COMPLETE);\n    } else if (ec == ErrorCode.ERR_NOT_MY_VBUCKET\n        && !getState().equals(OperationState.COMPLETE)) {\n      transitionState(OperationState.RETRY);\n    } else {\n      getCallback().receivedStatus(status);\n      transitionState(OperationState.COMPLETE);\n    }\n  }\n\n  /**\n   * Get the OperationStatus object for the given error code.\n   *\n   * @param errCode the error code\n   * @return the status to return, or null if this is an exceptional case\n   */\n  protected OperationStatus getStatusForErrorCode(short errCode, byte[] errPl)\n    throws IOException {\n    switch (ErrorCode.getErrorCode(errCode)) {\n    case SUCCESS:\n      return STATUS_OK;\n    case ERR_NOT_FOUND:\n      return new CASOperationStatus(false, new String(errPl),\n          CASResponse.NOT_FOUND);\n    case ERR_EXISTS:\n      return new CASOperationStatus(false, new String(errPl),\n          CASResponse.EXISTS);\n    case ERR_NOT_STORED:\n      return new CASOperationStatus(false, new String(errPl),\n          CASResponse.NOT_FOUND);\n    case ERR_2BIG:\n    case ERR_INTERNAL:\n      handleError(OperationErrorType.SERVER, new String(errPl));\n    case ERR_INVAL:\n    case ERR_DELTA_BADVAL:\n    case ERR_NOT_MY_VBUCKET:\n    case ERR_UNKNOWN_COMMAND:\n    case ERR_NO_MEM:\n    case ERR_NOT_SUPPORTED:\n    case ERR_BUSY:\n    case ERR_TEMP_FAIL:\n      return new OperationStatus(false, new String(errPl));\n    default:\n      return null;\n    }\n  }\n\n  /**\n   * Decode the given payload for this command.\n   *\n   * @param pl the payload.\n   */\n  protected void decodePayload(byte[] pl) {\n    assert pl.length == 0 : \"Payload has bytes, but decode isn't overridden\";\n    getCallback().receivedStatus(STATUS_OK);\n  }\n\n  /**\n   * Validate an opaque value from the header. This may be overridden from a\n   * subclass where the opaque isn't expected to always be the same as the\n   * request opaque.\n   */\n  protected boolean opaqueIsValid() {\n    if (responseOpaque != opaque) {\n      getLogger().warn(\"Expected opaque:  %d, got opaque:  %d\\n\",\n          responseOpaque, opaque);\n    }\n    return responseOpaque == opaque;\n  }\n\n  static int decodeShort(byte[] data, int i) {\n    return (data[i] & 0xff) << 8 | (data[i + 1] & 0xff);\n  }\n\n  static int decodeInt(byte[] data, int i) {\n    return (data[i] & 0xff) << 24\n      | (data[i + 1] & 0xff) << 16\n      | (data[i + 2] & 0xff) << 8\n      | (data[i + 3] & 0xff);\n  }\n\n  static long decodeUnsignedInt(byte[] data, int i) {\n    return ((long) (data[i] & 0xff) << 24)\n      | ((data[i + 1] & 0xff) << 16)\n      | ((data[i + 2] & 0xff) << 8)\n      | (data[i + 3] & 0xff);\n  }\n\n  static long decodeLong(byte[] data, int i) {\n    return (data[i] & 0xffL) << 56\n      | (data[i + 1] & 0xffL) << 48\n      | (data[i + 2] & 0xffL) << 40\n      | (data[i + 3] & 0xffL) << 32\n      | (data[i + 4] & 0xffL) << 24\n      | (data[i + 5] & 0xffL) << 16\n      | (data[i + 6] & 0xffL) << 8\n      | (data[i + 7] & 0xffL);\n  }\n\n  /**\n   * Prepare a send buffer.\n   *\n   * @param key the key (for keyed ops)\n   * @param cas the cas value\n   * @param val the data payload\n   * @param extraHeaders any additional headers that need to be sent\n   */\n  protected void prepareBuffer(String key, long cas, byte[] val,\n      Object... extraHeaders) {\n    int extraLen = 0;\n    for (Object o : extraHeaders) {\n      if (o instanceof Integer) {\n        extraLen += 4;\n      } else if (o instanceof byte[]) {\n        extraLen += ((byte[]) o).length;\n      } else if (o instanceof Long) {\n        extraLen += 8;\n      } else {\n        assert false : \"Unhandled extra header type:  \" + o.getClass();\n      }\n    }\n    final byte[] keyBytes = KeyUtil.getKeyBytes(key);\n    int bufSize = MIN_RECV_PACKET + keyBytes.length + val.length;\n\n    // # magic, opcode, keylen, extralen, datatype, [reserved],\n    // bodylen, opaque, cas\n    // REQ_PKT_FMT=\">BBHBBxxIIQ\"\n\n    // set up the initial header stuff\n    ByteBuffer bb = ByteBuffer.allocate(bufSize + extraLen);\n    assert bb.order() == ByteOrder.BIG_ENDIAN;\n    bb.put(REQ_MAGIC);\n    bb.put(cmd);\n    bb.putShort((short) keyBytes.length);\n    bb.put((byte) extraLen);\n    bb.put((byte) 0); // data type\n    bb.putShort(vbucket); // vbucket\n    bb.putInt(keyBytes.length + val.length + extraLen);\n    bb.putInt(opaque);\n    bb.putLong(cas);\n\n    // Add the extra headers.\n    for (Object o : extraHeaders) {\n      if (o instanceof Integer) {\n        bb.putInt((Integer) o);\n      } else if (o instanceof byte[]) {\n        bb.put((byte[]) o);\n      } else if (o instanceof Long) {\n        bb.putLong((Long) o);\n      } else {\n        assert false : \"Unhandled extra header type:  \" + o.getClass();\n      }\n    }\n\n    // Add the normal stuff\n    bb.put(keyBytes);\n    bb.put(val);\n\n    bb.flip();\n    setBuffer(bb);\n  }\n\n  /**\n   * Generate an opaque ID.\n   */\n  static int generateOpaque() {\n    int rv = SEQ_NUMBER.incrementAndGet();\n    while (rv < 0) {\n      SEQ_NUMBER.compareAndSet(rv, 0);\n      rv = SEQ_NUMBER.incrementAndGet();\n    }\n    return rv;\n  }\n\n  @Override\n  public String toString() {\n    return \"Cmd: \" + cmd + \" Opaque: \" + opaque;\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2006-2009 Dustin Sallings\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached.protocol.binary;\n\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\nimport java.nio.ByteOrder;\nimport java.util.concurrent.atomic.AtomicInteger;\n\nimport net.spy.memcached.CASResponse;\nimport net.spy.memcached.KeyUtil;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.ErrorCode;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.protocol.BaseOperationImpl;\n\n/**\n * Base class for binary operations.\n */\nabstract class OperationImpl extends BaseOperationImpl implements Operation {\n\n  protected static final byte REQ_MAGIC = (byte) 0x80;\n  protected static final byte RES_MAGIC = (byte) 0x81;\n  protected static final byte DUMMY_OPCODE = (byte)0xff;\n  protected static final int MIN_RECV_PACKET = 24;\n\n  protected static final byte[] EMPTY_BYTES = new byte[0];\n\n  protected static final OperationStatus STATUS_OK = new CASOperationStatus(\n      true, \"OK\", ErrorCode.SUCCESS, CASResponse.OK);\n\n  private static final AtomicInteger SEQ_NUMBER = new AtomicInteger(0);\n\n  // request header fields\n  private final byte cmd;\n  protected short vbucket = 0;\n  protected final int opaque;\n\n  private final byte[] header = new byte[MIN_RECV_PACKET];\n  private int headerOffset = 0;\n  private byte[] payload = null;\n\n  // Response header fields\n  protected int keyLen;\n  protected byte responseCmd;\n  protected short errorCode;\n  protected int responseOpaque;\n  protected long responseCas;\n\n  private int payloadOffset = 0;\n\n  /**\n   * Construct with opaque.\n   *\n   * @param o the opaque value.\n   * @param cb\n   */\n  protected OperationImpl(byte c, int o, OperationCallback cb) {\n    super();\n    cmd = c;\n    opaque = o;\n    setCallback(cb);\n  }\n\n  protected void resetInput() {\n    payload = null;\n    payloadOffset = 0;\n    headerOffset = 0;\n  }\n\n  // Base response packet format:\n  // 0 1 2 3 4 5 6 7 8 9 10 11\n  // # magic, opcode, keylen, extralen, datatype, status, bodylen,\n  // 12,3,4,5 16\n  // opaque, cas\n  // RES_PKT_FMT=\">BBHBBHIIQ\"\n\n  @Override\n  public void readFromBuffer(ByteBuffer b) throws IOException {\n    // First process headers if we haven't completed them yet\n    if (headerOffset < MIN_RECV_PACKET) {\n      int toRead = MIN_RECV_PACKET - headerOffset;\n      int available = b.remaining();\n      toRead = Math.min(toRead, available);\n      getLogger().debug(\"Reading %d header bytes\", toRead);\n      b.get(header, headerOffset, toRead);\n      headerOffset += toRead;\n\n      // We've completed reading the header. Prepare body read.\n      if (headerOffset == MIN_RECV_PACKET) {\n        int magic = header[0];\n        assert magic == RES_MAGIC : \"Invalid magic:  \" + magic;\n        responseCmd = header[1];\n        assert cmd == DUMMY_OPCODE || responseCmd == cmd\n          : \"Unexpected response command value\";\n        keyLen = decodeShort(header, 2);\n        // TODO: Examine extralen and datatype\n        errorCode = (short) decodeShort(header, 6);\n        int bytesToRead = decodeInt(header, 8);\n        payload = new byte[bytesToRead];\n        responseOpaque = decodeInt(header, 12);\n        responseCas = decodeLong(header, 16);\n        assert opaqueIsValid() : \"Opaque is not valid\";\n      }\n    }\n\n    // Now process the payload if we can.\n    if (headerOffset >= MIN_RECV_PACKET && payload == null) {\n      finishedPayload(EMPTY_BYTES);\n    } else if (payload != null) {\n      int toRead = payload.length - payloadOffset;\n      int available = b.remaining();\n      toRead = Math.min(toRead, available);\n      getLogger().debug(\"Reading %d payload bytes\", toRead);\n      b.get(payload, payloadOffset, toRead);\n      payloadOffset += toRead;\n\n      // Have we read it all?\n      if (payloadOffset == payload.length) {\n        finishedPayload(payload);\n      }\n    } else {\n      // Haven't read enough to make up a payload. Must read more.\n      getLogger().debug(\"Only read %d of the %d needed to fill a header\",\n          headerOffset, MIN_RECV_PACKET);\n    }\n\n  }\n\n  protected void finishedPayload(byte[] pl) throws IOException {\n    OperationStatus status = getStatusForErrorCode(errorCode, pl);\n    ErrorCode ec = ErrorCode.getErrorCode(errorCode);\n\n    if (status == null) {\n      handleError(OperationErrorType.SERVER, new String(pl));\n    } else if (ec == ErrorCode.SUCCESS) {\n      decodePayload(pl);\n      transitionState(OperationState.COMPLETE);\n    } else if (ec == ErrorCode.ERR_NOT_MY_VBUCKET\n        && !getState().equals(OperationState.COMPLETE)) {\n      transitionState(OperationState.RETRY);\n    } else {\n      getCallback().receivedStatus(status);\n      transitionState(OperationState.COMPLETE);\n    }\n  }\n\n  /**\n   * Get the OperationStatus object for the given error code.\n   *\n   * @param errCode the error code\n   * @return the status to return, or null if this is an exceptional case\n   */\n  protected OperationStatus getStatusForErrorCode(short errCode, byte[] errPl)\n    throws IOException {\n    ErrorCode ec = ErrorCode.getErrorCode(errCode);\n    switch (ec) {\n    case SUCCESS:\n      return STATUS_OK;\n    case ERR_NOT_FOUND:\n      return new CASOperationStatus(false, new String(errPl), ec,\n          CASResponse.NOT_FOUND);\n    case ERR_EXISTS:\n      return new CASOperationStatus(false, new String(errPl), ec,\n          CASResponse.EXISTS);\n    case ERR_NOT_STORED:\n      return new CASOperationStatus(false, new String(errPl), ec,\n          CASResponse.NOT_FOUND);\n    case ERR_2BIG:\n    case ERR_INTERNAL:\n      handleError(OperationErrorType.SERVER, new String(errPl));\n    case ERR_INVAL:\n    case ERR_DELTA_BADVAL:\n    case ERR_NOT_MY_VBUCKET:\n    case ERR_UNKNOWN_COMMAND:\n    case ERR_NO_MEM:\n    case ERR_NOT_SUPPORTED:\n    case ERR_BUSY:\n    case ERR_TEMP_FAIL:\n      return new OperationStatus(false, new String(errPl), ec);\n    default:\n      return null;\n    }\n  }\n\n  /**\n   * Decode the given payload for this command.\n   *\n   * @param pl the payload.\n   */\n  protected void decodePayload(byte[] pl) {\n    assert pl.length == 0 : \"Payload has bytes, but decode isn't overridden\";\n    getCallback().receivedStatus(STATUS_OK);\n  }\n\n  /**\n   * Validate an opaque value from the header. This may be overridden from a\n   * subclass where the opaque isn't expected to always be the same as the\n   * request opaque.\n   */\n  protected boolean opaqueIsValid() {\n    if (responseOpaque != opaque) {\n      getLogger().warn(\"Expected opaque:  %d, got opaque:  %d\\n\",\n          responseOpaque, opaque);\n    }\n    return responseOpaque == opaque;\n  }\n\n  static int decodeShort(byte[] data, int i) {\n    return (data[i] & 0xff) << 8 | (data[i + 1] & 0xff);\n  }\n\n  static int decodeInt(byte[] data, int i) {\n    return (data[i] & 0xff) << 24\n      | (data[i + 1] & 0xff) << 16\n      | (data[i + 2] & 0xff) << 8\n      | (data[i + 3] & 0xff);\n  }\n\n  static long decodeUnsignedInt(byte[] data, int i) {\n    return ((long) (data[i] & 0xff) << 24)\n      | ((data[i + 1] & 0xff) << 16)\n      | ((data[i + 2] & 0xff) << 8)\n      | (data[i + 3] & 0xff);\n  }\n\n  static long decodeLong(byte[] data, int i) {\n    return (data[i] & 0xffL) << 56\n      | (data[i + 1] & 0xffL) << 48\n      | (data[i + 2] & 0xffL) << 40\n      | (data[i + 3] & 0xffL) << 32\n      | (data[i + 4] & 0xffL) << 24\n      | (data[i + 5] & 0xffL) << 16\n      | (data[i + 6] & 0xffL) << 8\n      | (data[i + 7] & 0xffL);\n  }\n\n  /**\n   * Prepare a send buffer.\n   *\n   * @param key the key (for keyed ops)\n   * @param cas the cas value\n   * @param val the data payload\n   * @param extraHeaders any additional headers that need to be sent\n   */\n  protected void prepareBuffer(String key, long cas, byte[] val,\n      Object... extraHeaders) {\n    int extraLen = 0;\n    for (Object o : extraHeaders) {\n      if (o instanceof Integer) {\n        extraLen += 4;\n      } else if (o instanceof byte[]) {\n        extraLen += ((byte[]) o).length;\n      } else if (o instanceof Long) {\n        extraLen += 8;\n      } else {\n        assert false : \"Unhandled extra header type:  \" + o.getClass();\n      }\n    }\n    final byte[] keyBytes = KeyUtil.getKeyBytes(key);\n    int bufSize = MIN_RECV_PACKET + keyBytes.length + val.length;\n\n    // # magic, opcode, keylen, extralen, datatype, [reserved],\n    // bodylen, opaque, cas\n    // REQ_PKT_FMT=\">BBHBBxxIIQ\"\n\n    // set up the initial header stuff\n    ByteBuffer bb = ByteBuffer.allocate(bufSize + extraLen);\n    assert bb.order() == ByteOrder.BIG_ENDIAN;\n    bb.put(REQ_MAGIC);\n    bb.put(cmd);\n    bb.putShort((short) keyBytes.length);\n    bb.put((byte) extraLen);\n    bb.put((byte) 0); // data type\n    bb.putShort(vbucket); // vbucket\n    bb.putInt(keyBytes.length + val.length + extraLen);\n    bb.putInt(opaque);\n    bb.putLong(cas);\n\n    // Add the extra headers.\n    for (Object o : extraHeaders) {\n      if (o instanceof Integer) {\n        bb.putInt((Integer) o);\n      } else if (o instanceof byte[]) {\n        bb.put((byte[]) o);\n      } else if (o instanceof Long) {\n        bb.putLong((Long) o);\n      } else {\n        assert false : \"Unhandled extra header type:  \" + o.getClass();\n      }\n    }\n\n    // Add the normal stuff\n    bb.put(keyBytes);\n    bb.put(val);\n\n    bb.flip();\n    setBuffer(bb);\n  }\n\n  /**\n   * Generate an opaque ID.\n   */\n  static int generateOpaque() {\n    int rv = SEQ_NUMBER.incrementAndGet();\n    while (rv < 0) {\n      SEQ_NUMBER.compareAndSet(rv, 0);\n      rv = SEQ_NUMBER.incrementAndGet();\n    }\n    return rv;\n  }\n\n  @Override\n  public String toString() {\n    return \"Cmd: \" + cmd + \" Opaque: \" + opaque;\n  }\n}\n","lineNo":181}
{"Smelly Sample":"/**\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport net.spy.memcached.HashAlgorithm;\nimport net.spy.memcached.HashAlgorithmRegistry;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\n/**\n * A DefaultConfigFactory.\n */\npublic class DefaultConfigFactory implements ConfigFactory {\n\n  @Override\n  public Config create(File filename) {\n    if (filename == null || \"\".equals(filename.getName())) {\n      throw new IllegalArgumentException(\"Filename is empty.\");\n    }\n    StringBuilder sb = new StringBuilder();\n    try {\n      FileInputStream fis = new FileInputStream(filename);\n      BufferedReader reader = new BufferedReader(new InputStreamReader(fis));\n      String str;\n      while ((str = reader.readLine()) != null) {\n        sb.append(str);\n      }\n    } catch (IOException e) {\n      throw new ConfigParsingException(\"Exception reading input file: \"\n          + filename, e);\n    }\n    return create(sb.toString());\n  }\n\n  @Override\n  public Config create(String data) {\n    try {\n      JSONObject jsonObject = new JSONObject(data);\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \" + data,\n        e);\n    }\n  }\n\n  @Override\n  public Config create(JSONObject jsonObject) {\n    try {\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \"\n        + jsonObject, e);\n    }\n  }\n\n  private Config parseJSON(JSONObject jsonObject) throws JSONException {\n    // the incoming config could be cache or EP object types, JSON envelope\n    // picked apart\n    if (!jsonObject.has(\"vBucketServerMap\")) {\n      return parseCacheJSON(jsonObject);\n    }\n    return parseEpJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n  }\n\n  private Config parseCacheJSON(JSONObject jsonObject) throws JSONException {\n\n    JSONArray nodes = jsonObject.getJSONArray(\"nodes\");\n    if (nodes.length() <= 0) {\n      throw new ConfigParsingException(\"Empty nodes list.\");\n    }\n    int serversCount = nodes.length();\n\n    CacheConfig config = new CacheConfig(serversCount);\n    populateServers(config, nodes);\n\n    return config;\n  }\n\n  /* ep is for ep-engine, a.k.a. membase */\n  private Config parseEpJSON(JSONObject jsonObject) throws JSONException {\n    String algorithm = jsonObject.getString(\"hashAlgorithm\");\n    HashAlgorithm hashAlgorithm =\n        HashAlgorithmRegistry.lookupHashAlgorithm(algorithm);\n    if (hashAlgorithm == null) {\n      throw new IllegalArgumentException(\"Unhandled hash algorithm type: \"\n          + algorithm);\n    }\n    int replicasCount = jsonObject.getInt(\"numReplicas\");\n    if (replicasCount > VBucket.MAX_REPLICAS) {\n      throw new ConfigParsingException(\"Expected number <= \"\n          + VBucket.MAX_REPLICAS + \" for replicas.\");\n    }\n    JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n    if (servers.length() <= 0) {\n      throw new ConfigParsingException(\"Empty servers list.\");\n    }\n    int serversCount = servers.length();\n    JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n    int vbucketsCount = vbuckets.length();\n    if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n      throw new ConfigParsingException(\"Number of buckets must be a power of \"\n        + \"two, > 0 and <= \" + VBucket.MAX_BUCKETS);\n    }\n    List<String> populateServers = populateServers(servers);\n    List<VBucket> populateVbuckets = populateVbuckets(vbuckets);\n\n    DefaultConfig config = new DefaultConfig(hashAlgorithm, serversCount,\n      replicasCount, vbucketsCount, populateServers, populateVbuckets);\n\n    return config;\n  }\n\n  private List<String> populateServers(JSONArray servers) throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < servers.length(); i++) {\n      String server = servers.getString(i);\n      serverNames.add(server);\n    }\n    return serverNames;\n  }\n\n  private void populateServers(CacheConfig config, JSONArray nodes)\n    throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < nodes.length(); i++) {\n      JSONObject node = nodes.getJSONObject(i);\n      String webHostPort = node.getString(\"hostname\");\n      String[] splitHostPort = webHostPort.split(\":\");\n      JSONObject portsList = node.getJSONObject(\"ports\");\n      int port = portsList.getInt(\"direct\");\n      serverNames.add(splitHostPort[0] + \":\" + port);\n    }\n    config.setServers(serverNames);\n  }\n\n  private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n    throws JSONException {\n    List<VBucket> vBuckets = new ArrayList<VBucket>();\n    for (int i = 0; i < jsonVbuckets.length(); i++) {\n      JSONArray rows = jsonVbuckets.getJSONArray(i);\n      int master = rows.getInt(0);\n      int[] replicas = new int[VBucket.MAX_REPLICAS];\n      for (int j = 1; j < rows.length(); j++) {\n        replicas[j - 1] = rows.getInt(j);\n      }\n      vBuckets.add(new VBucket(master, replicas));\n    }\n    return vBuckets;\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.MalformedURLException;\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport net.spy.memcached.HashAlgorithm;\nimport net.spy.memcached.HashAlgorithmRegistry;\nimport net.spy.memcached.compat.SpyObject;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\n/**\n * A DefaultConfigFactory.\n */\npublic class DefaultConfigFactory extends SpyObject implements ConfigFactory {\n\n  @Override\n  public Config create(File filename) {\n    if (filename == null || \"\".equals(filename.getName())) {\n      throw new IllegalArgumentException(\"Filename is empty.\");\n    }\n    StringBuilder sb = new StringBuilder();\n    try {\n      FileInputStream fis = new FileInputStream(filename);\n      BufferedReader reader = new BufferedReader(new InputStreamReader(fis));\n      String str;\n      while ((str = reader.readLine()) != null) {\n        sb.append(str);\n      }\n    } catch (IOException e) {\n      throw new ConfigParsingException(\"Exception reading input file: \"\n          + filename, e);\n    }\n    return create(sb.toString());\n  }\n\n  @Override\n  public Config create(String data) {\n    try {\n      JSONObject jsonObject = new JSONObject(data);\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \" + data,\n        e);\n    }\n  }\n\n  @Override\n  public Config create(JSONObject jsonObject) {\n    try {\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \"\n        + jsonObject, e);\n    }\n  }\n\n  private Config parseJSON(JSONObject jsonObject) throws JSONException {\n    // the incoming config could be cache or EP object types, JSON envelope\n    // picked apart\n    if (!jsonObject.has(\"vBucketServerMap\")) {\n      return parseCacheJSON(jsonObject);\n    }\n    return parseEpJSON(jsonObject);\n  }\n\n  private Config parseCacheJSON(JSONObject jsonObject) throws JSONException {\n\n    JSONArray nodes = jsonObject.getJSONArray(\"nodes\");\n    if (nodes.length() <= 0) {\n      throw new ConfigParsingException(\"Empty nodes list.\");\n    }\n    int serversCount = nodes.length();\n\n    CacheConfig config = new CacheConfig(serversCount);\n    populateServers(config, nodes);\n\n    return config;\n  }\n\n  /* ep is for ep-engine, a.k.a. membase */\n  private Config parseEpJSON(JSONObject jsonObject) throws JSONException {\n    JSONObject vbMap = jsonObject.getJSONObject(\"vBucketServerMap\");\n    String algorithm = vbMap.getString(\"hashAlgorithm\");\n    HashAlgorithm hashAlgorithm =\n        HashAlgorithmRegistry.lookupHashAlgorithm(algorithm);\n    if (hashAlgorithm == null) {\n      throw new IllegalArgumentException(\"Unhandled hash algorithm type: \"\n          + algorithm);\n    }\n    int replicasCount = vbMap.getInt(\"numReplicas\");\n    if (replicasCount > VBucket.MAX_REPLICAS) {\n      throw new ConfigParsingException(\"Expected number <= \"\n          + VBucket.MAX_REPLICAS + \" for replicas.\");\n    }\n    JSONArray servers = vbMap.getJSONArray(\"serverList\");\n    if (servers.length() <= 0) {\n      throw new ConfigParsingException(\"Empty servers list.\");\n    }\n    int serversCount = servers.length();\n    JSONArray vbuckets = vbMap.getJSONArray(\"vBucketMap\");\n    int vbucketsCount = vbuckets.length();\n    if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n      throw new ConfigParsingException(\"Number of buckets must be a power of \"\n        + \"two, > 0 and <= \" + VBucket.MAX_BUCKETS);\n    }\n    List<String> populateServers = populateServers(servers);\n    List<VBucket> populateVbuckets = populateVbuckets(vbuckets);\n\n    List<URL> couchServers =\n      populateCouchServers(jsonObject.getJSONArray(\"nodes\"));\n\n    DefaultConfig config = new DefaultConfig(hashAlgorithm, serversCount,\n      replicasCount, vbucketsCount, populateServers, populateVbuckets,\n      couchServers);\n\n    return config;\n  }\n\n  private List<URL> populateCouchServers(JSONArray nodes) throws JSONException{\n    List<URL> nodeNames = new ArrayList<URL>();\n    for (int i = 0; i < nodes.length(); i++) {\n      JSONObject node = nodes.getJSONObject(i);\n      if (node.has(\"couchApiBase\")) {\n        try {\n          nodeNames.add(new URL(node.getString(\"couchApiBase\")));\n        } catch (MalformedURLException e) {\n          throw new JSONException(\"Got bad couchApiBase URL from config\");\n        }\n      }\n    }\n    return nodeNames;\n  }\n\n  private List<String> populateServers(JSONArray servers) throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < servers.length(); i++) {\n      String server = servers.getString(i);\n      serverNames.add(server);\n    }\n    return serverNames;\n  }\n\n  private void populateServers(CacheConfig config, JSONArray nodes)\n    throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < nodes.length(); i++) {\n      JSONObject node = nodes.getJSONObject(i);\n      String webHostPort = node.getString(\"hostname\");\n      String[] splitHostPort = webHostPort.split(\":\");\n      JSONObject portsList = node.getJSONObject(\"ports\");\n      int port = portsList.getInt(\"direct\");\n      serverNames.add(splitHostPort[0] + \":\" + port);\n    }\n    config.setServers(serverNames);\n  }\n\n  private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n    throws JSONException {\n    List<VBucket> vBuckets = new ArrayList<VBucket>();\n    for (int i = 0; i < jsonVbuckets.length(); i++) {\n      JSONArray rows = jsonVbuckets.getJSONArray(i);\n      int master = rows.getInt(0);\n      int[] replicas = new int[VBucket.MAX_REPLICAS];\n      for (int j = 1; j < rows.length(); j++) {\n        replicas[j - 1] = rows.getInt(j);\n      }\n      vBuckets.add(new VBucket(master, replicas));\n    }\n    return vBuckets;\n  }\n}\n","lineNo":114}
{"Smelly Sample":"/**\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport net.spy.memcached.DefaultHashAlgorithm;\nimport net.spy.memcached.HashAlgorithm;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\n/**\n * A DefaultConfigFactory.\n */\npublic class DefaultConfigFactory implements ConfigFactory {\n\n  @Override\n  public Config create(File filename) {\n    if (filename == null || \"\".equals(filename.getName())) {\n      throw new IllegalArgumentException(\"Filename is empty.\");\n    }\n    StringBuilder sb = new StringBuilder();\n    try {\n      FileInputStream fis = new FileInputStream(filename);\n      BufferedReader reader = new BufferedReader(new InputStreamReader(fis));\n      String str;\n      while ((str = reader.readLine()) != null) {\n        sb.append(str);\n      }\n    } catch (IOException e) {\n      throw new ConfigParsingException(\"Exception reading input file: \"\n          + filename, e);\n    }\n    return create(sb.toString());\n  }\n\n  @Override\n  public Config create(String data) {\n    try {\n      JSONObject jsonObject = new JSONObject(data);\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \" + data,\n        e);\n    }\n  }\n\n  @Override\n  public Config create(JSONObject jsonObject) {\n    try {\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \"\n        + jsonObject, e);\n    }\n  }\n\n  private HashAlgorithm lookupHashAlgorithm(String algorithm) {\n    HashAlgorithm ha = DefaultHashAlgorithm.NATIVE_HASH;\n    if (\"crc\".equalsIgnoreCase(algorithm)) {\n      ha = DefaultHashAlgorithm.CRC32_HASH;\n    } else if (\"fnv1_32\".equalsIgnoreCase(algorithm)) {\n      ha = DefaultHashAlgorithm.FNV1_32_HASH;\n    } else if (\"fnv1_64\".equalsIgnoreCase(algorithm)) {\n      ha = DefaultHashAlgorithm.FNV1_64_HASH;\n    } else if (\"fnv1a_32\".equalsIgnoreCase(algorithm)) {\n      ha = DefaultHashAlgorithm.FNV1A_32_HASH;\n    } else if (\"fnv1a_64\".equalsIgnoreCase(algorithm)) {\n      ha = DefaultHashAlgorithm.FNV1A_64_HASH;\n    } else if (\"md5\".equalsIgnoreCase(algorithm)) {\n      ha = DefaultHashAlgorithm.KETAMA_HASH;\n    } else {\n      throw new IllegalArgumentException(\"Unhandled algorithm type: \"\n        + algorithm);\n    }\n    return ha;\n  }\n\n  private Config parseJSON(JSONObject jsonObject) throws JSONException {\n    // the incoming config could be cache or EP object types, JSON envelope\n    // picked apart\n    if (!jsonObject.has(\"vBucketServerMap\")) {\n      return parseCacheJSON(jsonObject);\n    }\n    return parseEpJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n  }\n\n  private Config parseCacheJSON(JSONObject jsonObject) throws JSONException {\n\n    JSONArray nodes = jsonObject.getJSONArray(\"nodes\");\n    if (nodes.length() <= 0) {\n      throw new ConfigParsingException(\"Empty nodes list.\");\n    }\n    int serversCount = nodes.length();\n\n    CacheConfig config = new CacheConfig(serversCount);\n    populateServers(config, nodes);\n\n    return config;\n  }\n\n  /* ep is for ep-engine, a.k.a. membase */\n  private Config parseEpJSON(JSONObject jsonObject) throws JSONException {\n\n    HashAlgorithm hashAlgorithm =\n        lookupHashAlgorithm(jsonObject.getString(\"hashAlgorithm\"));\n    int replicasCount = jsonObject.getInt(\"numReplicas\");\n    if (replicasCount > VBucket.MAX_REPLICAS) {\n      throw new ConfigParsingException(\"Expected number <= \"\n          + VBucket.MAX_REPLICAS + \" for replicas.\");\n    }\n    JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n    if (servers.length() <= 0) {\n      throw new ConfigParsingException(\"Empty servers list.\");\n    }\n    int serversCount = servers.length();\n    JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n    int vbucketsCount = vbuckets.length();\n    if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n      throw new ConfigParsingException(\"Number of buckets must be a power of \"\n        + \"two, > 0 and <= \" + VBucket.MAX_BUCKETS);\n    }\n    List<String> populateServers = populateServers(servers);\n    List<VBucket> populateVbuckets = populateVbuckets(vbuckets);\n\n    DefaultConfig config = new DefaultConfig(hashAlgorithm, serversCount,\n      replicasCount, vbucketsCount, populateServers, populateVbuckets);\n\n    return config;\n  }\n\n  private List<String> populateServers(JSONArray servers) throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < servers.length(); i++) {\n      String server = servers.getString(i);\n      serverNames.add(server);\n    }\n    return serverNames;\n  }\n\n  private void populateServers(CacheConfig config, JSONArray nodes)\n    throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < nodes.length(); i++) {\n      JSONObject node = nodes.getJSONObject(i);\n      String webHostPort = node.getString(\"hostname\");\n      String[] splitHostPort = webHostPort.split(\":\");\n      JSONObject portsList = node.getJSONObject(\"ports\");\n      int port = portsList.getInt(\"direct\");\n      serverNames.add(splitHostPort[0] + \":\" + port);\n    }\n    config.setServers(serverNames);\n  }\n\n  private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n    throws JSONException {\n    List<VBucket> vBuckets = new ArrayList<VBucket>();\n    for (int i = 0; i < jsonVbuckets.length(); i++) {\n      JSONArray rows = jsonVbuckets.getJSONArray(i);\n      int master = rows.getInt(0);\n      int[] replicas = new int[VBucket.MAX_REPLICAS];\n      for (int j = 1; j < rows.length(); j++) {\n        replicas[j - 1] = rows.getInt(j);\n      }\n      vBuckets.add(new VBucket(master, replicas));\n    }\n    return vBuckets;\n  }\n}\n","Method after Refactoring":"/**\n * Copyright (C) 2009-2011 Couchbase, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALING\n * IN THE SOFTWARE.\n */\n\npackage net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport net.spy.memcached.HashAlgorithm;\nimport net.spy.memcached.HashAlgorithmRegistry;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\n/**\n * A DefaultConfigFactory.\n */\npublic class DefaultConfigFactory implements ConfigFactory {\n\n  @Override\n  public Config create(File filename) {\n    if (filename == null || \"\".equals(filename.getName())) {\n      throw new IllegalArgumentException(\"Filename is empty.\");\n    }\n    StringBuilder sb = new StringBuilder();\n    try {\n      FileInputStream fis = new FileInputStream(filename);\n      BufferedReader reader = new BufferedReader(new InputStreamReader(fis));\n      String str;\n      while ((str = reader.readLine()) != null) {\n        sb.append(str);\n      }\n    } catch (IOException e) {\n      throw new ConfigParsingException(\"Exception reading input file: \"\n          + filename, e);\n    }\n    return create(sb.toString());\n  }\n\n  @Override\n  public Config create(String data) {\n    try {\n      JSONObject jsonObject = new JSONObject(data);\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \" + data,\n        e);\n    }\n  }\n\n  @Override\n  public Config create(JSONObject jsonObject) {\n    try {\n      return parseJSON(jsonObject);\n    } catch (JSONException e) {\n      throw new ConfigParsingException(\"Exception parsing JSON data: \"\n        + jsonObject, e);\n    }\n  }\n\n  private Config parseJSON(JSONObject jsonObject) throws JSONException {\n    // the incoming config could be cache or EP object types, JSON envelope\n    // picked apart\n    if (!jsonObject.has(\"vBucketServerMap\")) {\n      return parseCacheJSON(jsonObject);\n    }\n    return parseEpJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n  }\n\n  private Config parseCacheJSON(JSONObject jsonObject) throws JSONException {\n\n    JSONArray nodes = jsonObject.getJSONArray(\"nodes\");\n    if (nodes.length() <= 0) {\n      throw new ConfigParsingException(\"Empty nodes list.\");\n    }\n    int serversCount = nodes.length();\n\n    CacheConfig config = new CacheConfig(serversCount);\n    populateServers(config, nodes);\n\n    return config;\n  }\n\n  /* ep is for ep-engine, a.k.a. membase */\n  private Config parseEpJSON(JSONObject jsonObject) throws JSONException {\n    String algorithm = jsonObject.getString(\"hashAlgorithm\");\n    HashAlgorithm hashAlgorithm =\n        HashAlgorithmRegistry.lookupHashAlgorithm(algorithm);\n    if (hashAlgorithm == null) {\n      throw new IllegalArgumentException(\"Unhandled hash algorithm type: \"\n          + algorithm);\n    }\n    int replicasCount = jsonObject.getInt(\"numReplicas\");\n    if (replicasCount > VBucket.MAX_REPLICAS) {\n      throw new ConfigParsingException(\"Expected number <= \"\n          + VBucket.MAX_REPLICAS + \" for replicas.\");\n    }\n    JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n    if (servers.length() <= 0) {\n      throw new ConfigParsingException(\"Empty servers list.\");\n    }\n    int serversCount = servers.length();\n    JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n    int vbucketsCount = vbuckets.length();\n    if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n      throw new ConfigParsingException(\"Number of buckets must be a power of \"\n        + \"two, > 0 and <= \" + VBucket.MAX_BUCKETS);\n    }\n    List<String> populateServers = populateServers(servers);\n    List<VBucket> populateVbuckets = populateVbuckets(vbuckets);\n\n    DefaultConfig config = new DefaultConfig(hashAlgorithm, serversCount,\n      replicasCount, vbucketsCount, populateServers, populateVbuckets);\n\n    return config;\n  }\n\n  private List<String> populateServers(JSONArray servers) throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < servers.length(); i++) {\n      String server = servers.getString(i);\n      serverNames.add(server);\n    }\n    return serverNames;\n  }\n\n  private void populateServers(CacheConfig config, JSONArray nodes)\n    throws JSONException {\n    List<String> serverNames = new ArrayList<String>();\n    for (int i = 0; i < nodes.length(); i++) {\n      JSONObject node = nodes.getJSONObject(i);\n      String webHostPort = node.getString(\"hostname\");\n      String[] splitHostPort = webHostPort.split(\":\");\n      JSONObject portsList = node.getJSONObject(\"ports\");\n      int port = portsList.getInt(\"direct\");\n      serverNames.add(splitHostPort[0] + \":\" + port);\n    }\n    config.setServers(serverNames);\n  }\n\n  private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n    throws JSONException {\n    List<VBucket> vBuckets = new ArrayList<VBucket>();\n    for (int i = 0; i < jsonVbuckets.length(); i++) {\n      JSONArray rows = jsonVbuckets.getJSONArray(i);\n      int master = rows.getInt(0);\n      int[] replicas = new int[VBucket.MAX_REPLICAS];\n      for (int j = 1; j < rows.length(); j++) {\n        replicas[j - 1] = rows.getInt(j);\n      }\n      vBuckets.add(new VBucket(master, replicas));\n    }\n    return vBuckets;\n  }\n}\n","lineNo":111}
{"Smelly Sample":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class DocsOperationImpl extends HttpOperationImpl implements\n\t\tDocsOperation {\n\n\tpublic DocsOperationImpl(HttpRequest r, DocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseWithDocs vr = parseDocsViewResult(json);\n\n\t\t\t((DocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseWithDocs parseDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowWithDocs> rows = new LinkedList<RowWithDocs>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tString id = ids.getJSONObject(i).getString(\"id\");\n\t\t\t\t\t\tString key = ids.getJSONObject(i).getString(\"key\");\n\t\t\t\t\t\tString value = ids.getJSONObject(i).getString(\"value\");\n\t\t\t\t\t\trows.add(new RowWithDocs(id, key, value, null));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseWithDocs(rows);\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class DocsOperationImpl extends HttpOperationImpl implements\n\t\tDocsOperation {\n\n\tpublic DocsOperationImpl(HttpRequest r, DocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseWithDocs vr = parseDocsViewResult(json);\n\n\t\t\t((DocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseWithDocs parseDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowWithDocs> rows = new LinkedList<RowWithDocs>();\n\t\tfinal Collection<RowError> errors = new LinkedList<RowError>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString id = elem.getString(\"id\");\n\t\t\t\t\t\tString key = elem.getString(\"key\");\n\t\t\t\t\t\tString value = elem.getString(\"value\");\n\t\t\t\t\t\trows.add(new RowWithDocs(id, key, value, null));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (base.has(\"errors\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"errors\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString from = elem.getString(\"from\");\n\t\t\t\t\t\tString reason = elem.getString(\"reason\");\n\t\t\t\t\t\terrors.add(new RowError(from, reason));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseWithDocs(rows, errors);\n\t}\n}\n","lineNo":51}
{"Smelly Sample":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class DocsOperationImpl extends HttpOperationImpl implements\n\t\tDocsOperation {\n\n\tpublic DocsOperationImpl(HttpRequest r, DocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseWithDocs vr = parseDocsViewResult(json);\n\n\t\t\t((DocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseWithDocs parseDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowWithDocs> rows = new LinkedList<RowWithDocs>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tString id = ids.getJSONObject(i).getString(\"id\");\n\t\t\t\t\t\tString key = ids.getJSONObject(i).getString(\"key\");\n\t\t\t\t\t\tString value = ids.getJSONObject(i).getString(\"value\");\n\t\t\t\t\t\trows.add(new RowWithDocs(id, key, value, null));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseWithDocs(rows);\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class DocsOperationImpl extends HttpOperationImpl implements\n\t\tDocsOperation {\n\n\tpublic DocsOperationImpl(HttpRequest r, DocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseWithDocs vr = parseDocsViewResult(json);\n\n\t\t\t((DocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseWithDocs parseDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowWithDocs> rows = new LinkedList<RowWithDocs>();\n\t\tfinal Collection<RowError> errors = new LinkedList<RowError>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString id = elem.getString(\"id\");\n\t\t\t\t\t\tString key = elem.getString(\"key\");\n\t\t\t\t\t\tString value = elem.getString(\"value\");\n\t\t\t\t\t\trows.add(new RowWithDocs(id, key, value, null));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (base.has(\"errors\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"errors\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString from = elem.getString(\"from\");\n\t\t\t\t\t\tString reason = elem.getString(\"reason\");\n\t\t\t\t\t\terrors.add(new RowError(from, reason));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseWithDocs(rows, errors);\n\t}\n}\n","lineNo":61}
{"Smelly Sample":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class NoDocsOperationImpl extends HttpOperationImpl implements\n\t\tNoDocsOperation {\n\n\tpublic NoDocsOperationImpl(HttpRequest r, NoDocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseNoDocs vr = parseNoDocsViewResult(json);\n\n\t\t\t((NoDocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseNoDocs parseNoDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowNoDocs> rows = new LinkedList<RowNoDocs>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tString id = ids.getJSONObject(i).getString(\"id\");\n\t\t\t\t\t\tString key = ids.getJSONObject(i).getString(\"key\");\n\t\t\t\t\t\tString value = ids.getJSONObject(i).getString(\"value\");\n\t\t\t\t\t\trows.add(new RowNoDocs(id, key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseNoDocs(rows);\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class NoDocsOperationImpl extends HttpOperationImpl implements\n\t\tNoDocsOperation {\n\n\tpublic NoDocsOperationImpl(HttpRequest r, NoDocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseNoDocs vr = parseNoDocsViewResult(json);\n\n\t\t\t((NoDocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseNoDocs parseNoDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowNoDocs> rows = new LinkedList<RowNoDocs>();\n\t\tfinal Collection<RowError> errors = new LinkedList<RowError>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString id = elem.getString(\"id\");\n\t\t\t\t\t\tString key = elem.getString(\"key\");\n\t\t\t\t\t\tString value = elem.getString(\"value\");\n\t\t\t\t\t\trows.add(new RowNoDocs(id, key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (base.has(\"errors\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"errors\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString from = elem.getString(\"from\");\n\t\t\t\t\t\tString reason = elem.getString(\"reason\");\n\t\t\t\t\t\terrors.add(new RowError(from, reason));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseNoDocs(rows, errors);\n\t}\n}\n","lineNo":51}
{"Smelly Sample":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class NoDocsOperationImpl extends HttpOperationImpl implements\n\t\tNoDocsOperation {\n\n\tpublic NoDocsOperationImpl(HttpRequest r, NoDocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseNoDocs vr = parseNoDocsViewResult(json);\n\n\t\t\t((NoDocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseNoDocs parseNoDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowNoDocs> rows = new LinkedList<RowNoDocs>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tString id = ids.getJSONObject(i).getString(\"id\");\n\t\t\t\t\t\tString key = ids.getJSONObject(i).getString(\"key\");\n\t\t\t\t\t\tString value = ids.getJSONObject(i).getString(\"value\");\n\t\t\t\t\t\trows.add(new RowNoDocs(id, key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseNoDocs(rows);\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class NoDocsOperationImpl extends HttpOperationImpl implements\n\t\tNoDocsOperation {\n\n\tpublic NoDocsOperationImpl(HttpRequest r, NoDocsCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseNoDocs vr = parseNoDocsViewResult(json);\n\n\t\t\t((NoDocsCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseNoDocs parseNoDocsViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowNoDocs> rows = new LinkedList<RowNoDocs>();\n\t\tfinal Collection<RowError> errors = new LinkedList<RowError>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString id = elem.getString(\"id\");\n\t\t\t\t\t\tString key = elem.getString(\"key\");\n\t\t\t\t\t\tString value = elem.getString(\"value\");\n\t\t\t\t\t\trows.add(new RowNoDocs(id, key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (base.has(\"errors\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"errors\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString from = elem.getString(\"from\");\n\t\t\t\t\t\tString reason = elem.getString(\"reason\");\n\t\t\t\t\t\terrors.add(new RowError(from, reason));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseNoDocs(rows, errors);\n\t}\n}\n","lineNo":61}
{"Smelly Sample":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class ReducedOperationImpl extends HttpOperationImpl implements\n\t\tReducedOperation {\n\n\tpublic ReducedOperationImpl(HttpRequest r, ReducedCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseReduced vr = parseReducedViewResult(json);\n\n\t\t\t((ReducedCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseReduced parseReducedViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowReduced> rows = new LinkedList<RowReduced>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tString key = ids.getJSONObject(i).getString(\"key\");\n\t\t\t\t\t\tString value = ids.getJSONObject(i).getString(\"value\");\n\t\t\t\t\t\trows.add(new RowReduced(key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseReduced(rows);\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class ReducedOperationImpl extends HttpOperationImpl implements\n\t\tReducedOperation {\n\n\tpublic ReducedOperationImpl(HttpRequest r, ReducedCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseReduced vr = parseReducedViewResult(json);\n\n\t\t\t((ReducedCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseReduced parseReducedViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowReduced> rows = new LinkedList<RowReduced>();\n\t\tfinal Collection<RowError> errors = new LinkedList<RowError>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString key = elem.getString(\"key\");\n\t\t\t\t\t\tString value = elem.getString(\"value\");\n\t\t\t\t\t\trows.add(new RowReduced(key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (base.has(\"errors\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"errors\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString from = elem.getString(\"from\");\n\t\t\t\t\t\tString reason = elem.getString(\"reason\");\n\t\t\t\t\t\terrors.add(new RowError(from, reason));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseReduced(rows, errors);\n\t}\n}\n","lineNo":51}
{"Smelly Sample":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class ReducedOperationImpl extends HttpOperationImpl implements\n\t\tReducedOperation {\n\n\tpublic ReducedOperationImpl(HttpRequest r, ReducedCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseReduced vr = parseReducedViewResult(json);\n\n\t\t\t((ReducedCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseReduced parseReducedViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowReduced> rows = new LinkedList<RowReduced>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tString key = ids.getJSONObject(i).getString(\"key\");\n\t\t\t\t\t\tString value = ids.getJSONObject(i).getString(\"value\");\n\t\t\t\t\t\trows.add(new RowReduced(key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseReduced(rows);\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.couchdb;\n\nimport java.text.ParseException;\nimport java.util.Collection;\nimport java.util.LinkedList;\n\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationStatus;\n\nimport org.apache.http.HttpRequest;\nimport org.apache.http.HttpResponse;\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\npublic class ReducedOperationImpl extends HttpOperationImpl implements\n\t\tReducedOperation {\n\n\tpublic ReducedOperationImpl(HttpRequest r, ReducedCallback cb) {\n\t\tsuper(r, cb);\n\t}\n\n\t@Override\n\tpublic void handleResponse(HttpResponse response) {\n\t\tString json = getEntityString(response);\n\t\tint errorcode = response.getStatusLine().getStatusCode();\n\t\ttry {\n\t\t\tOperationStatus status = parseViewForStatus(json, errorcode);\n\t\t\tViewResponseReduced vr = parseReducedViewResult(json);\n\n\t\t\t((ReducedCallback) callback).gotData(vr);\n\t\t\tcallback.receivedStatus(status);\n\t\t} catch (ParseException e) {\n\t\t\texception = new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\"Error parsing JSON\");\n\t\t}\n\t\tcallback.complete();\n\t}\n\n\tprivate ViewResponseReduced parseReducedViewResult(String json)\n\t\t\tthrows ParseException {\n\t\tfinal Collection<RowReduced> rows = new LinkedList<RowReduced>();\n\t\tfinal Collection<RowError> errors = new LinkedList<RowError>();\n\t\tif (json != null) {\n\t\t\ttry {\n\t\t\t\tJSONObject base = new JSONObject(json);\n\t\t\t\tif (base.has(\"rows\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"rows\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString key = elem.getString(\"key\");\n\t\t\t\t\t\tString value = elem.getString(\"value\");\n\t\t\t\t\t\trows.add(new RowReduced(key, value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (base.has(\"errors\")) {\n\t\t\t\t\tJSONArray ids = base.getJSONArray(\"errors\");\n\t\t\t\t\tfor (int i = 0; i < ids.length(); i++) {\n\t\t\t\t\t\tJSONObject elem = ids.getJSONObject(i);\n\t\t\t\t\t\tString from = elem.getString(\"from\");\n\t\t\t\t\t\tString reason = elem.getString(\"reason\");\n\t\t\t\t\t\terrors.add(new RowError(from, reason));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch (JSONException e) {\n\t\t\t\tthrow new ParseException(\"Cannot read json: \" + json, 0);\n\t\t\t}\n\t\t}\n\t\treturn new ViewResponseReduced(rows, errors);\n\t}\n}\n","lineNo":60}
{"Smelly Sample":"package net.spy.memcached.vbucket;\n\nimport org.jboss.netty.bootstrap.ClientBootstrap;\nimport org.jboss.netty.channel.Channel;\nimport org.jboss.netty.channel.ChannelFactory;\nimport org.jboss.netty.channel.ChannelFuture;\nimport org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;\nimport org.jboss.netty.handler.codec.http.DefaultHttpRequest;\nimport org.jboss.netty.handler.codec.http.HttpHeaders;\nimport org.jboss.netty.handler.codec.http.HttpMethod;\nimport org.jboss.netty.handler.codec.http.HttpRequest;\nimport org.jboss.netty.handler.codec.http.HttpVersion;\n\nimport java.net.InetSocketAddress;\nimport java.net.URI;\nimport java.util.Observable;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport java.text.ParseException;\n\nimport net.spy.memcached.vbucket.config.Bucket;\nimport net.spy.memcached.vbucket.config.ConfigurationParser;\n\n/**\n *\n * The BucketMonitor will open an HTTP comet stream to monitor for\n * changes to the list of nodes.  If the list of nodes changes\n */\npublic class BucketMonitor extends Observable {\n\n    private final URI cometStreamURI;\n    private Bucket bucket;\n    private final String httpUser;\n    private final String httpPass;\n    private final ChannelFactory factory;\n    private Channel channel;\n    private final String host;\n    private final int port;\n    private ConfigurationParser configParser;\n    private BucketUpdateResponseHandler handler;\n    /**\n     * The specification version which this client meets.  This will be included\n     * in requests to the server.\n     */\n    public static final String CLIENT_SPEC_VER = \"1.0\";\n\n    /**\n     *\n     * @param cometStreamURI the URI which will stream node changes\n     * @param bucketname the bucketToMonitor name we are monitoring\n     * @param username the username required for HTTP Basic Auth to the restful service\n     * @param password the password required for HTTP Basic Auth to the restful service\n     */\n    public BucketMonitor(URI cometStreamURI,  String bucketname, String username, String password, ConfigurationParser configParser) {\n        super();\n        if (cometStreamURI == null) {\n            throw new IllegalArgumentException(\"cometStreamURI cannot be NULL\");\n        }\n        String scheme = cometStreamURI.getScheme() == null ? \"http\" : cometStreamURI.getScheme();\n        if (!scheme.equals(\"http\")) {\n            // an SslHandler is needed in the pipeline\n            //System.err.println(\"Only HTTP is supported.\");\n            throw new UnsupportedOperationException(\"Only http is supported.\");\n        }\n\n        this.cometStreamURI = cometStreamURI;\n        this.httpUser = username;\n        this.httpPass = password;\n        this.configParser = configParser;\n        this.host = cometStreamURI.getHost();\n        this.port = cometStreamURI.getPort() == -1 ? 80 : cometStreamURI.getPort();\n        factory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool());\n    }\n\n    public void startMonitor() {\n        if (channel != null) {\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.WARNING,\n                    \"Bucket monitor is already started.\");\n            return;\n        }\n        createChannel();\n        this.handler = channel.getPipeline().get(BucketUpdateResponseHandler.class);\n        handler.setBucketMonitor(this);\n        HttpRequest request = prepareRequest(cometStreamURI, host);\n        channel.write(request);\n        try {\n            String response = this.handler.getLastResponse();\n            logFiner(\"Getting server list returns this last chunked response:\\n\" + response);\n            Bucket bucketToMonitor = this.configParser.parseBucket(response);\n            setBucket(bucketToMonitor);\n        } catch (ParseException ex) {\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.WARNING,\n                    \"Invalid client configuration received from server.  Staying with existing configuration.\", ex);\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.FINE,\n                    \"Invalid client configuration received:\\n\" + handler.getLastResponse() + \"\\n\");\n        }\n    }\n\n    protected void createChannel() {\n        // Configure the client.\n        ClientBootstrap bootstrap = new ClientBootstrap(factory);\n\n        // Set up the event pipeline factory.\n        bootstrap.setPipelineFactory(new BucketMonitorPipelineFactory());\n\n        // Start the connection attempt.\n        ChannelFuture future = bootstrap.connect(new InetSocketAddress(host, port));\n\n        // Wait until the connection attempt succeeds or fails.\n        channel = future.awaitUninterruptibly().getChannel();\n        if (!future.isSuccess()) {\n            bootstrap.releaseExternalResources();\n            throw new ConnectionException(\"Could not connect to any pool member.\");\n        }\n        assert(channel != null);\n    }\n\n    protected HttpRequest prepareRequest(URI uri, String h) {\n        // Send the HTTP request.\n        HttpRequest request = new DefaultHttpRequest(\n                HttpVersion.HTTP_1_1, HttpMethod.GET, uri.toASCIIString());\n        request.setHeader(HttpHeaders.Names.HOST, h);\n        if (getHttpUser() != null) {\n            request.setHeader(HttpHeaders.Names.AUTHORIZATION, ConfigurationProviderHTTP.buildAuthHeader(getHttpUser(), getHttpPass()));\n        }\n        request.setHeader(HttpHeaders.Names.CONNECTION, HttpHeaders.Values.CLOSE);  // No keep-alives for this\n        request.setHeader(HttpHeaders.Names.CACHE_CONTROL, HttpHeaders.Values.NO_CACHE);\n        request.setHeader(HttpHeaders.Names.ACCEPT, \"application/json\");\n        request.setHeader(HttpHeaders.Names.USER_AGENT, \"spymemcached vbucket client\");\n        request.setHeader(\"X-memcachekv-Store-Client-Specification-Version\", CLIENT_SPEC_VER);\n        return request;\n    }\n\n    /**\n     * Update the config if it has changed and notify our\n     * observers.\n     *\n     * @param bucketToMonitor the bucketToMonitor to set\n     */\n    private void setBucket(Bucket bucket) {\n        if (this.bucket == null || !this.bucket.equals(bucket)) {\n            this.bucket = bucket;\n            setChanged();\n            notifyObservers(this.bucket);\n        }\n    }\n\n    /**\n     * @return the httpUser\n     */\n    public String getHttpUser() {\n        return httpUser;\n    }\n\n    /**\n     * @return the httpPass\n     */\n    public String getHttpPass() {\n        return httpPass;\n    }\n\n    private void logFiner(String msg) {\n        Logger.getLogger(BucketMonitor.class.getName()).log(Level.FINER, msg);\n    }\n\n    /**\n     * Shut down the monitor in a graceful way (and release all resources)\n     */\n    public void shutdown() {\n        shutdown(-1, TimeUnit.MILLISECONDS);\n    }\n\n    /**\n     * Shut down this monitor in a graceful way\n     *\n     * @param timeout\n     * @param unit\n     */\n    public void shutdown(long timeout, TimeUnit unit) {\n        deleteObservers();\n        if (channel != null) {\n            channel.close().awaitUninterruptibly(timeout, unit);\n        }\n        factory.releaseExternalResources();\n    }\n\n    protected void invalidate() {\n        try {\n            String response = handler.getLastResponse();\n            Bucket updatedBucket = this.configParser.parseBucket(response);\n            setBucket(updatedBucket);\n        } catch (ParseException e) {\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.SEVERE,\n                    \"Invalid client configuration received from server.  Staying with existing configuration.\", e);\n        }\n    }\n\n    public void setConfigParser(ConfigurationParser configParser) {\n        this.configParser = configParser;\n    }\n}\n","Method after Refactoring":"package net.spy.memcached.vbucket;\n\nimport java.io.UnsupportedEncodingException;\nimport org.jboss.netty.bootstrap.ClientBootstrap;\nimport org.jboss.netty.channel.Channel;\nimport org.jboss.netty.channel.ChannelFactory;\nimport org.jboss.netty.channel.ChannelFuture;\nimport org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;\nimport org.jboss.netty.handler.codec.http.DefaultHttpRequest;\nimport org.jboss.netty.handler.codec.http.HttpHeaders;\nimport org.jboss.netty.handler.codec.http.HttpMethod;\nimport org.jboss.netty.handler.codec.http.HttpRequest;\nimport org.jboss.netty.handler.codec.http.HttpVersion;\n\nimport java.net.InetSocketAddress;\nimport java.net.URI;\nimport java.util.Observable;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport java.text.ParseException;\n\nimport net.spy.memcached.vbucket.config.Bucket;\nimport net.spy.memcached.vbucket.config.ConfigurationParser;\n\n/**\n *\n * The BucketMonitor will open an HTTP comet stream to monitor for\n * changes to the list of nodes.  If the list of nodes changes\n */\npublic class BucketMonitor extends Observable {\n\n    private final URI cometStreamURI;\n    private Bucket bucket;\n    private final String httpUser;\n    private final String httpPass;\n    private final ChannelFactory factory;\n    private Channel channel;\n    private final String host;\n    private final int port;\n    private ConfigurationParser configParser;\n    private BucketUpdateResponseHandler handler;\n    /**\n     * The specification version which this client meets.  This will be included\n     * in requests to the server.\n     */\n    public static final String CLIENT_SPEC_VER = \"1.0\";\n\n    /**\n     *\n     * @param cometStreamURI the URI which will stream node changes\n     * @param bucketname the bucketToMonitor name we are monitoring\n     * @param username the username required for HTTP Basic Auth to the restful service\n     * @param password the password required for HTTP Basic Auth to the restful service\n     */\n    public BucketMonitor(URI cometStreamURI,  String bucketname, String username, String password, ConfigurationParser configParser) {\n        super();\n        if (cometStreamURI == null) {\n            throw new IllegalArgumentException(\"cometStreamURI cannot be NULL\");\n        }\n        String scheme = cometStreamURI.getScheme() == null ? \"http\" : cometStreamURI.getScheme();\n        if (!scheme.equals(\"http\")) {\n            // an SslHandler is needed in the pipeline\n            //System.err.println(\"Only HTTP is supported.\");\n            throw new UnsupportedOperationException(\"Only http is supported.\");\n        }\n\n        this.cometStreamURI = cometStreamURI;\n        this.httpUser = username;\n        this.httpPass = password;\n        this.configParser = configParser;\n        this.host = cometStreamURI.getHost();\n        this.port = cometStreamURI.getPort() == -1 ? 80 : cometStreamURI.getPort();\n        factory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool());\n    }\n\n    public void startMonitor() {\n        if (channel != null) {\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.WARNING,\n                    \"Bucket monitor is already started.\");\n            return;\n        }\n        createChannel();\n        this.handler = channel.getPipeline().get(BucketUpdateResponseHandler.class);\n        handler.setBucketMonitor(this);\n        HttpRequest request = prepareRequest(cometStreamURI, host);\n        channel.write(request);\n        try {\n            String response = this.handler.getLastResponse();\n            logFiner(\"Getting server list returns this last chunked response:\\n\" + response);\n            Bucket bucketToMonitor = this.configParser.parseBucket(response);\n            setBucket(bucketToMonitor);\n        } catch (ParseException ex) {\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.WARNING,\n                    \"Invalid client configuration received from server.  Staying with existing configuration.\", ex);\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.FINE,\n                    \"Invalid client configuration received:\\n\" + handler.getLastResponse() + \"\\n\");\n        }\n    }\n\n    protected void createChannel() {\n        // Configure the client.\n        ClientBootstrap bootstrap = new ClientBootstrap(factory);\n\n        // Set up the event pipeline factory.\n        bootstrap.setPipelineFactory(new BucketMonitorPipelineFactory());\n\n        // Start the connection attempt.\n        ChannelFuture future = bootstrap.connect(new InetSocketAddress(host, port));\n\n        // Wait until the connection attempt succeeds or fails.\n        channel = future.awaitUninterruptibly().getChannel();\n        if (!future.isSuccess()) {\n            bootstrap.releaseExternalResources();\n            throw new ConnectionException(\"Could not connect to any pool member.\");\n        }\n        assert(channel != null);\n    }\n\n    protected HttpRequest prepareRequest(URI uri, String h) {\n        // Send the HTTP request.\n        HttpRequest request = new DefaultHttpRequest(\n                HttpVersion.HTTP_1_1, HttpMethod.GET, uri.toASCIIString());\n        request.setHeader(HttpHeaders.Names.HOST, h);\n        if (getHttpUser() != null) {\n\t    String basicAuthHeader;\n\t    try {\n\t\tbasicAuthHeader = ConfigurationProviderHTTP.buildAuthHeader(getHttpUser(), getHttpPass());\n\t\trequest.setHeader(HttpHeaders.Names.AUTHORIZATION, basicAuthHeader);\n\t    } catch (UnsupportedEncodingException ex) {\n\t\tthrow new RuntimeException(\"Could not encode specified credentials for HTTP request.\", ex);\n\t    }\n        }\n        request.setHeader(HttpHeaders.Names.CONNECTION, HttpHeaders.Values.CLOSE);  // No keep-alives for this\n        request.setHeader(HttpHeaders.Names.CACHE_CONTROL, HttpHeaders.Values.NO_CACHE);\n        request.setHeader(HttpHeaders.Names.ACCEPT, \"application/json\");\n        request.setHeader(HttpHeaders.Names.USER_AGENT, \"spymemcached vbucket client\");\n        request.setHeader(\"X-memcachekv-Store-Client-Specification-Version\", CLIENT_SPEC_VER);\n        return request;\n    }\n\n    /**\n     * Update the config if it has changed and notify our\n     * observers.\n     *\n     * @param bucketToMonitor the bucketToMonitor to set\n     */\n    private void setBucket(Bucket bucket) {\n        if (this.bucket == null || !this.bucket.equals(bucket)) {\n            this.bucket = bucket;\n            setChanged();\n            notifyObservers(this.bucket);\n        }\n    }\n\n    /**\n     * @return the httpUser\n     */\n    public String getHttpUser() {\n        return httpUser;\n    }\n\n    /**\n     * @return the httpPass\n     */\n    public String getHttpPass() {\n        return httpPass;\n    }\n\n    private void logFiner(String msg) {\n        Logger.getLogger(BucketMonitor.class.getName()).log(Level.FINER, msg);\n    }\n\n    /**\n     * Shut down the monitor in a graceful way (and release all resources)\n     */\n    public void shutdown() {\n        shutdown(-1, TimeUnit.MILLISECONDS);\n    }\n\n    /**\n     * Shut down this monitor in a graceful way\n     *\n     * @param timeout\n     * @param unit\n     */\n    public void shutdown(long timeout, TimeUnit unit) {\n        deleteObservers();\n        if (channel != null) {\n            channel.close().awaitUninterruptibly(timeout, unit);\n        }\n        factory.releaseExternalResources();\n    }\n\n    protected void invalidate() {\n        try {\n            String response = handler.getLastResponse();\n            Bucket updatedBucket = this.configParser.parseBucket(response);\n            setBucket(updatedBucket);\n        } catch (ParseException e) {\n            Logger.getLogger(BucketMonitor.class.getName()).log(Level.SEVERE,\n                    \"Invalid client configuration received from server.  Staying with existing configuration.\", e);\n        }\n    }\n\n    public void setConfigParser(ConfigurationParser configParser) {\n        this.configParser = configParser;\n    }\n}\n","lineNo":127}
{"Smelly Sample":"package net.spy.memcached;\n\nimport java.nio.ByteBuffer;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SyncThread;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.transcoders.SerializingTranscoder;\nimport net.spy.memcached.transcoders.Transcoder;\n\n\npublic abstract class ProtocolBaseCase extends ClientBaseCase {\n\n\tpublic void testAssertions() {\n\t\tboolean caught=false;\n\t\ttry {\n\t\t\tassert false;\n\t\t} catch(AssertionError e) {\n\t\t\tcaught=true;\n\t\t}\n\t\tassertTrue(\"Assertions are not enabled!\", caught);\n\t}\n\n\tpublic void testGetStats() throws Exception {\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats();\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"curr_items\"));\n\t}\n\n\tpublic void testGetStatsSlabs() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t    return;\n\t\t}\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no slabs to check.\n\t\tclient.set(\"slabinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"slabs\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"1:chunk_size\"));\n\t}\n\n\tpublic void testGetStatsSizes() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there may\n\t\t// be no sizes to check.  Note the protocol says\n\t\t// flushed/expired items may come back in stats sizes and we\n\t\t// use flush when testing, so we check that there's at least\n\t\t// one.\n\t\tclient.set(\"sizeinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"sizes\");\n\t\tSystem.out.println(\"Stats sizes:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString noItemsSmall = oneStat.get(\"96\");\n\t\tassertTrue(Integer.parseInt(noItemsSmall) >= 1);\n\t}\n\n\tpublic void testGetStatsCacheDump() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there\n\t\t// won't be anything to dump\n\t\tclient.set(\"dumpinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats =\n\t\t\t\tclient.getStats(\"cachedump 1 10000\");\n\t\tSystem.out.println(\"Stats cachedump:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString val = oneStat.get(\"dumpinitializer\");\n\t\tassertTrue(val + \"doesn't match\", val.matches(\"\\\\[2 b; \\\\d+ s\\\\]\"));\n\t}\n\n\tpublic void testDelayedFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tclient.flush(2);\n\t\tThread.sleep(2100);\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testNoop() {\n\t\t// This runs through the startup/flush cycle\n\t}\n\n\tpublic void testDoubleShutdown() {\n\t\tclient.shutdown();\n\t\tclient.shutdown();\n\t}\n\n\tpublic void testSimpleGet() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testSimpleCASGets() throws Exception {\n\t\tassertNull(client.gets(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.gets(\"test1\").getValue());\n\t}\n\n\tpublic void testCAS() throws Exception {\n\t\tfinal String key=\"castestkey\";\n\t\t// First, make sure it doesn't work for a non-existing value.\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\tCASResponse.NOT_FOUND,\n\t\t\tclient.cas(key, 0x7fffffffffL, \"bad value\"));\n\n\t\t// OK, stick a value in here.\n\t\tassertTrue(client.add(key, 5, \"original value\").get());\n\t\tCASValue<?> getsVal = client.gets(key);\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// Now try it with an existing value, but wrong CAS id\n\t\tassertSame(\"Expected error CASing with invalid id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas() + 1, \"broken value\"));\n\t\t// Validate the original value is still in tact.\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// OK, now do a valid update\n\t\tassertSame(\"Expected successful CAS with correct id (\"\n\t\t\t+ getsVal.getCas() + \")\",\n\t\t\tCASResponse.OK,\n\t\t\tclient.cas(key, getsVal.getCas(), \"new value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\n\t\t// Test a CAS replay\n\t\tassertSame(\"Expected unsuccessful CAS with replayed id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas(), \"crap value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\t}\n\n\tpublic void testReallyLongCASId() throws Exception {\n\t\tString key = \"this-is-my-key\";\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\t\tCASResponse.NOT_FOUND,\n\t\t\t\tclient.cas(key, 9223372036854775807l, \"bad value\"));\n\t}\n\n\tpublic void testExtendedUTF8Key() throws Exception {\n\t\tString key=\"\\u2013\\u00ba\\u2013\\u220f\\u2014\\u00c4\";\n\t\tassertNull(client.get(key));\n\t\tclient.set(key, 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(key));\n\t}\n\n\tpublic void testInvalidKey1() throws Exception {\n\t\ttry {\n\t\t\tclient.get(\"key with spaces\");\n\t\t\tfail(\"Expected IllegalArgumentException getting key with spaces\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey2() throws Exception {\n\t\ttry {\n\t\t\tStringBuilder longKey=new StringBuilder();\n\t\t\tfor(int i=0; i<251; i++) {\n\t\t\t\tlongKey.append(\"a\");\n\t\t\t}\n\t\t\tclient.get(longKey.toString());\n\t\t\tfail(\"Expected IllegalArgumentException getting too long of a key\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey3() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\n\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey4() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\r\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey5() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\0\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBlank() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBulk() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.getBulk(\"Key key2\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testParallelSetGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tMap<String, Object> m=client.getBulk(\"test0\", \"test1\", \"test2\",\n\t\t\t\t\t\"test3\", \"test4\", \"test5\", \"test6\", \"test7\", \"test8\",\n\t\t\t\t\t\"test9\", \"test10\"); // Yes, I intentionally ran over.\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, m.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetAutoMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tclient.set(\"testparallel\", 5, \"parallelvalue\");\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"parallelvalue\", client.get(\"testparallel\"));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testAdd() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\").get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\").get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testAddWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\", t).get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\", t).get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t}\n\n\tpublic void testAddNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.add(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testSetNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.set(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testReplaceNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.replace(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testUpdate() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.replace(\"test1\", 5, \"test1value\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testUpdateWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tclient.replace(\"test1\", 5, \"test1value\", t);\n\t\tassertNull(client.get(\"test1\", t));\n\t}\n\n\t// Just to make sure the sequence is being handled correctly\n\tpublic void testMixedSetsAndUpdates() throws Exception {\n\t\tCollection<Future<Boolean>> futures=new ArrayList<Future<Boolean>>();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<100; i++) {\n\t\t\tString key=\"k\" + i;\n\t\t\tfutures.add(client.set(key, 10, key));\n\t\t\tfutures.add(client.add(key, 10, \"a\" + i));\n\t\t\tkeys.add(key);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(100, m.size());\n\t\tfor(Map.Entry<String, Object> me : m.entrySet()) {\n\t\t\tassertEquals(me.getKey(), me.getValue());\n\t\t}\n\t\tfor(Iterator<Future<Boolean>> i=futures.iterator();i.hasNext();) {\n\t\t\tassertTrue(i.next().get());\n\t\t\tassertFalse(i.next().get());\n\t\t}\n\t}\n\n\tpublic void testGetBulk() throws Exception {\n\t\tCollection<String> keys=Arrays.asList(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(0, client.getBulk(keys).size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(keys);\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVararg() throws Exception {\n\t\tassertEquals(0, client.getBulk(\"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tMap<String, String> vals=client.getBulk(t, \"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(t,\n\t\t\t\t\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get().get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkWithTranscoderIterator() throws Exception {\n\t\tArrayList<String> keys = new ArrayList<String>();\n\t\tkeys.add(\"test1\");\n\t\tkeys.add(\"test2\");\n\t\tkeys.add(\"test3\");\n\n\t\tArrayList<Transcoder<String>> tcs = new ArrayList<Transcoder<String>>(keys.size());\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\t// Any transcoders listed after list of keys should be\n\t\t// ignored.\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\tassertEquals(0, client.asyncGetBulk(keys, tcs.listIterator()).get().size());\n\n\t\tclient.set(keys.get(0), 5, \"val1\", tcs.get(0));\n\t\tclient.set(keys.get(1), 5, \"val2\", tcs.get(1));\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(keys, tcs.listIterator());\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(keys.get(0)));\n\t\tassertEquals(\"val2\", vals.get().get(keys.get(1)));\n\n\t\t// Set with one transcoder with the proper key and get\n\t\t// with another transcoder with the wrong key.\n\t\tkeys.add(0, \"test4\");\n\t\tTranscoder<String> encodeTranscoder = new TestWithKeyTranscoder(keys.get(0));\n\t\tclient.set(keys.get(0), 5, \"val4\", encodeTranscoder).get();\n\n\t\tTranscoder<String> decodeTranscoder = new TestWithKeyTranscoder(\"not \" + keys.get(0));\n\t\ttcs.add(0, decodeTranscoder);\n\t\ttry {\n\t\t\tclient.asyncGetBulk(keys, tcs.listIterator()).get();\n\t\t\tfail(\"Expected ExecutionException caused by key mismatch\");\n\t\t} catch (java.util.concurrent.ExecutionException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testAvailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(new ArrayList<String>(\n\t\t\t\tCollections.singleton(getExpectedVersionSource())),\n\t\t\tstringify(client.getAvailableServers()));\n\t}\n\n\tpublic void testUnavailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(Collections.emptyList(), client.getUnavailableServers());\n\t}\n\n\tprotected abstract String getExpectedVersionSource();\n\n\tpublic void testGetVersions() throws Exception {\n\t\tMap<SocketAddress, String> vs=client.getVersions();\n\t\tassertEquals(1, vs.size());\n\t\tMap.Entry<SocketAddress, String> me=vs.entrySet().iterator().next();\n\t\tassertEquals(getExpectedVersionSource(), me.getKey().toString());\n\t\tassertNotNull(me.getValue());\n\t}\n\n\tpublic void testNonexistentMutate() throws Exception {\n\t\tassertEquals(-1, client.incr(\"nonexistent\", 1));\n\t\tassertEquals(-1, client.decr(\"nonexistent\", 1));\n\t}\n\n\tpublic void testMutateWithDefault() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9));\n\t}\n\n\tpublic void testMutateWithDefaultAndExp() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9, 1));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9, 1));\n\t\tThread.sleep(2000);\n\t\tassertNull(client.get(\"mtest\"));\n\t}\n\n\tpublic void testAsyncIncrement() throws Exception {\n\t\tString k=\"async-incr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(6, (long)f.get());\n\t}\n\n\tpublic void testAsyncIncrementNonExistent() throws Exception {\n\t\tString k=\"async-incr-non-existent\";\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrement() throws Exception {\n\t\tString k=\"async-decr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(4, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrementNonExistent() throws Exception {\n\t\tString k=\"async-decr-non-existent\";\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testConcurrentMutation() throws Throwable {\n\t\tint num=SyncThread.getDistinctResultCount(10, new Callable<Long>(){\n\t\t\tpublic Long call() throws Exception {\n\t\t\t\treturn client.incr(\"mtest\", 1, 11);\n\t\t\t}});\n\t\tassertEquals(10, num);\n\t}\n\n\tpublic void testImmediateDelete() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tclient.delete(\"test1\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassertTrue(client.flush().get());\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testGracefulShutdown() throws Exception {\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\t// Get a new client\n\t\tinitClient();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tkeys.add(\"t\" + i);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(1000, m.size());\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tassertEquals(i, m.get(\"t\" + i));\n\t\t}\n\t}\n\n\tpublic void testSyncGetTimeouts() throws Exception {\n\t\tfinal String key=\"timeoutTestKey\";\n\t\tfinal String value=\"timeoutTestValue\";\n\t\t// Shutting down the default client to get one with a short timeout.\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\tinitClient(new DefaultConnectionFactory() {\n\t\t\t@Override\n\t\t\tpublic long getOperationTimeout() {\n\t\t\t\treturn 2;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic int getTimeoutExceptionThreshold() {\n\t\t\t\treturn 1000000;\n\t\t\t}\n\t\t});\n\n\t\tThread.sleep(100); // allow connections to be established\n\n\t\tint j = 0;\n\t\tboolean set = false;\n\t\tdo {\n\t\t\tset = client.set(key, 0, value).get();\n\t\t\tj++;\n\t\t} while (!set && j < 10);\n\t\tassert set == true;\n\n\t\tint i = 0;\n\t\ttry {\n\t\t\tfor(i = 0; i < 1000000; i++) {\n\t\t\t\tclient.get(key);\n\t\t\t}\n\t\t\tthrow new Exception(\"Didn't get a timeout.\");\n\t\t} catch(RuntimeException e) {\n\t\t\tSystem.out.println(\"Got a timeout at iteration \" + i + \".\");\n\t\t}\n\t\tThread.sleep(100); // let whatever caused the timeout to pass\n\t\ttry {\n\t\t\tif (value.equals(client.asyncGet(key).get(30, TimeUnit.SECONDS))) {\n\t\t\tSystem.out.println(\"Got the right value.\");\n\t\t} else {\n\t\t\tthrow new Exception(\"Didn't get the expected value.\");\n\t\t}\n\t\t} catch (java.util.concurrent.TimeoutException timeoutException) {\n\t\t        debugNodeInfo(client.getNodeLocator().getAll());\n\t\t\tthrow new Exception(\"Unexpected timeout after 30 seconds waiting\", timeoutException);\n\t\t}\n\t}\n\n\tprivate void debugNodeInfo(Collection<MemcachedNode> nodes) {\n\t    System.err.println(\"Debug nodes:\");\n\t    for (MemcachedNode node : nodes) {\n\t\t    System.err.println(node);\n\t\t    System.err.println(\"Is active? \" + node.isActive());\n\t\t    System.err.println(\"Has read operation? \" + node.hasReadOp() + \" Has write operation? \" + node.hasWriteOp());\n\t\ttry {\n\t\t    System.err.println(\"Has timed out this many times: \" + node.getContinuousTimeout());\n\t\t    System.err.println(\"Write op: \" + node.getCurrentWriteOp());\n\t\t    System.err.println(\"Read op: \" + node.getCurrentReadOp());\n\t\t} catch (UnsupportedOperationException e) {\n\t\t    System.err.println(\"Node does not support full interface, likely read only.\");\n\t\t}\n\t    }\n\t}\n\n\tpublic void xtestGracefulShutdownTooSlow() throws Exception {\n\t\tfor(int i=0; i<10000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertFalse(\"Weird, shut down too fast\",\n\t\t\tclient.shutdown(1, TimeUnit.MILLISECONDS));\n\n\t\ttry {\n\t\t\tMap<SocketAddress, String> m = client.getVersions();\n\t\t\tfail(\"Expected failure, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\tassertEquals(\"Shutting down\", e.getMessage());\n\t\t}\n\n\t\t// Get a new client\n\t\tinitClient();\n\t}\n\n\tpublic void testStupidlyLargeSetAndSizeOverride() throws Exception {\n\t\tif (isMembase()) {\n\t\t    return;\n\t\t}\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder(Integer.MAX_VALUE);\n\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(ExecutionException e) {\n\t\t\tSystem.err.println(\"Successful failure setting bigassthing.  Ass size \" + data.length + \" bytes doesn't fit.\");\n\t\t\te.printStackTrace();\n\t\t\tOperationException oe=(OperationException)e.getCause();\n\t\t\tassertSame(OperationErrorType.SERVER, oe.getType());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testStupidlyLargeSet() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder();\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Cannot cache data larger than \"\n\t\t\t\t\t+ CachedData.MAX_SIZE + \" bytes \"\n\t\t\t\t\t+ \"(you tried to cache a \" + data.length + \" byte object)\",\n\t\t\t\te.getMessage());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testQueueAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tObject o=client.get(\"k\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + o);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testMultiReqAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tMap<String, ?> m=client.getBulk(\"k1\", \"k2\", \"k3\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testBroadcastAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tFuture<?> f=client.flush();\n\t\t\tfail(\"Expected IllegalStateException, got \" + f.get());\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testABunchOfCancelledOperations() throws Exception {\n\t\tfinal String k=\"bunchOCancel\";\n\t\tCollection<Future<?>> futures=new ArrayList<Future<?>>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tfutures.add(client.set(k, 5, \"xval\"));\n\t\t\tfutures.add(client.asyncGet(k));\n\t\t}\n\t\tFuture<Boolean> sf=client.set(k, 5, \"myxval\");\n\t\tFuture<Object> gf=client.asyncGet(k);\n\t\tfor(Future<?> f : futures) {\n\t\t\tf.cancel(true);\n\t\t}\n\t\tassertTrue(sf.get());\n\t\tassertEquals(\"myxval\", gf.get());\n\t}\n\n\tpublic void testUTF8Key() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testUTF8KeyDelete() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tassertTrue(client.delete(key).get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testUTF8MultiGet() throws Exception {\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<50; i++) {\n\t\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\"\n\t\t\t\t+ System.currentTimeMillis() + \".\" + i;\n\t\t\tassertTrue(client.set(key, 6000, value).get());\n\t\t\tkeys.add(key);\n\t\t}\n\n\t\tMap<String, Object> vals = client.getBulk(keys);\n\t\tassertEquals(keys.size(), vals.size());\n\t\tfor(Object o : vals.values()) {\n\t\t\tassertEquals(value, o);\n\t\t}\n\t\tassertTrue(keys.containsAll(vals.keySet()));\n\t}\n\n\tpublic void testUTF8Value() throws Exception {\n\t\tfinal String key = \"junit.plaintext.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ \"\n\t\t\t+ \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testAppend() throws Exception {\n\t\tfinal String key=\"append.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.append(0, key, \"es\").get());\n\t\tassertEquals(\"testes\", client.get(key));\n\t}\n\n\tpublic void testPrepend() throws Exception {\n\t\tfinal String key=\"prepend.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.prepend(0, key, \"es\").get());\n\t\tassertEquals(\"estest\", client.get(key));\n\t}\n\n\tpublic void testAppendNoSuchKey() throws Exception {\n\t\tfinal String key=\"append.missing\";\n\t\tassertFalse(client.append(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testPrependNoSuchKey() throws Exception {\n\t\tfinal String key=\"prepend.missing\";\n\t\tassertFalse(client.prepend(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tprivate static class TestTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885206;\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\t\t\treturn new String(d.getData());\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\treturn new CachedData(flags, o.getBytes(), getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate static class TestWithKeyTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885207;\n\n\t\tprivate final String key;\n\n\t\tTestWithKeyTranscoder(String k) {\n\t\t\tkey = k;\n\t\t}\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(d.getData());\n\n\t\t\tint keyLength = bb.getInt();\n\t\t\tbyte[] keyBytes = new byte[keyLength];\n\t\t\tbb.get(keyBytes);\n\t\t\tString k = new String(keyBytes);\n\n\t\t\tassertEquals(key, k);\n\n\t\t\tint valueLength = bb.getInt();\n\t\t\tbyte[] valueBytes = new byte[valueLength];\n\t\t\tbb.get(valueBytes);\n\n\t\t\treturn new String(valueBytes);\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\tbyte[] keyBytes = key.getBytes();\n\t\t\tbyte[] valueBytes = o.getBytes();\n\t\t\tint length = 4 + keyBytes.length + 4 + valueBytes.length;\n\t\t\tbyte[] bytes = new byte[length];\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(bytes);\n\t\t\tbb.putInt(keyBytes.length).put(keyBytes);\n\t\t\tbb.putInt(valueBytes.length).put(valueBytes);\n\n\t\t\treturn new CachedData(flags, bytes, getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached;\n\nimport java.nio.ByteBuffer;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SyncThread;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.transcoders.SerializingTranscoder;\nimport net.spy.memcached.transcoders.Transcoder;\n\n\npublic abstract class ProtocolBaseCase extends ClientBaseCase {\n\n\tpublic void testAssertions() {\n\t\tboolean caught=false;\n\t\ttry {\n\t\t\tassert false;\n\t\t} catch(AssertionError e) {\n\t\t\tcaught=true;\n\t\t}\n\t\tassertTrue(\"Assertions are not enabled!\", caught);\n\t}\n\n\tpublic void testGetStats() throws Exception {\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats();\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"curr_items\"));\n\t}\n\n\tpublic void testGetStatsSlabs() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t    return;\n\t\t}\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no slabs to check.\n\t\tclient.set(\"slabinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"slabs\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"1:chunk_size\"));\n\t}\n\n\tpublic void testGetStatsSizes() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there may\n\t\t// be no sizes to check.  Note the protocol says\n\t\t// flushed/expired items may come back in stats sizes and we\n\t\t// use flush when testing, so we check that there's at least\n\t\t// one.\n\t\tclient.set(\"sizeinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"sizes\");\n\t\tSystem.out.println(\"Stats sizes:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString noItemsSmall = oneStat.get(\"96\");\n\t\tassertTrue(Integer.parseInt(noItemsSmall) >= 1);\n\t}\n\n\tpublic void testGetStatsCacheDump() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there\n\t\t// won't be anything to dump\n\t\tclient.set(\"dumpinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats =\n\t\t\t\tclient.getStats(\"cachedump 1 10000\");\n\t\tSystem.out.println(\"Stats cachedump:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString val = oneStat.get(\"dumpinitializer\");\n\t\tassertTrue(val + \"doesn't match\", val.matches(\"\\\\[2 b; \\\\d+ s\\\\]\"));\n\t}\n\n\tpublic void testDelayedFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassert client.set(\"test1\", 5, \"test1value\").getStatus().isSuccess();\n\t\tassert client.set(\"test2\", 5, \"test2value\").getStatus().isSuccess();\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassert client.flush(2).getStatus().isSuccess();\n\t\tThread.sleep(2100);\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t\tassert !client.asyncGet(\"test1\").getStatus().isSuccess();\n\t\tassert !client.asyncGet(\"test2\").getStatus().isSuccess();\n\t}\n\n\tpublic void testNoop() {\n\t\t// This runs through the startup/flush cycle\n\t}\n\n\tpublic void testDoubleShutdown() {\n\t\tclient.shutdown();\n\t\tclient.shutdown();\n\t}\n\n\tpublic void testSimpleGet() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testSimpleCASGets() throws Exception {\n\t\tassertNull(client.gets(\"test1\"));\n\t\tassert client.set(\"test1\", 5, \"test1value\").getStatus().isSuccess();\n\t\tassertEquals(\"test1value\", client.gets(\"test1\").getValue());\n\t}\n\n\tpublic void testCAS() throws Exception {\n\t\tfinal String key=\"castestkey\";\n\t\t// First, make sure it doesn't work for a non-existing value.\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\tCASResponse.NOT_FOUND,\n\t\t\tclient.cas(key, 0x7fffffffffL, \"bad value\"));\n\n\t\t// OK, stick a value in here.\n\t\tassertTrue(client.add(key, 5, \"original value\").get());\n\t\tCASValue<?> getsVal = client.gets(key);\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// Now try it with an existing value, but wrong CAS id\n\t\tassertSame(\"Expected error CASing with invalid id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas() + 1, \"broken value\"));\n\t\t// Validate the original value is still in tact.\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// OK, now do a valid update\n\t\tassertSame(\"Expected successful CAS with correct id (\"\n\t\t\t+ getsVal.getCas() + \")\",\n\t\t\tCASResponse.OK,\n\t\t\tclient.cas(key, getsVal.getCas(), \"new value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\n\t\t// Test a CAS replay\n\t\tassertSame(\"Expected unsuccessful CAS with replayed id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas(), \"crap value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\t}\n\n\tpublic void testReallyLongCASId() throws Exception {\n\t\tString key = \"this-is-my-key\";\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\t\tCASResponse.NOT_FOUND,\n\t\t\t\tclient.cas(key, 9223372036854775807l, \"bad value\"));\n\t}\n\n\tpublic void testExtendedUTF8Key() throws Exception {\n\t\tString key=\"\\u2013\\u00ba\\u2013\\u220f\\u2014\\u00c4\";\n\t\tassertNull(client.get(key));\n\t\tassert client.set(key, 5, \"test1value\").getStatus().isSuccess();\n\t\tassertEquals(\"test1value\", client.get(key));\n\t}\n\n\tpublic void testInvalidKey1() throws Exception {\n\t\ttry {\n\t\t\tclient.get(\"key with spaces\");\n\t\t\tfail(\"Expected IllegalArgumentException getting key with spaces\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey2() throws Exception {\n\t\ttry {\n\t\t\tStringBuilder longKey=new StringBuilder();\n\t\t\tfor(int i=0; i<251; i++) {\n\t\t\t\tlongKey.append(\"a\");\n\t\t\t}\n\t\t\tclient.get(longKey.toString());\n\t\t\tfail(\"Expected IllegalArgumentException getting too long of a key\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey3() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\n\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey4() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\r\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey5() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\0\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBlank() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBulk() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.getBulk(\"Key key2\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testParallelSetGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassert client.set(\"test\" + i, 5, \"value\" + i).getStatus().isSuccess();\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassert client.set(\"test\" + i, 5, \"value\" + i).getStatus().isSuccess();\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tMap<String, Object> m=client.getBulk(\"test0\", \"test1\", \"test2\",\n\t\t\t\t\t\"test3\", \"test4\", \"test5\", \"test6\", \"test7\", \"test8\",\n\t\t\t\t\t\"test9\", \"test10\"); // Yes, I intentionally ran over.\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, m.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetAutoMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tassert client.set(\"testparallel\", 5, \"parallelvalue\").getStatus().isSuccess();\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"parallelvalue\", client.get(\"testparallel\"));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testAdd() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassert !client.asyncGet(\"test1\").getStatus().isSuccess();\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\").get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassert client.asyncGet(\"test1\").getStatus().isSuccess();\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\").get());\n\t\tassert !client.add(\"test1\", 5, \"ignoredvalue\").getStatus().isSuccess();\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testAddWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tassert !client.asyncGet(\"test1\", t).getStatus().isSuccess();\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\", t).get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\", t).get());\n\t\tassert !client.add(\"test1\", 5, \"ignoredvalue\", t).getStatus().isSuccess();\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t}\n\n\tpublic void testAddNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.add(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testSetNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.set(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testReplaceNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.replace(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testUpdate() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.replace(\"test1\", 5, \"test1value\");\n\t\tassert !client.replace(\"test1\", 5, \"test1value\").getStatus().isSuccess();\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testUpdateWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tclient.replace(\"test1\", 5, \"test1value\", t);\n\t\tassert !client.replace(\"test1\", 5, \"test1value\", t).getStatus().isSuccess();\n\t\tassertNull(client.get(\"test1\", t));\n\t}\n\n\t// Just to make sure the sequence is being handled correctly\n\tpublic void testMixedSetsAndUpdates() throws Exception {\n\t\tCollection<Future<Boolean>> futures=new ArrayList<Future<Boolean>>();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<100; i++) {\n\t\t\tString key=\"k\" + i;\n\t\t\tfutures.add(client.set(key, 10, key));\n\t\t\tfutures.add(client.add(key, 10, \"a\" + i));\n\t\t\tkeys.add(key);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(100, m.size());\n\t\tfor(Map.Entry<String, Object> me : m.entrySet()) {\n\t\t\tassertEquals(me.getKey(), me.getValue());\n\t\t}\n\t\tfor(Iterator<Future<Boolean>> i=futures.iterator();i.hasNext();) {\n\t\t\tassertTrue(i.next().get());\n\t\t\tassertFalse(i.next().get());\n\t\t}\n\t}\n\n\tpublic void testGetBulk() throws Exception {\n\t\tCollection<String> keys=Arrays.asList(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(0, client.getBulk(keys).size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(keys);\n\t\tassert client.asyncGetBulk(keys).getStatus().isSuccess();\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVararg() throws Exception {\n\t\tassertEquals(0, client.getBulk(\"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(\"test1\", \"test2\", \"test3\");\n\t\tassert client.asyncGetBulk(\"test1\", \"test2\", \"test3\").getStatus().isSuccess();\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tMap<String, String> vals=client.getBulk(t, \"test1\", \"test2\", \"test3\");\n\t\tassert client.asyncGetBulk(t, \"test1\", \"test2\", \"test3\").getStatus().isSuccess();\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tBulkFuture<Map<String, String>> vals=client.asyncGetBulk(t,\n\t\t\t\t\"test1\", \"test2\", \"test3\");\n\t\tassert vals.getStatus().isSuccess();\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get().get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkWithTranscoderIterator() throws Exception {\n\t\tArrayList<String> keys = new ArrayList<String>();\n\t\tkeys.add(\"test1\");\n\t\tkeys.add(\"test2\");\n\t\tkeys.add(\"test3\");\n\n\t\tArrayList<Transcoder<String>> tcs = new ArrayList<Transcoder<String>>(keys.size());\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\t// Any transcoders listed after list of keys should be\n\t\t// ignored.\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\tassertEquals(0, client.asyncGetBulk(keys, tcs.listIterator()).get().size());\n\n\t\tclient.set(keys.get(0), 5, \"val1\", tcs.get(0));\n\t\tclient.set(keys.get(1), 5, \"val2\", tcs.get(1));\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(keys, tcs.listIterator());\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(keys.get(0)));\n\t\tassertEquals(\"val2\", vals.get().get(keys.get(1)));\n\n\t\t// Set with one transcoder with the proper key and get\n\t\t// with another transcoder with the wrong key.\n\t\tkeys.add(0, \"test4\");\n\t\tTranscoder<String> encodeTranscoder = new TestWithKeyTranscoder(keys.get(0));\n\t\tclient.set(keys.get(0), 5, \"val4\", encodeTranscoder).get();\n\n\t\tTranscoder<String> decodeTranscoder = new TestWithKeyTranscoder(\"not \" + keys.get(0));\n\t\ttcs.add(0, decodeTranscoder);\n\t\ttry {\n\t\t\tclient.asyncGetBulk(keys, tcs.listIterator()).get();\n\t\t\tfail(\"Expected ExecutionException caused by key mismatch\");\n\t\t} catch (java.util.concurrent.ExecutionException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testAvailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(new ArrayList<String>(\n\t\t\t\tCollections.singleton(getExpectedVersionSource())),\n\t\t\tstringify(client.getAvailableServers()));\n\t}\n\n\tpublic void testUnavailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(Collections.emptyList(), client.getUnavailableServers());\n\t}\n\n\tprotected abstract String getExpectedVersionSource();\n\n\tpublic void testGetVersions() throws Exception {\n\t\tMap<SocketAddress, String> vs=client.getVersions();\n\t\tassertEquals(1, vs.size());\n\t\tMap.Entry<SocketAddress, String> me=vs.entrySet().iterator().next();\n\t\tassertEquals(getExpectedVersionSource(), me.getKey().toString());\n\t\tassertNotNull(me.getValue());\n\t}\n\n\tpublic void testNonexistentMutate() throws Exception {\n\t\tassertEquals(-1, client.incr(\"nonexistent\", 1));\n\t\tassert !client.asyncIncr(\"nonexistent\", 1).getStatus().isSuccess();\n\t\tassertEquals(-1, client.decr(\"nonexistent\", 1));\n\t\tassert !client.asyncDecr(\"nonexistent\", 1).getStatus().isSuccess();\n\t}\n\n\tpublic void testMutateWithDefault() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9));\n\t}\n\n\tpublic void testMutateWithDefaultAndExp() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9, 1));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9, 1));\n\t\tThread.sleep(2000);\n\t\tassertNull(client.get(\"mtest\"));\n\t\tassert ! client.asyncGet(\"mtest\").getStatus().isSuccess();\n\t}\n\n\tpublic void testAsyncIncrement() throws Exception {\n\t\tString k=\"async-incr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(6, (long)f.get());\n\t}\n\n\tpublic void testAsyncIncrementNonExistent() throws Exception {\n\t\tString k=\"async-incr-non-existent\";\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrement() throws Exception {\n\t\tString k=\"async-decr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(4, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrementNonExistent() throws Exception {\n\t\tString k=\"async-decr-non-existent\";\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testConcurrentMutation() throws Throwable {\n\t\tint num=SyncThread.getDistinctResultCount(10, new Callable<Long>(){\n\t\t\tpublic Long call() throws Exception {\n\t\t\t\treturn client.incr(\"mtest\", 1, 11);\n\t\t\t}});\n\t\tassertEquals(10, num);\n\t}\n\n\tpublic void testImmediateDelete() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassert client.delete(\"test1\").getStatus().isSuccess();\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassertTrue(client.flush().get());\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testGracefulShutdown() throws Exception {\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\t// Get a new client\n\t\tinitClient();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tkeys.add(\"t\" + i);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(1000, m.size());\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tassertEquals(i, m.get(\"t\" + i));\n\t\t}\n\t}\n\n\tpublic void testSyncGetTimeouts() throws Exception {\n\t\tfinal String key=\"timeoutTestKey\";\n\t\tfinal String value=\"timeoutTestValue\";\n\t\t// Shutting down the default client to get one with a short timeout.\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\tinitClient(new DefaultConnectionFactory() {\n\t\t\t@Override\n\t\t\tpublic long getOperationTimeout() {\n\t\t\t\treturn 2;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic int getTimeoutExceptionThreshold() {\n\t\t\t\treturn 1000000;\n\t\t\t}\n\t\t});\n\n\t\tThread.sleep(100); // allow connections to be established\n\n\t\tint j = 0;\n\t\tboolean set = false;\n\t\tdo {\n\t\t\tset = client.set(key, 0, value).get();\n\t\t\tj++;\n\t\t} while (!set && j < 10);\n\t\tassert set == true;\n\n\t\tint i = 0;\n\t\tGetFuture<Object> g = null;\n\t\ttry {\n\t\t\tfor(i = 0; i < 1000000; i++) {\n\t\t\t\tg = client.asyncGet(key);\n\t\t\t\tg.get();\n\t\t\t}\n\t\t\tthrow new Exception(\"Didn't get a timeout.\");\n\t\t} catch(Exception e) {\n\t\t\tassert !g.getStatus().isSuccess();\n\t\t\tSystem.out.println(\"Got a timeout at iteration \" + i + \".\");\n\t\t}\n\t\tThread.sleep(100); // let whatever caused the timeout to pass\n\t\ttry {\n\t\t\tif (value.equals(client.asyncGet(key).get(30, TimeUnit.SECONDS))) {\n\t\t\tSystem.out.println(\"Got the right value.\");\n\t\t} else {\n\t\t\tthrow new Exception(\"Didn't get the expected value.\");\n\t\t}\n\t\t} catch (java.util.concurrent.TimeoutException timeoutException) {\n\t\t        debugNodeInfo(client.getNodeLocator().getAll());\n\t\t\tthrow new Exception(\"Unexpected timeout after 30 seconds waiting\", timeoutException);\n\t\t}\n\t}\n\n\tprivate void debugNodeInfo(Collection<MemcachedNode> nodes) {\n\t    System.err.println(\"Debug nodes:\");\n\t    for (MemcachedNode node : nodes) {\n\t\t    System.err.println(node);\n\t\t    System.err.println(\"Is active? \" + node.isActive());\n\t\t    System.err.println(\"Has read operation? \" + node.hasReadOp() + \" Has write operation? \" + node.hasWriteOp());\n\t\ttry {\n\t\t    System.err.println(\"Has timed out this many times: \" + node.getContinuousTimeout());\n\t\t    System.err.println(\"Write op: \" + node.getCurrentWriteOp());\n\t\t    System.err.println(\"Read op: \" + node.getCurrentReadOp());\n\t\t} catch (UnsupportedOperationException e) {\n\t\t    System.err.println(\"Node does not support full interface, likely read only.\");\n\t\t}\n\t    }\n\t}\n\n\tpublic void xtestGracefulShutdownTooSlow() throws Exception {\n\t\tfor(int i=0; i<10000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertFalse(\"Weird, shut down too fast\",\n\t\t\tclient.shutdown(1, TimeUnit.MILLISECONDS));\n\n\t\ttry {\n\t\t\tMap<SocketAddress, String> m = client.getVersions();\n\t\t\tfail(\"Expected failure, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\tassertEquals(\"Shutting down\", e.getMessage());\n\t\t}\n\n\t\t// Get a new client\n\t\tinitClient();\n\t}\n\n\tpublic void testStupidlyLargeSetAndSizeOverride() throws Exception {\n\t\tif (isMembase()) {\n\t\t    return;\n\t\t}\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder(Integer.MAX_VALUE);\n\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(ExecutionException e) {\n\t\t\tSystem.err.println(\"Successful failure setting bigassthing.  Ass size \" + data.length + \" bytes doesn't fit.\");\n\t\t\te.printStackTrace();\n\t\t\tOperationException oe=(OperationException)e.getCause();\n\t\t\tassertSame(OperationErrorType.SERVER, oe.getType());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testStupidlyLargeSet() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder();\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Cannot cache data larger than \"\n\t\t\t\t\t+ CachedData.MAX_SIZE + \" bytes \"\n\t\t\t\t\t+ \"(you tried to cache a \" + data.length + \" byte object)\",\n\t\t\t\te.getMessage());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testQueueAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tObject o=client.get(\"k\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + o);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testMultiReqAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tMap<String, ?> m=client.getBulk(\"k1\", \"k2\", \"k3\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testBroadcastAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tFuture<?> f=client.flush();\n\t\t\tfail(\"Expected IllegalStateException, got \" + f.get());\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testABunchOfCancelledOperations() throws Exception {\n\t\tfinal String k=\"bunchOCancel\";\n\t\tCollection<Future<?>> futures=new ArrayList<Future<?>>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tfutures.add(client.set(k, 5, \"xval\"));\n\t\t\tfutures.add(client.asyncGet(k));\n\t\t}\n\t\tOperationFuture<Boolean> sf=client.set(k, 5, \"myxval\");\n\t\tGetFuture<Object> gf=client.asyncGet(k);\n\t\tfor(Future<?> f : futures) {\n\t\t\tf.cancel(true);\n\t\t}\n\t\tassertTrue(sf.get());\n\t\tassert sf.getStatus().isSuccess();\n\t\tassertEquals(\"myxval\", gf.get());\n\t\tassert gf.getStatus().isSuccess();\n\t}\n\n\tpublic void testUTF8Key() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testUTF8KeyDelete() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tassertTrue(client.delete(key).get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testUTF8MultiGet() throws Exception {\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<50; i++) {\n\t\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\"\n\t\t\t\t+ System.currentTimeMillis() + \".\" + i;\n\t\t\tassertTrue(client.set(key, 6000, value).get());\n\t\t\tkeys.add(key);\n\t\t}\n\n\t\tMap<String, Object> vals = client.getBulk(keys);\n\t\tassertEquals(keys.size(), vals.size());\n\t\tfor(Object o : vals.values()) {\n\t\t\tassertEquals(value, o);\n\t\t}\n\t\tassertTrue(keys.containsAll(vals.keySet()));\n\t}\n\n\tpublic void testUTF8Value() throws Exception {\n\t\tfinal String key = \"junit.plaintext.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ \"\n\t\t\t+ \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testAppend() throws Exception {\n\t\tfinal String key=\"append.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tOperationFuture<Boolean> op = client.append(0, key, \"es\");\n\t\tassertTrue(op.get());\n\t\tassert op.getStatus().isSuccess();\n\t\tassertEquals(\"testes\", client.get(key));\n\t}\n\n\tpublic void testPrepend() throws Exception {\n\t\tfinal String key=\"prepend.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tOperationFuture<Boolean> op = client.prepend(0, key, \"es\");\n\t\tassertTrue(op.get());\n\t\tassert op.getStatus().isSuccess();\n\t\tassertEquals(\"estest\", client.get(key));\n\t}\n\n\tpublic void testAppendNoSuchKey() throws Exception {\n\t\tfinal String key=\"append.missing\";\n\t\tassertFalse(client.append(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testPrependNoSuchKey() throws Exception {\n\t\tfinal String key=\"prepend.missing\";\n\t\tassertFalse(client.prepend(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tprivate static class TestTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885206;\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\t\t\treturn new String(d.getData());\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\treturn new CachedData(flags, o.getBytes(), getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate static class TestWithKeyTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885207;\n\n\t\tprivate final String key;\n\n\t\tTestWithKeyTranscoder(String k) {\n\t\t\tkey = k;\n\t\t}\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(d.getData());\n\n\t\t\tint keyLength = bb.getInt();\n\t\t\tbyte[] keyBytes = new byte[keyLength];\n\t\t\tbb.get(keyBytes);\n\t\t\tString k = new String(keyBytes);\n\n\t\t\tassertEquals(key, k);\n\n\t\t\tint valueLength = bb.getInt();\n\t\t\tbyte[] valueBytes = new byte[valueLength];\n\t\t\tbb.get(valueBytes);\n\n\t\t\treturn new String(valueBytes);\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\tbyte[] keyBytes = key.getBytes();\n\t\t\tbyte[] valueBytes = o.getBytes();\n\t\t\tint length = 4 + keyBytes.length + 4 + valueBytes.length;\n\t\t\tbyte[] bytes = new byte[length];\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(bytes);\n\t\t\tbb.putInt(keyBytes.length).put(keyBytes);\n\t\t\tbb.putInt(valueBytes.length).put(valueBytes);\n\n\t\t\treturn new CachedData(flags, bytes, getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n","lineNo":828}
{"Smelly Sample":"package net.spy.memcached;\n\nimport java.nio.ByteBuffer;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SyncThread;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.transcoders.SerializingTranscoder;\nimport net.spy.memcached.transcoders.Transcoder;\n\n\npublic abstract class ProtocolBaseCase extends ClientBaseCase {\n\n\tpublic void testAssertions() {\n\t\tboolean caught=false;\n\t\ttry {\n\t\t\tassert false;\n\t\t} catch(AssertionError e) {\n\t\t\tcaught=true;\n\t\t}\n\t\tassertTrue(\"Assertions are not enabled!\", caught);\n\t}\n\n\tpublic void testGetStats() throws Exception {\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats();\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"curr_items\"));\n\t}\n\n\tpublic void testGetStatsSlabs() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t    return;\n\t\t}\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no slabs to check.\n\t\tclient.set(\"slabinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"slabs\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"1:chunk_size\"));\n\t}\n\n\tpublic void testGetStatsSizes() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there may\n\t\t// be no sizes to check.  Note the protocol says\n\t\t// flushed/expired items may come back in stats sizes and we\n\t\t// use flush when testing, so we check that there's at least\n\t\t// one.\n\t\tclient.set(\"sizeinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"sizes\");\n\t\tSystem.out.println(\"Stats sizes:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString noItemsSmall = oneStat.get(\"96\");\n\t\tassertTrue(Integer.parseInt(noItemsSmall) >= 1);\n\t}\n\n\tpublic void testGetStatsCacheDump() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there\n\t\t// won't be anything to dump\n\t\tclient.set(\"dumpinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats =\n\t\t\t\tclient.getStats(\"cachedump 1 10000\");\n\t\tSystem.out.println(\"Stats cachedump:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString val = oneStat.get(\"dumpinitializer\");\n\t\tassertTrue(val + \"doesn't match\", val.matches(\"\\\\[2 b; \\\\d+ s\\\\]\"));\n\t}\n\n\tpublic void testDelayedFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tclient.flush(2);\n\t\tThread.sleep(2100);\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testNoop() {\n\t\t// This runs through the startup/flush cycle\n\t}\n\n\tpublic void testDoubleShutdown() {\n\t\tclient.shutdown();\n\t\tclient.shutdown();\n\t}\n\n\tpublic void testSimpleGet() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testSimpleCASGets() throws Exception {\n\t\tassertNull(client.gets(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.gets(\"test1\").getValue());\n\t}\n\n\tpublic void testCAS() throws Exception {\n\t\tfinal String key=\"castestkey\";\n\t\t// First, make sure it doesn't work for a non-existing value.\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\tCASResponse.NOT_FOUND,\n\t\t\tclient.cas(key, 0x7fffffffffL, \"bad value\"));\n\n\t\t// OK, stick a value in here.\n\t\tassertTrue(client.add(key, 5, \"original value\").get());\n\t\tCASValue<?> getsVal = client.gets(key);\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// Now try it with an existing value, but wrong CAS id\n\t\tassertSame(\"Expected error CASing with invalid id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas() + 1, \"broken value\"));\n\t\t// Validate the original value is still in tact.\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// OK, now do a valid update\n\t\tassertSame(\"Expected successful CAS with correct id (\"\n\t\t\t+ getsVal.getCas() + \")\",\n\t\t\tCASResponse.OK,\n\t\t\tclient.cas(key, getsVal.getCas(), \"new value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\n\t\t// Test a CAS replay\n\t\tassertSame(\"Expected unsuccessful CAS with replayed id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas(), \"crap value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\t}\n\n\tpublic void testReallyLongCASId() throws Exception {\n\t\tString key = \"this-is-my-key\";\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\t\tCASResponse.NOT_FOUND,\n\t\t\t\tclient.cas(key, 9223372036854775807l, \"bad value\"));\n\t}\n\n\tpublic void testExtendedUTF8Key() throws Exception {\n\t\tString key=\"\\u2013\\u00ba\\u2013\\u220f\\u2014\\u00c4\";\n\t\tassertNull(client.get(key));\n\t\tclient.set(key, 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(key));\n\t}\n\n\tpublic void testInvalidKey1() throws Exception {\n\t\ttry {\n\t\t\tclient.get(\"key with spaces\");\n\t\t\tfail(\"Expected IllegalArgumentException getting key with spaces\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey2() throws Exception {\n\t\ttry {\n\t\t\tStringBuilder longKey=new StringBuilder();\n\t\t\tfor(int i=0; i<251; i++) {\n\t\t\t\tlongKey.append(\"a\");\n\t\t\t}\n\t\t\tclient.get(longKey.toString());\n\t\t\tfail(\"Expected IllegalArgumentException getting too long of a key\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey3() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\n\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey4() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\r\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey5() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\0\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBlank() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBulk() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.getBulk(\"Key key2\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testParallelSetGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tMap<String, Object> m=client.getBulk(\"test0\", \"test1\", \"test2\",\n\t\t\t\t\t\"test3\", \"test4\", \"test5\", \"test6\", \"test7\", \"test8\",\n\t\t\t\t\t\"test9\", \"test10\"); // Yes, I intentionally ran over.\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, m.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetAutoMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tclient.set(\"testparallel\", 5, \"parallelvalue\");\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"parallelvalue\", client.get(\"testparallel\"));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testAdd() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\").get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\").get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testAddWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\", t).get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\", t).get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t}\n\n\tpublic void testAddNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.add(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testSetNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.set(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testReplaceNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.replace(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testUpdate() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.replace(\"test1\", 5, \"test1value\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testUpdateWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tclient.replace(\"test1\", 5, \"test1value\", t);\n\t\tassertNull(client.get(\"test1\", t));\n\t}\n\n\t// Just to make sure the sequence is being handled correctly\n\tpublic void testMixedSetsAndUpdates() throws Exception {\n\t\tCollection<Future<Boolean>> futures=new ArrayList<Future<Boolean>>();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<100; i++) {\n\t\t\tString key=\"k\" + i;\n\t\t\tfutures.add(client.set(key, 10, key));\n\t\t\tfutures.add(client.add(key, 10, \"a\" + i));\n\t\t\tkeys.add(key);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(100, m.size());\n\t\tfor(Map.Entry<String, Object> me : m.entrySet()) {\n\t\t\tassertEquals(me.getKey(), me.getValue());\n\t\t}\n\t\tfor(Iterator<Future<Boolean>> i=futures.iterator();i.hasNext();) {\n\t\t\tassertTrue(i.next().get());\n\t\t\tassertFalse(i.next().get());\n\t\t}\n\t}\n\n\tpublic void testGetBulk() throws Exception {\n\t\tCollection<String> keys=Arrays.asList(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(0, client.getBulk(keys).size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(keys);\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVararg() throws Exception {\n\t\tassertEquals(0, client.getBulk(\"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tMap<String, String> vals=client.getBulk(t, \"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(t,\n\t\t\t\t\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get().get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkWithTranscoderIterator() throws Exception {\n\t\tArrayList<String> keys = new ArrayList<String>();\n\t\tkeys.add(\"test1\");\n\t\tkeys.add(\"test2\");\n\t\tkeys.add(\"test3\");\n\n\t\tArrayList<Transcoder<String>> tcs = new ArrayList<Transcoder<String>>(keys.size());\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\t// Any transcoders listed after list of keys should be\n\t\t// ignored.\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\tassertEquals(0, client.asyncGetBulk(keys, tcs.listIterator()).get().size());\n\n\t\tclient.set(keys.get(0), 5, \"val1\", tcs.get(0));\n\t\tclient.set(keys.get(1), 5, \"val2\", tcs.get(1));\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(keys, tcs.listIterator());\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(keys.get(0)));\n\t\tassertEquals(\"val2\", vals.get().get(keys.get(1)));\n\n\t\t// Set with one transcoder with the proper key and get\n\t\t// with another transcoder with the wrong key.\n\t\tkeys.add(0, \"test4\");\n\t\tTranscoder<String> encodeTranscoder = new TestWithKeyTranscoder(keys.get(0));\n\t\tclient.set(keys.get(0), 5, \"val4\", encodeTranscoder).get();\n\n\t\tTranscoder<String> decodeTranscoder = new TestWithKeyTranscoder(\"not \" + keys.get(0));\n\t\ttcs.add(0, decodeTranscoder);\n\t\ttry {\n\t\t\tclient.asyncGetBulk(keys, tcs.listIterator()).get();\n\t\t\tfail(\"Expected ExecutionException caused by key mismatch\");\n\t\t} catch (java.util.concurrent.ExecutionException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testAvailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(new ArrayList<String>(\n\t\t\t\tCollections.singleton(getExpectedVersionSource())),\n\t\t\tstringify(client.getAvailableServers()));\n\t}\n\n\tpublic void testUnavailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(Collections.emptyList(), client.getUnavailableServers());\n\t}\n\n\tprotected abstract String getExpectedVersionSource();\n\n\tpublic void testGetVersions() throws Exception {\n\t\tMap<SocketAddress, String> vs=client.getVersions();\n\t\tassertEquals(1, vs.size());\n\t\tMap.Entry<SocketAddress, String> me=vs.entrySet().iterator().next();\n\t\tassertEquals(getExpectedVersionSource(), me.getKey().toString());\n\t\tassertNotNull(me.getValue());\n\t}\n\n\tpublic void testNonexistentMutate() throws Exception {\n\t\tassertEquals(-1, client.incr(\"nonexistent\", 1));\n\t\tassertEquals(-1, client.decr(\"nonexistent\", 1));\n\t}\n\n\tpublic void testMutateWithDefault() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9));\n\t}\n\n\tpublic void testMutateWithDefaultAndExp() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9, 1));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9, 1));\n\t\tThread.sleep(2000);\n\t\tassertNull(client.get(\"mtest\"));\n\t}\n\n\tpublic void testAsyncIncrement() throws Exception {\n\t\tString k=\"async-incr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(6, (long)f.get());\n\t}\n\n\tpublic void testAsyncIncrementNonExistent() throws Exception {\n\t\tString k=\"async-incr-non-existent\";\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrement() throws Exception {\n\t\tString k=\"async-decr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(4, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrementNonExistent() throws Exception {\n\t\tString k=\"async-decr-non-existent\";\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testConcurrentMutation() throws Throwable {\n\t\tint num=SyncThread.getDistinctResultCount(10, new Callable<Long>(){\n\t\t\tpublic Long call() throws Exception {\n\t\t\t\treturn client.incr(\"mtest\", 1, 11);\n\t\t\t}});\n\t\tassertEquals(10, num);\n\t}\n\n\tpublic void testImmediateDelete() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tclient.delete(\"test1\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassertTrue(client.flush().get());\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testGracefulShutdown() throws Exception {\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\t// Get a new client\n\t\tinitClient();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tkeys.add(\"t\" + i);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(1000, m.size());\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tassertEquals(i, m.get(\"t\" + i));\n\t\t}\n\t}\n\n\tpublic void testSyncGetTimeouts() throws Exception {\n\t\tfinal String key=\"timeoutTestKey\";\n\t\tfinal String value=\"timeoutTestValue\";\n\t\t// Shutting down the default client to get one with a short timeout.\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\tinitClient(new DefaultConnectionFactory() {\n\t\t\t@Override\n\t\t\tpublic long getOperationTimeout() {\n\t\t\t\treturn 2;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic int getTimeoutExceptionThreshold() {\n\t\t\t\treturn 1000000;\n\t\t\t}\n\t\t});\n\n\t\tThread.sleep(100); // allow connections to be established\n\n\t\tint j = 0;\n\t\tboolean set = false;\n\t\tdo {\n\t\t\tset = client.set(key, 0, value).get();\n\t\t\tj++;\n\t\t} while (!set && j < 10);\n\t\tassert set == true;\n\n\t\tint i = 0;\n\t\ttry {\n\t\t\tfor(i = 0; i < 1000000; i++) {\n\t\t\t\tclient.get(key);\n\t\t\t}\n\t\t\tthrow new Exception(\"Didn't get a timeout.\");\n\t\t} catch(RuntimeException e) {\n\t\t\tSystem.out.println(\"Got a timeout at iteration \" + i + \".\");\n\t\t}\n\t\tThread.sleep(100); // let whatever caused the timeout to pass\n\t\ttry {\n\t\t\tif (value.equals(client.asyncGet(key).get(30, TimeUnit.SECONDS))) {\n\t\t\tSystem.out.println(\"Got the right value.\");\n\t\t} else {\n\t\t\tthrow new Exception(\"Didn't get the expected value.\");\n\t\t}\n\t\t} catch (java.util.concurrent.TimeoutException timeoutException) {\n\t\t        debugNodeInfo(client.getNodeLocator().getAll());\n\t\t\tthrow new Exception(\"Unexpected timeout after 30 seconds waiting\", timeoutException);\n\t\t}\n\t}\n\n\tprivate void debugNodeInfo(Collection<MemcachedNode> nodes) {\n\t    System.err.println(\"Debug nodes:\");\n\t    for (MemcachedNode node : nodes) {\n\t\t    System.err.println(node);\n\t\t    System.err.println(\"Is active? \" + node.isActive());\n\t\t    System.err.println(\"Has read operation? \" + node.hasReadOp() + \" Has write operation? \" + node.hasWriteOp());\n\t\ttry {\n\t\t    System.err.println(\"Has timed out this many times: \" + node.getContinuousTimeout());\n\t\t    System.err.println(\"Write op: \" + node.getCurrentWriteOp());\n\t\t    System.err.println(\"Read op: \" + node.getCurrentReadOp());\n\t\t} catch (UnsupportedOperationException e) {\n\t\t    System.err.println(\"Node does not support full interface, likely read only.\");\n\t\t}\n\t    }\n\t}\n\n\tpublic void xtestGracefulShutdownTooSlow() throws Exception {\n\t\tfor(int i=0; i<10000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertFalse(\"Weird, shut down too fast\",\n\t\t\tclient.shutdown(1, TimeUnit.MILLISECONDS));\n\n\t\ttry {\n\t\t\tMap<SocketAddress, String> m = client.getVersions();\n\t\t\tfail(\"Expected failure, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\tassertEquals(\"Shutting down\", e.getMessage());\n\t\t}\n\n\t\t// Get a new client\n\t\tinitClient();\n\t}\n\n\tpublic void testStupidlyLargeSetAndSizeOverride() throws Exception {\n\t\tif (isMembase()) {\n\t\t    return;\n\t\t}\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder(Integer.MAX_VALUE);\n\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(ExecutionException e) {\n\t\t\tSystem.err.println(\"Successful failure setting bigassthing.  Ass size \" + data.length + \" bytes doesn't fit.\");\n\t\t\te.printStackTrace();\n\t\t\tOperationException oe=(OperationException)e.getCause();\n\t\t\tassertSame(OperationErrorType.SERVER, oe.getType());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testStupidlyLargeSet() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder();\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Cannot cache data larger than \"\n\t\t\t\t\t+ CachedData.MAX_SIZE + \" bytes \"\n\t\t\t\t\t+ \"(you tried to cache a \" + data.length + \" byte object)\",\n\t\t\t\te.getMessage());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testQueueAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tObject o=client.get(\"k\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + o);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testMultiReqAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tMap<String, ?> m=client.getBulk(\"k1\", \"k2\", \"k3\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testBroadcastAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tFuture<?> f=client.flush();\n\t\t\tfail(\"Expected IllegalStateException, got \" + f.get());\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testABunchOfCancelledOperations() throws Exception {\n\t\tfinal String k=\"bunchOCancel\";\n\t\tCollection<Future<?>> futures=new ArrayList<Future<?>>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tfutures.add(client.set(k, 5, \"xval\"));\n\t\t\tfutures.add(client.asyncGet(k));\n\t\t}\n\t\tFuture<Boolean> sf=client.set(k, 5, \"myxval\");\n\t\tFuture<Object> gf=client.asyncGet(k);\n\t\tfor(Future<?> f : futures) {\n\t\t\tf.cancel(true);\n\t\t}\n\t\tassertTrue(sf.get());\n\t\tassertEquals(\"myxval\", gf.get());\n\t}\n\n\tpublic void testUTF8Key() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testUTF8KeyDelete() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tassertTrue(client.delete(key).get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testUTF8MultiGet() throws Exception {\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<50; i++) {\n\t\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\"\n\t\t\t\t+ System.currentTimeMillis() + \".\" + i;\n\t\t\tassertTrue(client.set(key, 6000, value).get());\n\t\t\tkeys.add(key);\n\t\t}\n\n\t\tMap<String, Object> vals = client.getBulk(keys);\n\t\tassertEquals(keys.size(), vals.size());\n\t\tfor(Object o : vals.values()) {\n\t\t\tassertEquals(value, o);\n\t\t}\n\t\tassertTrue(keys.containsAll(vals.keySet()));\n\t}\n\n\tpublic void testUTF8Value() throws Exception {\n\t\tfinal String key = \"junit.plaintext.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ \"\n\t\t\t+ \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testAppend() throws Exception {\n\t\tfinal String key=\"append.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.append(0, key, \"es\").get());\n\t\tassertEquals(\"testes\", client.get(key));\n\t}\n\n\tpublic void testPrepend() throws Exception {\n\t\tfinal String key=\"prepend.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.prepend(0, key, \"es\").get());\n\t\tassertEquals(\"estest\", client.get(key));\n\t}\n\n\tpublic void testAppendNoSuchKey() throws Exception {\n\t\tfinal String key=\"append.missing\";\n\t\tassertFalse(client.append(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testPrependNoSuchKey() throws Exception {\n\t\tfinal String key=\"prepend.missing\";\n\t\tassertFalse(client.prepend(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tprivate static class TestTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885206;\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\t\t\treturn new String(d.getData());\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\treturn new CachedData(flags, o.getBytes(), getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate static class TestWithKeyTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885207;\n\n\t\tprivate final String key;\n\n\t\tTestWithKeyTranscoder(String k) {\n\t\t\tkey = k;\n\t\t}\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(d.getData());\n\n\t\t\tint keyLength = bb.getInt();\n\t\t\tbyte[] keyBytes = new byte[keyLength];\n\t\t\tbb.get(keyBytes);\n\t\t\tString k = new String(keyBytes);\n\n\t\t\tassertEquals(key, k);\n\n\t\t\tint valueLength = bb.getInt();\n\t\t\tbyte[] valueBytes = new byte[valueLength];\n\t\t\tbb.get(valueBytes);\n\n\t\t\treturn new String(valueBytes);\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\tbyte[] keyBytes = key.getBytes();\n\t\t\tbyte[] valueBytes = o.getBytes();\n\t\t\tint length = 4 + keyBytes.length + 4 + valueBytes.length;\n\t\t\tbyte[] bytes = new byte[length];\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(bytes);\n\t\t\tbb.putInt(keyBytes.length).put(keyBytes);\n\t\t\tbb.putInt(valueBytes.length).put(valueBytes);\n\n\t\t\treturn new CachedData(flags, bytes, getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached;\n\nimport java.nio.ByteBuffer;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SyncThread;\nimport net.spy.memcached.internal.BulkFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.transcoders.SerializingTranscoder;\nimport net.spy.memcached.transcoders.Transcoder;\n\n\npublic abstract class ProtocolBaseCase extends ClientBaseCase {\n\n\tpublic void testAssertions() {\n\t\tboolean caught=false;\n\t\ttry {\n\t\t\tassert false;\n\t\t} catch(AssertionError e) {\n\t\t\tcaught=true;\n\t\t}\n\t\tassertTrue(\"Assertions are not enabled!\", caught);\n\t}\n\n\tpublic void testGetStats() throws Exception {\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats();\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"curr_items\"));\n\t}\n\n\tpublic void testGetStatsSlabs() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t    return;\n\t\t}\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no slabs to check.\n\t\tclient.set(\"slabinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"slabs\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"1:chunk_size\"));\n\t}\n\n\tpublic void testGetStatsSizes() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there may\n\t\t// be no sizes to check.  Note the protocol says\n\t\t// flushed/expired items may come back in stats sizes and we\n\t\t// use flush when testing, so we check that there's at least\n\t\t// one.\n\t\tclient.set(\"sizeinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"sizes\");\n\t\tSystem.out.println(\"Stats sizes:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString noItemsSmall = oneStat.get(\"96\");\n\t\tassertTrue(Integer.parseInt(noItemsSmall) >= 1);\n\t}\n\n\tpublic void testGetStatsCacheDump() throws Exception {\n\t\tif (isMembase() || isMoxi()) {\n\t\t\treturn;\n\t\t}\n\t\t// There needs to at least have been one value set or there\n\t\t// won't be anything to dump\n\t\tclient.set(\"dumpinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats =\n\t\t\t\tclient.getStats(\"cachedump 1 10000\");\n\t\tSystem.out.println(\"Stats cachedump:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString val = oneStat.get(\"dumpinitializer\");\n\t\tassertTrue(val + \"doesn't match\", val.matches(\"\\\\[2 b; \\\\d+ s\\\\]\"));\n\t}\n\n\tpublic void testDelayedFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassert client.set(\"test1\", 5, \"test1value\").getStatus().isSuccess();\n\t\tassert client.set(\"test2\", 5, \"test2value\").getStatus().isSuccess();\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassert client.flush(2).getStatus().isSuccess();\n\t\tThread.sleep(2100);\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t\tassert !client.asyncGet(\"test1\").getStatus().isSuccess();\n\t\tassert !client.asyncGet(\"test2\").getStatus().isSuccess();\n\t}\n\n\tpublic void testNoop() {\n\t\t// This runs through the startup/flush cycle\n\t}\n\n\tpublic void testDoubleShutdown() {\n\t\tclient.shutdown();\n\t\tclient.shutdown();\n\t}\n\n\tpublic void testSimpleGet() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testSimpleCASGets() throws Exception {\n\t\tassertNull(client.gets(\"test1\"));\n\t\tassert client.set(\"test1\", 5, \"test1value\").getStatus().isSuccess();\n\t\tassertEquals(\"test1value\", client.gets(\"test1\").getValue());\n\t}\n\n\tpublic void testCAS() throws Exception {\n\t\tfinal String key=\"castestkey\";\n\t\t// First, make sure it doesn't work for a non-existing value.\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\tCASResponse.NOT_FOUND,\n\t\t\tclient.cas(key, 0x7fffffffffL, \"bad value\"));\n\n\t\t// OK, stick a value in here.\n\t\tassertTrue(client.add(key, 5, \"original value\").get());\n\t\tCASValue<?> getsVal = client.gets(key);\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// Now try it with an existing value, but wrong CAS id\n\t\tassertSame(\"Expected error CASing with invalid id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas() + 1, \"broken value\"));\n\t\t// Validate the original value is still in tact.\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// OK, now do a valid update\n\t\tassertSame(\"Expected successful CAS with correct id (\"\n\t\t\t+ getsVal.getCas() + \")\",\n\t\t\tCASResponse.OK,\n\t\t\tclient.cas(key, getsVal.getCas(), \"new value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\n\t\t// Test a CAS replay\n\t\tassertSame(\"Expected unsuccessful CAS with replayed id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas(), \"crap value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\t}\n\n\tpublic void testReallyLongCASId() throws Exception {\n\t\tString key = \"this-is-my-key\";\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\t\tCASResponse.NOT_FOUND,\n\t\t\t\tclient.cas(key, 9223372036854775807l, \"bad value\"));\n\t}\n\n\tpublic void testExtendedUTF8Key() throws Exception {\n\t\tString key=\"\\u2013\\u00ba\\u2013\\u220f\\u2014\\u00c4\";\n\t\tassertNull(client.get(key));\n\t\tassert client.set(key, 5, \"test1value\").getStatus().isSuccess();\n\t\tassertEquals(\"test1value\", client.get(key));\n\t}\n\n\tpublic void testInvalidKey1() throws Exception {\n\t\ttry {\n\t\t\tclient.get(\"key with spaces\");\n\t\t\tfail(\"Expected IllegalArgumentException getting key with spaces\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey2() throws Exception {\n\t\ttry {\n\t\t\tStringBuilder longKey=new StringBuilder();\n\t\t\tfor(int i=0; i<251; i++) {\n\t\t\t\tlongKey.append(\"a\");\n\t\t\t}\n\t\t\tclient.get(longKey.toString());\n\t\t\tfail(\"Expected IllegalArgumentException getting too long of a key\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey3() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\n\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey4() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\r\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey5() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\0\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBlank() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBulk() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.getBulk(\"Key key2\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testParallelSetGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassert client.set(\"test\" + i, 5, \"value\" + i).getStatus().isSuccess();\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassert client.set(\"test\" + i, 5, \"value\" + i).getStatus().isSuccess();\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tMap<String, Object> m=client.getBulk(\"test0\", \"test1\", \"test2\",\n\t\t\t\t\t\"test3\", \"test4\", \"test5\", \"test6\", \"test7\", \"test8\",\n\t\t\t\t\t\"test9\", \"test10\"); // Yes, I intentionally ran over.\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, m.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetAutoMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tassert client.set(\"testparallel\", 5, \"parallelvalue\").getStatus().isSuccess();\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"parallelvalue\", client.get(\"testparallel\"));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testAdd() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassert !client.asyncGet(\"test1\").getStatus().isSuccess();\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\").get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassert client.asyncGet(\"test1\").getStatus().isSuccess();\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\").get());\n\t\tassert !client.add(\"test1\", 5, \"ignoredvalue\").getStatus().isSuccess();\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testAddWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tassert !client.asyncGet(\"test1\", t).getStatus().isSuccess();\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\", t).get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\", t).get());\n\t\tassert !client.add(\"test1\", 5, \"ignoredvalue\", t).getStatus().isSuccess();\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t}\n\n\tpublic void testAddNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.add(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testSetNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.set(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testReplaceNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.replace(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testUpdate() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.replace(\"test1\", 5, \"test1value\");\n\t\tassert !client.replace(\"test1\", 5, \"test1value\").getStatus().isSuccess();\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testUpdateWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tclient.replace(\"test1\", 5, \"test1value\", t);\n\t\tassert !client.replace(\"test1\", 5, \"test1value\", t).getStatus().isSuccess();\n\t\tassertNull(client.get(\"test1\", t));\n\t}\n\n\t// Just to make sure the sequence is being handled correctly\n\tpublic void testMixedSetsAndUpdates() throws Exception {\n\t\tCollection<Future<Boolean>> futures=new ArrayList<Future<Boolean>>();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<100; i++) {\n\t\t\tString key=\"k\" + i;\n\t\t\tfutures.add(client.set(key, 10, key));\n\t\t\tfutures.add(client.add(key, 10, \"a\" + i));\n\t\t\tkeys.add(key);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(100, m.size());\n\t\tfor(Map.Entry<String, Object> me : m.entrySet()) {\n\t\t\tassertEquals(me.getKey(), me.getValue());\n\t\t}\n\t\tfor(Iterator<Future<Boolean>> i=futures.iterator();i.hasNext();) {\n\t\t\tassertTrue(i.next().get());\n\t\t\tassertFalse(i.next().get());\n\t\t}\n\t}\n\n\tpublic void testGetBulk() throws Exception {\n\t\tCollection<String> keys=Arrays.asList(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(0, client.getBulk(keys).size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(keys);\n\t\tassert client.asyncGetBulk(keys).getStatus().isSuccess();\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVararg() throws Exception {\n\t\tassertEquals(0, client.getBulk(\"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(\"test1\", \"test2\", \"test3\");\n\t\tassert client.asyncGetBulk(\"test1\", \"test2\", \"test3\").getStatus().isSuccess();\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tMap<String, String> vals=client.getBulk(t, \"test1\", \"test2\", \"test3\");\n\t\tassert client.asyncGetBulk(t, \"test1\", \"test2\", \"test3\").getStatus().isSuccess();\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tBulkFuture<Map<String, String>> vals=client.asyncGetBulk(t,\n\t\t\t\t\"test1\", \"test2\", \"test3\");\n\t\tassert vals.getStatus().isSuccess();\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get().get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkWithTranscoderIterator() throws Exception {\n\t\tArrayList<String> keys = new ArrayList<String>();\n\t\tkeys.add(\"test1\");\n\t\tkeys.add(\"test2\");\n\t\tkeys.add(\"test3\");\n\n\t\tArrayList<Transcoder<String>> tcs = new ArrayList<Transcoder<String>>(keys.size());\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\t// Any transcoders listed after list of keys should be\n\t\t// ignored.\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\tassertEquals(0, client.asyncGetBulk(keys, tcs.listIterator()).get().size());\n\n\t\tclient.set(keys.get(0), 5, \"val1\", tcs.get(0));\n\t\tclient.set(keys.get(1), 5, \"val2\", tcs.get(1));\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(keys, tcs.listIterator());\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(keys.get(0)));\n\t\tassertEquals(\"val2\", vals.get().get(keys.get(1)));\n\n\t\t// Set with one transcoder with the proper key and get\n\t\t// with another transcoder with the wrong key.\n\t\tkeys.add(0, \"test4\");\n\t\tTranscoder<String> encodeTranscoder = new TestWithKeyTranscoder(keys.get(0));\n\t\tclient.set(keys.get(0), 5, \"val4\", encodeTranscoder).get();\n\n\t\tTranscoder<String> decodeTranscoder = new TestWithKeyTranscoder(\"not \" + keys.get(0));\n\t\ttcs.add(0, decodeTranscoder);\n\t\ttry {\n\t\t\tclient.asyncGetBulk(keys, tcs.listIterator()).get();\n\t\t\tfail(\"Expected ExecutionException caused by key mismatch\");\n\t\t} catch (java.util.concurrent.ExecutionException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testAvailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(new ArrayList<String>(\n\t\t\t\tCollections.singleton(getExpectedVersionSource())),\n\t\t\tstringify(client.getAvailableServers()));\n\t}\n\n\tpublic void testUnavailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(Collections.emptyList(), client.getUnavailableServers());\n\t}\n\n\tprotected abstract String getExpectedVersionSource();\n\n\tpublic void testGetVersions() throws Exception {\n\t\tMap<SocketAddress, String> vs=client.getVersions();\n\t\tassertEquals(1, vs.size());\n\t\tMap.Entry<SocketAddress, String> me=vs.entrySet().iterator().next();\n\t\tassertEquals(getExpectedVersionSource(), me.getKey().toString());\n\t\tassertNotNull(me.getValue());\n\t}\n\n\tpublic void testNonexistentMutate() throws Exception {\n\t\tassertEquals(-1, client.incr(\"nonexistent\", 1));\n\t\tassert !client.asyncIncr(\"nonexistent\", 1).getStatus().isSuccess();\n\t\tassertEquals(-1, client.decr(\"nonexistent\", 1));\n\t\tassert !client.asyncDecr(\"nonexistent\", 1).getStatus().isSuccess();\n\t}\n\n\tpublic void testMutateWithDefault() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9));\n\t}\n\n\tpublic void testMutateWithDefaultAndExp() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9, 1));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9, 1));\n\t\tThread.sleep(2000);\n\t\tassertNull(client.get(\"mtest\"));\n\t\tassert ! client.asyncGet(\"mtest\").getStatus().isSuccess();\n\t}\n\n\tpublic void testAsyncIncrement() throws Exception {\n\t\tString k=\"async-incr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(6, (long)f.get());\n\t}\n\n\tpublic void testAsyncIncrementNonExistent() throws Exception {\n\t\tString k=\"async-incr-non-existent\";\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrement() throws Exception {\n\t\tString k=\"async-decr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(4, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrementNonExistent() throws Exception {\n\t\tString k=\"async-decr-non-existent\";\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testConcurrentMutation() throws Throwable {\n\t\tint num=SyncThread.getDistinctResultCount(10, new Callable<Long>(){\n\t\t\tpublic Long call() throws Exception {\n\t\t\t\treturn client.incr(\"mtest\", 1, 11);\n\t\t\t}});\n\t\tassertEquals(10, num);\n\t}\n\n\tpublic void testImmediateDelete() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassert client.delete(\"test1\").getStatus().isSuccess();\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassertTrue(client.flush().get());\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testGracefulShutdown() throws Exception {\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\t// Get a new client\n\t\tinitClient();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tkeys.add(\"t\" + i);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(1000, m.size());\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tassertEquals(i, m.get(\"t\" + i));\n\t\t}\n\t}\n\n\tpublic void testSyncGetTimeouts() throws Exception {\n\t\tfinal String key=\"timeoutTestKey\";\n\t\tfinal String value=\"timeoutTestValue\";\n\t\t// Shutting down the default client to get one with a short timeout.\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\tinitClient(new DefaultConnectionFactory() {\n\t\t\t@Override\n\t\t\tpublic long getOperationTimeout() {\n\t\t\t\treturn 2;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic int getTimeoutExceptionThreshold() {\n\t\t\t\treturn 1000000;\n\t\t\t}\n\t\t});\n\n\t\tThread.sleep(100); // allow connections to be established\n\n\t\tint j = 0;\n\t\tboolean set = false;\n\t\tdo {\n\t\t\tset = client.set(key, 0, value).get();\n\t\t\tj++;\n\t\t} while (!set && j < 10);\n\t\tassert set == true;\n\n\t\tint i = 0;\n\t\tGetFuture<Object> g = null;\n\t\ttry {\n\t\t\tfor(i = 0; i < 1000000; i++) {\n\t\t\t\tg = client.asyncGet(key);\n\t\t\t\tg.get();\n\t\t\t}\n\t\t\tthrow new Exception(\"Didn't get a timeout.\");\n\t\t} catch(Exception e) {\n\t\t\tassert !g.getStatus().isSuccess();\n\t\t\tSystem.out.println(\"Got a timeout at iteration \" + i + \".\");\n\t\t}\n\t\tThread.sleep(100); // let whatever caused the timeout to pass\n\t\ttry {\n\t\t\tif (value.equals(client.asyncGet(key).get(30, TimeUnit.SECONDS))) {\n\t\t\tSystem.out.println(\"Got the right value.\");\n\t\t} else {\n\t\t\tthrow new Exception(\"Didn't get the expected value.\");\n\t\t}\n\t\t} catch (java.util.concurrent.TimeoutException timeoutException) {\n\t\t        debugNodeInfo(client.getNodeLocator().getAll());\n\t\t\tthrow new Exception(\"Unexpected timeout after 30 seconds waiting\", timeoutException);\n\t\t}\n\t}\n\n\tprivate void debugNodeInfo(Collection<MemcachedNode> nodes) {\n\t    System.err.println(\"Debug nodes:\");\n\t    for (MemcachedNode node : nodes) {\n\t\t    System.err.println(node);\n\t\t    System.err.println(\"Is active? \" + node.isActive());\n\t\t    System.err.println(\"Has read operation? \" + node.hasReadOp() + \" Has write operation? \" + node.hasWriteOp());\n\t\ttry {\n\t\t    System.err.println(\"Has timed out this many times: \" + node.getContinuousTimeout());\n\t\t    System.err.println(\"Write op: \" + node.getCurrentWriteOp());\n\t\t    System.err.println(\"Read op: \" + node.getCurrentReadOp());\n\t\t} catch (UnsupportedOperationException e) {\n\t\t    System.err.println(\"Node does not support full interface, likely read only.\");\n\t\t}\n\t    }\n\t}\n\n\tpublic void xtestGracefulShutdownTooSlow() throws Exception {\n\t\tfor(int i=0; i<10000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertFalse(\"Weird, shut down too fast\",\n\t\t\tclient.shutdown(1, TimeUnit.MILLISECONDS));\n\n\t\ttry {\n\t\t\tMap<SocketAddress, String> m = client.getVersions();\n\t\t\tfail(\"Expected failure, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\tassertEquals(\"Shutting down\", e.getMessage());\n\t\t}\n\n\t\t// Get a new client\n\t\tinitClient();\n\t}\n\n\tpublic void testStupidlyLargeSetAndSizeOverride() throws Exception {\n\t\tif (isMembase()) {\n\t\t    return;\n\t\t}\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder(Integer.MAX_VALUE);\n\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(ExecutionException e) {\n\t\t\tSystem.err.println(\"Successful failure setting bigassthing.  Ass size \" + data.length + \" bytes doesn't fit.\");\n\t\t\te.printStackTrace();\n\t\t\tOperationException oe=(OperationException)e.getCause();\n\t\t\tassertSame(OperationErrorType.SERVER, oe.getType());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testStupidlyLargeSet() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder();\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[21*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Cannot cache data larger than \"\n\t\t\t\t\t+ CachedData.MAX_SIZE + \" bytes \"\n\t\t\t\t\t+ \"(you tried to cache a \" + data.length + \" byte object)\",\n\t\t\t\te.getMessage());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testQueueAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tObject o=client.get(\"k\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + o);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testMultiReqAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tMap<String, ?> m=client.getBulk(\"k1\", \"k2\", \"k3\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testBroadcastAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tFuture<?> f=client.flush();\n\t\t\tfail(\"Expected IllegalStateException, got \" + f.get());\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testABunchOfCancelledOperations() throws Exception {\n\t\tfinal String k=\"bunchOCancel\";\n\t\tCollection<Future<?>> futures=new ArrayList<Future<?>>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tfutures.add(client.set(k, 5, \"xval\"));\n\t\t\tfutures.add(client.asyncGet(k));\n\t\t}\n\t\tOperationFuture<Boolean> sf=client.set(k, 5, \"myxval\");\n\t\tGetFuture<Object> gf=client.asyncGet(k);\n\t\tfor(Future<?> f : futures) {\n\t\t\tf.cancel(true);\n\t\t}\n\t\tassertTrue(sf.get());\n\t\tassert sf.getStatus().isSuccess();\n\t\tassertEquals(\"myxval\", gf.get());\n\t\tassert gf.getStatus().isSuccess();\n\t}\n\n\tpublic void testUTF8Key() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testUTF8KeyDelete() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tassertTrue(client.delete(key).get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testUTF8MultiGet() throws Exception {\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<50; i++) {\n\t\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\"\n\t\t\t\t+ System.currentTimeMillis() + \".\" + i;\n\t\t\tassertTrue(client.set(key, 6000, value).get());\n\t\t\tkeys.add(key);\n\t\t}\n\n\t\tMap<String, Object> vals = client.getBulk(keys);\n\t\tassertEquals(keys.size(), vals.size());\n\t\tfor(Object o : vals.values()) {\n\t\t\tassertEquals(value, o);\n\t\t}\n\t\tassertTrue(keys.containsAll(vals.keySet()));\n\t}\n\n\tpublic void testUTF8Value() throws Exception {\n\t\tfinal String key = \"junit.plaintext.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ \"\n\t\t\t+ \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testAppend() throws Exception {\n\t\tfinal String key=\"append.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tOperationFuture<Boolean> op = client.append(0, key, \"es\");\n\t\tassertTrue(op.get());\n\t\tassert op.getStatus().isSuccess();\n\t\tassertEquals(\"testes\", client.get(key));\n\t}\n\n\tpublic void testPrepend() throws Exception {\n\t\tfinal String key=\"prepend.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tOperationFuture<Boolean> op = client.prepend(0, key, \"es\");\n\t\tassertTrue(op.get());\n\t\tassert op.getStatus().isSuccess();\n\t\tassertEquals(\"estest\", client.get(key));\n\t}\n\n\tpublic void testAppendNoSuchKey() throws Exception {\n\t\tfinal String key=\"append.missing\";\n\t\tassertFalse(client.append(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testPrependNoSuchKey() throws Exception {\n\t\tfinal String key=\"prepend.missing\";\n\t\tassertFalse(client.prepend(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tprivate static class TestTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885206;\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\t\t\treturn new String(d.getData());\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\treturn new CachedData(flags, o.getBytes(), getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate static class TestWithKeyTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885207;\n\n\t\tprivate final String key;\n\n\t\tTestWithKeyTranscoder(String k) {\n\t\t\tkey = k;\n\t\t}\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(d.getData());\n\n\t\t\tint keyLength = bb.getInt();\n\t\t\tbyte[] keyBytes = new byte[keyLength];\n\t\t\tbb.get(keyBytes);\n\t\t\tString k = new String(keyBytes);\n\n\t\t\tassertEquals(key, k);\n\n\t\t\tint valueLength = bb.getInt();\n\t\t\tbyte[] valueBytes = new byte[valueLength];\n\t\t\tbb.get(valueBytes);\n\n\t\t\treturn new String(valueBytes);\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\tbyte[] keyBytes = key.getBytes();\n\t\t\tbyte[] valueBytes = o.getBytes();\n\t\t\tint length = 4 + keyBytes.length + 4 + valueBytes.length;\n\t\t\tbyte[] bytes = new byte[length];\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(bytes);\n\t\t\tbb.putInt(keyBytes.length).put(keyBytes);\n\t\t\tbb.putInt(valueBytes.length).put(valueBytes);\n\n\t\t\treturn new CachedData(flags, bytes, getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n","lineNo":837}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\n\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.vbucket.VBucketNodeLocator;\nimport net.spy.memcached.vbucket.Reconfigurable;\nimport net.spy.memcached.vbucket.config.Bucket;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic final class MemcachedConnection extends SpyObject implements Reconfigurable {\n\n\t// The number of empty selects we'll allow before assuming we may have\n\t// missed one and should check the current selectors.  This generally\n\t// indicates a bug, but we'll check it nonetheless.\n\tprivate static final int DOUBLE_CHECK_EMPTY = 256;\n\t// The number of empty selects we'll allow before blowing up.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 0x1000000;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, optimization will collapse multiple sequential get ops\n\tprivate final boolean shouldOptimize;\n\tprivate Selector selector=null;\n\tprivate final NodeLocator locator;\n\tprivate final FailureMode failureMode;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate final long maxDelay;\n\tprivate int emptySelects=0;\n\tprivate final int bufSize;\n\tprivate final ConnectionFactory connectionFactory;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n\tprivate final Collection<ConnectionObserver> connObservers =\n\t\tnew ConcurrentLinkedQueue<ConnectionObserver>();\n\tprivate final OperationFactory opFact;\n\tprivate final int timeoutExceptionThreshold;\n        private final Collection<Operation> retryOps;\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n\t\t\tFailureMode fm, OperationFactory opfactory)\n\t\tthrows IOException {\n\t\tconnObservers.addAll(obs);\n\t\treconnectQueue=new TreeMap<Long, MemcachedNode>();\n\t\taddedQueue=new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tfailureMode = fm;\n\t\tshouldOptimize = f.shouldOptimize();\n\t\tmaxDelay = f.getMaxReconnectDelay();\n\t\topFact = opfactory;\n\t\ttimeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n\t\tselector=Selector.open();\n\t\tretryOps = new ArrayList<Operation>();\n\t\tnodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tthis.bufSize = bufSize;\n\t\tthis.connectionFactory = f;\n\t\tList<MemcachedNode> connections = createConnections(a);\n\t\tlocator=f.createLocator(connections);\n\t\t}\n\n\tprivate List<MemcachedNode> createConnections(final Collection<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\tList<MemcachedNode> connections=new ArrayList<MemcachedNode>(a.size());\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tMemcachedNode qa=this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n\t\t\tint ops=0;\n\t\t\tch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n\t\t\t// Initially I had attempted to skirt this by queueing every\n\t\t\t// connect, but it considerably slowed down start time.\n\t\t\ttry {\n\t\t\t\tif(ch.connect(sa)) {\n\t\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\t\tconnected(qa);\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t}\n\t\t\t\tqa.setSk(ch.register(selector, ops, qa));\n\t\t\t\tassert ch.isConnected()\n\t\t\t\t\t|| qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Socket error on initial connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t\tconnections.add(qa);\n\t\t}\n\t\treturn connections;\n\t}\n\n\tpublic void reconfigure(Bucket bucket) {\n\t\ttry {\n\t\t\tif (!(locator instanceof VBucketNodeLocator)) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// get a new collection of addresses from the received config\n\t\t\tList<String> servers = bucket.getVbuckets().getServers();\n\t\t\tHashSet<SocketAddress> newServerAddresses = new HashSet<SocketAddress>();\n\t\t\tArrayList<InetSocketAddress> newServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (String server : servers) {\n\t\t\t\tint finalColon = server.lastIndexOf(':');\n\t\t\t\tif (finalColon < 1) {\n\t\t\t\t\tthrow new IllegalArgumentException(\"Invalid server ``\"\n\t\t\t\t\t+ server + \"'' in vbucket's server list\");\n\t\t\t\t}\n\t\t\t\tString hostPart = server.substring(0, finalColon);\n\t\t\t\tString portNum = server.substring(finalColon + 1);\n\n\t\t\t\tInetSocketAddress address = new InetSocketAddress(hostPart,\n\t\t\t\tInteger.parseInt(portNum));\n\t\t\t\t// add parsed address to our collections\n\t\t\t\tnewServerAddresses.add(address);\n\t\t\t\tnewServers.add(address);\n\n\t\t\t}\n\n\t\t\t// split current nodes to \"odd nodes\" and \"stay nodes\"\n\t\t\tArrayList<MemcachedNode> oddNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<MemcachedNode> stayNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<InetSocketAddress> stayServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (MemcachedNode current : locator.getAll()) {\n\t\t\t\tif (newServerAddresses.contains(current.getSocketAddress())) {\n\t\t\t\t\tstayNodes.add(current);\n\t\t\t\t\tstayServers.add((InetSocketAddress) current.getSocketAddress());\n\t\t\t\t} else {\n\t\t\t\t\toddNodes.add(current);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// prepare a collection of addresses for new nodes\n\t\t\tnewServers.removeAll(stayServers);\n\n\t\t\t// create a collection of new nodes\n\t\t\tList<MemcachedNode> newNodes = createConnections(newServers);\n\n\t\t\t// merge stay nodes with new nodes\n\t\t\tList<MemcachedNode> mergedNodes = new ArrayList<MemcachedNode>();\n\t\t\tmergedNodes.addAll(stayNodes);\n\t\t\tmergedNodes.addAll(newNodes);\n\n\t\t\t// call update locator with new nodes list and vbucket config\n\t\t\t((VBucketNodeLocator) locator).updateLocator(mergedNodes, bucket.getVbuckets());\n\n\t\t\t// schedule shutdown for the oddNodes\n\t\t\tnodesToShutdown.addAll(oddNodes);\n\t\t} catch (IOException e) {\n\t\t    getLogger().error(\"Connection reconfiguration failed\", e);\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getSk() != null && qa.getSk().isValid()) {\n\t\t\t\tif(qa.getChannel().isConnected()) {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty() && !shutDown) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > DOUBLE_CHECK_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlostConnection((MemcachedNode)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\thandleIO(sk);\n\t\t\t}\n\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\n\t\t// see if any connections blew up with large number of timeouts\n\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\tMemcachedNode mn = (MemcachedNode)sk.attachment();\n\t\t\tif (mn.getContinuousTimeout() > timeoutExceptionThreshold)\n\t\t\t{\n\t\t\t\tgetLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n\t\t\t\tlostConnection(mn);\n\t\t\t}\n\t\t}\n\n\t\tif(!shutDown && !reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n        // rehash operations that in retry state\n        redistributeOperations(retryOps);\n        retryOps.clear();\n\n        // try to shutdown odd nodes\n        for (MemcachedNode qa : nodesToShutdown) {\n            if (!addedQueue.contains(qa)) {\n                nodesToShutdown.remove(qa);\n                Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n                if (qa.getChannel() != null) {\n                    qa.getChannel().close();\n                    qa.setSk(null);\n                    if (qa.getBytesRemainingToWrite() > 0) {\n                        getLogger().warn(\n                                \"Shut down with %d bytes remaining to write\",\n                                qa.getBytesRemainingToWrite());\n                    }\n                    getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n                }\n                redistributeOperations(notCompletedOperations);\n            }\n        }\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<MemcachedNode> toAdd=new HashSet<MemcachedNode>();\n\t\t\t// Transfer the queue into a hashset.  There are very likely more\n\t\t\t// additions than there are nodes.\n\t\t\tCollection<MemcachedNode> todo=new HashSet<MemcachedNode>();\n\t\t\ttry {\n\t\t\t\tMemcachedNode qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\ttodo.add(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// Found everything\n\t\t\t}\n\n\t\t\t// Now process the queue.\n\t\t\tfor(MemcachedNode qa : todo) {\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\tif(qa.isActive()) {\n\t\t\t\t\tif(qa.getCurrentWriteOp() != null) {\n\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t}\n\t\t\t\tqa.copyInputQueue();\n\t\t\t\tif(readyForIO) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\t\thandleWrites(qa.getSk(), qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\tlostConnection(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tqa.fixupOps();\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return whether the observer was successfully added\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn connObservers.add(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed and now doesn't\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn connObservers.remove(obs);\n\t}\n\n\tprivate void connected(MemcachedNode qa) {\n\t\tassert qa.getChannel().isConnected() : \"Not connected.\";\n\t\tint rt = qa.getReconnectCount();\n\t\tqa.connected();\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionEstablished(qa.getSocketAddress(), rt);\n\t\t}\n\t}\n\n\tprivate void lostConnection(MemcachedNode qa) {\n\t\tqueueReconnect(qa);\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionLost(qa.getSocketAddress());\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tMemcachedNode qa=(MemcachedNode)sk.attachment();\n\t\ttry {\n\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\tif(sk.isConnectable()) {\n\t\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\t\tfinal SocketChannel channel=qa.getChannel();\n\t\t\t\tif(channel.finishConnect()) {\n\t\t\t\t\tconnected(qa);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t}\n\t\t\t\tif(sk.isWritable()) {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(ClosedChannelException e) {\n\t\t\t// Note, not all channel closes end up here\n\t\t\tif(!shutDown) {\n\t\t\t\tgetLogger().info(\"Closed channel and not shutting down.  \"\n\t\t\t\t\t+ \"Queueing reconnect on %s\", qa, e);\n\t\t\t\tlostConnection(qa);\n\t\t\t}\n\t\t} catch(ConnectException e) {\n\t\t\t// Failures to establish a connection should attempt a reconnect\n\t\t\t// without signaling the observers.\n\t\t\tgetLogger().info(\"Reconnecting due to failure to connect to %s\",\n\t\t\t\t\tqa, e);\n\t\t\tqueueReconnect(qa);\n\t\t} catch (OperationException e) {\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnection due to exception \" +\n\t\t\t\t\"handling a memcached operation on %s.  \" +\n\t\t\t\t\"This may be due to an authentication failure.\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t} catch(Exception e) {\n\t\t\t// Any particular error processing an item should simply\n\t\t\t// cause us to reconnect to the server.\n\t\t\t//\n\t\t\t// One cause is just network oddness or servers\n\t\t\t// restarting, which lead here with IOException\n\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnecting due to exception on %s\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t}\n\t\tqa.fixupOps();\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\tboolean canWriteMore=qa.getBytesRemainingToWrite() > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.writeSome();\n\t\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\t\tcanWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tByteBuffer rbuf=qa.getRbuf();\n\t\tfinal SocketChannel channel = qa.getChannel();\n\t\tint read=channel.read(rbuf);\n\t\tif (read < 0) {\n\t\t    // our model is to keep the connection alive for future ops\n\t\t    // so we'll queue a reconnect if disconnected via an IOException\n\t\t    throw new IOException(\"Disconnected unexpected, will reconnect.\");\n\t\t}\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\trbuf.flip();\n\t\t\twhile(rbuf.remaining() > 0) {\n\t\t\t\tif(currentOp == null) {\n\t\t\t\t\tthrow new IllegalStateException(\"No read operation.\");\n\t\t\t\t}\n\t\t\t\tcurrentOp.readFromBuffer(rbuf);\n\t\t\t\tif(currentOp.getState() == OperationState.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t} else if (currentOp.getState() == OperationState.RETRY) {\n                    getLogger().debug(\n                            \"Reschedule read op due to NOT_MY_VBUCKET error: %s \",\n                            currentOp);\n                    Operation op=qa.removeCurrentReadOp();\n                    assert op == currentOp\n                    : \"Expected to pop \" + currentOp + \" got \" + op;\n                    retryOps.add(currentOp);\n                    currentOp=qa.getCurrentReadOp();\n\n\t\t\t\t}\n\t\t\t}\n\t\t\trbuf.clear();\n\t\t\tread=channel.read(rbuf);\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(MemcachedNode qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.getReconnectCount());\n\t\t\tif(qa.getSk() != null) {\n\t\t\t\tqa.getSk().cancel();\n\t\t\t\tassert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnecting();\n\t\t\ttry {\n\t\t\t\tif(qa.getChannel() != null && qa.getChannel().socket() != null) {\n\t\t\t\t\tqa.getChannel().socket().close();\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"The channel or socket was null for %s\",\n\t\t\t\t\t\tqa);\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.setChannel(null);\n\n\t\t\tlong delay = (long)Math.min(maxDelay,\n\t\t\t\t\tMath.pow(2, qa.getReconnectCount())) * 1000;\n\t\t\tlong reconTime = System.currentTimeMillis() + delay;\n\n\t\t\t// Avoid potential condition where two connections are scheduled\n\t\t\t// for reconnect at the exact same time.  This is expected to be\n\t\t\t// a rare situation.\n\t\t\twhile(reconnectQueue.containsKey(reconTime)) {\n\t\t\t\treconTime++;\n\t\t\t}\n\n\t\t\treconnectQueue.put(reconTime, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\n\t\t\tif(failureMode == FailureMode.Redistribute) {\n\t\t\t\tredistributeOperations(qa.destroyInputQueue());\n\t\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\t\tcancelOperations(qa.destroyInputQueue());\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void cancelOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\top.cancel();\n\t\t}\n\t}\n\n\tprivate void redistributeOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\tif(op instanceof KeyedOperation) {\n\t\t\t\tKeyedOperation ko = (KeyedOperation)op;\n\t\t\t\tint added = 0;\n\t\t\t\tfor(String k : ko.getKeys()) {\n\t\t\t\t\tfor(Operation newop : opFact.clone(ko)) {\n\t\t\t\t\t\taddOperation(k, newop);\n\t\t\t\t\t\tadded++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert added > 0\n\t\t\t\t\t: \"Didn't add any new operations when redistributing\";\n\t\t\t} else {\n\t\t\t\t// Cancel things that don't have definite targets.\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tfinal long now=System.currentTimeMillis();\n\t\tfinal Map<MemcachedNode, Boolean> seen=\n\t\t\tnew IdentityHashMap<MemcachedNode, Boolean>();\n\t\tfinal List<MemcachedNode> rereQueue=new ArrayList<MemcachedNode>();\n\t\tSocketChannel ch = null;\n\t\tfor(Iterator<MemcachedNode> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tfinal MemcachedNode qa=i.next();\n\t\t\ti.remove();\n\t\t\ttry {\n\t\t\t\tif(!seen.containsKey(qa)) {\n\t\t\t\t\tseen.put(qa, Boolean.TRUE);\n\t\t\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\t\t\tch=SocketChannel.open();\n\t\t\t\t\tch.configureBlocking(false);\n\t\t\t\t\tint ops=0;\n\t\t\t\t\tif(ch.connect(qa.getSocketAddress())) {\n\t\t\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\t\t\tassert ch.isConnected();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t\t}\n\t\t\t\t\tqa.registerChannel(ch, ch.register(selector, ops, qa));\n\t\t\t\t\tassert qa.getChannel() == ch : \"Channel was lost.\";\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Skipping duplicate reconnect request for %s\", qa);\n\t\t\t\t}\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Error on reconnect\", e);\n\t\t\t\trereQueue.add(qa);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n                getLogger().error(\"Exception on reconnect, lost node %s\", qa, e);\n            } finally {\n                //it's possible that above code will leak file descriptors under abnormal\n                //conditions (when ch.open() fails and throws IOException.\n                //always close non connected channel\n                if (ch != null && !ch.isConnected()\n                        && !ch.isConnectionPending()) {\n                    try {\n                        ch.close();\n                    } catch (IOException x) {\n                        getLogger().error(\"Exception closing channel: %s\", qa, x);\n                    }\n                }\n            }\n\t\t}\n\t\t// Requeue any fast-failed connects.\n\t\tfor(MemcachedNode n : rereQueue) {\n\t\t\tqueueReconnect(n);\n\t\t}\n\t}\n\n\t/**\n\t * Get the node locator used by this connection.\n\t */\n\tNodeLocator getLocator() {\n\t\treturn locator;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t *\n\t * @param key the key the operation is operating upon\n\t * @param o the operation\n\t */\n\tpublic void addOperation(final String key, final Operation o) {\n\t\tMemcachedNode placeIn=null;\n\t\tMemcachedNode primary = locator.getPrimary(key);\n\t\tif(primary.isActive() || failureMode == FailureMode.Retry) {\n\t\t\tplaceIn=primary;\n\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\to.cancel();\n\t\t} else {\n\t\t\t// Look for another node in sequence that is ready.\n\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\tplaceIn == null && i.hasNext(); ) {\n\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\tif(n.isActive()) {\n\t\t\t\t\tplaceIn=n;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we didn't find an active node, queue it in the primary node\n\t\t\t// and wait for it to come back online.\n\t\t\tif(placeIn == null) {\n\t\t\t\tplaceIn = primary;\n\t\t\t\tthis.getLogger().warn(\"Could not redistribute \" +\n\t\t\t\t\t\"to another node, retrying primary node for %s.\", key);\n\t\t\t}\n\t\t}\n\n\t\tassert o.isCancelled() || placeIn != null\n\t\t\t: \"No node found for key \" + key;\n\t\tif(placeIn != null) {\n\t\t\t// add the vbucketIndex to the operation\n\t\t\tif (locator instanceof VBucketNodeLocator) {\n\t\t\t\tint vbucketIndex = ((VBucketNodeLocator) locator).getVBucketIndex(key);\n\t\t\t\tif (o instanceof VBucketAware) {\n\t\t\t\t\t((VBucketAware) o).setVBucket(vbucketIndex);\n\t\t\t\t}\n\t\t\t}\n\t\t\taddOperation(placeIn, o);\n\t\t} else {\n\t\t\tassert o.isCancelled() : \"No node found for \"\n\t\t\t\t+ key + \" (and not immediately cancelled)\";\n\t\t}\n\t}\n\n\tpublic void insertOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.insertOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.addOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n\t\tfor(Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n\t\t\tfinal MemcachedNode node=me.getKey();\n\t\t\tOperation o=me.getValue();\n\t\t\to.setHandlingNode(node);\n\t\t\to.initialize();\n\t\t\tnode.addOp(o);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t}\n\n\t/**\n\t * Broadcast an operation to all nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(BroadcastOpFactory of) {\n\t\treturn broadcastOperation(of, locator.getAll());\n\t}\n\n\t/**\n\t * Broadcast an operation to a specific collection of nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(locator.getAll().size());\n\t\tfor(MemcachedNode node : nodes) {\n\t\t\tOperation op = of.newOp(node, latch);\n\t\t\top.initialize();\n\t\t\tnode.addOp(op);\n\t\t\top.setHandlingNode(node);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\treturn latch;\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tshutDown=true;\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getChannel() != null) {\n\t\t\t\tqa.getChannel().close();\n\t\t\t\tqa.setSk(null);\n\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.getBytesRemainingToWrite());\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.getChannel());\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.getSocketAddress());\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n    /**\n     * helper method: increase timeout count on node attached to this op\n     *\n     * @param op\n     */\n    public static void opTimedOut(Operation op) {\n        MemcachedConnection.setTimeout(op, true);\n    }\n\n    /**\n     * helper method: reset timeout counter\n     *\n     * @param op\n     */\n    public static void opSucceeded(Operation op) {\n        MemcachedConnection.setTimeout(op, false);\n    }\n\n    /**\n     * helper method: do some error checking and set timeout boolean\n     *\n     * @param op\n     * @param isTimeout\n     */\n    private static void setTimeout(Operation op, boolean isTimeout) {\n        try {\n            if (op == null || op.isTimedOutUnsent()) {\n                return; // op may be null in some cases, e.g. flush\n            }\n            MemcachedNode node = op.getHandlingNode();\n            if (node == null) {\n                LoggerFactory.getLogger(MemcachedConnection.class).warn(\"handling node for operation is not set\");\n            }\n            else {\n                node.setContinuousTimeout(isTimeout);\n            }\n        } catch (Exception e) {\n            LoggerFactory.getLogger(MemcachedConnection.class).error(e.getMessage());\n        }\n    }\n\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\n\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.vbucket.VBucketNodeLocator;\nimport net.spy.memcached.vbucket.Reconfigurable;\nimport net.spy.memcached.vbucket.config.Bucket;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic final class MemcachedConnection extends SpyObject implements Reconfigurable {\n\n\t// The number of empty selects we'll allow before assuming we may have\n\t// missed one and should check the current selectors.  This generally\n\t// indicates a bug, but we'll check it nonetheless.\n\tprivate static final int DOUBLE_CHECK_EMPTY = 256;\n\t// The number of empty selects we'll allow before blowing up.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 0x1000000;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, optimization will collapse multiple sequential get ops\n\tprivate final boolean shouldOptimize;\n\tprivate Selector selector=null;\n\tprivate final NodeLocator locator;\n\tprivate final FailureMode failureMode;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate final long maxDelay;\n\tprivate int emptySelects=0;\n\tprivate final int bufSize;\n\tprivate final ConnectionFactory connectionFactory;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n\tprivate final Collection<ConnectionObserver> connObservers =\n\t\tnew ConcurrentLinkedQueue<ConnectionObserver>();\n\tprivate final OperationFactory opFact;\n\tprivate final int timeoutExceptionThreshold;\n        private final Collection<Operation> retryOps;\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n\t\t\tFailureMode fm, OperationFactory opfactory)\n\t\tthrows IOException {\n\t\tconnObservers.addAll(obs);\n\t\treconnectQueue=new TreeMap<Long, MemcachedNode>();\n\t\taddedQueue=new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tfailureMode = fm;\n\t\tshouldOptimize = f.shouldOptimize();\n\t\tmaxDelay = f.getMaxReconnectDelay();\n\t\topFact = opfactory;\n\t\ttimeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n\t\tselector=Selector.open();\n\t\tretryOps = new ArrayList<Operation>();\n\t\tnodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tthis.bufSize = bufSize;\n\t\tthis.connectionFactory = f;\n\t\tList<MemcachedNode> connections = createConnections(a);\n\t\tlocator=f.createLocator(connections);\n\t\t}\n\n\tprivate List<MemcachedNode> createConnections(final Collection<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\tList<MemcachedNode> connections=new ArrayList<MemcachedNode>(a.size());\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tMemcachedNode qa=this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n\t\t\tint ops=0;\n\t\t\tch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n\t\t\t// Initially I had attempted to skirt this by queueing every\n\t\t\t// connect, but it considerably slowed down start time.\n\t\t\ttry {\n\t\t\t\tif(ch.connect(sa)) {\n\t\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\t\tconnected(qa);\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t}\n\t\t\t\tqa.setSk(ch.register(selector, ops, qa));\n\t\t\t\tassert ch.isConnected()\n\t\t\t\t\t|| qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Socket error on initial connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t\tconnections.add(qa);\n\t\t}\n\t\treturn connections;\n\t}\n\n\tpublic void reconfigure(Bucket bucket) {\n\t\ttry {\n\t\t\tif (!(locator instanceof VBucketNodeLocator)) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// get a new collection of addresses from the received config\n\t\t\tList<String> servers = bucket.getVbuckets().getServers();\n\t\t\tHashSet<SocketAddress> newServerAddresses = new HashSet<SocketAddress>();\n\t\t\tArrayList<InetSocketAddress> newServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (String server : servers) {\n\t\t\t\tint finalColon = server.lastIndexOf(':');\n\t\t\t\tif (finalColon < 1) {\n\t\t\t\t\tthrow new IllegalArgumentException(\"Invalid server ``\"\n\t\t\t\t\t+ server + \"'' in vbucket's server list\");\n\t\t\t\t}\n\t\t\t\tString hostPart = server.substring(0, finalColon);\n\t\t\t\tString portNum = server.substring(finalColon + 1);\n\n\t\t\t\tInetSocketAddress address = new InetSocketAddress(hostPart,\n\t\t\t\tInteger.parseInt(portNum));\n\t\t\t\t// add parsed address to our collections\n\t\t\t\tnewServerAddresses.add(address);\n\t\t\t\tnewServers.add(address);\n\n\t\t\t}\n\n\t\t\t// split current nodes to \"odd nodes\" and \"stay nodes\"\n\t\t\tArrayList<MemcachedNode> oddNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<MemcachedNode> stayNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<InetSocketAddress> stayServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (MemcachedNode current : locator.getAll()) {\n\t\t\t\tif (newServerAddresses.contains(current.getSocketAddress())) {\n\t\t\t\t\tstayNodes.add(current);\n\t\t\t\t\tstayServers.add((InetSocketAddress) current.getSocketAddress());\n\t\t\t\t} else {\n\t\t\t\t\toddNodes.add(current);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// prepare a collection of addresses for new nodes\n\t\t\tnewServers.removeAll(stayServers);\n\n\t\t\t// create a collection of new nodes\n\t\t\tList<MemcachedNode> newNodes = createConnections(newServers);\n\n\t\t\t// merge stay nodes with new nodes\n\t\t\tList<MemcachedNode> mergedNodes = new ArrayList<MemcachedNode>();\n\t\t\tmergedNodes.addAll(stayNodes);\n\t\t\tmergedNodes.addAll(newNodes);\n\n\t\t\t// call update locator with new nodes list and vbucket config\n\t\t\t((VBucketNodeLocator) locator).updateLocator(mergedNodes, bucket.getVbuckets());\n\n\t\t\t// schedule shutdown for the oddNodes\n\t\t\tnodesToShutdown.addAll(oddNodes);\n\t\t} catch (IOException e) {\n\t\t    getLogger().error(\"Connection reconfiguration failed\", e);\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getSk() != null && qa.getSk().isValid()) {\n\t\t\t\tif(qa.getChannel().isConnected()) {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty() && !shutDown) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > DOUBLE_CHECK_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlostConnection((MemcachedNode)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\thandleIO(sk);\n\t\t\t}\n\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\n\t\t// see if any connections blew up with large number of timeouts\n\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\tMemcachedNode mn = (MemcachedNode)sk.attachment();\n\t\t\tif (mn.getContinuousTimeout() > timeoutExceptionThreshold)\n\t\t\t{\n\t\t\t\tgetLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n\t\t\t\tlostConnection(mn);\n\t\t\t}\n\t\t}\n\n\t\tif(!shutDown && !reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n        // rehash operations that in retry state\n        redistributeOperations(retryOps);\n        retryOps.clear();\n\n        // try to shutdown odd nodes\n        for (MemcachedNode qa : nodesToShutdown) {\n            if (!addedQueue.contains(qa)) {\n                nodesToShutdown.remove(qa);\n                Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n                if (qa.getChannel() != null) {\n                    qa.getChannel().close();\n                    qa.setSk(null);\n                    if (qa.getBytesRemainingToWrite() > 0) {\n                        getLogger().warn(\n                                \"Shut down with %d bytes remaining to write\",\n                                qa.getBytesRemainingToWrite());\n                    }\n                    getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n                }\n                redistributeOperations(notCompletedOperations);\n            }\n        }\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<MemcachedNode> toAdd=new HashSet<MemcachedNode>();\n\t\t\t// Transfer the queue into a hashset.  There are very likely more\n\t\t\t// additions than there are nodes.\n\t\t\tCollection<MemcachedNode> todo=new HashSet<MemcachedNode>();\n\t\t\ttry {\n\t\t\t\tMemcachedNode qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\ttodo.add(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// Found everything\n\t\t\t}\n\n\t\t\t// Now process the queue.\n\t\t\tfor(MemcachedNode qa : todo) {\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\tif(qa.isActive()) {\n\t\t\t\t\tif(qa.getCurrentWriteOp() != null) {\n\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t}\n\t\t\t\tqa.copyInputQueue();\n\t\t\t\tif(readyForIO) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\t\thandleWrites(qa.getSk(), qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\tlostConnection(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tqa.fixupOps();\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return whether the observer was successfully added\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn connObservers.add(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed and now doesn't\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn connObservers.remove(obs);\n\t}\n\n\tprivate void connected(MemcachedNode qa) {\n\t\tassert qa.getChannel().isConnected() : \"Not connected.\";\n\t\tint rt = qa.getReconnectCount();\n\t\tqa.connected();\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionEstablished(qa.getSocketAddress(), rt);\n\t\t}\n\t}\n\n\tprivate void lostConnection(MemcachedNode qa) {\n\t\tqueueReconnect(qa);\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionLost(qa.getSocketAddress());\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tMemcachedNode qa=(MemcachedNode)sk.attachment();\n\t\ttry {\n\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\tif(sk.isConnectable()) {\n\t\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\t\tfinal SocketChannel channel=qa.getChannel();\n\t\t\t\tif(channel.finishConnect()) {\n\t\t\t\t\tconnected(qa);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t}\n\t\t\t\tif(sk.isWritable()) {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(ClosedChannelException e) {\n\t\t\t// Note, not all channel closes end up here\n\t\t\tif(!shutDown) {\n\t\t\t\tgetLogger().info(\"Closed channel and not shutting down.  \"\n\t\t\t\t\t+ \"Queueing reconnect on %s\", qa, e);\n\t\t\t\tlostConnection(qa);\n\t\t\t}\n\t\t} catch(ConnectException e) {\n\t\t\t// Failures to establish a connection should attempt a reconnect\n\t\t\t// without signaling the observers.\n\t\t\tgetLogger().info(\"Reconnecting due to failure to connect to %s\",\n\t\t\t\t\tqa, e);\n\t\t\tqueueReconnect(qa);\n\t\t} catch (OperationException e) {\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnection due to exception \" +\n\t\t\t\t\"handling a memcached operation on %s.  \" +\n\t\t\t\t\"This may be due to an authentication failure.\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t} catch(Exception e) {\n\t\t\t// Any particular error processing an item should simply\n\t\t\t// cause us to reconnect to the server.\n\t\t\t//\n\t\t\t// One cause is just network oddness or servers\n\t\t\t// restarting, which lead here with IOException\n\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnecting due to exception on %s\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t}\n\t\tqa.fixupOps();\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\tboolean canWriteMore=qa.getBytesRemainingToWrite() > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.writeSome();\n\t\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\t\tcanWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tByteBuffer rbuf=qa.getRbuf();\n\t\tfinal SocketChannel channel = qa.getChannel();\n\t\tint read=channel.read(rbuf);\n\t\tif (read < 0) {\n\t\t    // our model is to keep the connection alive for future ops\n\t\t    // so we'll queue a reconnect if disconnected via an IOException\n\t\t    throw new IOException(\"Disconnected unexpected, will reconnect.\");\n\t\t}\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\trbuf.flip();\n\t\t\twhile(rbuf.remaining() > 0) {\n\t\t\t\tif(currentOp == null) {\n\t\t\t\t\tthrow new IllegalStateException(\"No read operation.\");\n\t\t\t\t}\n\t\t\t\tcurrentOp.readFromBuffer(rbuf);\n\t\t\t\tif(currentOp.getState() == OperationState.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t} else if (currentOp.getState() == OperationState.RETRY) {\n                    getLogger().debug(\n                            \"Reschedule read op due to NOT_MY_VBUCKET error: %s \",\n                            currentOp);\n                    ((VBucketAware) currentOp).addNotMyVbucketNode(currentOp.getHandlingNode());\n                    Operation op=qa.removeCurrentReadOp();\n                    assert op == currentOp\n                    : \"Expected to pop \" + currentOp + \" got \" + op;\n                    retryOps.add(currentOp);\n                    currentOp=qa.getCurrentReadOp();\n\n\t\t\t\t}\n\t\t\t}\n\t\t\trbuf.clear();\n\t\t\tread=channel.read(rbuf);\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(MemcachedNode qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.getReconnectCount());\n\t\t\tif(qa.getSk() != null) {\n\t\t\t\tqa.getSk().cancel();\n\t\t\t\tassert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnecting();\n\t\t\ttry {\n\t\t\t\tif(qa.getChannel() != null && qa.getChannel().socket() != null) {\n\t\t\t\t\tqa.getChannel().socket().close();\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"The channel or socket was null for %s\",\n\t\t\t\t\t\tqa);\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.setChannel(null);\n\n\t\t\tlong delay = (long)Math.min(maxDelay,\n\t\t\t\t\tMath.pow(2, qa.getReconnectCount())) * 1000;\n\t\t\tlong reconTime = System.currentTimeMillis() + delay;\n\n\t\t\t// Avoid potential condition where two connections are scheduled\n\t\t\t// for reconnect at the exact same time.  This is expected to be\n\t\t\t// a rare situation.\n\t\t\twhile(reconnectQueue.containsKey(reconTime)) {\n\t\t\t\treconTime++;\n\t\t\t}\n\n\t\t\treconnectQueue.put(reconTime, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\n\t\t\tif(failureMode == FailureMode.Redistribute) {\n\t\t\t\tredistributeOperations(qa.destroyInputQueue());\n\t\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\t\tcancelOperations(qa.destroyInputQueue());\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void cancelOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\top.cancel();\n\t\t}\n\t}\n\n\tprivate void redistributeOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\tif(op instanceof KeyedOperation) {\n\t\t\t\tKeyedOperation ko = (KeyedOperation)op;\n\t\t\t\tint added = 0;\n\t\t\t\tfor(String k : ko.getKeys()) {\n\t\t\t\t\tfor(Operation newop : opFact.clone(ko)) {\n\t\t\t\t\t\taddOperation(k, newop);\n\t\t\t\t\t\tadded++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert added > 0\n\t\t\t\t\t: \"Didn't add any new operations when redistributing\";\n\t\t\t} else {\n\t\t\t\t// Cancel things that don't have definite targets.\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tfinal long now=System.currentTimeMillis();\n\t\tfinal Map<MemcachedNode, Boolean> seen=\n\t\t\tnew IdentityHashMap<MemcachedNode, Boolean>();\n\t\tfinal List<MemcachedNode> rereQueue=new ArrayList<MemcachedNode>();\n\t\tSocketChannel ch = null;\n\t\tfor(Iterator<MemcachedNode> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tfinal MemcachedNode qa=i.next();\n\t\t\ti.remove();\n\t\t\ttry {\n\t\t\t\tif(!seen.containsKey(qa)) {\n\t\t\t\t\tseen.put(qa, Boolean.TRUE);\n\t\t\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\t\t\tch=SocketChannel.open();\n\t\t\t\t\tch.configureBlocking(false);\n\t\t\t\t\tint ops=0;\n\t\t\t\t\tif(ch.connect(qa.getSocketAddress())) {\n\t\t\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\t\t\tassert ch.isConnected();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t\t}\n\t\t\t\t\tqa.registerChannel(ch, ch.register(selector, ops, qa));\n\t\t\t\t\tassert qa.getChannel() == ch : \"Channel was lost.\";\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Skipping duplicate reconnect request for %s\", qa);\n\t\t\t\t}\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Error on reconnect\", e);\n\t\t\t\trereQueue.add(qa);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n                getLogger().error(\"Exception on reconnect, lost node %s\", qa, e);\n            } finally {\n                //it's possible that above code will leak file descriptors under abnormal\n                //conditions (when ch.open() fails and throws IOException.\n                //always close non connected channel\n                if (ch != null && !ch.isConnected()\n                        && !ch.isConnectionPending()) {\n                    try {\n                        ch.close();\n                    } catch (IOException x) {\n                        getLogger().error(\"Exception closing channel: %s\", qa, x);\n                    }\n                }\n            }\n\t\t}\n\t\t// Requeue any fast-failed connects.\n\t\tfor(MemcachedNode n : rereQueue) {\n\t\t\tqueueReconnect(n);\n\t\t}\n\t}\n\n\t/**\n\t * Get the node locator used by this connection.\n\t */\n\tNodeLocator getLocator() {\n\t\treturn locator;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t *\n\t * @param key the key the operation is operating upon\n\t * @param o the operation\n\t */\n\tpublic void addOperation(final String key, final Operation o) {\n\t\tMemcachedNode placeIn=null;\n\t\tMemcachedNode primary = locator.getPrimary(key);\n\t\tif(primary.isActive() || failureMode == FailureMode.Retry) {\n\t\t\tplaceIn=primary;\n\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\to.cancel();\n\t\t} else {\n\t\t\t// Look for another node in sequence that is ready.\n\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\tplaceIn == null && i.hasNext(); ) {\n\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\tif(n.isActive()) {\n\t\t\t\t\tplaceIn=n;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we didn't find an active node, queue it in the primary node\n\t\t\t// and wait for it to come back online.\n\t\t\tif(placeIn == null) {\n\t\t\t\tplaceIn = primary;\n\t\t\t\tthis.getLogger().warn(\"Could not redistribute \" +\n\t\t\t\t\t\"to another node, retrying primary node for %s.\", key);\n\t\t\t}\n\t\t}\n\n\t\tassert o.isCancelled() || placeIn != null\n\t\t\t: \"No node found for key \" + key;\n\t\tif(placeIn != null) {\n\t\t\t// add the vbucketIndex to the operation\n\t\t\tif (locator instanceof VBucketNodeLocator) {\n\t\t\t\tVBucketNodeLocator vbucketLocator = (VBucketNodeLocator) locator;\n\t\t\t\tint vbucketIndex = vbucketLocator.getVBucketIndex(key);\n\t\t\t\tif (o instanceof VBucketAware) {\n\t\t\t\t\tVBucketAware vbucketAwareOp = (VBucketAware) o;\n\t\t\t\t\tvbucketAwareOp.setVBucket(vbucketIndex);\n\t\t\t\t\tif (!vbucketAwareOp.getNotMyVbucketNodes().isEmpty()) {\n\t\t\t\t\t\tMemcachedNode alternative = vbucketLocator.\n\t\t\t\t\t\tgetAlternative(key, vbucketAwareOp.getNotMyVbucketNodes());\n\t\t\t\t\t\t\tif (alternative != null) {\n\t\t\t\t\t\t\t\tplaceIn = alternative;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\taddOperation(placeIn, o);\n\t\t\t} else {\n\t\t\t\tassert o.isCancelled() : \"No node found for \"\n\t\t\t\t\t+ key + \" (and not immediately cancelled)\";\n\t\t\t}\n\t}\n\n\tpublic void insertOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.insertOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.addOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n\t\tfor(Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n\t\t\tfinal MemcachedNode node=me.getKey();\n\t\t\tOperation o=me.getValue();\n\t\t\to.setHandlingNode(node);\n\t\t\to.initialize();\n\t\t\tnode.addOp(o);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t}\n\n\t/**\n\t * Broadcast an operation to all nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(BroadcastOpFactory of) {\n\t\treturn broadcastOperation(of, locator.getAll());\n\t}\n\n\t/**\n\t * Broadcast an operation to a specific collection of nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(locator.getAll().size());\n\t\tfor(MemcachedNode node : nodes) {\n\t\t\tOperation op = of.newOp(node, latch);\n\t\t\top.initialize();\n\t\t\tnode.addOp(op);\n\t\t\top.setHandlingNode(node);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\treturn latch;\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tshutDown=true;\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getChannel() != null) {\n\t\t\t\tqa.getChannel().close();\n\t\t\t\tqa.setSk(null);\n\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.getBytesRemainingToWrite());\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.getChannel());\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.getSocketAddress());\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n    /**\n     * helper method: increase timeout count on node attached to this op\n     *\n     * @param op\n     */\n    public static void opTimedOut(Operation op) {\n        MemcachedConnection.setTimeout(op, true);\n    }\n\n    /**\n     * helper method: reset timeout counter\n     *\n     * @param op\n     */\n    public static void opSucceeded(Operation op) {\n        MemcachedConnection.setTimeout(op, false);\n    }\n\n    /**\n     * helper method: do some error checking and set timeout boolean\n     *\n     * @param op\n     * @param isTimeout\n     */\n    private static void setTimeout(Operation op, boolean isTimeout) {\n        try {\n            if (op == null || op.isTimedOutUnsent()) {\n                return; // op may be null in some cases, e.g. flush\n            }\n            MemcachedNode node = op.getHandlingNode();\n            if (node == null) {\n                LoggerFactory.getLogger(MemcachedConnection.class).warn(\"handling node for operation is not set\");\n            }\n            else {\n                node.setContinuousTimeout(isTimeout);\n            }\n        } catch (Exception e) {\n            LoggerFactory.getLogger(MemcachedConnection.class).error(e.getMessage());\n        }\n    }\n\n}\n","lineNo":705}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\n\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.vbucket.VBucketNodeLocator;\nimport net.spy.memcached.vbucket.Reconfigurable;\nimport net.spy.memcached.vbucket.config.Bucket;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic final class MemcachedConnection extends SpyObject implements Reconfigurable {\n\n\t// The number of empty selects we'll allow before assuming we may have\n\t// missed one and should check the current selectors.  This generally\n\t// indicates a bug, but we'll check it nonetheless.\n\tprivate static final int DOUBLE_CHECK_EMPTY = 256;\n\t// The number of empty selects we'll allow before blowing up.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 0x1000000;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, optimization will collapse multiple sequential get ops\n\tprivate final boolean shouldOptimize;\n\tprivate Selector selector=null;\n\tprivate final NodeLocator locator;\n\tprivate final FailureMode failureMode;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate final long maxDelay;\n\tprivate int emptySelects=0;\n\tprivate final int bufSize;\n\tprivate final ConnectionFactory connectionFactory;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n\tprivate final Collection<ConnectionObserver> connObservers =\n\t\tnew ConcurrentLinkedQueue<ConnectionObserver>();\n\tprivate final OperationFactory opFact;\n\tprivate final int timeoutExceptionThreshold;\n        private final Collection<Operation> retryOps;\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n\t\t\tFailureMode fm, OperationFactory opfactory)\n\t\tthrows IOException {\n\t\tconnObservers.addAll(obs);\n\t\treconnectQueue=new TreeMap<Long, MemcachedNode>();\n\t\taddedQueue=new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tfailureMode = fm;\n\t\tshouldOptimize = f.shouldOptimize();\n\t\tmaxDelay = f.getMaxReconnectDelay();\n\t\topFact = opfactory;\n\t\ttimeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n\t\tselector=Selector.open();\n\t\tretryOps = new ArrayList<Operation>();\n\t\tnodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tthis.bufSize = bufSize;\n\t\tthis.connectionFactory = f;\n\t\tList<MemcachedNode> connections = createConnections(a);\n\t\tlocator=f.createLocator(connections);\n\t\t}\n\n\tprivate List<MemcachedNode> createConnections(final Collection<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\tList<MemcachedNode> connections=new ArrayList<MemcachedNode>(a.size());\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tMemcachedNode qa=this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n\t\t\tint ops=0;\n\t\t\tch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n\t\t\t// Initially I had attempted to skirt this by queueing every\n\t\t\t// connect, but it considerably slowed down start time.\n\t\t\ttry {\n\t\t\t\tif(ch.connect(sa)) {\n\t\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\t\tconnected(qa);\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t}\n\t\t\t\tqa.setSk(ch.register(selector, ops, qa));\n\t\t\t\tassert ch.isConnected()\n\t\t\t\t\t|| qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Socket error on initial connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t\tconnections.add(qa);\n\t\t}\n\t\treturn connections;\n\t}\n\n\tpublic void reconfigure(Bucket bucket) {\n\t\ttry {\n\t\t\tif (!(locator instanceof VBucketNodeLocator)) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// get a new collection of addresses from the received config\n\t\t\tList<String> servers = bucket.getVbuckets().getServers();\n\t\t\tHashSet<SocketAddress> newServerAddresses = new HashSet<SocketAddress>();\n\t\t\tArrayList<InetSocketAddress> newServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (String server : servers) {\n\t\t\t\tint finalColon = server.lastIndexOf(':');\n\t\t\t\tif (finalColon < 1) {\n\t\t\t\t\tthrow new IllegalArgumentException(\"Invalid server ``\"\n\t\t\t\t\t+ server + \"'' in vbucket's server list\");\n\t\t\t\t}\n\t\t\t\tString hostPart = server.substring(0, finalColon);\n\t\t\t\tString portNum = server.substring(finalColon + 1);\n\n\t\t\t\tInetSocketAddress address = new InetSocketAddress(hostPart,\n\t\t\t\tInteger.parseInt(portNum));\n\t\t\t\t// add parsed address to our collections\n\t\t\t\tnewServerAddresses.add(address);\n\t\t\t\tnewServers.add(address);\n\n\t\t\t}\n\n\t\t\t// split current nodes to \"odd nodes\" and \"stay nodes\"\n\t\t\tArrayList<MemcachedNode> oddNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<MemcachedNode> stayNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<InetSocketAddress> stayServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (MemcachedNode current : locator.getAll()) {\n\t\t\t\tif (newServerAddresses.contains(current.getSocketAddress())) {\n\t\t\t\t\tstayNodes.add(current);\n\t\t\t\t\tstayServers.add((InetSocketAddress) current.getSocketAddress());\n\t\t\t\t} else {\n\t\t\t\t\toddNodes.add(current);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// prepare a collection of addresses for new nodes\n\t\t\tnewServers.removeAll(stayServers);\n\n\t\t\t// create a collection of new nodes\n\t\t\tList<MemcachedNode> newNodes = createConnections(newServers);\n\n\t\t\t// merge stay nodes with new nodes\n\t\t\tList<MemcachedNode> mergedNodes = new ArrayList<MemcachedNode>();\n\t\t\tmergedNodes.addAll(stayNodes);\n\t\t\tmergedNodes.addAll(newNodes);\n\n\t\t\t// call update locator with new nodes list and vbucket config\n\t\t\t((VBucketNodeLocator) locator).updateLocator(mergedNodes, bucket.getVbuckets());\n\n\t\t\t// schedule shutdown for the oddNodes\n\t\t\tnodesToShutdown.addAll(oddNodes);\n\t\t} catch (IOException e) {\n\t\t    getLogger().error(\"Connection reconfiguration failed\", e);\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getSk() != null && qa.getSk().isValid()) {\n\t\t\t\tif(qa.getChannel().isConnected()) {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty() && !shutDown) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > DOUBLE_CHECK_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlostConnection((MemcachedNode)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\thandleIO(sk);\n\t\t\t}\n\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\n\t\t// see if any connections blew up with large number of timeouts\n\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\tMemcachedNode mn = (MemcachedNode)sk.attachment();\n\t\t\tif (mn.getContinuousTimeout() > timeoutExceptionThreshold)\n\t\t\t{\n\t\t\t\tgetLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n\t\t\t\tlostConnection(mn);\n\t\t\t}\n\t\t}\n\n\t\tif(!shutDown && !reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n        // rehash operations that in retry state\n        redistributeOperations(retryOps);\n        retryOps.clear();\n\n        // try to shutdown odd nodes\n        for (MemcachedNode qa : nodesToShutdown) {\n            if (!addedQueue.contains(qa)) {\n                nodesToShutdown.remove(qa);\n                Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n                if (qa.getChannel() != null) {\n                    qa.getChannel().close();\n                    qa.setSk(null);\n                    if (qa.getBytesRemainingToWrite() > 0) {\n                        getLogger().warn(\n                                \"Shut down with %d bytes remaining to write\",\n                                qa.getBytesRemainingToWrite());\n                    }\n                    getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n                }\n                redistributeOperations(notCompletedOperations);\n            }\n        }\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<MemcachedNode> toAdd=new HashSet<MemcachedNode>();\n\t\t\t// Transfer the queue into a hashset.  There are very likely more\n\t\t\t// additions than there are nodes.\n\t\t\tCollection<MemcachedNode> todo=new HashSet<MemcachedNode>();\n\t\t\ttry {\n\t\t\t\tMemcachedNode qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\ttodo.add(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// Found everything\n\t\t\t}\n\n\t\t\t// Now process the queue.\n\t\t\tfor(MemcachedNode qa : todo) {\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\tif(qa.isActive()) {\n\t\t\t\t\tif(qa.getCurrentWriteOp() != null) {\n\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t}\n\t\t\t\tqa.copyInputQueue();\n\t\t\t\tif(readyForIO) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\t\thandleWrites(qa.getSk(), qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\tlostConnection(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tqa.fixupOps();\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return whether the observer was successfully added\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn connObservers.add(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed and now doesn't\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn connObservers.remove(obs);\n\t}\n\n\tprivate void connected(MemcachedNode qa) {\n\t\tassert qa.getChannel().isConnected() : \"Not connected.\";\n\t\tint rt = qa.getReconnectCount();\n\t\tqa.connected();\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionEstablished(qa.getSocketAddress(), rt);\n\t\t}\n\t}\n\n\tprivate void lostConnection(MemcachedNode qa) {\n\t\tqueueReconnect(qa);\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionLost(qa.getSocketAddress());\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tMemcachedNode qa=(MemcachedNode)sk.attachment();\n\t\ttry {\n\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\tif(sk.isConnectable()) {\n\t\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\t\tfinal SocketChannel channel=qa.getChannel();\n\t\t\t\tif(channel.finishConnect()) {\n\t\t\t\t\tconnected(qa);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t}\n\t\t\t\tif(sk.isWritable()) {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(ClosedChannelException e) {\n\t\t\t// Note, not all channel closes end up here\n\t\t\tif(!shutDown) {\n\t\t\t\tgetLogger().info(\"Closed channel and not shutting down.  \"\n\t\t\t\t\t+ \"Queueing reconnect on %s\", qa, e);\n\t\t\t\tlostConnection(qa);\n\t\t\t}\n\t\t} catch(ConnectException e) {\n\t\t\t// Failures to establish a connection should attempt a reconnect\n\t\t\t// without signaling the observers.\n\t\t\tgetLogger().info(\"Reconnecting due to failure to connect to %s\",\n\t\t\t\t\tqa, e);\n\t\t\tqueueReconnect(qa);\n\t\t} catch (OperationException e) {\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnection due to exception \" +\n\t\t\t\t\"handling a memcached operation on %s.  \" +\n\t\t\t\t\"This may be due to an authentication failure.\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t} catch(Exception e) {\n\t\t\t// Any particular error processing an item should simply\n\t\t\t// cause us to reconnect to the server.\n\t\t\t//\n\t\t\t// One cause is just network oddness or servers\n\t\t\t// restarting, which lead here with IOException\n\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnecting due to exception on %s\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t}\n\t\tqa.fixupOps();\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\tboolean canWriteMore=qa.getBytesRemainingToWrite() > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.writeSome();\n\t\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\t\tcanWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tByteBuffer rbuf=qa.getRbuf();\n\t\tfinal SocketChannel channel = qa.getChannel();\n\t\tint read=channel.read(rbuf);\n\t\tif (read < 0) {\n\t\t    // our model is to keep the connection alive for future ops\n\t\t    // so we'll queue a reconnect if disconnected via an IOException\n\t\t    throw new IOException(\"Disconnected unexpected, will reconnect.\");\n\t\t}\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\trbuf.flip();\n\t\t\twhile(rbuf.remaining() > 0) {\n\t\t\t\tif(currentOp == null) {\n\t\t\t\t\tthrow new IllegalStateException(\"No read operation.\");\n\t\t\t\t}\n\t\t\t\tcurrentOp.readFromBuffer(rbuf);\n\t\t\t\tif(currentOp.getState() == OperationState.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t} else if (currentOp.getState() == OperationState.RETRY) {\n                    getLogger().debug(\n                            \"Reschedule read op due to NOT_MY_VBUCKET error: %s \",\n                            currentOp);\n                    Operation op=qa.removeCurrentReadOp();\n                    assert op == currentOp\n                    : \"Expected to pop \" + currentOp + \" got \" + op;\n                    retryOps.add(currentOp);\n                    currentOp=qa.getCurrentReadOp();\n\n\t\t\t\t}\n\t\t\t}\n\t\t\trbuf.clear();\n\t\t\tread=channel.read(rbuf);\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(MemcachedNode qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.getReconnectCount());\n\t\t\tif(qa.getSk() != null) {\n\t\t\t\tqa.getSk().cancel();\n\t\t\t\tassert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnecting();\n\t\t\ttry {\n\t\t\t\tif(qa.getChannel() != null && qa.getChannel().socket() != null) {\n\t\t\t\t\tqa.getChannel().socket().close();\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"The channel or socket was null for %s\",\n\t\t\t\t\t\tqa);\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.setChannel(null);\n\n\t\t\tlong delay = (long)Math.min(maxDelay,\n\t\t\t\t\tMath.pow(2, qa.getReconnectCount())) * 1000;\n\t\t\tlong reconTime = System.currentTimeMillis() + delay;\n\n\t\t\t// Avoid potential condition where two connections are scheduled\n\t\t\t// for reconnect at the exact same time.  This is expected to be\n\t\t\t// a rare situation.\n\t\t\twhile(reconnectQueue.containsKey(reconTime)) {\n\t\t\t\treconTime++;\n\t\t\t}\n\n\t\t\treconnectQueue.put(reconTime, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\n\t\t\tif(failureMode == FailureMode.Redistribute) {\n\t\t\t\tredistributeOperations(qa.destroyInputQueue());\n\t\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\t\tcancelOperations(qa.destroyInputQueue());\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void cancelOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\top.cancel();\n\t\t}\n\t}\n\n\tprivate void redistributeOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\tif(op instanceof KeyedOperation) {\n\t\t\t\tKeyedOperation ko = (KeyedOperation)op;\n\t\t\t\tint added = 0;\n\t\t\t\tfor(String k : ko.getKeys()) {\n\t\t\t\t\tfor(Operation newop : opFact.clone(ko)) {\n\t\t\t\t\t\taddOperation(k, newop);\n\t\t\t\t\t\tadded++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert added > 0\n\t\t\t\t\t: \"Didn't add any new operations when redistributing\";\n\t\t\t} else {\n\t\t\t\t// Cancel things that don't have definite targets.\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tfinal long now=System.currentTimeMillis();\n\t\tfinal Map<MemcachedNode, Boolean> seen=\n\t\t\tnew IdentityHashMap<MemcachedNode, Boolean>();\n\t\tfinal List<MemcachedNode> rereQueue=new ArrayList<MemcachedNode>();\n\t\tSocketChannel ch = null;\n\t\tfor(Iterator<MemcachedNode> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tfinal MemcachedNode qa=i.next();\n\t\t\ti.remove();\n\t\t\ttry {\n\t\t\t\tif(!seen.containsKey(qa)) {\n\t\t\t\t\tseen.put(qa, Boolean.TRUE);\n\t\t\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\t\t\tch=SocketChannel.open();\n\t\t\t\t\tch.configureBlocking(false);\n\t\t\t\t\tint ops=0;\n\t\t\t\t\tif(ch.connect(qa.getSocketAddress())) {\n\t\t\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\t\t\tassert ch.isConnected();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t\t}\n\t\t\t\t\tqa.registerChannel(ch, ch.register(selector, ops, qa));\n\t\t\t\t\tassert qa.getChannel() == ch : \"Channel was lost.\";\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Skipping duplicate reconnect request for %s\", qa);\n\t\t\t\t}\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Error on reconnect\", e);\n\t\t\t\trereQueue.add(qa);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n                getLogger().error(\"Exception on reconnect, lost node %s\", qa, e);\n            } finally {\n                //it's possible that above code will leak file descriptors under abnormal\n                //conditions (when ch.open() fails and throws IOException.\n                //always close non connected channel\n                if (ch != null && !ch.isConnected()\n                        && !ch.isConnectionPending()) {\n                    try {\n                        ch.close();\n                    } catch (IOException x) {\n                        getLogger().error(\"Exception closing channel: %s\", qa, x);\n                    }\n                }\n            }\n\t\t}\n\t\t// Requeue any fast-failed connects.\n\t\tfor(MemcachedNode n : rereQueue) {\n\t\t\tqueueReconnect(n);\n\t\t}\n\t}\n\n\t/**\n\t * Get the node locator used by this connection.\n\t */\n\tNodeLocator getLocator() {\n\t\treturn locator;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t *\n\t * @param key the key the operation is operating upon\n\t * @param o the operation\n\t */\n\tpublic void addOperation(final String key, final Operation o) {\n\t\tMemcachedNode placeIn=null;\n\t\tMemcachedNode primary = locator.getPrimary(key);\n\t\tif(primary.isActive() || failureMode == FailureMode.Retry) {\n\t\t\tplaceIn=primary;\n\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\to.cancel();\n\t\t} else {\n\t\t\t// Look for another node in sequence that is ready.\n\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\tplaceIn == null && i.hasNext(); ) {\n\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\tif(n.isActive()) {\n\t\t\t\t\tplaceIn=n;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we didn't find an active node, queue it in the primary node\n\t\t\t// and wait for it to come back online.\n\t\t\tif(placeIn == null) {\n\t\t\t\tplaceIn = primary;\n\t\t\t\tthis.getLogger().warn(\"Could not redistribute \" +\n\t\t\t\t\t\"to another node, retrying primary node for %s.\", key);\n\t\t\t}\n\t\t}\n\n\t\tassert o.isCancelled() || placeIn != null\n\t\t\t: \"No node found for key \" + key;\n\t\tif(placeIn != null) {\n\t\t\t// add the vbucketIndex to the operation\n\t\t\tif (locator instanceof VBucketNodeLocator) {\n\t\t\t\tint vbucketIndex = ((VBucketNodeLocator) locator).getVBucketIndex(key);\n\t\t\t\tif (o instanceof VBucketAware) {\n\t\t\t\t\t((VBucketAware) o).setVBucket(vbucketIndex);\n\t\t\t\t}\n\t\t\t}\n\t\t\taddOperation(placeIn, o);\n\t\t} else {\n\t\t\tassert o.isCancelled() : \"No node found for \"\n\t\t\t\t+ key + \" (and not immediately cancelled)\";\n\t\t}\n\t}\n\n\tpublic void insertOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.insertOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.addOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n\t\tfor(Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n\t\t\tfinal MemcachedNode node=me.getKey();\n\t\t\tOperation o=me.getValue();\n\t\t\to.setHandlingNode(node);\n\t\t\to.initialize();\n\t\t\tnode.addOp(o);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t}\n\n\t/**\n\t * Broadcast an operation to all nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(BroadcastOpFactory of) {\n\t\treturn broadcastOperation(of, locator.getAll());\n\t}\n\n\t/**\n\t * Broadcast an operation to a specific collection of nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(locator.getAll().size());\n\t\tfor(MemcachedNode node : nodes) {\n\t\t\tOperation op = of.newOp(node, latch);\n\t\t\top.initialize();\n\t\t\tnode.addOp(op);\n\t\t\top.setHandlingNode(node);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\treturn latch;\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tshutDown=true;\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getChannel() != null) {\n\t\t\t\tqa.getChannel().close();\n\t\t\t\tqa.setSk(null);\n\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.getBytesRemainingToWrite());\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.getChannel());\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.getSocketAddress());\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n    /**\n     * helper method: increase timeout count on node attached to this op\n     *\n     * @param op\n     */\n    public static void opTimedOut(Operation op) {\n        MemcachedConnection.setTimeout(op, true);\n    }\n\n    /**\n     * helper method: reset timeout counter\n     *\n     * @param op\n     */\n    public static void opSucceeded(Operation op) {\n        MemcachedConnection.setTimeout(op, false);\n    }\n\n    /**\n     * helper method: do some error checking and set timeout boolean\n     *\n     * @param op\n     * @param isTimeout\n     */\n    private static void setTimeout(Operation op, boolean isTimeout) {\n        try {\n            if (op == null || op.isTimedOutUnsent()) {\n                return; // op may be null in some cases, e.g. flush\n            }\n            MemcachedNode node = op.getHandlingNode();\n            if (node == null) {\n                LoggerFactory.getLogger(MemcachedConnection.class).warn(\"handling node for operation is not set\");\n            }\n            else {\n                node.setContinuousTimeout(isTimeout);\n            }\n        } catch (Exception e) {\n            LoggerFactory.getLogger(MemcachedConnection.class).error(e.getMessage());\n        }\n    }\n\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.net.SocketException;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\n\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.compat.log.LoggerFactory;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.VBucketAware;\nimport net.spy.memcached.vbucket.VBucketNodeLocator;\nimport net.spy.memcached.vbucket.Reconfigurable;\nimport net.spy.memcached.vbucket.config.Bucket;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic final class MemcachedConnection extends SpyObject implements Reconfigurable {\n\n\t// The number of empty selects we'll allow before assuming we may have\n\t// missed one and should check the current selectors.  This generally\n\t// indicates a bug, but we'll check it nonetheless.\n\tprivate static final int DOUBLE_CHECK_EMPTY = 256;\n\t// The number of empty selects we'll allow before blowing up.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 0x1000000;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, optimization will collapse multiple sequential get ops\n\tprivate final boolean shouldOptimize;\n\tprivate Selector selector=null;\n\tprivate final NodeLocator locator;\n\tprivate final FailureMode failureMode;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate final long maxDelay;\n\tprivate int emptySelects=0;\n\tprivate final int bufSize;\n\tprivate final ConnectionFactory connectionFactory;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n\tprivate final Collection<ConnectionObserver> connObservers =\n\t\tnew ConcurrentLinkedQueue<ConnectionObserver>();\n\tprivate final OperationFactory opFact;\n\tprivate final int timeoutExceptionThreshold;\n        private final Collection<Operation> retryOps;\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> nodesToShutdown;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n\t\t\tFailureMode fm, OperationFactory opfactory)\n\t\tthrows IOException {\n\t\tconnObservers.addAll(obs);\n\t\treconnectQueue=new TreeMap<Long, MemcachedNode>();\n\t\taddedQueue=new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tfailureMode = fm;\n\t\tshouldOptimize = f.shouldOptimize();\n\t\tmaxDelay = f.getMaxReconnectDelay();\n\t\topFact = opfactory;\n\t\ttimeoutExceptionThreshold = f.getTimeoutExceptionThreshold();\n\t\tselector=Selector.open();\n\t\tretryOps = new ArrayList<Operation>();\n\t\tnodesToShutdown = new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tthis.bufSize = bufSize;\n\t\tthis.connectionFactory = f;\n\t\tList<MemcachedNode> connections = createConnections(a);\n\t\tlocator=f.createLocator(connections);\n\t\t}\n\n\tprivate List<MemcachedNode> createConnections(final Collection<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\tList<MemcachedNode> connections=new ArrayList<MemcachedNode>(a.size());\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tMemcachedNode qa=this.connectionFactory.createMemcachedNode(sa, ch, bufSize);\n\t\t\tint ops=0;\n\t\t\tch.socket().setTcpNoDelay(!this.connectionFactory.useNagleAlgorithm());\n\t\t\t// Initially I had attempted to skirt this by queueing every\n\t\t\t// connect, but it considerably slowed down start time.\n\t\t\ttry {\n\t\t\t\tif(ch.connect(sa)) {\n\t\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\t\tconnected(qa);\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t}\n\t\t\t\tqa.setSk(ch.register(selector, ops, qa));\n\t\t\t\tassert ch.isConnected()\n\t\t\t\t\t|| qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Socket error on initial connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t\tconnections.add(qa);\n\t\t}\n\t\treturn connections;\n\t}\n\n\tpublic void reconfigure(Bucket bucket) {\n\t\ttry {\n\t\t\tif (!(locator instanceof VBucketNodeLocator)) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// get a new collection of addresses from the received config\n\t\t\tList<String> servers = bucket.getVbuckets().getServers();\n\t\t\tHashSet<SocketAddress> newServerAddresses = new HashSet<SocketAddress>();\n\t\t\tArrayList<InetSocketAddress> newServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (String server : servers) {\n\t\t\t\tint finalColon = server.lastIndexOf(':');\n\t\t\t\tif (finalColon < 1) {\n\t\t\t\t\tthrow new IllegalArgumentException(\"Invalid server ``\"\n\t\t\t\t\t+ server + \"'' in vbucket's server list\");\n\t\t\t\t}\n\t\t\t\tString hostPart = server.substring(0, finalColon);\n\t\t\t\tString portNum = server.substring(finalColon + 1);\n\n\t\t\t\tInetSocketAddress address = new InetSocketAddress(hostPart,\n\t\t\t\tInteger.parseInt(portNum));\n\t\t\t\t// add parsed address to our collections\n\t\t\t\tnewServerAddresses.add(address);\n\t\t\t\tnewServers.add(address);\n\n\t\t\t}\n\n\t\t\t// split current nodes to \"odd nodes\" and \"stay nodes\"\n\t\t\tArrayList<MemcachedNode> oddNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<MemcachedNode> stayNodes = new ArrayList<MemcachedNode>();\n\t\t\tArrayList<InetSocketAddress> stayServers = new ArrayList<InetSocketAddress>();\n\t\t\tfor (MemcachedNode current : locator.getAll()) {\n\t\t\t\tif (newServerAddresses.contains(current.getSocketAddress())) {\n\t\t\t\t\tstayNodes.add(current);\n\t\t\t\t\tstayServers.add((InetSocketAddress) current.getSocketAddress());\n\t\t\t\t} else {\n\t\t\t\t\toddNodes.add(current);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// prepare a collection of addresses for new nodes\n\t\t\tnewServers.removeAll(stayServers);\n\n\t\t\t// create a collection of new nodes\n\t\t\tList<MemcachedNode> newNodes = createConnections(newServers);\n\n\t\t\t// merge stay nodes with new nodes\n\t\t\tList<MemcachedNode> mergedNodes = new ArrayList<MemcachedNode>();\n\t\t\tmergedNodes.addAll(stayNodes);\n\t\t\tmergedNodes.addAll(newNodes);\n\n\t\t\t// call update locator with new nodes list and vbucket config\n\t\t\t((VBucketNodeLocator) locator).updateLocator(mergedNodes, bucket.getVbuckets());\n\n\t\t\t// schedule shutdown for the oddNodes\n\t\t\tnodesToShutdown.addAll(oddNodes);\n\t\t} catch (IOException e) {\n\t\t    getLogger().error(\"Connection reconfiguration failed\", e);\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getSk() != null && qa.getSk().isValid()) {\n\t\t\t\tif(qa.getChannel().isConnected()) {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty() && !shutDown) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > DOUBLE_CHECK_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlostConnection((MemcachedNode)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\thandleIO(sk);\n\t\t\t}\n\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\n\t\t// see if any connections blew up with large number of timeouts\n\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\tMemcachedNode mn = (MemcachedNode)sk.attachment();\n\t\t\tif (mn.getContinuousTimeout() > timeoutExceptionThreshold)\n\t\t\t{\n\t\t\t\tgetLogger().warn(\"%s exceeded continuous timeout threshold\", sk);\n\t\t\t\tlostConnection(mn);\n\t\t\t}\n\t\t}\n\n\t\tif(!shutDown && !reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n        // rehash operations that in retry state\n        redistributeOperations(retryOps);\n        retryOps.clear();\n\n        // try to shutdown odd nodes\n        for (MemcachedNode qa : nodesToShutdown) {\n            if (!addedQueue.contains(qa)) {\n                nodesToShutdown.remove(qa);\n                Collection<Operation> notCompletedOperations = qa.destroyInputQueue();\n                if (qa.getChannel() != null) {\n                    qa.getChannel().close();\n                    qa.setSk(null);\n                    if (qa.getBytesRemainingToWrite() > 0) {\n                        getLogger().warn(\n                                \"Shut down with %d bytes remaining to write\",\n                                qa.getBytesRemainingToWrite());\n                    }\n                    getLogger().debug(\"Shut down channel %s\", qa.getChannel());\n                }\n                redistributeOperations(notCompletedOperations);\n            }\n        }\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<MemcachedNode> toAdd=new HashSet<MemcachedNode>();\n\t\t\t// Transfer the queue into a hashset.  There are very likely more\n\t\t\t// additions than there are nodes.\n\t\t\tCollection<MemcachedNode> todo=new HashSet<MemcachedNode>();\n\t\t\ttry {\n\t\t\t\tMemcachedNode qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\ttodo.add(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// Found everything\n\t\t\t}\n\n\t\t\t// Now process the queue.\n\t\t\tfor(MemcachedNode qa : todo) {\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\tif(qa.isActive()) {\n\t\t\t\t\tif(qa.getCurrentWriteOp() != null) {\n\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t}\n\t\t\t\tqa.copyInputQueue();\n\t\t\t\tif(readyForIO) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\t\thandleWrites(qa.getSk(), qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\tlostConnection(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tqa.fixupOps();\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return whether the observer was successfully added\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn connObservers.add(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed and now doesn't\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn connObservers.remove(obs);\n\t}\n\n\tprivate void connected(MemcachedNode qa) {\n\t\tassert qa.getChannel().isConnected() : \"Not connected.\";\n\t\tint rt = qa.getReconnectCount();\n\t\tqa.connected();\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionEstablished(qa.getSocketAddress(), rt);\n\t\t}\n\t}\n\n\tprivate void lostConnection(MemcachedNode qa) {\n\t\tqueueReconnect(qa);\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionLost(qa.getSocketAddress());\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tMemcachedNode qa=(MemcachedNode)sk.attachment();\n\t\ttry {\n\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\tif(sk.isConnectable()) {\n\t\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\t\tfinal SocketChannel channel=qa.getChannel();\n\t\t\t\tif(channel.finishConnect()) {\n\t\t\t\t\tconnected(qa);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t}\n\t\t\t\tif(sk.isWritable()) {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(ClosedChannelException e) {\n\t\t\t// Note, not all channel closes end up here\n\t\t\tif(!shutDown) {\n\t\t\t\tgetLogger().info(\"Closed channel and not shutting down.  \"\n\t\t\t\t\t+ \"Queueing reconnect on %s\", qa, e);\n\t\t\t\tlostConnection(qa);\n\t\t\t}\n\t\t} catch(ConnectException e) {\n\t\t\t// Failures to establish a connection should attempt a reconnect\n\t\t\t// without signaling the observers.\n\t\t\tgetLogger().info(\"Reconnecting due to failure to connect to %s\",\n\t\t\t\t\tqa, e);\n\t\t\tqueueReconnect(qa);\n\t\t} catch (OperationException e) {\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnection due to exception \" +\n\t\t\t\t\"handling a memcached operation on %s.  \" +\n\t\t\t\t\"This may be due to an authentication failure.\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t} catch(Exception e) {\n\t\t\t// Any particular error processing an item should simply\n\t\t\t// cause us to reconnect to the server.\n\t\t\t//\n\t\t\t// One cause is just network oddness or servers\n\t\t\t// restarting, which lead here with IOException\n\n\t\t\tqa.setupForAuth(); // noop if !shouldAuth\n\t\t\tgetLogger().info(\"Reconnecting due to exception on %s\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t}\n\t\tqa.fixupOps();\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\tboolean canWriteMore=qa.getBytesRemainingToWrite() > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.writeSome();\n\t\t\tqa.fillWriteBuffer(shouldOptimize);\n\t\t\tcanWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tByteBuffer rbuf=qa.getRbuf();\n\t\tfinal SocketChannel channel = qa.getChannel();\n\t\tint read=channel.read(rbuf);\n\t\tif (read < 0) {\n\t\t    // our model is to keep the connection alive for future ops\n\t\t    // so we'll queue a reconnect if disconnected via an IOException\n\t\t    throw new IOException(\"Disconnected unexpected, will reconnect.\");\n\t\t}\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\trbuf.flip();\n\t\t\twhile(rbuf.remaining() > 0) {\n\t\t\t\tif(currentOp == null) {\n\t\t\t\t\tthrow new IllegalStateException(\"No read operation.\");\n\t\t\t\t}\n\t\t\t\tcurrentOp.readFromBuffer(rbuf);\n\t\t\t\tif(currentOp.getState() == OperationState.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t} else if (currentOp.getState() == OperationState.RETRY) {\n                    getLogger().debug(\n                            \"Reschedule read op due to NOT_MY_VBUCKET error: %s \",\n                            currentOp);\n                    ((VBucketAware) currentOp).addNotMyVbucketNode(currentOp.getHandlingNode());\n                    Operation op=qa.removeCurrentReadOp();\n                    assert op == currentOp\n                    : \"Expected to pop \" + currentOp + \" got \" + op;\n                    retryOps.add(currentOp);\n                    currentOp=qa.getCurrentReadOp();\n\n\t\t\t\t}\n\t\t\t}\n\t\t\trbuf.clear();\n\t\t\tread=channel.read(rbuf);\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(MemcachedNode qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.getReconnectCount());\n\t\t\tif(qa.getSk() != null) {\n\t\t\t\tqa.getSk().cancel();\n\t\t\t\tassert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnecting();\n\t\t\ttry {\n\t\t\t\tif(qa.getChannel() != null && qa.getChannel().socket() != null) {\n\t\t\t\t\tqa.getChannel().socket().close();\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"The channel or socket was null for %s\",\n\t\t\t\t\t\tqa);\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.setChannel(null);\n\n\t\t\tlong delay = (long)Math.min(maxDelay,\n\t\t\t\t\tMath.pow(2, qa.getReconnectCount())) * 1000;\n\t\t\tlong reconTime = System.currentTimeMillis() + delay;\n\n\t\t\t// Avoid potential condition where two connections are scheduled\n\t\t\t// for reconnect at the exact same time.  This is expected to be\n\t\t\t// a rare situation.\n\t\t\twhile(reconnectQueue.containsKey(reconTime)) {\n\t\t\t\treconTime++;\n\t\t\t}\n\n\t\t\treconnectQueue.put(reconTime, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\n\t\t\tif(failureMode == FailureMode.Redistribute) {\n\t\t\t\tredistributeOperations(qa.destroyInputQueue());\n\t\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\t\tcancelOperations(qa.destroyInputQueue());\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void cancelOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\top.cancel();\n\t\t}\n\t}\n\n\tprivate void redistributeOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\tif(op instanceof KeyedOperation) {\n\t\t\t\tKeyedOperation ko = (KeyedOperation)op;\n\t\t\t\tint added = 0;\n\t\t\t\tfor(String k : ko.getKeys()) {\n\t\t\t\t\tfor(Operation newop : opFact.clone(ko)) {\n\t\t\t\t\t\taddOperation(k, newop);\n\t\t\t\t\t\tadded++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert added > 0\n\t\t\t\t\t: \"Didn't add any new operations when redistributing\";\n\t\t\t} else {\n\t\t\t\t// Cancel things that don't have definite targets.\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tfinal long now=System.currentTimeMillis();\n\t\tfinal Map<MemcachedNode, Boolean> seen=\n\t\t\tnew IdentityHashMap<MemcachedNode, Boolean>();\n\t\tfinal List<MemcachedNode> rereQueue=new ArrayList<MemcachedNode>();\n\t\tSocketChannel ch = null;\n\t\tfor(Iterator<MemcachedNode> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tfinal MemcachedNode qa=i.next();\n\t\t\ti.remove();\n\t\t\ttry {\n\t\t\t\tif(!seen.containsKey(qa)) {\n\t\t\t\t\tseen.put(qa, Boolean.TRUE);\n\t\t\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\t\t\tch=SocketChannel.open();\n\t\t\t\t\tch.configureBlocking(false);\n\t\t\t\t\tint ops=0;\n\t\t\t\t\tif(ch.connect(qa.getSocketAddress())) {\n\t\t\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\t\t\tassert ch.isConnected();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t\t}\n\t\t\t\t\tqa.registerChannel(ch, ch.register(selector, ops, qa));\n\t\t\t\t\tassert qa.getChannel() == ch : \"Channel was lost.\";\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Skipping duplicate reconnect request for %s\", qa);\n\t\t\t\t}\n\t\t\t} catch(SocketException e) {\n\t\t\t\tgetLogger().warn(\"Error on reconnect\", e);\n\t\t\t\trereQueue.add(qa);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n                getLogger().error(\"Exception on reconnect, lost node %s\", qa, e);\n            } finally {\n                //it's possible that above code will leak file descriptors under abnormal\n                //conditions (when ch.open() fails and throws IOException.\n                //always close non connected channel\n                if (ch != null && !ch.isConnected()\n                        && !ch.isConnectionPending()) {\n                    try {\n                        ch.close();\n                    } catch (IOException x) {\n                        getLogger().error(\"Exception closing channel: %s\", qa, x);\n                    }\n                }\n            }\n\t\t}\n\t\t// Requeue any fast-failed connects.\n\t\tfor(MemcachedNode n : rereQueue) {\n\t\t\tqueueReconnect(n);\n\t\t}\n\t}\n\n\t/**\n\t * Get the node locator used by this connection.\n\t */\n\tNodeLocator getLocator() {\n\t\treturn locator;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t *\n\t * @param key the key the operation is operating upon\n\t * @param o the operation\n\t */\n\tpublic void addOperation(final String key, final Operation o) {\n\t\tMemcachedNode placeIn=null;\n\t\tMemcachedNode primary = locator.getPrimary(key);\n\t\tif(primary.isActive() || failureMode == FailureMode.Retry) {\n\t\t\tplaceIn=primary;\n\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\to.cancel();\n\t\t} else {\n\t\t\t// Look for another node in sequence that is ready.\n\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\tplaceIn == null && i.hasNext(); ) {\n\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\tif(n.isActive()) {\n\t\t\t\t\tplaceIn=n;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we didn't find an active node, queue it in the primary node\n\t\t\t// and wait for it to come back online.\n\t\t\tif(placeIn == null) {\n\t\t\t\tplaceIn = primary;\n\t\t\t\tthis.getLogger().warn(\"Could not redistribute \" +\n\t\t\t\t\t\"to another node, retrying primary node for %s.\", key);\n\t\t\t}\n\t\t}\n\n\t\tassert o.isCancelled() || placeIn != null\n\t\t\t: \"No node found for key \" + key;\n\t\tif(placeIn != null) {\n\t\t\t// add the vbucketIndex to the operation\n\t\t\tif (locator instanceof VBucketNodeLocator) {\n\t\t\t\tVBucketNodeLocator vbucketLocator = (VBucketNodeLocator) locator;\n\t\t\t\tint vbucketIndex = vbucketLocator.getVBucketIndex(key);\n\t\t\t\tif (o instanceof VBucketAware) {\n\t\t\t\t\tVBucketAware vbucketAwareOp = (VBucketAware) o;\n\t\t\t\t\tvbucketAwareOp.setVBucket(vbucketIndex);\n\t\t\t\t\tif (!vbucketAwareOp.getNotMyVbucketNodes().isEmpty()) {\n\t\t\t\t\t\tMemcachedNode alternative = vbucketLocator.\n\t\t\t\t\t\tgetAlternative(key, vbucketAwareOp.getNotMyVbucketNodes());\n\t\t\t\t\t\t\tif (alternative != null) {\n\t\t\t\t\t\t\t\tplaceIn = alternative;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\taddOperation(placeIn, o);\n\t\t\t} else {\n\t\t\t\tassert o.isCancelled() : \"No node found for \"\n\t\t\t\t\t+ key + \" (and not immediately cancelled)\";\n\t\t\t}\n\t}\n\n\tpublic void insertOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.insertOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperation(final MemcachedNode node, final Operation o) {\n\t\to.setHandlingNode(node);\n\t\to.initialize();\n\t\tnode.addOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n\t\tfor(Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n\t\t\tfinal MemcachedNode node=me.getKey();\n\t\t\tOperation o=me.getValue();\n\t\t\to.setHandlingNode(node);\n\t\t\to.initialize();\n\t\t\tnode.addOp(o);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t}\n\n\t/**\n\t * Broadcast an operation to all nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(BroadcastOpFactory of) {\n\t\treturn broadcastOperation(of, locator.getAll());\n\t}\n\n\t/**\n\t * Broadcast an operation to a specific collection of nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(final BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(locator.getAll().size());\n\t\tfor(MemcachedNode node : nodes) {\n\t\t\tOperation op = of.newOp(node, latch);\n\t\t\top.initialize();\n\t\t\tnode.addOp(op);\n\t\t\top.setHandlingNode(node);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\treturn latch;\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tshutDown=true;\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getChannel() != null) {\n\t\t\t\tqa.getChannel().close();\n\t\t\t\tqa.setSk(null);\n\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.getBytesRemainingToWrite());\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.getChannel());\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.getSocketAddress());\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n    /**\n     * helper method: increase timeout count on node attached to this op\n     *\n     * @param op\n     */\n    public static void opTimedOut(Operation op) {\n        MemcachedConnection.setTimeout(op, true);\n    }\n\n    /**\n     * helper method: reset timeout counter\n     *\n     * @param op\n     */\n    public static void opSucceeded(Operation op) {\n        MemcachedConnection.setTimeout(op, false);\n    }\n\n    /**\n     * helper method: do some error checking and set timeout boolean\n     *\n     * @param op\n     * @param isTimeout\n     */\n    private static void setTimeout(Operation op, boolean isTimeout) {\n        try {\n            if (op == null || op.isTimedOutUnsent()) {\n                return; // op may be null in some cases, e.g. flush\n            }\n            MemcachedNode node = op.getHandlingNode();\n            if (node == null) {\n                LoggerFactory.getLogger(MemcachedConnection.class).warn(\"handling node for operation is not set\");\n            }\n            else {\n                node.setContinuousTimeout(isTimeout);\n            }\n        } catch (Exception e) {\n            LoggerFactory.getLogger(MemcachedConnection.class).error(e.getMessage());\n        }\n    }\n\n}\n","lineNo":708}
{"Smelly Sample":"package net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\nimport net.spy.memcached.HashAlgorithm;\n\npublic class DefaultConfigFactory implements ConfigFactory {\n\n    @Override\n    public Config create(File filename) {\n        if (filename == null || \"\".equals(filename.getName())) {\n            throw new IllegalArgumentException(\"Filename is empty.\");\n        }\n        StringBuilder sb = new StringBuilder();\n        try {\n            FileInputStream fis = new FileInputStream(filename);\n            BufferedReader reader = new BufferedReader(new InputStreamReader(\n                    fis));\n            String str;\n            while ((str = reader.readLine()) != null) {\n                sb.append(str);\n            }\n        } catch (IOException e) {\n            throw new ConfigParsingException(\"Exception reading input file: \"\n                    + filename, e);\n        }\n        return create(sb.toString());\n    }\n\n    @Override\n    public Config create(String data) {\n        try {\n            JSONObject jsonObject = new JSONObject(data);\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + data, e);\n        }\n    }\n\n    @Override\n    public Config create(JSONObject jsonObject) {\n        try {\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + jsonObject, e);\n        }\n    }\n\n    private HashAlgorithm lookupHashAlgorithm(String algorithm) {\n        HashAlgorithm ha = HashAlgorithm.NATIVE_HASH;\n        if (\"crc\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.CRC32_HASH;\n        } else if (\"fnv1_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_32_HASH;\n        } else if (\"fnv1_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_64_HASH;\n        } else if (\"fnv1a_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_32_HASH;\n        } else if (\"fnv1a_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_64_HASH;\n        } else if (\"md5\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.KETAMA_HASH;\n        } else {\n            throw new IllegalArgumentException(\"Unhandled algorithm type: \"\n                    + algorithm);\n        }\n        return ha;\n    }\n\n    private Config parseJSON(JSONObject jsonObject) throws JSONException {\n        // Allows clients to have a JSON envelope.\n        if (jsonObject.has(\"vBucketServerMap\")) {\n            return parseJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n        }\n        HashAlgorithm hashAlgorithm = lookupHashAlgorithm(jsonObject\n                .getString(\"hashAlgorithm\"));\n        int replicasCount = jsonObject.getInt(\"numReplicas\");\n        if (replicasCount > VBucket.MAX_REPLICAS) {\n            throw new ConfigParsingException(\"Expected number <= \"\n                    + VBucket.MAX_REPLICAS + \" for replicas.\");\n        }\n        JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n        if (servers.length() <= 0) {\n            throw new ConfigParsingException(\"Empty servers list.\");\n        }\n        int serversCount = servers.length();\n        JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n        int vbucketsCount = vbuckets.length();\n        if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n            throw new ConfigParsingException(\n                    \"Number of buckets must be a power of two, > 0 and <= \"\n                            + VBucket.MAX_BUCKETS);\n        }\n\n        Config config = new DefaultConfig(hashAlgorithm, serversCount,\n                replicasCount, vbucketsCount, populateServers(servers),\n                populateVbuckets(servers));\n\n        return config;\n    }\n\n    private List<String> populateServers(JSONArray servers)\n            throws JSONException {\n        List<String> serverNames = new ArrayList<String>();\n        for (int i = 0; i < servers.length(); i++) {\n            String server = servers.getString(i);\n            serverNames.add(server);\n        }\n        return serverNames;\n    }\n\n    private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n            throws JSONException {\n        List<VBucket> vBuckets = new ArrayList<VBucket>();\n        for (int i = 0; i < jsonVbuckets.length(); i++) {\n            JSONArray rows = jsonVbuckets.getJSONArray(i);\n            int master = rows.getInt(0);\n            int replicas[] = new int[VBucket.MAX_REPLICAS];\n            for (int j = 1; j < rows.length(); j++) {\n                replicas[j-1] = rows.getInt(j);\n            }\n            vBuckets.add(new VBucket(master, replicas));\n        }\n        return vBuckets;\n    }\n\n}\n","Method after Refactoring":"package net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\nimport net.spy.memcached.HashAlgorithm;\n\npublic class DefaultConfigFactory implements ConfigFactory {\n\n    @Override\n    public Config create(File filename) {\n        if (filename == null || \"\".equals(filename.getName())) {\n            throw new IllegalArgumentException(\"Filename is empty.\");\n        }\n        StringBuilder sb = new StringBuilder();\n        try {\n            FileInputStream fis = new FileInputStream(filename);\n            BufferedReader reader = new BufferedReader(new InputStreamReader(\n                    fis));\n            String str;\n            while ((str = reader.readLine()) != null) {\n                sb.append(str);\n            }\n        } catch (IOException e) {\n            throw new ConfigParsingException(\"Exception reading input file: \"\n                    + filename, e);\n        }\n        return create(sb.toString());\n    }\n\n    @Override\n    public Config create(String data) {\n        try {\n            JSONObject jsonObject = new JSONObject(data);\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + data, e);\n        }\n    }\n\n    @Override\n    public Config create(JSONObject jsonObject) {\n        try {\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + jsonObject, e);\n        }\n    }\n\n    private HashAlgorithm lookupHashAlgorithm(String algorithm) {\n        HashAlgorithm ha = HashAlgorithm.NATIVE_HASH;\n        if (\"crc\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.CRC32_HASH;\n        } else if (\"fnv1_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_32_HASH;\n        } else if (\"fnv1_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_64_HASH;\n        } else if (\"fnv1a_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_32_HASH;\n        } else if (\"fnv1a_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_64_HASH;\n        } else if (\"md5\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.KETAMA_HASH;\n        } else {\n            throw new IllegalArgumentException(\"Unhandled algorithm type: \"\n                    + algorithm);\n        }\n        return ha;\n    }\n\n    private Config parseJSON(JSONObject jsonObject) throws JSONException {\n        // Allows clients to have a JSON envelope.\n        if (jsonObject.has(\"vBucketServerMap\")) {\n            return parseJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n        }\n        HashAlgorithm hashAlgorithm = lookupHashAlgorithm(jsonObject\n                .getString(\"hashAlgorithm\"));\n        int replicasCount = jsonObject.getInt(\"numReplicas\");\n        if (replicasCount > VBucket.MAX_REPLICAS) {\n            throw new ConfigParsingException(\"Expected number <= \"\n                    + VBucket.MAX_REPLICAS + \" for replicas.\");\n        }\n        JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n        if (servers.length() <= 0) {\n            throw new ConfigParsingException(\"Empty servers list.\");\n        }\n        int serversCount = servers.length();\n        JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n        int vbucketsCount = vbuckets.length();\n        if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n            throw new ConfigParsingException(\n                    \"Number of buckets must be a power of two, > 0 and <= \"\n                            + VBucket.MAX_BUCKETS);\n        }\n\tList<String> populateServers = populateServers(servers);\n\tList<VBucket> populateVbuckets = populateVbuckets(vbuckets);\n        Config config = new DefaultConfig(hashAlgorithm, serversCount,\n                replicasCount, vbucketsCount, populateServers,\n                populateVbuckets);\n\n        return config;\n    }\n\n    private List<String> populateServers(JSONArray servers)\n            throws JSONException {\n        List<String> serverNames = new ArrayList<String>();\n        for (int i = 0; i < servers.length(); i++) {\n            String server = servers.getString(i);\n            serverNames.add(server);\n        }\n        return serverNames;\n    }\n\n    private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n            throws JSONException {\n        List<VBucket> vBuckets = new ArrayList<VBucket>();\n        for (int i = 0; i < jsonVbuckets.length(); i++) {\n            JSONArray rows = jsonVbuckets.getJSONArray(i);\n            int master = rows.getInt(0);\n            int replicas[] = new int[VBucket.MAX_REPLICAS];\n            for (int j = 1; j < rows.length(); j++) {\n                replicas[j-1] = rows.getInt(j);\n            }\n            vBuckets.add(new VBucket(master, replicas));\n        }\n        return vBuckets;\n    }\n\n}\n","lineNo":106}
{"Smelly Sample":"package net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\nimport net.spy.memcached.HashAlgorithm;\n\npublic class DefaultConfigFactory implements ConfigFactory {\n\n    @Override\n    public Config create(File filename) {\n        if (filename == null || \"\".equals(filename.getName())) {\n            throw new IllegalArgumentException(\"Filename is empty.\");\n        }\n        StringBuilder sb = new StringBuilder();\n        try {\n            FileInputStream fis = new FileInputStream(filename);\n            BufferedReader reader = new BufferedReader(new InputStreamReader(\n                    fis));\n            String str;\n            while ((str = reader.readLine()) != null) {\n                sb.append(str);\n            }\n        } catch (IOException e) {\n            throw new ConfigParsingException(\"Exception reading input file: \"\n                    + filename, e);\n        }\n        return create(sb.toString());\n    }\n\n    @Override\n    public Config create(String data) {\n        try {\n            JSONObject jsonObject = new JSONObject(data);\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + data, e);\n        }\n    }\n\n    @Override\n    public Config create(JSONObject jsonObject) {\n        try {\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + jsonObject, e);\n        }\n    }\n\n    private HashAlgorithm lookupHashAlgorithm(String algorithm) {\n        HashAlgorithm ha = HashAlgorithm.NATIVE_HASH;\n        if (\"crc\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.CRC32_HASH;\n        } else if (\"fnv1_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_32_HASH;\n        } else if (\"fnv1_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_64_HASH;\n        } else if (\"fnv1a_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_32_HASH;\n        } else if (\"fnv1a_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_64_HASH;\n        } else if (\"md5\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.KETAMA_HASH;\n        } else {\n            throw new IllegalArgumentException(\"Unhandled algorithm type: \"\n                    + algorithm);\n        }\n        return ha;\n    }\n\n    private Config parseJSON(JSONObject jsonObject) throws JSONException {\n        // Allows clients to have a JSON envelope.\n        if (jsonObject.has(\"vBucketServerMap\")) {\n            return parseJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n        }\n        HashAlgorithm hashAlgorithm = lookupHashAlgorithm(jsonObject\n                .getString(\"hashAlgorithm\"));\n        int replicasCount = jsonObject.getInt(\"numReplicas\");\n        if (replicasCount > VBucket.MAX_REPLICAS) {\n            throw new ConfigParsingException(\"Expected number <= \"\n                    + VBucket.MAX_REPLICAS + \" for replicas.\");\n        }\n        JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n        if (servers.length() <= 0) {\n            throw new ConfigParsingException(\"Empty servers list.\");\n        }\n        int serversCount = servers.length();\n        JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n        int vbucketsCount = vbuckets.length();\n        if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n            throw new ConfigParsingException(\n                    \"Number of buckets must be a power of two, > 0 and <= \"\n                            + VBucket.MAX_BUCKETS);\n        }\n\n        Config config = new DefaultConfig(hashAlgorithm, serversCount,\n                replicasCount, vbucketsCount, populateServers(servers),\n                populateVbuckets(servers));\n\n        return config;\n    }\n\n    private List<String> populateServers(JSONArray servers)\n            throws JSONException {\n        List<String> serverNames = new ArrayList<String>();\n        for (int i = 0; i < servers.length(); i++) {\n            String server = servers.getString(i);\n            serverNames.add(server);\n        }\n        return serverNames;\n    }\n\n    private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n            throws JSONException {\n        List<VBucket> vBuckets = new ArrayList<VBucket>();\n        for (int i = 0; i < jsonVbuckets.length(); i++) {\n            JSONArray rows = jsonVbuckets.getJSONArray(i);\n            int master = rows.getInt(0);\n            int replicas[] = new int[VBucket.MAX_REPLICAS];\n            for (int j = 1; j < rows.length(); j++) {\n                replicas[j-1] = rows.getInt(j);\n            }\n            vBuckets.add(new VBucket(master, replicas));\n        }\n        return vBuckets;\n    }\n\n}\n","Method after Refactoring":"package net.spy.memcached.vbucket.config;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.codehaus.jettison.json.JSONArray;\nimport org.codehaus.jettison.json.JSONException;\nimport org.codehaus.jettison.json.JSONObject;\n\nimport net.spy.memcached.HashAlgorithm;\n\npublic class DefaultConfigFactory implements ConfigFactory {\n\n    @Override\n    public Config create(File filename) {\n        if (filename == null || \"\".equals(filename.getName())) {\n            throw new IllegalArgumentException(\"Filename is empty.\");\n        }\n        StringBuilder sb = new StringBuilder();\n        try {\n            FileInputStream fis = new FileInputStream(filename);\n            BufferedReader reader = new BufferedReader(new InputStreamReader(\n                    fis));\n            String str;\n            while ((str = reader.readLine()) != null) {\n                sb.append(str);\n            }\n        } catch (IOException e) {\n            throw new ConfigParsingException(\"Exception reading input file: \"\n                    + filename, e);\n        }\n        return create(sb.toString());\n    }\n\n    @Override\n    public Config create(String data) {\n        try {\n            JSONObject jsonObject = new JSONObject(data);\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + data, e);\n        }\n    }\n\n    @Override\n    public Config create(JSONObject jsonObject) {\n        try {\n            return parseJSON(jsonObject);\n        } catch (JSONException e) {\n            throw new ConfigParsingException(\"Exception parsing JSON data: \"\n                    + jsonObject, e);\n        }\n    }\n\n    private HashAlgorithm lookupHashAlgorithm(String algorithm) {\n        HashAlgorithm ha = HashAlgorithm.NATIVE_HASH;\n        if (\"crc\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.CRC32_HASH;\n        } else if (\"fnv1_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_32_HASH;\n        } else if (\"fnv1_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1_64_HASH;\n        } else if (\"fnv1a_32\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_32_HASH;\n        } else if (\"fnv1a_64\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.FNV1A_64_HASH;\n        } else if (\"md5\".equalsIgnoreCase(algorithm)) {\n            ha = HashAlgorithm.KETAMA_HASH;\n        } else {\n            throw new IllegalArgumentException(\"Unhandled algorithm type: \"\n                    + algorithm);\n        }\n        return ha;\n    }\n\n    private Config parseJSON(JSONObject jsonObject) throws JSONException {\n        // Allows clients to have a JSON envelope.\n        if (jsonObject.has(\"vBucketServerMap\")) {\n            return parseJSON(jsonObject.getJSONObject(\"vBucketServerMap\"));\n        }\n        HashAlgorithm hashAlgorithm = lookupHashAlgorithm(jsonObject\n                .getString(\"hashAlgorithm\"));\n        int replicasCount = jsonObject.getInt(\"numReplicas\");\n        if (replicasCount > VBucket.MAX_REPLICAS) {\n            throw new ConfigParsingException(\"Expected number <= \"\n                    + VBucket.MAX_REPLICAS + \" for replicas.\");\n        }\n        JSONArray servers = jsonObject.getJSONArray(\"serverList\");\n        if (servers.length() <= 0) {\n            throw new ConfigParsingException(\"Empty servers list.\");\n        }\n        int serversCount = servers.length();\n        JSONArray vbuckets = jsonObject.getJSONArray(\"vBucketMap\");\n        int vbucketsCount = vbuckets.length();\n        if (vbucketsCount == 0 || (vbucketsCount & (vbucketsCount - 1)) != 0) {\n            throw new ConfigParsingException(\n                    \"Number of buckets must be a power of two, > 0 and <= \"\n                            + VBucket.MAX_BUCKETS);\n        }\n\tList<String> populateServers = populateServers(servers);\n\tList<VBucket> populateVbuckets = populateVbuckets(vbuckets);\n        Config config = new DefaultConfig(hashAlgorithm, serversCount,\n                replicasCount, vbucketsCount, populateServers,\n                populateVbuckets);\n\n        return config;\n    }\n\n    private List<String> populateServers(JSONArray servers)\n            throws JSONException {\n        List<String> serverNames = new ArrayList<String>();\n        for (int i = 0; i < servers.length(); i++) {\n            String server = servers.getString(i);\n            serverNames.add(server);\n        }\n        return serverNames;\n    }\n\n    private List<VBucket> populateVbuckets(JSONArray jsonVbuckets)\n            throws JSONException {\n        List<VBucket> vBuckets = new ArrayList<VBucket>();\n        for (int i = 0; i < jsonVbuckets.length(); i++) {\n            JSONArray rows = jsonVbuckets.getJSONArray(i);\n            int master = rows.getInt(0);\n            int replicas[] = new int[VBucket.MAX_REPLICAS];\n            for (int j = 1; j < rows.length(); j++) {\n                replicas[j-1] = rows.getInt(j);\n            }\n            vBuckets.add(new VBucket(master, replicas));\n        }\n        return vBuckets;\n    }\n\n}\n","lineNo":107}
{"Smelly Sample":"package net.spy.memcached.protocol.ascii;\n\nimport java.nio.ByteBuffer;\nimport java.util.Collection;\n\nimport net.spy.memcached.KeyUtil;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\n\n/**\n * Base class for get and gets handlers.\n */\nabstract class BaseGetOpImpl extends OperationImpl {\n\n\tprivate static final OperationStatus END = new OperationStatus(true, \"END\");\n\tprivate static final byte[] RN_BYTES = \"\\r\\n\".getBytes();\n\tprivate final String cmd;\n\tprivate final Collection<String> keys;\n\tprivate String currentKey = null;\n\tprivate long casValue=0;\n\tprivate int currentFlags = 0;\n\tprivate byte[] data = null;\n\tprivate int readOffset = 0;\n\tprivate byte lookingFor = '\\0';\n\n\tpublic BaseGetOpImpl(String c,\n\t\t\tOperationCallback cb, Collection<String> k) {\n\t\tsuper(cb);\n\t\tcmd=c;\n\t\tkeys=k;\n\t}\n\n\t/**\n\t * Get the keys this GetOperation is looking for.\n\t */\n\tpublic final Collection<String> getKeys() {\n\t\treturn keys;\n\t}\n\n\t@Override\n\tpublic final void handleLine(String line) {\n\t\tif(line.equals(\"END\")) {\n\t\t\tgetLogger().debug(\"Get complete!\");\n\t\t\tgetCallback().receivedStatus(END);\n\t\t\ttransitionState(OperationState.COMPLETE);\n\t\t\tdata=null;\n\t\t} else if(line.startsWith(\"VALUE \")) {\n\t\t\tgetLogger().debug(\"Got line %s\", line);\n\t\t\tString[] stuff=line.split(\" \");\n\t\t\tassert stuff[0].equals(\"VALUE\");\n\t\t\tcurrentKey=stuff[1];\n\t\t\tcurrentFlags=Integer.parseInt(stuff[2]);\n\t\t\tdata=new byte[Integer.parseInt(stuff[3])];\n\t\t\tif(stuff.length > 4) {\n\t\t\t\tcasValue=Long.parseLong(stuff[4]);\n\t\t\t}\n\t\t\treadOffset=0;\n\t\t\tgetLogger().debug(\"Set read type to data\");\n\t\t\tsetReadType(OperationReadType.DATA);\n\t\t} else {\n\t\t\tassert false : \"Unknown line type: \" + line;\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void handleRead(ByteBuffer b) {\n\t\tassert currentKey != null;\n\t\tassert data != null;\n\t\t// This will be the case, because we'll clear them when it's not.\n\t\tassert readOffset <= data.length\n\t\t\t: \"readOffset is \" + readOffset + \" data.length is \" + data.length;\n\n\t\tgetLogger().debug(\"readOffset: %d, length: %d\",\n\t\t\t\treadOffset, data.length);\n\t\t// If we're not looking for termination, we're still looking for data\n\t\tif(lookingFor == '\\0') {\n\t\t\tint toRead=data.length - readOffset;\n\t\t\tint available=b.remaining();\n\t\t\ttoRead=Math.min(toRead, available);\n\t\t\tgetLogger().debug(\"Reading %d bytes\", toRead);\n\t\t\tb.get(data, readOffset, toRead);\n\t\t\treadOffset+=toRead;\n\t\t}\n\t\t// Transition us into a ``looking for \\r\\n'' kind of state if we've\n\t\t// read enough and are still in a data state.\n\t\tif(readOffset == data.length && lookingFor == '\\0') {\n\t\t\t// The callback is most likely a get callback.  If it's not, then\n\t\t\t// it's a gets callback.\n\t\t\ttry {\n\t\t\t\tGetOperation.Callback gcb=(GetOperation.Callback)getCallback();\n\t\t\t\tgcb.gotData(currentKey, currentFlags, data);\n\t\t\t} catch(ClassCastException e) {\n\t\t\t\tGetsOperation.Callback gcb=(GetsOperation.Callback)\n\t\t\t\t\tgetCallback();\n\t\t\t\tgcb.gotData(currentKey, currentFlags, casValue, data);\n\t\t\t}\n\t\t\tlookingFor='\\r';\n\t\t}\n\t\t// If we're looking for an ending byte, let's go find it.\n\t\tif(lookingFor != '\\0' && b.hasRemaining()) {\n\t\t\tdo {\n\t\t\t\tbyte tmp=b.get();\n\t\t\t\tassert tmp == lookingFor : \"Expecting \" + lookingFor + \", got \"\n\t\t\t\t\t+ (char)tmp;\n\t\t\t\tswitch(lookingFor) {\n\t\t\t\t\tcase '\\r': lookingFor='\\n'; break;\n\t\t\t\t\tcase '\\n': lookingFor='\\0'; break;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tassert false: \"Looking for unexpected char: \"\n\t\t\t\t\t\t\t+ (char)lookingFor;\n\t\t\t\t}\n\t\t\t} while(lookingFor != '\\0' && b.hasRemaining());\n\t\t\t// Completed the read, reset stuff.\n\t\t\tif(lookingFor == '\\0') {\n\t\t\t\tcurrentKey=null;\n\t\t\t\tdata=null;\n\t\t\t\treadOffset=0;\n\t\t\t\tcurrentFlags=0;\n\t\t\t\tgetLogger().debug(\"Setting read type back to line.\");\n\t\t\t\tsetReadType(OperationReadType.LINE);\n\t\t\t}\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void initialize() {\n\t\t// Figure out the length of the request\n\t\tint size=6; // Enough for gets\\r\\n\n\t\tCollection<byte[]> keyBytes=KeyUtil.getKeyBytes(keys);\n\t\tfor(byte[] k : keyBytes) {\n\t\t\tsize+=k.length;\n\t\t\tsize++;\n\t\t}\n\t\tByteBuffer b=ByteBuffer.allocate(size);\n\t\tb.put(cmd.getBytes());\n\t\tfor(byte[] k : keyBytes) {\n\t\t\tb.put((byte)' ');\n\t\t\tb.put(k);\n\t\t}\n\t\tb.put(RN_BYTES);\n\t\tb.flip();\n\t\tsetBuffer(b);\n\t}\n\n\t@Override\n\tprotected final void wasCancelled() {\n\t\tgetCallback().receivedStatus(CANCELLED);\n\t}\n\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.ascii;\n\nimport java.nio.ByteBuffer;\nimport java.util.Collection;\nimport java.util.Collections;\n\nimport net.spy.memcached.KeyUtil;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetlOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\n\n/**\n * Base class for get and gets handlers.\n */\nabstract class BaseGetOpImpl extends OperationImpl {\n\n\tprivate static final OperationStatus END = new OperationStatus(true, \"END\");\n\tprivate static final byte[] RN_BYTES = \"\\r\\n\".getBytes();\n\tprivate final String cmd;\n\tprivate final Collection<String> keys;\n\tprivate String currentKey = null;\n\tprivate final int exp;\n\tprivate final boolean hasExp;\n\tprivate long casValue=0;\n\tprivate int currentFlags = 0;\n\tprivate byte[] data = null;\n\tprivate int readOffset = 0;\n\tprivate byte lookingFor = '\\0';\n\n\tpublic BaseGetOpImpl(String c,\n\t\t\tOperationCallback cb, Collection<String> k) {\n\t\tsuper(cb);\n\t\tcmd=c;\n\t\tkeys=k;\n\t\texp=0;\n\t\thasExp=false;\n\t}\n\n\tpublic BaseGetOpImpl(String c, int e, GetlOperation.Callback cb,\n\t\t\tString k) {\n\t\tsuper(cb);\n\t\tcmd=c;\n\t\tkeys=Collections.singleton(k);\n\t\texp=e;\n\t\thasExp=true;\n\t}\n\n\t/**\n\t * Get the keys this GetOperation is looking for.\n\t */\n\tpublic final Collection<String> getKeys() {\n\t\treturn keys;\n\t}\n\n\t@Override\n\tpublic final void handleLine(String line) {\n\t\tif(line.equals(\"END\")) {\n\t\t\tgetLogger().debug(\"Get complete!\");\n\t\t\tgetCallback().receivedStatus(END);\n\t\t\ttransitionState(OperationState.COMPLETE);\n\t\t\tdata=null;\n\t\t} else if(line.startsWith(\"VALUE \")) {\n\t\t\tgetLogger().debug(\"Got line %s\", line);\n\t\t\tString[] stuff=line.split(\" \");\n\t\t\tassert stuff[0].equals(\"VALUE\");\n\t\t\tcurrentKey=stuff[1];\n\t\t\tcurrentFlags=Integer.parseInt(stuff[2]);\n\t\t\tdata=new byte[Integer.parseInt(stuff[3])];\n\t\t\tif(stuff.length > 4) {\n\t\t\t\tcasValue=Long.parseLong(stuff[4]);\n\t\t\t}\n\t\t\treadOffset=0;\n\t\t\tgetLogger().debug(\"Set read type to data\");\n\t\t\tsetReadType(OperationReadType.DATA);\n\t\t} else {\n\t\t\tassert false : \"Unknown line type: \" + line;\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void handleRead(ByteBuffer b) {\n\t\tassert currentKey != null;\n\t\tassert data != null;\n\t\t// This will be the case, because we'll clear them when it's not.\n\t\tassert readOffset <= data.length\n\t\t\t: \"readOffset is \" + readOffset + \" data.length is \" + data.length;\n\n\t\tgetLogger().debug(\"readOffset: %d, length: %d\",\n\t\t\t\treadOffset, data.length);\n\t\t// If we're not looking for termination, we're still looking for data\n\t\tif(lookingFor == '\\0') {\n\t\t\tint toRead=data.length - readOffset;\n\t\t\tint available=b.remaining();\n\t\t\ttoRead=Math.min(toRead, available);\n\t\t\tgetLogger().debug(\"Reading %d bytes\", toRead);\n\t\t\tb.get(data, readOffset, toRead);\n\t\t\treadOffset+=toRead;\n\t\t}\n\t\t// Transition us into a ``looking for \\r\\n'' kind of state if we've\n\t\t// read enough and are still in a data state.\n\t\tif(readOffset == data.length && lookingFor == '\\0') {\n\t\t\t// The callback is most likely a get callback.  If it's not, then\n\t\t\t// it's a gets callback.\n\t\t\tOperationCallback cb = getCallback();\n\t\t\tif (cb instanceof GetOperation.Callback) {\n\t\t\t\tGetOperation.Callback gcb=(GetOperation.Callback)cb;\n\t\t\t\tgcb.gotData(currentKey, currentFlags, data);\n\t\t\t} else if (cb instanceof GetsOperation.Callback) {\n\t\t\t\tGetsOperation.Callback gcb=(GetsOperation.Callback)cb;\n\t\t\t\tgcb.gotData(currentKey, currentFlags, casValue, data);\n\t\t\t} else if (cb instanceof GetlOperation.Callback) {\n\t\t\t\tGetlOperation.Callback gcb=(GetlOperation.Callback)cb;\n\t\t\t\tgcb.gotData(currentKey, currentFlags, casValue, data);\n\t\t\t} else {\n\t\t\t\tthrow new ClassCastException(\"Couldn't convert \" + cb + \"to a relevent op\");\n\t\t\t}\n\t\t\tlookingFor='\\r';\n\t\t}\n\t\t// If we're looking for an ending byte, let's go find it.\n\t\tif(lookingFor != '\\0' && b.hasRemaining()) {\n\t\t\tdo {\n\t\t\t\tbyte tmp=b.get();\n\t\t\t\tassert tmp == lookingFor : \"Expecting \" + lookingFor + \", got \"\n\t\t\t\t\t+ (char)tmp;\n\t\t\t\tswitch(lookingFor) {\n\t\t\t\t\tcase '\\r': lookingFor='\\n'; break;\n\t\t\t\t\tcase '\\n': lookingFor='\\0'; break;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tassert false: \"Looking for unexpected char: \"\n\t\t\t\t\t\t\t+ (char)lookingFor;\n\t\t\t\t}\n\t\t\t} while(lookingFor != '\\0' && b.hasRemaining());\n\t\t\t// Completed the read, reset stuff.\n\t\t\tif(lookingFor == '\\0') {\n\t\t\t\tcurrentKey=null;\n\t\t\t\tdata=null;\n\t\t\t\treadOffset=0;\n\t\t\t\tcurrentFlags=0;\n\t\t\t\tgetLogger().debug(\"Setting read type back to line.\");\n\t\t\t\tsetReadType(OperationReadType.LINE);\n\t\t\t}\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void initialize() {\n\t\t// Figure out the length of the request\n\t\tint size=6; // Enough for gets\\r\\n\n\t\tCollection<byte[]> keyBytes=KeyUtil.getKeyBytes(keys);\n\t\tfor(byte[] k : keyBytes) {\n\t\t\tsize+=k.length;\n\t\t\tsize++;\n\t\t}\n\t\tbyte[] e = String.valueOf(exp).getBytes();\n\t\tif (hasExp) {\n\t\t\tsize+=e.length + 1;\n\t\t}\n\t\tByteBuffer b=ByteBuffer.allocate(size);\n\t\tb.put(cmd.getBytes());\n\t\tfor(byte[] k : keyBytes) {\n\t\t\tb.put((byte)' ');\n\t\t\tb.put(k);\n\t\t}\n\t\tif (hasExp) {\n\t\t\tb.put((byte)' ');\n\t\t\tb.put(e);\n\t\t}\n\t\tb.put(RN_BYTES);\n\t\tb.flip();\n\t\tsetBuffer(b);\n\t}\n\n\t@Override\n\tprotected final void wasCancelled() {\n\t\tgetCallback().receivedStatus(CANCELLED);\n\t}\n\n}\n","lineNo":107}
{"Smelly Sample":"package net.spy.memcached;\n\nimport java.util.Iterator;\n\nimport org.jmock.Mock;\nimport org.jmock.MockObjectTestCase;\n\npublic abstract class AbstractNodeLocationCase extends MockObjectTestCase {\n\n\tprotected MemcachedNode[] nodes;\n\tprotected Mock[] nodeMocks;\n\tprotected NodeLocator locator;\n\n\tprivate void runSequenceAssertion(NodeLocator l, String k, int... seq) {\n\t\tint pos=0;\n\t\tfor(Iterator<MemcachedNode> i=l.getSequence(k); i.hasNext(); ) {\n\t\t\tassertEquals(\"At position \" + pos, nodes[seq[pos]].toString(),\n\t\t\t\ti.next().toString());\n\t\t\ttry {\n\t\t\t\ti.remove();\n\t\t\t\tfail(\"Allowed a removal from a sequence.\");\n\t\t\t} catch(UnsupportedOperationException e) {\n\t\t\t\t// pass\n\t\t\t}\n\t\t\tpos++;\n\t\t}\n\t\tassertEquals(\"Incorrect sequence size for \" + k, seq.length, pos);\n\t}\n\n\tpublic final void testCloningGetPrimary() {\n\t\tsetupNodes(5);\n\t\tassertTrue(locator.getReadonlyCopy().getPrimary(\"hi\")\n\t\t\tinstanceof MemcachedNodeROImpl);\n\t}\n\n\tpublic final void testCloningGetAll() {\n\t\tsetupNodes(5);\n\t\tassertTrue(locator.getReadonlyCopy().getAll().iterator().next()\n\t\t\tinstanceof MemcachedNodeROImpl);\n\t}\n\n\tpublic final void testCloningGetSequence() {\n\t\tsetupNodes(5);\n\t\tassertTrue(locator.getReadonlyCopy().getSequence(\"hi\").next()\n\t\t\tinstanceof MemcachedNodeROImpl);\n\t}\n\n\tprotected final void assertSequence(String k, int... seq) {\n\t\trunSequenceAssertion(locator, k, seq);\n\t\trunSequenceAssertion(locator.getReadonlyCopy(), k, seq);\n\t}\n\n\tprotected void setupNodes(int n) {\n\t\tnodes=new MemcachedNode[n];\n\t\tnodeMocks=new Mock[nodes.length];\n\n\t\tfor(int i=0; i<nodeMocks.length; i++) {\n\t\t\tnodeMocks[i]=mock(MemcachedNode.class, \"node#\" + i);\n\t\t\tnodes[i]=(MemcachedNode)nodeMocks[i].proxy();\n\t\t}\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached;\n\nimport java.util.Iterator;\n\nimport org.jmock.Mock;\nimport org.jmock.MockObjectTestCase;\n\npublic abstract class AbstractNodeLocationCase extends MockObjectTestCase {\n\n\tprotected MemcachedNode[] nodes;\n\tprotected Mock[] nodeMocks;\n\tprotected NodeLocator locator;\n\n\tprivate void runSequenceAssertion(NodeLocator l, String k, int... seq) {\n\t\tint pos=0;\n\t\tfor(Iterator<MemcachedNode> i=l.getSequence(k); i.hasNext(); ) {\n\t\t\tString loc = i.next().toString();\n\t\t\ttry {\n\t\t\t\tassertEquals(\"At position \" + pos, nodes[seq[pos]].toString(),\n\t\t\t\t\tloc);\n\t\t\t\ti.remove();\n\t\t\t\tfail(\"Allowed a removal from a sequence.\");\n\t\t\t} catch(UnsupportedOperationException e) {\n\t\t\t\t// pass\n\t\t\t}\n\t\t\tcatch (ArrayIndexOutOfBoundsException ex) {\n\t\t\t    throw new RuntimeException(\"Tried to access nodes[\" + seq + \"[\" + pos + \"]] erroneously.\", ex);\n\t\t\t}\n\t\t\tpos++;\n\t\t}\n\t\tassertEquals(\"Incorrect sequence size for \" + k, seq.length, pos);\n\t}\n\n\tpublic final void testCloningGetPrimary() {\n\t\tsetupNodes(5);\n\t\tassertTrue(locator.getReadonlyCopy().getPrimary(\"hi\")\n\t\t\tinstanceof MemcachedNodeROImpl);\n\t}\n\n\tpublic final void testCloningGetAll() {\n\t\tsetupNodes(5);\n\t\tassertTrue(locator.getReadonlyCopy().getAll().iterator().next()\n\t\t\tinstanceof MemcachedNodeROImpl);\n\t}\n\n\tpublic final void testCloningGetSequence() {\n\t\tsetupNodes(5);\n\t\tassertTrue(locator.getReadonlyCopy().getSequence(\"hi\").next()\n\t\t\tinstanceof MemcachedNodeROImpl);\n\t}\n\n\tprotected final void assertSequence(String k, int... seq) {\n\t\trunSequenceAssertion(locator, k, seq);\n\t\trunSequenceAssertion(locator.getReadonlyCopy(), k, seq);\n\t}\n\n\tprotected void setupNodes(int n) {\n\t\tnodes=new MemcachedNode[n];\n\t\tnodeMocks=new Mock[nodes.length];\n\n\t\tfor(int i=0; i<nodeMocks.length; i++) {\n\t\t\tnodeMocks[i]=mock(MemcachedNode.class, \"node#\" + i);\n\t\t\tnodes[i]=(MemcachedNode)nodeMocks[i].proxy();\n\t\t}\n\t}\n}\n","lineNo":17}
{"Smelly Sample":"package net.spy.memcached;\n\nimport java.nio.ByteBuffer;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SyncThread;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.transcoders.SerializingTranscoder;\nimport net.spy.memcached.transcoders.Transcoder;\n\n\npublic abstract class ProtocolBaseCase extends ClientBaseCase {\n\n\tpublic void testAssertions() {\n\t\tboolean caught=false;\n\t\ttry {\n\t\t\tassert false;\n\t\t} catch(AssertionError e) {\n\t\t\tcaught=true;\n\t\t}\n\t\tassertTrue(\"Assertions are not enabled!\", caught);\n\t}\n\n\tpublic void testGetStats() throws Exception {\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats();\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"total_items\"));\n\t}\n\n\tpublic void testGetStatsSlabs() throws Exception {\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no slabs to check.\n\t\tclient.set(\"slabinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"slabs\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"1:chunk_size\"));\n\t}\n\n\tpublic void testGetStatsSizes() throws Exception {\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no sizes to check.\n\t\tclient.set(\"sizeinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"sizes\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertEquals(\"1\", oneStat.get(\"96\"));\n\t}\n\n\tpublic void testGetStatsCacheDump() throws Exception {\n\t\t// There needs to at least have been one value set or there\n\t\t// won't be anything to dump\n\t\tclient.set(\"dumpinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats =\n\t\t\t\tclient.getStats(\"cachedump 1 10000\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString val = oneStat.get(\"dumpinitializer\");\n\t\tassertTrue(val + \"doesn't match\", val.matches(\"\\\\[2 b; \\\\d+ s\\\\]\"));\n\t}\n\n\tpublic void testDelayedFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tclient.flush(2);\n\t\tThread.sleep(2100);\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testNoop() {\n\t\t// This runs through the startup/flush cycle\n\t}\n\n\tpublic void testDoubleShutdown() {\n\t\tclient.shutdown();\n\t\tclient.shutdown();\n\t}\n\n\tpublic void testSimpleGet() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testSimpleCASGets() throws Exception {\n\t\tassertNull(client.gets(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.gets(\"test1\").getValue());\n\t}\n\n\tpublic void testCAS() throws Exception {\n\t\tfinal String key=\"castestkey\";\n\t\t// First, make sure it doesn't work for a non-existing value.\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\tCASResponse.NOT_FOUND,\n\t\t\tclient.cas(key, 0x7fffffffffL, \"bad value\"));\n\n\t\t// OK, stick a value in here.\n\t\tassertTrue(client.add(key, 5, \"original value\").get());\n\t\tCASValue<?> getsVal = client.gets(key);\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// Now try it with an existing value, but wrong CAS id\n\t\tassertSame(\"Expected error CASing with invalid id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas() + 1, \"broken value\"));\n\t\t// Validate the original value is still in tact.\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// OK, now do a valid update\n\t\tassertSame(\"Expected successful CAS with correct id (\"\n\t\t\t+ getsVal.getCas() + \")\",\n\t\t\tCASResponse.OK,\n\t\t\tclient.cas(key, getsVal.getCas(), \"new value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\n\t\t// Test a CAS replay\n\t\tassertSame(\"Expected unsuccessful CAS with replayed id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas(), \"crap value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\t}\n\n\tpublic void testReallyLongCASId() throws Exception {\n\t\tString key = \"this-is-my-key\";\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\t\tCASResponse.NOT_FOUND,\n\t\t\t\tclient.cas(key, 9223372036854775807l, \"bad value\"));\n\t}\n\n\tpublic void testExtendedUTF8Key() throws Exception {\n\t\tString key=\"\\u2013\\u00ba\\u2013\\u220f\\u2014\\u00c4\";\n\t\tassertNull(client.get(key));\n\t\tclient.set(key, 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(key));\n\t}\n\n\tpublic void testInvalidKey1() throws Exception {\n\t\ttry {\n\t\t\tclient.get(\"key with spaces\");\n\t\t\tfail(\"Expected IllegalArgumentException getting key with spaces\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey2() throws Exception {\n\t\ttry {\n\t\t\tStringBuilder longKey=new StringBuilder();\n\t\t\tfor(int i=0; i<251; i++) {\n\t\t\t\tlongKey.append(\"a\");\n\t\t\t}\n\t\t\tclient.get(longKey.toString());\n\t\t\tfail(\"Expected IllegalArgumentException getting too long of a key\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey3() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\n\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey4() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\r\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey5() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\0\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBlank() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBulk() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.getBulk(\"Key key2\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testParallelSetGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tMap<String, Object> m=client.getBulk(\"test0\", \"test1\", \"test2\",\n\t\t\t\t\t\"test3\", \"test4\", \"test5\", \"test6\", \"test7\", \"test8\",\n\t\t\t\t\t\"test9\", \"test10\"); // Yes, I intentionally ran over.\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, m.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetAutoMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tclient.set(\"testparallel\", 5, \"parallelvalue\");\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"parallelvalue\", client.get(\"testparallel\"));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testAdd() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\").get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\").get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testAddWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\", t).get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\", t).get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t}\n\n\tpublic void testAddNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.add(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testSetNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.set(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testReplaceNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.replace(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testUpdate() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.replace(\"test1\", 5, \"test1value\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testUpdateWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tclient.replace(\"test1\", 5, \"test1value\", t);\n\t\tassertNull(client.get(\"test1\", t));\n\t}\n\n\t// Just to make sure the sequence is being handled correctly\n\tpublic void testMixedSetsAndUpdates() throws Exception {\n\t\tCollection<Future<Boolean>> futures=new ArrayList<Future<Boolean>>();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<100; i++) {\n\t\t\tString key=\"k\" + i;\n\t\t\tfutures.add(client.set(key, 10, key));\n\t\t\tfutures.add(client.add(key, 10, \"a\" + i));\n\t\t\tkeys.add(key);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(100, m.size());\n\t\tfor(Map.Entry<String, Object> me : m.entrySet()) {\n\t\t\tassertEquals(me.getKey(), me.getValue());\n\t\t}\n\t\tfor(Iterator<Future<Boolean>> i=futures.iterator();i.hasNext();) {\n\t\t\tassertTrue(i.next().get());\n\t\t\tassertFalse(i.next().get());\n\t\t}\n\t}\n\n\tpublic void testGetBulk() throws Exception {\n\t\tCollection<String> keys=Arrays.asList(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(0, client.getBulk(keys).size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(keys);\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVararg() throws Exception {\n\t\tassertEquals(0, client.getBulk(\"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tMap<String, String> vals=client.getBulk(t, \"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(t,\n\t\t\t\t\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get().get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkWithTranscoderIterator() throws Exception {\n\t\tArrayList<String> keys = new ArrayList<String>();\n\t\tkeys.add(\"test1\");\n\t\tkeys.add(\"test2\");\n\t\tkeys.add(\"test3\");\n\n\t\tArrayList<Transcoder<String>> tcs = new ArrayList<Transcoder<String>>(keys.size());\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\t// Any transcoders listed after list of keys should be\n\t\t// ignored.\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\tassertEquals(0, client.asyncGetBulk(keys, tcs.listIterator()).get().size());\n\n\t\tclient.set(keys.get(0), 5, \"val1\", tcs.get(0));\n\t\tclient.set(keys.get(1), 5, \"val2\", tcs.get(1));\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(keys, tcs.listIterator());\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(keys.get(0)));\n\t\tassertEquals(\"val2\", vals.get().get(keys.get(1)));\n\n\t\t// Set with one transcoder with the proper key and get\n\t\t// with another transcoder with the wrong key.\n\t\tkeys.add(0, \"test4\");\n\t\tTranscoder<String> encodeTranscoder = new TestWithKeyTranscoder(keys.get(0));\n\t\tclient.set(keys.get(0), 5, \"val4\", encodeTranscoder).get();\n\n\t\tTranscoder<String> decodeTranscoder = new TestWithKeyTranscoder(\"not \" + keys.get(0));\n\t\ttcs.add(0, decodeTranscoder);\n\t\ttry {\n\t\t\tclient.asyncGetBulk(keys, tcs.listIterator()).get();\n\t\t\tfail(\"Expected ExecutionException caused by key mismatch\");\n\t\t} catch (java.util.concurrent.ExecutionException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testAvailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(new ArrayList<String>(\n\t\t\t\tCollections.singleton(getExpectedVersionSource())),\n\t\t\tstringify(client.getAvailableServers()));\n\t}\n\n\tpublic void testUnavailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(Collections.emptyList(), client.getUnavailableServers());\n\t}\n\n\tprotected abstract String getExpectedVersionSource();\n\n\tpublic void testGetVersions() throws Exception {\n\t\tMap<SocketAddress, String> vs=client.getVersions();\n\t\tassertEquals(1, vs.size());\n\t\tMap.Entry<SocketAddress, String> me=vs.entrySet().iterator().next();\n\t\tassertEquals(getExpectedVersionSource(), me.getKey().toString());\n\t\tassertNotNull(me.getValue());\n\t}\n\n\tpublic void testNonexistentMutate() throws Exception {\n\t\tassertEquals(-1, client.incr(\"nonexistent\", 1));\n\t\tassertEquals(-1, client.decr(\"nonexistent\", 1));\n\t}\n\n\tpublic void testMutateWithDefault() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9));\n\t}\n\n\tpublic void testMutateWithDefaultAndExp() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9, 1));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9, 1));\n\t\tThread.sleep(2000);\n\t\tassertNull(client.get(\"mtest\"));\n\t}\n\n\tpublic void testAsyncIncrement() throws Exception {\n\t\tString k=\"async-incr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(6, (long)f.get());\n\t}\n\n\tpublic void testAsyncIncrementNonExistent() throws Exception {\n\t\tString k=\"async-incr-non-existent\";\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrement() throws Exception {\n\t\tString k=\"async-decr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(4, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrementNonExistent() throws Exception {\n\t\tString k=\"async-decr-non-existent\";\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testConcurrentMutation() throws Throwable {\n\t\tint num=SyncThread.getDistinctResultCount(10, new Callable<Long>(){\n\t\t\tpublic Long call() throws Exception {\n\t\t\t\treturn client.incr(\"mtest\", 1, 11);\n\t\t\t}});\n\t\tassertEquals(10, num);\n\t}\n\n\tpublic void testImmediateDelete() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tclient.delete(\"test1\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassertTrue(client.flush().get());\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testGracefulShutdown() throws Exception {\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\t// Get a new client\n\t\tinitClient();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tkeys.add(\"t\" + i);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(1000, m.size());\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tassertEquals(i, m.get(\"t\" + i));\n\t\t}\n\t}\n\n\tpublic void testSyncGetTimeouts() throws Exception {\n\t\tfinal String key=\"timeoutTestKey\";\n\t\tfinal String value=\"timeoutTestValue\";\n\t\t// Shutting down the default client to get one with a short timeout.\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\tinitClient(new DefaultConnectionFactory() {\n\t\t\t@Override\n\t\t\tpublic long getOperationTimeout() {\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t});\n\n\t\tclient.set(key, 0, value);\n\t\ttry {\n\t\t\tfor(int i=0; i<1000000; i++) {\n\t\t\t\tclient.get(key);\n\t\t\t}\n\t\t\tthrow new Exception(\"Didn't get a timeout.\");\n\t\t} catch(OperationTimeoutException e) {\n\t\t\tSystem.out.println(\"Got a timeout.\");\n\t\t}\n\t\tif(value.equals(client.asyncGet(key).get(1, TimeUnit.SECONDS))) {\n\t\t\tSystem.out.println(\"Got the right value.\");\n\t\t} else {\n\t\t\tthrow new Exception(\"Didn't get the expected value.\");\n\t\t}\n\t}\n\n\tpublic void xtestGracefulShutdownTooSlow() throws Exception {\n\t\tfor(int i=0; i<10000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertFalse(\"Weird, shut down too fast\",\n\t\t\tclient.shutdown(1, TimeUnit.MILLISECONDS));\n\n\t\ttry {\n\t\t\tMap<SocketAddress, String> m = client.getVersions();\n\t\t\tfail(\"Expected failure, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\tassertEquals(\"Shutting down\", e.getMessage());\n\t\t}\n\n\t\t// Get a new client\n\t\tinitClient();\n\t}\n\n\tpublic void testStupidlyLargeSetAndSizeOverride() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder(Integer.MAX_VALUE);\n\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[10*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(ExecutionException e) {\n\t\t\te.printStackTrace();\n\t\t\tOperationException oe=(OperationException)e.getCause();\n\t\t\tassertSame(OperationErrorType.SERVER, oe.getType());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testStupidlyLargeSet() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder();\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[10*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Cannot cache data larger than \"\n\t\t\t\t\t+ CachedData.MAX_SIZE + \" bytes \"\n\t\t\t\t\t+ \"(you tried to cache a \" + data.length + \" byte object)\",\n\t\t\t\te.getMessage());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testQueueAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tObject o=client.get(\"k\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + o);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testMultiReqAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tMap<String, ?> m=client.getBulk(\"k1\", \"k2\", \"k3\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testBroadcastAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tFuture<?> f=client.flush();\n\t\t\tfail(\"Expected IllegalStateException, got \" + f.get());\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testABunchOfCancelledOperations() throws Exception {\n\t\tfinal String k=\"bunchOCancel\";\n\t\tCollection<Future<?>> futures=new ArrayList<Future<?>>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tfutures.add(client.set(k, 5, \"xval\"));\n\t\t\tfutures.add(client.asyncGet(k));\n\t\t}\n\t\tFuture<Boolean> sf=client.set(k, 5, \"myxval\");\n\t\tFuture<Object> gf=client.asyncGet(k);\n\t\tfor(Future<?> f : futures) {\n\t\t\tf.cancel(true);\n\t\t}\n\t\tassertTrue(sf.get());\n\t\tassertEquals(\"myxval\", gf.get());\n\t}\n\n\tpublic void testUTF8Key() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testUTF8KeyDelete() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tassertTrue(client.delete(key).get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testUTF8MultiGet() throws Exception {\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<50; i++) {\n\t\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\"\n\t\t\t\t+ System.currentTimeMillis() + \".\" + i;\n\t\t\tassertTrue(client.set(key, 6000, value).get());\n\t\t\tkeys.add(key);\n\t\t}\n\n\t\tMap<String, Object> vals = client.getBulk(keys);\n\t\tassertEquals(keys.size(), vals.size());\n\t\tfor(Object o : vals.values()) {\n\t\t\tassertEquals(value, o);\n\t\t}\n\t\tassertTrue(keys.containsAll(vals.keySet()));\n\t}\n\n\tpublic void testUTF8Value() throws Exception {\n\t\tfinal String key = \"junit.plaintext.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ \"\n\t\t\t+ \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testAppend() throws Exception {\n\t\tfinal String key=\"append.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.append(0, key, \"es\").get());\n\t\tassertEquals(\"testes\", client.get(key));\n\t}\n\n\tpublic void testPrepend() throws Exception {\n\t\tfinal String key=\"prepend.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.prepend(0, key, \"es\").get());\n\t\tassertEquals(\"estest\", client.get(key));\n\t}\n\n\tpublic void testAppendNoSuchKey() throws Exception {\n\t\tfinal String key=\"append.missing\";\n\t\tassertFalse(client.append(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testPrependNoSuchKey() throws Exception {\n\t\tfinal String key=\"prepend.missing\";\n\t\tassertFalse(client.prepend(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tprivate static class TestTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885206;\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\t\t\treturn new String(d.getData());\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\treturn new CachedData(flags, o.getBytes(), getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate static class TestWithKeyTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885207;\n\n\t\tprivate final String key;\n\n\t\tTestWithKeyTranscoder(String k) {\n\t\t\tkey = k;\n\t\t}\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(d.getData());\n\n\t\t\tint keyLength = bb.getInt();\n\t\t\tbyte[] keyBytes = new byte[keyLength];\n\t\t\tbb.get(keyBytes);\n\t\t\tString k = new String(keyBytes);\n\n\t\t\tassertEquals(key, k);\n\n\t\t\tint valueLength = bb.getInt();\n\t\t\tbyte[] valueBytes = new byte[valueLength];\n\t\t\tbb.get(valueBytes);\n\n\t\t\treturn new String(valueBytes);\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\tbyte[] keyBytes = key.getBytes();\n\t\t\tbyte[] valueBytes = o.getBytes();\n\t\t\tint length = 4 + keyBytes.length + 4 + valueBytes.length;\n\t\t\tbyte[] bytes = new byte[length];\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(bytes);\n\t\t\tbb.putInt(keyBytes.length).put(keyBytes);\n\t\t\tbb.putInt(valueBytes.length).put(valueBytes);\n\n\t\t\treturn new CachedData(flags, bytes, getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached;\n\nimport java.nio.ByteBuffer;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\n\nimport net.spy.memcached.compat.SyncThread;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.transcoders.SerializingTranscoder;\nimport net.spy.memcached.transcoders.Transcoder;\n\n\npublic abstract class ProtocolBaseCase extends ClientBaseCase {\n\n\tpublic void testAssertions() {\n\t\tboolean caught=false;\n\t\ttry {\n\t\t\tassert false;\n\t\t} catch(AssertionError e) {\n\t\t\tcaught=true;\n\t\t}\n\t\tassertTrue(\"Assertions are not enabled!\", caught);\n\t}\n\n\tpublic void testGetStats() throws Exception {\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats();\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"total_items\"));\n\t}\n\n\tpublic void testGetStatsSlabs() throws Exception {\n\t\t// There needs to at least have been one value set or there may be\n\t\t// no slabs to check.\n\t\tclient.set(\"slabinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"slabs\");\n\t\tSystem.out.println(\"Stats:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tassertTrue(oneStat.containsKey(\"1:chunk_size\"));\n\t}\n\n\tpublic void testGetStatsSizes() throws Exception {\n\t\t// There needs to at least have been one value set or there may\n\t\t// be no sizes to check.  Note the protocol says\n\t\t// flushed/expired items may come back in stats sizes and we\n\t\t// use flush when testing, so we check that there's at least\n\t\t// one.\n\t\tclient.set(\"sizeinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats = client.getStats(\"sizes\");\n\t\tSystem.out.println(\"Stats sizes:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString noItemsSmall = oneStat.get(\"96\");\n\t\tassertTrue(Integer.parseInt(noItemsSmall) >= 1);\n\t}\n\n\tpublic void testGetStatsCacheDump() throws Exception {\n\t\t// There needs to at least have been one value set or there\n\t\t// won't be anything to dump\n\t\tclient.set(\"dumpinitializer\", 0, \"hi\");\n\t\tMap<SocketAddress, Map<String, String>> stats =\n\t\t\t\tclient.getStats(\"cachedump 1 10000\");\n\t\tSystem.out.println(\"Stats cachedump:  \" + stats);\n\t\tassertEquals(1, stats.size());\n\t\tMap<String, String> oneStat=stats.values().iterator().next();\n\t\tString val = oneStat.get(\"dumpinitializer\");\n\t\tassertTrue(val + \"doesn't match\", val.matches(\"\\\\[2 b; \\\\d+ s\\\\]\"));\n\t}\n\n\tpublic void testDelayedFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tclient.flush(2);\n\t\tThread.sleep(2100);\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testNoop() {\n\t\t// This runs through the startup/flush cycle\n\t}\n\n\tpublic void testDoubleShutdown() {\n\t\tclient.shutdown();\n\t\tclient.shutdown();\n\t}\n\n\tpublic void testSimpleGet() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testSimpleCASGets() throws Exception {\n\t\tassertNull(client.gets(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.gets(\"test1\").getValue());\n\t}\n\n\tpublic void testCAS() throws Exception {\n\t\tfinal String key=\"castestkey\";\n\t\t// First, make sure it doesn't work for a non-existing value.\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\tCASResponse.NOT_FOUND,\n\t\t\tclient.cas(key, 0x7fffffffffL, \"bad value\"));\n\n\t\t// OK, stick a value in here.\n\t\tassertTrue(client.add(key, 5, \"original value\").get());\n\t\tCASValue<?> getsVal = client.gets(key);\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// Now try it with an existing value, but wrong CAS id\n\t\tassertSame(\"Expected error CASing with invalid id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas() + 1, \"broken value\"));\n\t\t// Validate the original value is still in tact.\n\t\tassertEquals(\"original value\", getsVal.getValue());\n\n\t\t// OK, now do a valid update\n\t\tassertSame(\"Expected successful CAS with correct id (\"\n\t\t\t+ getsVal.getCas() + \")\",\n\t\t\tCASResponse.OK,\n\t\t\tclient.cas(key, getsVal.getCas(), \"new value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\n\t\t// Test a CAS replay\n\t\tassertSame(\"Expected unsuccessful CAS with replayed id\",\n\t\t\tCASResponse.EXISTS,\n\t\t\tclient.cas(key, getsVal.getCas(), \"crap value\"));\n\t\tassertEquals(\"new value\", client.get(key));\n\t}\n\n\tpublic void testReallyLongCASId() throws Exception {\n\t\tString key = \"this-is-my-key\";\n\t\tassertSame(\"Expected error CASing with no existing value.\",\n\t\t\t\tCASResponse.NOT_FOUND,\n\t\t\t\tclient.cas(key, 9223372036854775807l, \"bad value\"));\n\t}\n\n\tpublic void testExtendedUTF8Key() throws Exception {\n\t\tString key=\"\\u2013\\u00ba\\u2013\\u220f\\u2014\\u00c4\";\n\t\tassertNull(client.get(key));\n\t\tclient.set(key, 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(key));\n\t}\n\n\tpublic void testInvalidKey1() throws Exception {\n\t\ttry {\n\t\t\tclient.get(\"key with spaces\");\n\t\t\tfail(\"Expected IllegalArgumentException getting key with spaces\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey2() throws Exception {\n\t\ttry {\n\t\t\tStringBuilder longKey=new StringBuilder();\n\t\t\tfor(int i=0; i<251; i++) {\n\t\t\t\tlongKey.append(\"a\");\n\t\t\t}\n\t\t\tclient.get(longKey.toString());\n\t\t\tfail(\"Expected IllegalArgumentException getting too long of a key\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey3() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\n\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey4() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\r\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKey5() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"Key\\0\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBlank() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.get(\"\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testInvalidKeyBulk() throws Exception {\n\t\ttry {\n\t\t\tObject val=client.getBulk(\"Key key2\");\n\t\t\tfail(\"Expected IllegalArgumentException, got \" + val);\n\t\t} catch(IllegalArgumentException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testParallelSetGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tclient.set(\"test\" + i, 5, \"value\" + i);\n\t\t\t\t\tassertEquals(\"value\" + i, client.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\tMap<String, Object> m=client.getBulk(\"test0\", \"test1\", \"test2\",\n\t\t\t\t\t\"test3\", \"test4\", \"test5\", \"test6\", \"test7\", \"test8\",\n\t\t\t\t\t\"test9\", \"test10\"); // Yes, I intentionally ran over.\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"value\" + i, m.get(\"test\" + i));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testParallelSetAutoMultiGet() throws Throwable {\n\t\tint cnt=SyncThread.getDistinctResultCount(10, new Callable<Boolean>(){\n\t\t\tpublic Boolean call() throws Exception {\n\t\t\t\tclient.set(\"testparallel\", 5, \"parallelvalue\");\n\t\t\t\tfor(int i=0; i<10; i++) {\n\t\t\t\t\tassertEquals(\"parallelvalue\", client.get(\"testparallel\"));\n\t\t\t\t}\n\t\t\t\treturn Boolean.TRUE;\n\t\t\t}});\n\t\tassertEquals(1, cnt);\n\t}\n\n\tpublic void testAdd() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\").get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\").get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t}\n\n\tpublic void testAddWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tassertTrue(client.set(\"test1\", 5, \"test1value\", t).get());\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t\tassertFalse(client.add(\"test1\", 5, \"ignoredvalue\", t).get());\n\t\t// Should return the original value\n\t\tassertEquals(\"test1value\", client.get(\"test1\", t));\n\t}\n\n\tpublic void testAddNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.add(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testSetNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.set(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testReplaceNotSerializable() throws Exception {\n\t\ttry {\n\t\t\tclient.replace(\"t1\", 5, new Object());\n\t\t\tfail(\"expected illegal argument exception\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Non-serializable object\", e.getMessage());\n\t\t}\n\t}\n\n\tpublic void testUpdate() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.replace(\"test1\", 5, \"test1value\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testUpdateWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertNull(client.get(\"test1\", t));\n\t\tclient.replace(\"test1\", 5, \"test1value\", t);\n\t\tassertNull(client.get(\"test1\", t));\n\t}\n\n\t// Just to make sure the sequence is being handled correctly\n\tpublic void testMixedSetsAndUpdates() throws Exception {\n\t\tCollection<Future<Boolean>> futures=new ArrayList<Future<Boolean>>();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<100; i++) {\n\t\t\tString key=\"k\" + i;\n\t\t\tfutures.add(client.set(key, 10, key));\n\t\t\tfutures.add(client.add(key, 10, \"a\" + i));\n\t\t\tkeys.add(key);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(100, m.size());\n\t\tfor(Map.Entry<String, Object> me : m.entrySet()) {\n\t\t\tassertEquals(me.getKey(), me.getValue());\n\t\t}\n\t\tfor(Iterator<Future<Boolean>> i=futures.iterator();i.hasNext();) {\n\t\t\tassertTrue(i.next().get());\n\t\t\tassertFalse(i.next().get());\n\t\t}\n\t}\n\n\tpublic void testGetBulk() throws Exception {\n\t\tCollection<String> keys=Arrays.asList(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(0, client.getBulk(keys).size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(keys);\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVararg() throws Exception {\n\t\tassertEquals(0, client.getBulk(\"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\");\n\t\tclient.set(\"test2\", 5, \"val2\");\n\t\tMap<String, Object> vals=client.getBulk(\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tMap<String, String> vals=client.getBulk(t, \"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.size());\n\t\tassertEquals(\"val1\", vals.get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkVarargWithTranscoder() throws Exception {\n\t\tTranscoder<String> t=new TestTranscoder();\n\t\tassertEquals(0, client.getBulk(t, \"test1\", \"test2\", \"test3\").size());\n\t\tclient.set(\"test1\", 5, \"val1\", t);\n\t\tclient.set(\"test2\", 5, \"val2\", t);\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(t,\n\t\t\t\t\"test1\", \"test2\", \"test3\");\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(\"test1\"));\n\t\tassertEquals(\"val2\", vals.get().get(\"test2\"));\n\t}\n\n\tpublic void testAsyncGetBulkWithTranscoderIterator() throws Exception {\n\t\tArrayList<String> keys = new ArrayList<String>();\n\t\tkeys.add(\"test1\");\n\t\tkeys.add(\"test2\");\n\t\tkeys.add(\"test3\");\n\n\t\tArrayList<Transcoder<String>> tcs = new ArrayList<Transcoder<String>>(keys.size());\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\t// Any transcoders listed after list of keys should be\n\t\t// ignored.\n\t\tfor (String key : keys) {\n\t\t\ttcs.add(new TestWithKeyTranscoder(key));\n\t\t}\n\n\t\tassertEquals(0, client.asyncGetBulk(keys, tcs.listIterator()).get().size());\n\n\t\tclient.set(keys.get(0), 5, \"val1\", tcs.get(0));\n\t\tclient.set(keys.get(1), 5, \"val2\", tcs.get(1));\n\t\tFuture<Map<String, String>> vals=client.asyncGetBulk(keys, tcs.listIterator());\n\t\tassertEquals(2, vals.get().size());\n\t\tassertEquals(\"val1\", vals.get().get(keys.get(0)));\n\t\tassertEquals(\"val2\", vals.get().get(keys.get(1)));\n\n\t\t// Set with one transcoder with the proper key and get\n\t\t// with another transcoder with the wrong key.\n\t\tkeys.add(0, \"test4\");\n\t\tTranscoder<String> encodeTranscoder = new TestWithKeyTranscoder(keys.get(0));\n\t\tclient.set(keys.get(0), 5, \"val4\", encodeTranscoder).get();\n\n\t\tTranscoder<String> decodeTranscoder = new TestWithKeyTranscoder(\"not \" + keys.get(0));\n\t\ttcs.add(0, decodeTranscoder);\n\t\ttry {\n\t\t\tclient.asyncGetBulk(keys, tcs.listIterator()).get();\n\t\t\tfail(\"Expected ExecutionException caused by key mismatch\");\n\t\t} catch (java.util.concurrent.ExecutionException e) {\n\t\t\t// pass\n\t\t}\n\t}\n\n\tpublic void testAvailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(new ArrayList<String>(\n\t\t\t\tCollections.singleton(getExpectedVersionSource())),\n\t\t\tstringify(client.getAvailableServers()));\n\t}\n\n\tpublic void testUnavailableServers() {\n\t\tclient.getVersions();\n\t\tassertEquals(Collections.emptyList(), client.getUnavailableServers());\n\t}\n\n\tprotected abstract String getExpectedVersionSource();\n\n\tpublic void testGetVersions() throws Exception {\n\t\tMap<SocketAddress, String> vs=client.getVersions();\n\t\tassertEquals(1, vs.size());\n\t\tMap.Entry<SocketAddress, String> me=vs.entrySet().iterator().next();\n\t\tassertEquals(getExpectedVersionSource(), me.getKey().toString());\n\t\tassertNotNull(me.getValue());\n\t}\n\n\tpublic void testNonexistentMutate() throws Exception {\n\t\tassertEquals(-1, client.incr(\"nonexistent\", 1));\n\t\tassertEquals(-1, client.decr(\"nonexistent\", 1));\n\t}\n\n\tpublic void testMutateWithDefault() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9));\n\t}\n\n\tpublic void testMutateWithDefaultAndExp() throws Exception {\n\t\tassertEquals(3, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(4, client.incr(\"mtest\", 1, 3, 1));\n\t\tassertEquals(3, client.decr(\"mtest\", 1, 9, 1));\n\t\tassertEquals(9, client.decr(\"mtest2\", 1, 9, 1));\n\t\tThread.sleep(2000);\n\t\tassertNull(client.get(\"mtest\"));\n\t}\n\n\tpublic void testAsyncIncrement() throws Exception {\n\t\tString k=\"async-incr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(6, (long)f.get());\n\t}\n\n\tpublic void testAsyncIncrementNonExistent() throws Exception {\n\t\tString k=\"async-incr-non-existent\";\n\t\tFuture<Long> f = client.asyncIncr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrement() throws Exception {\n\t\tString k=\"async-decr\";\n\t\tclient.set(k, 0, \"5\");\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(4, (long)f.get());\n\t}\n\n\tpublic void testAsyncDecrementNonExistent() throws Exception {\n\t\tString k=\"async-decr-non-existent\";\n\t\tFuture<Long> f = client.asyncDecr(k, 1);\n\t\tassertEquals(-1, (long)f.get());\n\t}\n\n\tpublic void testConcurrentMutation() throws Throwable {\n\t\tint num=SyncThread.getDistinctResultCount(10, new Callable<Long>(){\n\t\t\tpublic Long call() throws Exception {\n\t\t\t\treturn client.incr(\"mtest\", 1, 11);\n\t\t\t}});\n\t\tassertEquals(10, num);\n\t}\n\n\tpublic void testImmediateDelete() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tclient.delete(\"test1\");\n\t\tassertNull(client.get(\"test1\"));\n\t}\n\n\tpublic void testFlush() throws Exception {\n\t\tassertNull(client.get(\"test1\"));\n\t\tclient.set(\"test1\", 5, \"test1value\");\n\t\tclient.set(\"test2\", 5, \"test2value\");\n\t\tassertEquals(\"test1value\", client.get(\"test1\"));\n\t\tassertEquals(\"test2value\", client.get(\"test2\"));\n\t\tassertTrue(client.flush().get());\n\t\tassertNull(client.get(\"test1\"));\n\t\tassertNull(client.get(\"test2\"));\n\t}\n\n\tpublic void testGracefulShutdown() throws Exception {\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\t// Get a new client\n\t\tinitClient();\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tkeys.add(\"t\" + i);\n\t\t}\n\t\tMap<String, Object> m=client.getBulk(keys);\n\t\tassertEquals(1000, m.size());\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tassertEquals(i, m.get(\"t\" + i));\n\t\t}\n\t}\n\n\tpublic void testSyncGetTimeouts() throws Exception {\n\t\tfinal String key=\"timeoutTestKey\";\n\t\tfinal String value=\"timeoutTestValue\";\n\t\t// Shutting down the default client to get one with a short timeout.\n\t\tassertTrue(\"Couldn't shut down within five seconds\",\n\t\t\tclient.shutdown(5, TimeUnit.SECONDS));\n\n\t\tinitClient(new DefaultConnectionFactory() {\n\t\t\t@Override\n\t\t\tpublic long getOperationTimeout() {\n\t\t\t\treturn 20;\n\t\t\t}\n\t\t});\n\n\t\tclient.set(key, 0, value);\n\t\ttry {\n\t\t\tfor(int i=0; i<1000000; i++) {\n\t\t\t\tclient.get(key);\n\t\t\t}\n\t\t\tthrow new Exception(\"Didn't get a timeout.\");\n\t\t} catch(OperationTimeoutException e) {\n\t\t\tSystem.out.println(\"Got a timeout.\");\n\t\t}\n\t\tif(value.equals(client.asyncGet(key).get(1, TimeUnit.SECONDS))) {\n\t\t\tSystem.out.println(\"Got the right value.\");\n\t\t} else {\n\t\t\tthrow new Exception(\"Didn't get the expected value.\");\n\t\t}\n\t}\n\n\tpublic void xtestGracefulShutdownTooSlow() throws Exception {\n\t\tfor(int i=0; i<10000; i++) {\n\t\t\tclient.set(\"t\" + i, 10, i);\n\t\t}\n\t\tassertFalse(\"Weird, shut down too fast\",\n\t\t\tclient.shutdown(1, TimeUnit.MILLISECONDS));\n\n\t\ttry {\n\t\t\tMap<SocketAddress, String> m = client.getVersions();\n\t\t\tfail(\"Expected failure, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\tassertEquals(\"Shutting down\", e.getMessage());\n\t\t}\n\n\t\t// Get a new client\n\t\tinitClient();\n\t}\n\n\tpublic void testStupidlyLargeSetAndSizeOverride() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder(Integer.MAX_VALUE);\n\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[10*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(ExecutionException e) {\n\t\t\te.printStackTrace();\n\t\t\tOperationException oe=(OperationException)e.getCause();\n\t\t\tassertSame(OperationErrorType.SERVER, oe.getType());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testStupidlyLargeSet() throws Exception {\n\t\tRandom r=new Random();\n\t\tSerializingTranscoder st=new SerializingTranscoder();\n\t\tst.setCompressionThreshold(Integer.MAX_VALUE);\n\n\t\tbyte data[]=new byte[10*1024*1024];\n\t\tr.nextBytes(data);\n\n\t\ttry {\n\t\t\tclient.set(\"bigassthing\", 60, data, st).get();\n\t\t\tfail(\"Didn't fail setting bigass thing.\");\n\t\t} catch(IllegalArgumentException e) {\n\t\t\tassertEquals(\"Cannot cache data larger than \"\n\t\t\t\t\t+ CachedData.MAX_SIZE + \" bytes \"\n\t\t\t\t\t+ \"(you tried to cache a \" + data.length + \" byte object)\",\n\t\t\t\te.getMessage());\n\t\t}\n\n\t\t// But I should still be able to do something.\n\t\tclient.set(\"k\", 5, \"Blah\");\n\t\tassertEquals(\"Blah\", client.get(\"k\"));\n\t}\n\n\tpublic void testQueueAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tObject o=client.get(\"k\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + o);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testMultiReqAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tMap<String, ?> m=client.getBulk(\"k1\", \"k2\", \"k3\");\n\t\t\tfail(\"Expected IllegalStateException, got \" + m);\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testBroadcastAfterShutdown() throws Exception {\n\t\tclient.shutdown();\n\t\ttry {\n\t\t\tFuture<?> f=client.flush();\n\t\t\tfail(\"Expected IllegalStateException, got \" + f.get());\n\t\t} catch(IllegalStateException e) {\n\t\t\t// OK\n\t\t} finally {\n\t\t\tinitClient(); // init for tearDown\n\t\t}\n\t}\n\n\tpublic void testABunchOfCancelledOperations() throws Exception {\n\t\tfinal String k=\"bunchOCancel\";\n\t\tCollection<Future<?>> futures=new ArrayList<Future<?>>();\n\t\tfor(int i=0; i<1000; i++) {\n\t\t\tfutures.add(client.set(k, 5, \"xval\"));\n\t\t\tfutures.add(client.asyncGet(k));\n\t\t}\n\t\tFuture<Boolean> sf=client.set(k, 5, \"myxval\");\n\t\tFuture<Object> gf=client.asyncGet(k);\n\t\tfor(Future<?> f : futures) {\n\t\t\tf.cancel(true);\n\t\t}\n\t\tassertTrue(sf.get());\n\t\tassertEquals(\"myxval\", gf.get());\n\t}\n\n\tpublic void testUTF8Key() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testUTF8KeyDelete() throws Exception {\n\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tassertTrue(client.delete(key).get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testUTF8MultiGet() throws Exception {\n\t\tfinal String value = \"Skiing rocks if you can find the time to go!\";\n\t\tCollection<String> keys=new ArrayList<String>();\n\t\tfor(int i=0; i<50; i++) {\n\t\t\tfinal String key = \"junit.Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ.\"\n\t\t\t\t+ System.currentTimeMillis() + \".\" + i;\n\t\t\tassertTrue(client.set(key, 6000, value).get());\n\t\t\tkeys.add(key);\n\t\t}\n\n\t\tMap<String, Object> vals = client.getBulk(keys);\n\t\tassertEquals(keys.size(), vals.size());\n\t\tfor(Object o : vals.values()) {\n\t\t\tassertEquals(value, o);\n\t\t}\n\t\tassertTrue(keys.containsAll(vals.keySet()));\n\t}\n\n\tpublic void testUTF8Value() throws Exception {\n\t\tfinal String key = \"junit.plaintext.\" + System.currentTimeMillis();\n\t\tfinal String value = \"Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ Ð\u2014Ð´Ñ\u20acÐ°Ð²Ñ?Ñ\u201aÐ²ÑÐ¹Ñ\u201aÐµ \"\n\t\t\t+ \"Skiing rocks if you can find the time to go!\";\n\n\t\tassertTrue(client.set(key, 6000, value).get());\n\t\tObject output = client.get(key);\n\t\tassertNotNull(\"output is null\", output);\n\t\tassertEquals(\"output is not equal\", value, output);\n\t}\n\n\tpublic void testAppend() throws Exception {\n\t\tfinal String key=\"append.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.append(0, key, \"es\").get());\n\t\tassertEquals(\"testes\", client.get(key));\n\t}\n\n\tpublic void testPrepend() throws Exception {\n\t\tfinal String key=\"prepend.key\";\n\t\tassertTrue(client.set(key, 5, \"test\").get());\n\t\tassertTrue(client.prepend(0, key, \"es\").get());\n\t\tassertEquals(\"estest\", client.get(key));\n\t}\n\n\tpublic void testAppendNoSuchKey() throws Exception {\n\t\tfinal String key=\"append.missing\";\n\t\tassertFalse(client.append(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tpublic void testPrependNoSuchKey() throws Exception {\n\t\tfinal String key=\"prepend.missing\";\n\t\tassertFalse(client.prepend(0, key, \"es\").get());\n\t\tassertNull(client.get(key));\n\t}\n\n\tprivate static class TestTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885206;\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\t\t\treturn new String(d.getData());\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\treturn new CachedData(flags, o.getBytes(), getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tprivate static class TestWithKeyTranscoder implements Transcoder<String> {\n\t\tprivate static final int flags=238885207;\n\n\t\tprivate final String key;\n\n\t\tTestWithKeyTranscoder(String k) {\n\t\t\tkey = k;\n\t\t}\n\n\t\tpublic String decode(CachedData d) {\n\t\t\tassert d.getFlags() == flags\n\t\t\t\t: \"expected \" + flags + \" got \" + d.getFlags();\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(d.getData());\n\n\t\t\tint keyLength = bb.getInt();\n\t\t\tbyte[] keyBytes = new byte[keyLength];\n\t\t\tbb.get(keyBytes);\n\t\t\tString k = new String(keyBytes);\n\n\t\t\tassertEquals(key, k);\n\n\t\t\tint valueLength = bb.getInt();\n\t\t\tbyte[] valueBytes = new byte[valueLength];\n\t\t\tbb.get(valueBytes);\n\n\t\t\treturn new String(valueBytes);\n\t\t}\n\n\t\tpublic CachedData encode(String o) {\n\t\t\tbyte[] keyBytes = key.getBytes();\n\t\t\tbyte[] valueBytes = o.getBytes();\n\t\t\tint length = 4 + keyBytes.length + 4 + valueBytes.length;\n\t\t\tbyte[] bytes = new byte[length];\n\n\t\t\tByteBuffer bb = ByteBuffer.wrap(bytes);\n\t\t\tbb.putInt(keyBytes.length).put(keyBytes);\n\t\t\tbb.putInt(valueBytes.length).put(valueBytes);\n\n\t\t\treturn new CachedData(flags, bytes, getMaxSize());\n\t\t}\n\n\t\tpublic int getMaxSize() {\n\t\t\treturn CachedData.MAX_SIZE;\n\t\t}\n\n\t\tpublic boolean asyncDecode(CachedData d) {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n","lineNo":66}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport javax.security.auth.callback.CallbackHandler;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tnew InetSocketAddress(\"hostname\", portNum));\n *\n *\t// Store a value (async) for one hour\n *\tc.set(\"someKey\", 3600, someObject);\n *\t// Retrieve a value.\n *\tObject myObject=c.get(\"someKey\");\n *\t<\/pre>\n *\n *\t<h2>Advanced Usage<\/h2>\n *\n *\t<p>\n *\t MemcachedClient may be processing a great deal of asynchronous messages or\n *\t possibly dealing with an unreachable memcached, which may delay processing.\n *\t If a memcached is disabled, for example, MemcachedConnection will continue\n *\t to attempt to reconnect and replay pending operations until it comes back\n *\t up.  To prevent this from causing your application to hang, you can use\n *\t one of the asynchronous mechanisms to time out a request and cancel the\n *\t operation to the server.\n *\t<\/p>\n *\n *\t<pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n */\npublic class MemcachedClient extends SpyThread implements MemcachedClientIF {\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final long operationTimeout;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tfinal Transcoder<Object> transcoder;\n\n\tfinal TranscodeService tcService;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param cf the connection factory to configure connections for this client\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tif(cf == null) {\n\t\t\tthrow new NullPointerException(\"Connection factory required\");\n\t\t}\n\t\tif(addrs == null) {\n\t\t\tthrow new NullPointerException(\"Server list required\");\n\t\t}\n\t\tif(addrs.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"You must have at least one server to connect to\");\n\t\t}\n\t\tif(cf.getOperationTimeout() <= 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Operation timeout must be positive.\");\n\t\t}\n\t\ttcService = new TranscodeService();\n\t\ttranscoder=cf.getDefaultTranscoder();\n\t\topFact=cf.getOperationFactory();\n\t\tassert opFact != null : \"Connection factory failed to make op factory\";\n\t\tconn=cf.createConnection(addrs);\n\t\tassert conn != null : \"Connection factory failed to make a connection\";\n\t\toperationTimeout = cf.getOperationTimeout();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tsetDaemon(cf.isDaemon());\n\t\tstart();\n\t}\n\n\t/**\n\t * Get the addresses of available servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getAvailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the addresses of unavailable servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getUnavailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(!node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get a read-only wrapper around the node locator wrapping this instance.\n\t */\n\tpublic NodeLocator getNodeLocator() {\n\t\treturn conn.getLocator().getReadonlyCopy();\n\t}\n\n\t/**\n\t * Get the default transcoder that's in use.\n\t */\n\tpublic Transcoder<Object> getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tbyte[] keyBytes=KeyUtil.getKeyBytes(key);\n\t\tif(keyBytes.length > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\tif(keyBytes.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Key must contain at least one character.\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(byte b : keyBytes) {\n\t\t\tif(b == ' ' || b == '\\n' || b == '\\r' || b == 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void checkState() {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tvalidateKey(key);\n\t\tcheckState();\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, conn.getLocator().getAll(), true);\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes) {\n\t\treturn broadcastOp(of, nodes, true);\n\t}\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of, nodes);\n\t}\n\n\tprivate <T> Future<Boolean> asyncStore(StoreType storeType, String key,\n\t\t\t\t\t\t   int exp, T value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\treturn asyncStore(storeType, key, exp, value, transcoder);\n\t}\n\n\tprivate <T> Future<Boolean> asyncCat(\n\t\t\tConcatenationType catType, long cas, String key,\n\t\t\tT value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.cat(catType, cas, key, co.getData(),\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\trv.set(val.isSuccess());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> append(long cas, String key, Object val) {\n\t\treturn append(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> append(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.append, cas, key, val, tc);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> prepend(long cas, String key, Object val) {\n\t\treturn prepend(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> prepend(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n\t}\n\n\t/**\n     * Asynchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a future that will indicate the status of the CAS\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return asyncCAS(key, casId, 0, value, tc);\n\t}\n\n\t/**\n\t * Asynchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASResponse> asyncCAS(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASResponse> rv=new OperationFuture<CASResponse>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op=opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n\t\t\t\tco.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\tif(val instanceof CASOperationStatus) {\n\t\t\t\t\t\t\trv.set(((CASOperationStatus)val).getCASResponse());\n\t\t\t\t\t\t} else if(val instanceof CancelledOperationStatus) {\n\t\t\t\t\t\t\t// Cancelled, ignore and let it float up\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\"Unhandled state: \" + val);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asynchronous CAS operation using the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n\t\treturn asyncCAS(key, casId, value, transcoder);\n\t}\n\n\t/**\n     * Perform a synchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a CASResponse\n     * @throws OperationTimeoutException if global operation timeout is\n     *         exceeded\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> CASResponse cas(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return cas(key, casId, 0, value, tc);\n    }\n\n\t/**\n\t * Perform a synchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if global operation timeout is\n\t *         exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASResponse cas(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n\t\t\t\t\tTimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Perform a synchronous CAS operation with the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASResponse cas(String key, long casId, Object value) {\n\t\treturn cas(key, casId, value, transcoder);\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> add(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Add an object to the cache (using the default transcoder)\n\t * iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> set(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Set an object in the cache (using the default transcoder)\n\t * regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> replace(String key, int exp, T o,\n\t\tTranscoder<T> tc) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Replace an object with the given value (transcoded with the default\n\t * transcoder) iff there is already a value for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal GetFuture<T> rv=new GetFuture<T>(latch, operationTimeout);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate Future<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=tcService.decode(tc,\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize()));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the given key asynchronously and decode with the default\n\t * transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\t\treturn asyncGet(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASValue<T>> asyncGets(final String key,\n\t\t\tfinal Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASValue<T>> rv=\n\t\t\tnew OperationFuture<CASValue<T>>(latch, operationTimeout);\n\n\t\tOperation op=opFact.gets(key,\n\t\t\t\tnew GetsOperation.Callback() {\n\t\t\tprivate CASValue<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, long cas, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tassert cas > 0 : \"CAS was less than zero:  \" + cas;\n\t\t\t\tval=new CASValue<T>(cas, tc.decode(\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously and decode using\n\t * the default transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASValue<Object>> asyncGets(final String key) {\n\t\treturn asyncGets(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if global operation timeout is\n\t * \t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGets(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASValue<Object> gets(String key) {\n\t\treturn gets(key, transcoder);\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> T get(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGet(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get with a single key and decode using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Object get(String key) {\n\t\treturn get(key, transcoder);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Collection<String> keys,\n\t\tfinal Transcoder<T> tc) {\n\t\tfinal Map<String, Future<T>> m=new ConcurrentHashMap<String, Future<T>>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, tcService.decode(tc,\n\t\t\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\n\t\t// Now that we know how many servers it breaks down into, and the latch\n\t\t// is all set up, convert all of these strings collections to operations\n\t\tfinal Map<MemcachedNode, Operation> mops=\n\t\t\tnew HashMap<MemcachedNode, Operation>();\n\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tOperation op=opFact.get(me.getValue(), cb);\n\t\t\tmops.put(me.getKey(), op);\n\t\t\tops.add(op);\n\t\t}\n\t\tassert mops.size() == chunks.size();\n\t\tcheckState();\n\t\tconn.addOperations(mops);\n\t\treturn new BulkGetFuture<T>(m, ops, latch);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache and decode them\n\t * with the given transcoder.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\treturn asyncGetBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n\t\tString... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Collection<String> keys,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\"Timeout waiting for bulkvalues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\treturn getBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\t/**\n\t * Get a set of stats from all connections.\n\t *\n\t * @param arg which stats to get\n\t * @return a Map of the server SocketAddress to a map of String stat\n\t *\t\t   keys to String stat values.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:\t%s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol\n\t\t\t\t\t\t// Allows us to avoid string processing.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tif (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Mutate operation timed out, unable to modify counter [\"\n\t\t\t\t\t\t+ key + \"]\");\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, exp);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, exp);\n\t}\n\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, exp, String.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, exp);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Timeout waiting to mutate or init value\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Future<Long> asyncMutate(Mutator m, String key, int by, long def,\n\t\t\tint exp) {\n\t\tfinal CountDownLatch latch = new CountDownLatch(1);\n\t\tfinal OperationFuture<Long> rv = new OperationFuture<Long>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op = addOp(key, opFact.mutate(m, key, by, def, exp,\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\trv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}));\n\t\trv.setOperation(op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asychronous increment.\n\t *\n\t * @return a future with the incremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncIncr(String key, int by) {\n\t\treturn asyncMutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Asynchronous decrement.\n\t *\n\t * @return a future with the decremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncDecr(String key, int by) {\n\t\treturn asyncMutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * <p>\n\t * The hold argument specifies the amount of time in seconds (or Unix time\n\t * until which) the client wishes the server to refuse \"add\" and \"replace\"\n\t * commands with this key. For this amount of item, the item is put into a\n\t * delete queue, which means that it won't possible to retrieve it by the\n\t * \"get\" command, but \"add\" and \"replace\" command with this key will also\n\t * fail (the \"set\" command will succeed, however). After the time passes,\n\t * the item is finally deleted from server memory.\n\t * <\/p>\n\t *\n\t * @param key the key to delete\n\t * @param hold how long the key should be unavailable to add commands\n\t *\n\t * @deprecated Hold values are no longer honored.\n\t */\n\t@Deprecated\n\tpublic Future<Boolean> delete(String key, int hold) {\n\t\treturn delete(key);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\toperationTimeout);\n\t\tDeleteOperation op=opFact.delete(key,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult,\n\t\t\t\toperationTimeout) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedClientIF#authenticate(java.lang.String[], javax.security.auth.callback.CallbackHandler)\n\t */\n\tpublic void authenticate(final String[] mechs, final CallbackHandler cbh)\n\t\tthrows OperationException {\n\t\tfinal ConcurrentHashMap<MemcachedNode, OperationStatus> statuses =\n\t\t\tnew ConcurrentHashMap<MemcachedNode, OperationStatus>();\n\n\t\tCollection<MemcachedNode> todo = conn.getLocator().getAll();\n\n\t\tBroadcastOpFactory bfact = new BroadcastOpFactory() {\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tif(statuses.containsKey(n)) {\n\t\t\t\t\treturn opFact.saslStep(null); // TODO\n\t\t\t\t} else {\n\t\t\t\t\treturn opFact.saslAuth(mechs,\n\t\t\t\t\t\t\tn.toString(), null, cbh,new OperationCallback() {\n\t\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\t\tstatuses.put(n, status);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\n\t\tCountDownLatch blatch = broadcastOp(bfact, todo);\n\n\t\ttry {\n\t\t\tblatch.await();\n\t\t} catch(InterruptedException e) {\n\t\t\tThread.currentThread().interrupt();\n\t\t}\n\n\t\tfor(OperationStatus status : statuses.values()) {\n\t        if (!status.isSuccess()) {\n\t                throw new OperationException(\n\t                        OperationErrorType.GENERAL, status.getMessage());\n\t        }\n\t}\n\n\t}\n\n\tprivate void logRunException(Exception e) {\n\t\tif(shuttingDown) {\n\t\t\t// There are a couple types of errors that occur during the\n\t\t\t// shutdown sequence that are considered OK.  Log at debug.\n\t\t\tgetLogger().debug(\"Exception occurred during shutdown\", e);\n\t\t} else {\n\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(CancelledKeyException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(ClosedSelectorException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(IllegalStateException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\t// Guard against double shutdowns (bug 8).\n\t\tif(shuttingDown) {\n\t\t\tgetLogger().info(\"Suppressing duplicate attempt to shut down\");\n\t\t\treturn false;\n\t\t}\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t\ttcService.shutdown();\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t *\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, conn.getLocator().getAll(), false);\n\t\ttry {\n\t\t\t// XXX:  Perhaps IllegalStateException should be caught here\n\t\t\t// and the check retried.\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return true if the observer was added.\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn conn.addObserver(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed, but no longer does\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn conn.removeObserver(obs);\n\t}\n\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport javax.security.auth.callback.CallbackHandler;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.internal.BulkGetFuture;\nimport net.spy.memcached.internal.GetFuture;\nimport net.spy.memcached.internal.OperationFuture;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationErrorType;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tnew InetSocketAddress(\"hostname\", portNum));\n *\n *\t// Store a value (async) for one hour\n *\tc.set(\"someKey\", 3600, someObject);\n *\t// Retrieve a value.\n *\tObject myObject=c.get(\"someKey\");\n *\t<\/pre>\n *\n *\t<h2>Advanced Usage<\/h2>\n *\n *\t<p>\n *\t MemcachedClient may be processing a great deal of asynchronous messages or\n *\t possibly dealing with an unreachable memcached, which may delay processing.\n *\t If a memcached is disabled, for example, MemcachedConnection will continue\n *\t to attempt to reconnect and replay pending operations until it comes back\n *\t up.  To prevent this from causing your application to hang, you can use\n *\t one of the asynchronous mechanisms to time out a request and cancel the\n *\t operation to the server.\n *\t<\/p>\n *\n *\t<pre>\n *      // Get a memcached client connected to several servers\n *      // over the binary protocol\n *      MemcachedClient c = new MemcachedClient(new BinaryConnectionFactory(),\n *              AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *      // Try to get a value, for up to 5 seconds, and cancel if it\n *      // doesn't return\n *      Object myObj = null;\n *      Future&lt;Object&gt; f = c.asyncGet(\"someKey\");\n *      try {\n *          myObj = f.get(5, TimeUnit.SECONDS);\n *      // throws expecting InterruptedException, ExecutionException\n *      // or TimeoutException\n *      } catch (Exception e) {  /*  /\n *          // Since we don't need this, go ahead and cancel the operation.\n *          // This is not strictly necessary, but it'll save some work on\n *          // the server.  It is okay to cancel it if running.\n *          f.cancel(true);\n *          // Do other timeout related stuff\n *      }\n * <\/pre>\n */\npublic class MemcachedClient extends SpyThread implements MemcachedClientIF {\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final long operationTimeout;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tfinal Transcoder<Object> transcoder;\n\n\tfinal TranscodeService tcService;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param cf the connection factory to configure connections for this client\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tif(cf == null) {\n\t\t\tthrow new NullPointerException(\"Connection factory required\");\n\t\t}\n\t\tif(addrs == null) {\n\t\t\tthrow new NullPointerException(\"Server list required\");\n\t\t}\n\t\tif(addrs.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"You must have at least one server to connect to\");\n\t\t}\n\t\tif(cf.getOperationTimeout() <= 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Operation timeout must be positive.\");\n\t\t}\n\t\ttcService = new TranscodeService();\n\t\ttranscoder=cf.getDefaultTranscoder();\n\t\topFact=cf.getOperationFactory();\n\t\tassert opFact != null : \"Connection factory failed to make op factory\";\n\t\tconn=cf.createConnection(addrs);\n\t\tassert conn != null : \"Connection factory failed to make a connection\";\n\t\toperationTimeout = cf.getOperationTimeout();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tsetDaemon(cf.isDaemon());\n\t\tstart();\n\t}\n\n\t/**\n\t * Get the addresses of available servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getAvailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the addresses of unavailable servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getUnavailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(!node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get a read-only wrapper around the node locator wrapping this instance.\n\t */\n\tpublic NodeLocator getNodeLocator() {\n\t\treturn conn.getLocator().getReadonlyCopy();\n\t}\n\n\t/**\n\t * Get the default transcoder that's in use.\n\t */\n\tpublic Transcoder<Object> getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tbyte[] keyBytes=KeyUtil.getKeyBytes(key);\n\t\tif(keyBytes.length > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\tif(keyBytes.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Key must contain at least one character.\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(byte b : keyBytes) {\n\t\t\tif(b == ' ' || b == '\\n' || b == '\\r' || b == 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void checkState() {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tvalidateKey(key);\n\t\tcheckState();\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, conn.getLocator().getAll(), true);\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes) {\n\t\treturn broadcastOp(of, nodes, true);\n\t}\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tCollection<MemcachedNode> nodes,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of, nodes);\n\t}\n\n\tprivate <T> Future<Boolean> asyncStore(StoreType storeType, String key,\n\t\t\t\t\t\t   int exp, T value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\treturn asyncStore(storeType, key, exp, value, transcoder);\n\t}\n\n\tprivate <T> Future<Boolean> asyncCat(\n\t\t\tConcatenationType catType, long cas, String key,\n\t\t\tT value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.cat(catType, cas, key, co.getData(),\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\trv.set(val.isSuccess());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> append(long cas, String key, Object val) {\n\t\treturn append(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> append(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.append, cas, key, val, tc);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> prepend(long cas, String key, Object val) {\n\t\treturn prepend(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> prepend(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n\t}\n\n\t/**\n     * Asynchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a future that will indicate the status of the CAS\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return asyncCAS(key, casId, 0, value, tc);\n\t}\n\n\t/**\n\t * Asynchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASResponse> asyncCAS(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASResponse> rv=new OperationFuture<CASResponse>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op=opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n\t\t\t\tco.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\tif(val instanceof CASOperationStatus) {\n\t\t\t\t\t\t\trv.set(((CASOperationStatus)val).getCASResponse());\n\t\t\t\t\t\t} else if(val instanceof CancelledOperationStatus) {\n\t\t\t\t\t\t\t// Cancelled, ignore and let it float up\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\"Unhandled state: \" + val);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asynchronous CAS operation using the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n\t\treturn asyncCAS(key, casId, value, transcoder);\n\t}\n\n\t/**\n     * Perform a synchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a CASResponse\n     * @throws OperationTimeoutException if global operation timeout is\n     *         exceeded\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> CASResponse cas(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return cas(key, casId, 0, value, tc);\n    }\n\n\t/**\n\t * Perform a synchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if global operation timeout is\n\t *         exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASResponse cas(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n\t\t\t\t\tTimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Perform a synchronous CAS operation with the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASResponse cas(String key, long casId, Object value) {\n\t\treturn cas(key, casId, value, transcoder);\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> add(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Add an object to the cache (using the default transcoder)\n\t * iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> set(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Set an object in the cache (using the default transcoder)\n\t * regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> replace(String key, int exp, T o,\n\t\tTranscoder<T> tc) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Replace an object with the given value (transcoded with the default\n\t * transcoder) iff there is already a value for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal GetFuture<T> rv=new GetFuture<T>(latch, operationTimeout);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate Future<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=tcService.decode(tc,\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize()));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the given key asynchronously and decode with the default\n\t * transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\t\treturn asyncGet(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASValue<T>> asyncGets(final String key,\n\t\t\tfinal Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASValue<T>> rv=\n\t\t\tnew OperationFuture<CASValue<T>>(latch, operationTimeout);\n\n\t\tOperation op=opFact.gets(key,\n\t\t\t\tnew GetsOperation.Callback() {\n\t\t\tprivate CASValue<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, long cas, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tassert cas > 0 : \"CAS was less than zero:  \" + cas;\n\t\t\t\tval=new CASValue<T>(cas, tc.decode(\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously and decode using\n\t * the default transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASValue<Object>> asyncGets(final String key) {\n\t\treturn asyncGets(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if global operation timeout is\n\t * \t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGets(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASValue<Object> gets(String key) {\n\t\treturn gets(key, transcoder);\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> T get(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGet(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get with a single key and decode using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Object get(String key) {\n\t\treturn get(key, transcoder);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Collection<String> keys,\n\t\tfinal Transcoder<T> tc) {\n\t\tfinal Map<String, Future<T>> m=new ConcurrentHashMap<String, Future<T>>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, tcService.decode(tc,\n\t\t\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\n\t\t// Now that we know how many servers it breaks down into, and the latch\n\t\t// is all set up, convert all of these strings collections to operations\n\t\tfinal Map<MemcachedNode, Operation> mops=\n\t\t\tnew HashMap<MemcachedNode, Operation>();\n\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tOperation op=opFact.get(me.getValue(), cb);\n\t\t\tmops.put(me.getKey(), op);\n\t\t\tops.add(op);\n\t\t}\n\t\tassert mops.size() == chunks.size();\n\t\tcheckState();\n\t\tconn.addOperations(mops);\n\t\treturn new BulkGetFuture<T>(m, ops, latch);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache and decode them\n\t * with the given transcoder.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\treturn asyncGetBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n\t\tString... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Collection<String> keys,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\"Timeout waiting for bulkvalues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\treturn getBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\t/**\n\t * Get a set of stats from all connections.\n\t *\n\t * @param arg which stats to get\n\t * @return a Map of the server SocketAddress to a map of String stat\n\t *\t\t   keys to String stat values.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:\t%s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol\n\t\t\t\t\t\t// Allows us to avoid string processing.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tif (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Mutate operation timed out, unable to modify counter [\"\n\t\t\t\t\t\t+ key + \"]\");\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, exp);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, exp);\n\t}\n\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, exp, String.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, exp);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Timeout waiting to mutate or init value\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Future<Long> asyncMutate(Mutator m, String key, int by, long def,\n\t\t\tint exp) {\n\t\tfinal CountDownLatch latch = new CountDownLatch(1);\n\t\tfinal OperationFuture<Long> rv = new OperationFuture<Long>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op = addOp(key, opFact.mutate(m, key, by, def, exp,\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\trv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}));\n\t\trv.setOperation(op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asychronous increment.\n\t *\n\t * @return a future with the incremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncIncr(String key, int by) {\n\t\treturn asyncMutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Asynchronous decrement.\n\t *\n\t * @return a future with the decremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncDecr(String key, int by) {\n\t\treturn asyncMutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * <p>\n\t * The hold argument specifies the amount of time in seconds (or Unix time\n\t * until which) the client wishes the server to refuse \"add\" and \"replace\"\n\t * commands with this key. For this amount of item, the item is put into a\n\t * delete queue, which means that it won't possible to retrieve it by the\n\t * \"get\" command, but \"add\" and \"replace\" command with this key will also\n\t * fail (the \"set\" command will succeed, however). After the time passes,\n\t * the item is finally deleted from server memory.\n\t * <\/p>\n\t *\n\t * @param key the key to delete\n\t * @param hold how long the key should be unavailable to add commands\n\t *\n\t * @deprecated Hold values are no longer honored.\n\t */\n\t@Deprecated\n\tpublic Future<Boolean> delete(String key, int hold) {\n\t\treturn delete(key);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\toperationTimeout);\n\t\tDeleteOperation op=opFact.delete(key,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult,\n\t\t\t\toperationTimeout) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedClientIF#authenticate(java.lang.String[], javax.security.auth.callback.CallbackHandler)\n\t */\n\tpublic void authenticate(final String[] mechs, final CallbackHandler cbh)\n\t\tthrows OperationException {\n\t\tfinal ConcurrentHashMap<MemcachedNode, OperationStatus> statuses =\n\t\t\tnew ConcurrentHashMap<MemcachedNode, OperationStatus>();\n\n\t\tCollection<MemcachedNode> todo = new ArrayList<MemcachedNode>(\n\t\t\t\tconn.getLocator().getAll());\n\n\t\tBroadcastOpFactory bfact = new BroadcastOpFactory() {\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal OperationCallback cb = new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tstatuses.put(n, status);\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif(statuses.containsKey(n)) {\n\t\t\t\t\tOperationStatus priorStatus=statuses.remove(n);\n\t\t\t\t\treturn opFact.saslStep(mechs, priorStatus.getMessage(),\n\t\t\t\t\t\t\tn.toString(), null, cbh, cb);\n\t\t\t\t} else {\n\t\t\t\t\treturn opFact.saslAuth(mechs, n.toString(), null, cbh, cb);\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\n\t\tboolean done = false;\n\t\twhile (!done) {\n\t\t\tCountDownLatch blatch=broadcastOp(bfact, todo);\n\t\t\ttodo.clear();\n\n\t\t\ttry {\n\t\t\t\tblatch.await();\n\t\t\t} catch(InterruptedException e) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\n\t\t\tdone = true;\n\t\t\tfor(Map.Entry<MemcachedNode, OperationStatus> me\n\t\t\t\t\t: statuses.entrySet()) {\n\t\t\t\tif(!me.getValue().isSuccess()) {\n\t\t\t\t\tthrow new OperationException(OperationErrorType.GENERAL,\n\t\t\t\t\t\t\tme.getValue().getMessage());\n\t\t\t\t}\n\n\t\t\t\tif(me.getValue().getMessage().length() == 0) {\n\t\t\t\t\tstatuses.remove(me.getKey());\n\t\t\t\t} else {\n\t\t\t\t\ttodo.add(me.getKey());\n\t\t\t\t\tdone = false;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t}\n\n\tprivate void logRunException(Exception e) {\n\t\tif(shuttingDown) {\n\t\t\t// There are a couple types of errors that occur during the\n\t\t\t// shutdown sequence that are considered OK.  Log at debug.\n\t\t\tgetLogger().debug(\"Exception occurred during shutdown\", e);\n\t\t} else {\n\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(CancelledKeyException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(ClosedSelectorException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(IllegalStateException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\t// Guard against double shutdowns (bug 8).\n\t\tif(shuttingDown) {\n\t\t\tgetLogger().info(\"Suppressing duplicate attempt to shut down\");\n\t\t\treturn false;\n\t\t}\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t\ttcService.shutdown();\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t *\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, conn.getLocator().getAll(), false);\n\t\ttry {\n\t\t\t// XXX:  Perhaps IllegalStateException should be caught here\n\t\t\t// and the check retried.\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return true if the observer was added.\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn conn.addObserver(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed, but no longer does\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn conn.removeObserver(obs);\n\t}\n\n}\n","lineNo":1465}
{"Smelly Sample":"package net.spy.memcached.protocol;\n\nimport java.io.IOException;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.concurrent.BlockingQueue;\n\nimport net.spy.memcached.MemcachedNode;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\n\n/**\n * Represents a node with the memcached cluster, along with buffering and\n * operation queues.\n */\npublic abstract class TCPMemcachedNodeImpl extends SpyObject\n\timplements MemcachedNode {\n\n\tprivate final SocketAddress socketAddress;\n\tprivate final ByteBuffer rbuf;\n\tprivate final ByteBuffer wbuf;\n\tprotected final BlockingQueue<Operation> writeQ;\n\tprivate final BlockingQueue<Operation> readQ;\n\tprivate final BlockingQueue<Operation> inputQueue;\n\t// This has been declared volatile so it can be used as an availability\n\t// indicator.\n\tprivate volatile int reconnectAttempt=1;\n\tprivate SocketChannel channel;\n\tprivate int toWrite=0;\n\tprotected Operation optimizedOp=null;\n\tprivate volatile SelectionKey sk=null;\n\n\tpublic TCPMemcachedNodeImpl(SocketAddress sa, SocketChannel c,\n\t\t\tint bufSize, BlockingQueue<Operation> rq,\n\t\t\tBlockingQueue<Operation> wq, BlockingQueue<Operation> iq) {\n\t\tsuper();\n\t\tassert sa != null : \"No SocketAddress\";\n\t\tassert c != null : \"No SocketChannel\";\n\t\tassert bufSize > 0 : \"Invalid buffer size: \" + bufSize;\n\t\tassert rq != null : \"No operation read queue\";\n\t\tassert wq != null : \"No operation write queue\";\n\t\tassert iq != null : \"No input queue\";\n\t\tsocketAddress=sa;\n\t\tsetChannel(c);\n\t\trbuf=ByteBuffer.allocate(bufSize);\n\t\twbuf=ByteBuffer.allocate(bufSize);\n\t\tgetWbuf().clear();\n\t\treadQ=rq;\n\t\twriteQ=wq;\n\t\tinputQueue=iq;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#copyInputQueue()\n\t */\n\tpublic final void copyInputQueue() {\n\t\tCollection<Operation> tmp=new ArrayList<Operation>();\n\n\t\t// don't drain more than we have space to place\n\t\tinputQueue.drainTo(tmp, writeQ.remainingCapacity());\n\n\t\twriteQ.addAll(tmp);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#destroyInputQueue()\n\t */\n\tpublic Collection<Operation> destroyInputQueue() {\n\t\tCollection<Operation> rv=new ArrayList<Operation>();\n\t\tinputQueue.drainTo(rv);\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setupResend()\n\t */\n\tpublic final void setupResend() {\n\t\t// First, reset the current write op.\n\t\tOperation op=getCurrentWriteOp();\n\t\tif(op != null) {\n\t\t\top.getBuffer().reset();\n\t\t}\n\t\t// Now cancel all the pending read operations.  Might be better to\n\t\t// to requeue them.\n\t\twhile(hasReadOp()) {\n\t\t\top=removeCurrentReadOp();\n\t\t\tgetLogger().warn(\"Discarding partially completed op: %s\", op);\n\t\t\top.cancel();\n\t\t}\n\n\t\tgetWbuf().clear();\n\t\tgetRbuf().clear();\n\t\ttoWrite=0;\n\t}\n\n\t// Prepare the pending operations.  Return true if there are any pending\n\t// ops\n\tprivate boolean preparePending() {\n\t\t// Copy the input queue into the write queue.\n\t\tcopyInputQueue();\n\n\t\t// Now check the ops\n\t\tOperation nextOp=getCurrentWriteOp();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\", nextOp);\n\t\t\tremoveCurrentWriteOp();\n\t\t\tnextOp=getCurrentWriteOp();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#fillWriteBuffer(boolean)\n\t */\n\tpublic final void fillWriteBuffer(boolean shouldOptimize) {\n\t\tif(toWrite == 0 && readQ.remainingCapacity() > 0) {\n\t\t\tgetWbuf().clear();\n\t\t\tOperation o=getCurrentWriteOp();\n\t\t\twhile(o != null && toWrite < getWbuf().capacity()) {\n\t\t\t\tassert o.getState() == OperationState.WRITING;\n\t\t\t\tByteBuffer obuf=o.getBuffer();\n\t\t\t\tint bytesToCopy=Math.min(getWbuf().remaining(),\n\t\t\t\t\t\tobuf.remaining());\n\t\t\t\tbyte b[]=new byte[bytesToCopy];\n\t\t\t\tobuf.get(b);\n\t\t\t\tgetWbuf().put(b);\n\t\t\t\tgetLogger().debug(\"After copying stuff from %s: %s\",\n\t\t\t\t\t\to, getWbuf());\n\t\t\t\tif(!o.getBuffer().hasRemaining()) {\n\t\t\t\t\to.writeComplete();\n\t\t\t\t\ttransitionWriteItem();\n\n\t\t\t\t\tpreparePending();\n\t\t\t\t\tif(shouldOptimize) {\n\t\t\t\t\t\toptimize();\n\t\t\t\t\t}\n\n\t\t\t\t\to=getCurrentWriteOp();\n\t\t\t\t}\n\t\t\t\ttoWrite += bytesToCopy;\n\t\t\t}\n\t\t\tgetWbuf().flip();\n\t\t\tassert toWrite <= getWbuf().capacity()\n\t\t\t\t: \"toWrite exceeded capacity: \" + this;\n\t\t\tassert toWrite == getWbuf().remaining()\n\t\t\t\t: \"Expected \" + toWrite + \" remaining, got \"\n\t\t\t\t+ getWbuf().remaining();\n\t\t} else {\n\t\t\tgetLogger().debug(\"Buffer is full, skipping\");\n\t\t}\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#transitionWriteItem()\n\t */\n\tpublic final void transitionWriteItem() {\n\t\tOperation op=removeCurrentWriteOp();\n\t\tassert op != null : \"There is no write item to transition\";\n\t\tgetLogger().debug(\"Transitioning %s to read\", op);\n\t\treadQ.add(op);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#optimize()\n\t */\n\tprotected abstract void optimize();\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentReadOp()\n\t */\n\tpublic final Operation getCurrentReadOp() {\n\t\treturn readQ.peek();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentReadOp()\n\t */\n\tpublic final Operation removeCurrentReadOp() {\n\t\treturn readQ.remove();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentWriteOp()\n\t */\n\tpublic final Operation getCurrentWriteOp() {\n\t\treturn optimizedOp == null ? writeQ.peek() : optimizedOp;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentWriteOp()\n\t */\n\tpublic final Operation removeCurrentWriteOp() {\n\t\tOperation rv=optimizedOp;\n\t\tif(rv == null) {\n\t\t\trv=writeQ.remove();\n\t\t} else {\n\t\t\toptimizedOp=null;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasReadOp()\n\t */\n\tpublic final boolean hasReadOp() {\n\t\treturn !readQ.isEmpty();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasWriteOp()\n\t */\n\tpublic final boolean hasWriteOp() {\n\t\treturn !(optimizedOp == null && writeQ.isEmpty());\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#addOp(net.spy.memcached.ops.Operation)\n\t */\n\tpublic final void addOp(Operation op) {\n\t\tboolean added=inputQueue.add(op);\n\t\tassert added; // documented to throw an IllegalStateException\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSelectionOps()\n\t */\n\tpublic final int getSelectionOps() {\n\t\tint rv=0;\n\t\tif(getChannel().isConnected()) {\n\t\t\tif(hasReadOp()) {\n\t\t\t\trv |= SelectionKey.OP_READ;\n\t\t\t}\n\t\t\tif(toWrite > 0 || hasWriteOp()) {\n\t\t\t\trv |= SelectionKey.OP_WRITE;\n\t\t\t}\n\t\t} else {\n\t\t\trv = SelectionKey.OP_CONNECT;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getRbuf()\n\t */\n\tpublic final ByteBuffer getRbuf() {\n\t\treturn rbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getWbuf()\n\t */\n\tpublic final ByteBuffer getWbuf() {\n\t\treturn wbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSocketAddress()\n\t */\n\tpublic final SocketAddress getSocketAddress() {\n\t\treturn socketAddress;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#isActive()\n\t */\n\tpublic final boolean isActive() {\n\t\treturn reconnectAttempt == 0\n\t\t\t&& getChannel() != null && getChannel().isConnected();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#reconnecting()\n\t */\n\tpublic final void reconnecting() {\n\t\treconnectAttempt++;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#connected()\n\t */\n\tpublic final void connected() {\n\t\treconnectAttempt=0;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getReconnectCount()\n\t */\n\tpublic final int getReconnectCount() {\n\t\treturn reconnectAttempt;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#toString()\n\t */\n\t@Override\n\tpublic final String toString() {\n\t\tint sops=0;\n\t\tif(getSk()!= null && getSk().isValid()) {\n\t\t\tsops=getSk().interestOps();\n\t\t}\n\t\tint rsize=readQ.size() + (optimizedOp == null ? 0 : 1);\n\t\tint wsize=writeQ.size();\n\t\tint isize=inputQueue.size();\n\t\treturn \"{QA sa=\" + getSocketAddress() + \", #Rops=\" + rsize\n\t\t\t+ \", #Wops=\" + wsize\n\t\t\t+ \", #iq=\" + isize\n\t\t\t+ \", topRop=\" + getCurrentReadOp()\n\t\t\t+ \", topWop=\" + getCurrentWriteOp()\n\t\t\t+ \", toWrite=\" + toWrite\n\t\t\t+ \", interested=\" + sops + \"}\";\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#registerChannel(java.nio.channels.SocketChannel, java.nio.channels.SelectionKey)\n\t */\n\tpublic final void registerChannel(SocketChannel ch, SelectionKey skey) {\n\t\tsetChannel(ch);\n\t\tsetSk(skey);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setChannel(java.nio.channels.SocketChannel)\n\t */\n\tpublic final void setChannel(SocketChannel to) {\n\t\tassert channel == null || !channel.isOpen()\n\t\t\t: \"Attempting to overwrite channel\";\n\t\tchannel = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getChannel()\n\t */\n\tpublic final SocketChannel getChannel() {\n\t\treturn channel;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setSk(java.nio.channels.SelectionKey)\n\t */\n\tpublic final void setSk(SelectionKey to) {\n\t\tsk = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSk()\n\t */\n\tpublic final SelectionKey getSk() {\n\t\treturn sk;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getBytesRemainingInBuffer()\n\t */\n\tpublic final int getBytesRemainingToWrite() {\n\t\treturn toWrite;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#writeSome()\n\t */\n\tpublic final int writeSome() throws IOException {\n\t\tint wrote=channel.write(wbuf);\n\t\tassert wrote >= 0 : \"Wrote negative bytes?\";\n\t\ttoWrite -= wrote;\n\t\tassert toWrite >= 0\n\t\t\t: \"toWrite went negative after writing \" + wrote\n\t\t\t\t+ \" bytes for \" + this;\n\t\tgetLogger().debug(\"Wrote %d bytes\", wrote);\n\t\treturn wrote;\n\t}\n\n\n\tpublic final void fixupOps() {\n\t\t// As the selection key can be changed at any point due to node\n\t\t// failure, we'll grab the current volatile value and configure it.\n\t\tSelectionKey s = sk;\n\t\tif(s != null && s.isValid()) {\n\t\t\tint iops=getSelectionOps();\n\t\t\tgetLogger().debug(\"Setting interested opts to %d\", iops);\n\t\t\ts.interestOps(iops);\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selection key is not valid.\");\n\t\t}\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol;\n\nimport java.io.IOException;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.concurrent.BlockingQueue;\n\nimport net.spy.memcached.MemcachedNode;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\n\n/**\n * Represents a node with the memcached cluster, along with buffering and\n * operation queues.\n */\npublic abstract class TCPMemcachedNodeImpl extends SpyObject\n\timplements MemcachedNode {\n\n\tprivate final SocketAddress socketAddress;\n\tprivate final ByteBuffer rbuf;\n\tprivate final ByteBuffer wbuf;\n\tprotected final BlockingQueue<Operation> writeQ;\n\tprivate final BlockingQueue<Operation> readQ;\n\tprivate final BlockingQueue<Operation> inputQueue;\n\t// This has been declared volatile so it can be used as an availability\n\t// indicator.\n\tprivate volatile int reconnectAttempt=1;\n\tprivate SocketChannel channel;\n\tprivate int toWrite=0;\n\tprotected Operation optimizedOp=null;\n\tprivate volatile SelectionKey sk=null;\n\n\tpublic TCPMemcachedNodeImpl(SocketAddress sa, SocketChannel c,\n\t\t\tint bufSize, BlockingQueue<Operation> rq,\n\t\t\tBlockingQueue<Operation> wq, BlockingQueue<Operation> iq) {\n\t\tsuper();\n\t\tassert sa != null : \"No SocketAddress\";\n\t\tassert c != null : \"No SocketChannel\";\n\t\tassert bufSize > 0 : \"Invalid buffer size: \" + bufSize;\n\t\tassert rq != null : \"No operation read queue\";\n\t\tassert wq != null : \"No operation write queue\";\n\t\tassert iq != null : \"No input queue\";\n\t\tsocketAddress=sa;\n\t\tsetChannel(c);\n\t\trbuf=ByteBuffer.allocate(bufSize);\n\t\twbuf=ByteBuffer.allocate(bufSize);\n\t\tgetWbuf().clear();\n\t\treadQ=rq;\n\t\twriteQ=wq;\n\t\tinputQueue=iq;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#copyInputQueue()\n\t */\n\tpublic final void copyInputQueue() {\n\t\tCollection<Operation> tmp=new ArrayList<Operation>();\n\n\t\t// don't drain more than we have space to place\n\t\tinputQueue.drainTo(tmp, writeQ.remainingCapacity());\n\n\t\twriteQ.addAll(tmp);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#destroyInputQueue()\n\t */\n\tpublic Collection<Operation> destroyInputQueue() {\n\t\tCollection<Operation> rv=new ArrayList<Operation>();\n\t\tinputQueue.drainTo(rv);\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setupResend()\n\t */\n\tpublic final void setupResend() {\n\t\t// First, reset the current write op.\n\t\tOperation op=getCurrentWriteOp();\n\t\tif(op != null) {\n\t\t\tByteBuffer buf=op.getBuffer();\n\t\t\tif(buf != null) {\n\t\t\t\tbuf.reset();\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"No buffer for current write op, removing\");\n\t\t\t\tremoveCurrentWriteOp();\n\t\t\t}\n\t\t}\n\t\t// Now cancel all the pending read operations.  Might be better to\n\t\t// to requeue them.\n\t\twhile(hasReadOp()) {\n\t\t\top=removeCurrentReadOp();\n\t\t\tif (op != getCurrentWriteOp()) {\n\t\t\t\tgetLogger().warn(\"Discarding partially completed op: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\n\t\tgetWbuf().clear();\n\t\tgetRbuf().clear();\n\t\ttoWrite=0;\n\t}\n\n\t// Prepare the pending operations.  Return true if there are any pending\n\t// ops\n\tprivate boolean preparePending() {\n\t\t// Copy the input queue into the write queue.\n\t\tcopyInputQueue();\n\n\t\t// Now check the ops\n\t\tOperation nextOp=getCurrentWriteOp();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\", nextOp);\n\t\t\tremoveCurrentWriteOp();\n\t\t\tnextOp=getCurrentWriteOp();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#fillWriteBuffer(boolean)\n\t */\n\tpublic final void fillWriteBuffer(boolean shouldOptimize) {\n\t\tif(toWrite == 0 && readQ.remainingCapacity() > 0) {\n\t\t\tgetWbuf().clear();\n\t\t\tOperation o=getCurrentWriteOp();\n\t\t\twhile(o != null && toWrite < getWbuf().capacity()) {\n\t\t\t\tassert o.getState() == OperationState.WRITING;\n\t\t\t\t// This isn't the most optimal way to do this, but it hints\n\t\t\t\t// at a larger design problem that may need to be taken care\n\t\t\t\t// if in the bowels of the client.\n\t\t\t\t// In practice, readQ should be small, however.\n\t\t\t\tif(!readQ.contains(o)) {\n\t\t\t\t\treadQ.add(o);\n\t\t\t\t}\n\n\t\t\t\tByteBuffer obuf=o.getBuffer();\n\t\t\t\tassert obuf != null : \"Didn't get a write buffer from \" + o;\n\t\t\t\tint bytesToCopy=Math.min(getWbuf().remaining(),\n\t\t\t\t\t\tobuf.remaining());\n\t\t\t\tbyte b[]=new byte[bytesToCopy];\n\t\t\t\tobuf.get(b);\n\t\t\t\tgetWbuf().put(b);\n\t\t\t\tgetLogger().debug(\"After copying stuff from %s: %s\",\n\t\t\t\t\t\to, getWbuf());\n\t\t\t\tif(!o.getBuffer().hasRemaining()) {\n\t\t\t\t\to.writeComplete();\n\t\t\t\t\ttransitionWriteItem();\n\n\t\t\t\t\tpreparePending();\n\t\t\t\t\tif(shouldOptimize) {\n\t\t\t\t\t\toptimize();\n\t\t\t\t\t}\n\n\t\t\t\t\to=getCurrentWriteOp();\n\t\t\t\t}\n\t\t\t\ttoWrite += bytesToCopy;\n\t\t\t}\n\t\t\tgetWbuf().flip();\n\t\t\tassert toWrite <= getWbuf().capacity()\n\t\t\t\t: \"toWrite exceeded capacity: \" + this;\n\t\t\tassert toWrite == getWbuf().remaining()\n\t\t\t\t: \"Expected \" + toWrite + \" remaining, got \"\n\t\t\t\t+ getWbuf().remaining();\n\t\t} else {\n\t\t\tgetLogger().debug(\"Buffer is full, skipping\");\n\t\t}\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#transitionWriteItem()\n\t */\n\tpublic final void transitionWriteItem() {\n\t\tOperation op=removeCurrentWriteOp();\n\t\tassert op != null : \"There is no write item to transition\";\n\t\tgetLogger().debug(\"Finished writing %s\", op);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#optimize()\n\t */\n\tprotected abstract void optimize();\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentReadOp()\n\t */\n\tpublic final Operation getCurrentReadOp() {\n\t\treturn readQ.peek();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentReadOp()\n\t */\n\tpublic final Operation removeCurrentReadOp() {\n\t\treturn readQ.remove();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentWriteOp()\n\t */\n\tpublic final Operation getCurrentWriteOp() {\n\t\treturn optimizedOp == null ? writeQ.peek() : optimizedOp;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentWriteOp()\n\t */\n\tpublic final Operation removeCurrentWriteOp() {\n\t\tOperation rv=optimizedOp;\n\t\tif(rv == null) {\n\t\t\trv=writeQ.remove();\n\t\t} else {\n\t\t\toptimizedOp=null;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasReadOp()\n\t */\n\tpublic final boolean hasReadOp() {\n\t\treturn !readQ.isEmpty();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasWriteOp()\n\t */\n\tpublic final boolean hasWriteOp() {\n\t\treturn !(optimizedOp == null && writeQ.isEmpty());\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#addOp(net.spy.memcached.ops.Operation)\n\t */\n\tpublic final void addOp(Operation op) {\n\t\tboolean added=inputQueue.add(op);\n\t\tassert added; // documented to throw an IllegalStateException\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSelectionOps()\n\t */\n\tpublic final int getSelectionOps() {\n\t\tint rv=0;\n\t\tif(getChannel().isConnected()) {\n\t\t\tif(hasReadOp()) {\n\t\t\t\trv |= SelectionKey.OP_READ;\n\t\t\t}\n\t\t\tif(toWrite > 0 || hasWriteOp()) {\n\t\t\t\trv |= SelectionKey.OP_WRITE;\n\t\t\t}\n\t\t} else {\n\t\t\trv = SelectionKey.OP_CONNECT;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getRbuf()\n\t */\n\tpublic final ByteBuffer getRbuf() {\n\t\treturn rbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getWbuf()\n\t */\n\tpublic final ByteBuffer getWbuf() {\n\t\treturn wbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSocketAddress()\n\t */\n\tpublic final SocketAddress getSocketAddress() {\n\t\treturn socketAddress;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#isActive()\n\t */\n\tpublic final boolean isActive() {\n\t\treturn reconnectAttempt == 0\n\t\t\t&& getChannel() != null && getChannel().isConnected();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#reconnecting()\n\t */\n\tpublic final void reconnecting() {\n\t\treconnectAttempt++;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#connected()\n\t */\n\tpublic final void connected() {\n\t\treconnectAttempt=0;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getReconnectCount()\n\t */\n\tpublic final int getReconnectCount() {\n\t\treturn reconnectAttempt;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#toString()\n\t */\n\t@Override\n\tpublic final String toString() {\n\t\tint sops=0;\n\t\tif(getSk()!= null && getSk().isValid()) {\n\t\t\tsops=getSk().interestOps();\n\t\t}\n\t\tint rsize=readQ.size() + (optimizedOp == null ? 0 : 1);\n\t\tint wsize=writeQ.size();\n\t\tint isize=inputQueue.size();\n\t\treturn \"{QA sa=\" + getSocketAddress() + \", #Rops=\" + rsize\n\t\t\t+ \", #Wops=\" + wsize\n\t\t\t+ \", #iq=\" + isize\n\t\t\t+ \", topRop=\" + getCurrentReadOp()\n\t\t\t+ \", topWop=\" + getCurrentWriteOp()\n\t\t\t+ \", toWrite=\" + toWrite\n\t\t\t+ \", interested=\" + sops + \"}\";\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#registerChannel(java.nio.channels.SocketChannel, java.nio.channels.SelectionKey)\n\t */\n\tpublic final void registerChannel(SocketChannel ch, SelectionKey skey) {\n\t\tsetChannel(ch);\n\t\tsetSk(skey);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setChannel(java.nio.channels.SocketChannel)\n\t */\n\tpublic final void setChannel(SocketChannel to) {\n\t\tassert channel == null || !channel.isOpen()\n\t\t\t: \"Attempting to overwrite channel\";\n\t\tchannel = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getChannel()\n\t */\n\tpublic final SocketChannel getChannel() {\n\t\treturn channel;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setSk(java.nio.channels.SelectionKey)\n\t */\n\tpublic final void setSk(SelectionKey to) {\n\t\tsk = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSk()\n\t */\n\tpublic final SelectionKey getSk() {\n\t\treturn sk;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getBytesRemainingInBuffer()\n\t */\n\tpublic final int getBytesRemainingToWrite() {\n\t\treturn toWrite;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#writeSome()\n\t */\n\tpublic final int writeSome() throws IOException {\n\t\tint wrote=channel.write(wbuf);\n\t\tassert wrote >= 0 : \"Wrote negative bytes?\";\n\t\ttoWrite -= wrote;\n\t\tassert toWrite >= 0\n\t\t\t: \"toWrite went negative after writing \" + wrote\n\t\t\t\t+ \" bytes for \" + this;\n\t\tgetLogger().debug(\"Wrote %d bytes\", wrote);\n\t\treturn wrote;\n\t}\n\n\n\tpublic final void fixupOps() {\n\t\t// As the selection key can be changed at any point due to node\n\t\t// failure, we'll grab the current volatile value and configure it.\n\t\tSelectionKey s = sk;\n\t\tif(s != null && s.isValid()) {\n\t\t\tint iops=getSelectionOps();\n\t\t\tgetLogger().debug(\"Setting interested opts to %d\", iops);\n\t\t\ts.interestOps(iops);\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selection key is not valid.\");\n\t\t}\n\t}\n}\n","lineNo":86}
{"Smelly Sample":"package net.spy.memcached.internal;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.concurrent.TimeoutException;\n\nimport net.spy.memcached.ops.Operation;\n\n/**\n * Timeout exception that tracks the original operation.\n */\npublic class CheckedOperationTimeoutException extends TimeoutException {\n\n\tprivate final Collection<Operation> operations;\n\n\t/**\n\t * Construct a CheckedOperationTimeoutException with the given message\n\t * and operation.\n\t *\n\t * @param message the message\n\t * @param op the operation that timed out\n\t */\n\tpublic CheckedOperationTimeoutException(String message, Operation op) {\n\t\tthis(message, Collections.singleton(op));\n\t}\n\n\tpublic CheckedOperationTimeoutException(String message,\n\t\t\tCollection<Operation> ops) {\n\t\tsuper(createMessage(message, ops));\n\t\toperations = ops;\n\t}\n\n\tprivate static String createMessage(String message,\n\t\t\tCollection<Operation> ops) {\n\t\tStringBuilder rv = new StringBuilder(message);\n\t\trv.append(\" - failing node\");\n\t\trv.append(ops.size() == 1 ? \": \" : \"s: \");\n\t\tboolean first = true;\n\t\tfor(Operation op : ops) {\n\t\t\tif(first) {\n\t\t\t\tfirst = false;\n\t\t\t} else {\n\t\t\t\trv.append(\", \");\n\t\t\t}\n\t\t\trv.append(op.getHandlingNode().getSocketAddress());\n\t\t}\n\t\treturn rv.toString();\n\t}\n\n\t/**\n\t * Get the operation that timed out.\n\t */\n\tpublic Collection<Operation> getOperations() {\n\t\treturn operations;\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.internal;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.concurrent.TimeoutException;\n\nimport net.spy.memcached.MemcachedNode;\nimport net.spy.memcached.ops.Operation;\n\n/**\n * Timeout exception that tracks the original operation.\n */\npublic class CheckedOperationTimeoutException extends TimeoutException {\n\n\tprivate final Collection<Operation> operations;\n\n\t/**\n\t * Construct a CheckedOperationTimeoutException with the given message\n\t * and operation.\n\t *\n\t * @param message the message\n\t * @param op the operation that timed out\n\t */\n\tpublic CheckedOperationTimeoutException(String message, Operation op) {\n\t\tthis(message, Collections.singleton(op));\n\t}\n\n\tpublic CheckedOperationTimeoutException(String message,\n\t\t\tCollection<Operation> ops) {\n\t\tsuper(createMessage(message, ops));\n\t\toperations = ops;\n\t}\n\n\tprivate static String createMessage(String message,\n\t\t\tCollection<Operation> ops) {\n\t\tStringBuilder rv = new StringBuilder(message);\n\t\trv.append(\" - failing node\");\n\t\trv.append(ops.size() == 1 ? \": \" : \"s: \");\n\t\tboolean first = true;\n\t\tfor(Operation op : ops) {\n\t\t\tif(first) {\n\t\t\t\tfirst = false;\n\t\t\t} else {\n\t\t\t\trv.append(\", \");\n\t\t\t}\n\t\t\tMemcachedNode node = op == null ? null : op.getHandlingNode();\n\t\t\trv.append(node == null ? \"<unknown>\" : node.getSocketAddress());\n\t\t}\n\t\treturn rv.toString();\n\t}\n\n\t/**\n\t * Get the operation that timed out.\n\t */\n\tpublic Collection<Operation> getOperations() {\n\t\treturn operations;\n\t}\n}\n","lineNo":46}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.transcoders.Transcoder;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tnew InetSocketAddress(\"hostname\", portNum));\n *\n *\t// Store a value (async) for one hour\n *\tc.set(\"someKey\", 3600, someObject);\n *\t// Retrieve a value.\n *\tObject myObject=c.get(\"someKey\");\n *\t<\/pre>\n *\n *\t<h2>Advanced Usage<\/h2>\n *\n *\t<p>\n *\t MemcachedClient may be processing a great deal of asynchronous messages or\n *\t possibly dealing with an unreachable memcached, which may delay processing.\n *\t If a memcached is disabled, for example, MemcachedConnection will continue\n *\t to attempt to reconnect and replay pending operations until it comes back\n *\t up.  To prevent this from causing your application to hang, you can use\n *\t one of the asynchronous mechanisms to time out a request and cancel the\n *\t operation to the server.\n *\t<\/p>\n *\n *\t<pre>\n *\t// Get a memcached client connected to several servers\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tAddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *\t// Try to get a value, for up to 5 seconds, and cancel if it doesn't return\n *\tObject myObj=null;\n *\tFuture&lt;Object&gt; f=c.asyncGet(\"someKey\");\n *\ttry {\n *\t\tmyObj=f.get(5, TimeUnit.SECONDS);\n *\t} catch(TimeoutException e) {\n *\t\t// Since we don't need this, go ahead and cancel the operation.  This\n *\t\t// is not strictly necessary, but it'll save some work on the server.\n *\t\tf.cancel();\n *\t\t// Do other timeout related stuff\n *\t}\n * <\/pre>\n */\npublic final class MemcachedClient extends SpyThread implements MemcachedClientIF {\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final long operationTimeout;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tfinal Transcoder<Object> transcoder;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param bufSize read buffer size per connection (in bytes)\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tif(cf == null) {\n\t\t\tthrow new NullPointerException(\"Connection factory required\");\n\t\t}\n\t\tif(addrs == null) {\n\t\t\tthrow new NullPointerException(\"Server list required\");\n\t\t}\n\t\tif(addrs.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"You must have at least one server to connect to\");\n\t\t}\n\t\tif(cf.getOperationTimeout() <= 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Operation timeout must be positive.\");\n\t\t}\n\t\ttranscoder=cf.getDefaultTranscoder();\n\t\topFact=cf.getOperationFactory();\n\t\tassert opFact != null : \"Connection factory failed to make op factory\";\n\t\tconn=cf.createConnection(addrs);\n\t\tassert conn != null : \"Connection factory failed to make a connection\";\n\t\toperationTimeout = cf.getOperationTimeout();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tsetDaemon(cf.isDaemon());\n\t\tstart();\n\t}\n\n\t/**\n\t * Get the addresses of available servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getAvailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the addresses of unavailable servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getUnavailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(!node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get a read-only wrapper around the node locator wrapping this instance.\n\t */\n\tpublic NodeLocator getNodeLocator() {\n\t\treturn conn.getLocator().getReadonlyCopy();\n\t}\n\n\t/**\n\t * Get the default transcoder that's in use.\n\t */\n\tpublic Transcoder<Object> getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tbyte[] keyBytes=KeyUtil.getKeyBytes(key);\n\t\tif(keyBytes.length > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\tif(keyBytes.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Key must contain at least one character.\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(byte b : keyBytes) {\n\t\t\tif(b == ' ' || b == '\\n' || b == '\\r' || b == 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void checkState() {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tvalidateKey(key);\n\t\tcheckState();\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, true);\n\t}\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of);\n\t}\n\n\tprivate <T> Future<Boolean> asyncStore(StoreType storeType, String key,\n\t\t\t\t\t\t   int exp, T value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\treturn asyncStore(storeType, key, exp, value, transcoder);\n\t}\n\n\tprivate <T> Future<Boolean> asyncCat(\n\t\t\tConcatenationType catType, long cas, String key,\n\t\t\tT value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.cat(catType, cas, key, co.getData(),\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\trv.set(val.isSuccess());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> append(long cas, String key, Object val) {\n\t\treturn append(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> append(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.append, cas, key, val, tc);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> prepend(long cas, String key, Object val) {\n\t\treturn prepend(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> prepend(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n\t}\n\n\t/**\n     * Asynchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a future that will indicate the status of the CAS\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return asyncCAS(key, casId, 0, value, tc);\n\t}\n\n\t/**\n\t * Asynchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASResponse> asyncCAS(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASResponse> rv=new OperationFuture<CASResponse>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op=opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n\t\t\t\tco.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\tif(val instanceof CASOperationStatus) {\n\t\t\t\t\t\t\trv.set(((CASOperationStatus)val).getCASResponse());\n\t\t\t\t\t\t} else if(val instanceof CancelledOperationStatus) {\n\t\t\t\t\t\t\t// Cancelled, ignore and let it float up\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\"Unhandled state: \" + val);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asynchronous CAS operation using the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n\t\treturn asyncCAS(key, casId, value, transcoder);\n\t}\n\n\t/**\n     * Perform a synchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a CASResponse\n     * @throws OperationTimeoutException if global operation timeout is\n     *         exceeded\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> CASResponse cas(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return cas(key, casId, 0, value, tc);\n    }\n\n\t/**\n\t * Perform a synchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if global operation timeout is\n\t *         exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASResponse cas(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n\t\t\t\t\tTimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Perform a synchronous CAS operation with the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASResponse cas(String key, long casId, Object value) {\n\t\treturn cas(key, casId, value, transcoder);\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> add(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Add an object to the cache (using the default transcoder)\n\t * iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> set(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Set an object in the cache (using the default transcoder)\n\t * regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> replace(String key, int exp, T o,\n\t\tTranscoder<T> tc) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Replace an object with the given value (transcoded with the default\n\t * transcoder) iff there is already a value for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal GetFuture<T> rv=new GetFuture<T>(tc, latch, operationTimeout);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate CachedData val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=new CachedData(flags, data, tc.getMaxSize());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the given key asynchronously and decode with the default\n\t * transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\t\treturn asyncGet(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASValue<T>> asyncGets(final String key,\n\t\t\tfinal Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASValue<T>> rv=\n\t\t\tnew OperationFuture<CASValue<T>>(latch, operationTimeout);\n\n\t\tOperation op=opFact.gets(key,\n\t\t\t\tnew GetsOperation.Callback() {\n\t\t\tprivate CASValue<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, long cas, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tassert cas > 0 : \"CAS was less than zero:  \" + cas;\n\t\t\t\tval=new CASValue<T>(cas, tc.decode(\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously and decode using\n\t * the default transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASValue<Object>> asyncGets(final String key) {\n\t\treturn asyncGets(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if global operation timeout is\n\t * \t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGets(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASValue<Object> gets(String key) {\n\t\treturn gets(key, transcoder);\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> T get(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGet(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get with a single key and decode using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Object get(String key) {\n\t\treturn get(key, transcoder);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Collection<String> keys,\n\t\tfinal Transcoder<T> tc) {\n\t\tfinal Map<String, CachedData> m=new ConcurrentHashMap<String, CachedData>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, new CachedData(flags, data, tc.getMaxSize()));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\n\t\t// Now that we know how many servers it breaks down into, and the latch\n\t\t// is all set up, convert all of these strings collections to operations\n\t\tfinal Map<MemcachedNode, Operation> mops=\n\t\t\tnew HashMap<MemcachedNode, Operation>();\n\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tOperation op=opFact.get(me.getValue(), cb);\n\t\t\tmops.put(me.getKey(), op);\n\t\t\tops.add(op);\n\t\t}\n\t\tassert mops.size() == chunks.size();\n\t\tcheckState();\n\t\tconn.addOperations(mops);\n\t\treturn new BulkGetFuture<T>(tc, m, ops, latch);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache and decode them\n\t * with the given transcoder.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\treturn asyncGetBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n\t\tString... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Collection<String> keys,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\"Timeout waiting for bulkvalues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\treturn getBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\t/**\n\t * Get a set of stats from all connections.\n\t *\n\t * @param arg which stats to get\n\t * @return a Map of the server SocketAddress to a map of String stat\n\t *\t\t   keys to String stat values.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:\t%s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol\n\t\t\t\t\t\t// Allows us to avoid string processing.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tif (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Mutate operation timed out, unable to modify counter [\"\n\t\t\t\t\t\t+ key + \"]\");\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, exp);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, exp);\n\t}\n\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, exp, String.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, exp);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Timeout waiting to mutate or init value\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Future<Long> asyncMutate(Mutator m, String key, int by, long def,\n\t\t\tint exp) {\n\t\tfinal CountDownLatch latch = new CountDownLatch(1);\n\t\tfinal OperationFuture<Long> rv = new OperationFuture<Long>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op = addOp(key, opFact.mutate(m, key, by, def, exp,\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\trv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}));\n\t\trv.setOperation(op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asychronous increment.\n\t *\n\t * @return a future with the incremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncIncr(String key, int by) {\n\t\treturn asyncMutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Asynchronous decrement.\n\t *\n\t * @return a future with the decremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncDecr(String key, int by) {\n\t\treturn asyncMutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * <p>\n\t * The hold argument specifies the amount of time in seconds (or Unix time\n\t * until which) the client wishes the server to refuse \"add\" and \"replace\"\n\t * commands with this key. For this amount of item, the item is put into a\n\t * delete queue, which means that it won't possible to retrieve it by the\n\t * \"get\" command, but \"add\" and \"replace\" command with this key will also\n\t * fail (the \"set\" command will succeed, however). After the time passes,\n\t * the item is finally deleted from server memory.\n\t * <\/p>\n\t *\n\t * @param key the key to delete\n\t * @param hold how long the key should be unavailable to add commands\n\t *\n\t * @deprecated Hold values are no longer honored.\n\t */\n\t@Deprecated\n\tpublic Future<Boolean> delete(String key, int hold) {\n\t\treturn delete(key);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\toperationTimeout);\n\t\tDeleteOperation op=opFact.delete(key,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult,\n\t\t\t\toperationTimeout) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\tprivate void logRunException(Exception e) {\n\t\tif(shuttingDown) {\n\t\t\t// There are a couple types of errors that occur during the\n\t\t\t// shutdown sequence that are considered OK.  Log at debug.\n\t\t\tgetLogger().debug(\"Exception occurred during shutdown\", e);\n\t\t} else {\n\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(CancelledKeyException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(ClosedSelectorException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(IllegalStateException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\t// Guard against double shutdowns (bug 8).\n\t\tif(shuttingDown) {\n\t\t\tgetLogger().info(\"Suppressing duplicate attempt to shut down\");\n\t\t\treturn false;\n\t\t}\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t *\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, false);\n\t\ttry {\n\t\t\t// XXX:  Perhaps IllegalStateException should be caught here\n\t\t\t// and the check retried.\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return true if the observer was added.\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn conn.addObserver(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed, but no longer does\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn conn.removeObserver(obs);\n\t}\n\n\tstatic class BulkGetFuture<T> implements Future<Map<String, T>> {\n\t\tprivate final Transcoder<T> tc;\n\t\tprivate final Map<String, CachedData> rvMap;\n\t\tprivate final Collection<Operation> ops;\n\t\tprivate final CountDownLatch latch;\n\t\tprivate boolean cancelled=false;\n\n\t\tpublic BulkGetFuture(Transcoder<T> tc, Map<String, CachedData> m,\n\t\t\t\tCollection<Operation> getOps, CountDownLatch l) {\n\t\t\tsuper();\n\t\t\tthis.tc = tc;\n\t\t\trvMap = m;\n\t\t\tops = getOps;\n\t\t\tlatch=l;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tboolean rv=false;\n\t\t\tfor(Operation op : ops) {\n\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t\tcancelled=true;\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic Map<String, T> get()\n\t\t\tthrows InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\"Timed out waiting forever\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic Map<String, T> get(long timeout, TimeUnit unit)\n\t\t\tthrows InterruptedException,\n\t\t\tExecutionException, TimeoutException {\n\t\t\tif(!latch.await(timeout, unit)) {\n\t\t\t\tthrow new TimeoutException(\"Operation timed out.\");\n\t\t\t}\n\t\t\tfor(Operation op : ops) {\n\t\t\t\tif(op.isCancelled()) {\n\t\t\t\t\tthrow new ExecutionException(\n\t\t\t\t\t\t\tnew RuntimeException(\"Cancelled\"));\n\t\t\t\t}\n\t\t\t\tif(op.hasErrored()) {\n\t\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t\t}\n\t\t\t}\n\t\t\tMap<String, T> m = new HashMap<String, T>();\n\t\t\tfor (Map.Entry<String, CachedData> me : rvMap.entrySet()) {\n\t\t\t\tT val = tc.decode(me.getValue());\n\t\t\t\t// val may be null if the transcoder did not understand\n\t\t\t\t// the value.\n\t\t\t\tif(val != null) {\n\t\t\t\t\tm.put(me.getKey(), val);\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn m;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn cancelled;\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn latch.getCount() == 0;\n\t\t}\n\t}\n\n\tstatic class OperationFuture<T> implements Future<T> {\n\n\t\tprivate final CountDownLatch latch;\n\t\tprivate final AtomicReference<T> objRef;\n\t\tprivate Operation op;\n\t\tprivate final long globalOperationTimeout;\n\n\t\tpublic OperationFuture(CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis(l, new AtomicReference<T>(null), globalOperationTimeout);\n\t\t}\n\n\t\tpublic OperationFuture(CountDownLatch l, AtomicReference<T> oref,\n\t\t\tlong timeout) {\n\t\t\tsuper();\n\t\t\tlatch=l;\n\t\t\tobjRef=oref;\n\t\t\tglobalOperationTimeout = timeout;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tassert op != null : \"No operation\";\n\t\t\top.cancel();\n\t\t\t// This isn't exactly correct, but it's close enough.  If we're in\n\t\t\t// a writing state, we *probably* haven't started.\n\t\t\treturn op.getState() == OperationState.WRITING;\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(globalOperationTimeout, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Timed out waiting for operation\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\tif(!latch.await(duration, units)) {\n\t\t\t\tthrow new TimeoutException(\"Timed out waiting for operation\");\n\t\t\t}\n\t\t\tif(op != null && op.hasErrored()) {\n\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t}\n\t\t\tif(isCancelled()) {\n\t\t\t\tthrow new ExecutionException(new RuntimeException(\"Cancelled\"));\n\t\t\t}\n\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tvoid set(T o) {\n\t\t\tobjRef.set(o);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\top=to;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn op.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn latch.getCount() == 0 ||\n\t\t\t\top.isCancelled() || op.getState() == OperationState.COMPLETE;\n\t\t}\n\n\t}\n\n\tstatic class GetFuture<T> implements Future<T> {\n\t\tprivate final Transcoder<T> tc;\n\t\tprivate final OperationFuture<CachedData> rv;\n\n\t\tpublic GetFuture(Transcoder<T> tc, CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis.tc = tc;\n\t\t\tthis.rv = new OperationFuture<CachedData>(l, globalOperationTimeout);\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\treturn rv.cancel(ign);\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\treturn decode(rv.get());\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\treturn decode(rv.get(duration, units));\n\t\t}\n\n\t\tprivate T decode(CachedData d) {\n\t\t\treturn d == null ? null : tc.decode(d);\n\t\t}\n\n\t\tvoid set(CachedData d) {\n\t\t\trv.set(d);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\trv.setOperation(to);\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn rv.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn rv.isDone();\n\t\t}\n\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tnew InetSocketAddress(\"hostname\", portNum));\n *\n *\t// Store a value (async) for one hour\n *\tc.set(\"someKey\", 3600, someObject);\n *\t// Retrieve a value.\n *\tObject myObject=c.get(\"someKey\");\n *\t<\/pre>\n *\n *\t<h2>Advanced Usage<\/h2>\n *\n *\t<p>\n *\t MemcachedClient may be processing a great deal of asynchronous messages or\n *\t possibly dealing with an unreachable memcached, which may delay processing.\n *\t If a memcached is disabled, for example, MemcachedConnection will continue\n *\t to attempt to reconnect and replay pending operations until it comes back\n *\t up.  To prevent this from causing your application to hang, you can use\n *\t one of the asynchronous mechanisms to time out a request and cancel the\n *\t operation to the server.\n *\t<\/p>\n *\n *\t<pre>\n *\t// Get a memcached client connected to several servers\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tAddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *\t// Try to get a value, for up to 5 seconds, and cancel if it doesn't return\n *\tObject myObj=null;\n *\tFuture&lt;Object&gt; f=c.asyncGet(\"someKey\");\n *\ttry {\n *\t\tmyObj=f.get(5, TimeUnit.SECONDS);\n *\t} catch(TimeoutException e) {\n *\t\t// Since we don't need this, go ahead and cancel the operation.  This\n *\t\t// is not strictly necessary, but it'll save some work on the server.\n *\t\tf.cancel();\n *\t\t// Do other timeout related stuff\n *\t}\n * <\/pre>\n */\npublic final class MemcachedClient extends SpyThread implements MemcachedClientIF {\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final long operationTimeout;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tfinal Transcoder<Object> transcoder;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param bufSize read buffer size per connection (in bytes)\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tif(cf == null) {\n\t\t\tthrow new NullPointerException(\"Connection factory required\");\n\t\t}\n\t\tif(addrs == null) {\n\t\t\tthrow new NullPointerException(\"Server list required\");\n\t\t}\n\t\tif(addrs.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"You must have at least one server to connect to\");\n\t\t}\n\t\tif(cf.getOperationTimeout() <= 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Operation timeout must be positive.\");\n\t\t}\n\t\ttranscoder=cf.getDefaultTranscoder();\n\t\topFact=cf.getOperationFactory();\n\t\tassert opFact != null : \"Connection factory failed to make op factory\";\n\t\tconn=cf.createConnection(addrs);\n\t\tassert conn != null : \"Connection factory failed to make a connection\";\n\t\toperationTimeout = cf.getOperationTimeout();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tsetDaemon(cf.isDaemon());\n\t\tstart();\n\t}\n\n\t/**\n\t * Get the addresses of available servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getAvailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the addresses of unavailable servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getUnavailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(!node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get a read-only wrapper around the node locator wrapping this instance.\n\t */\n\tpublic NodeLocator getNodeLocator() {\n\t\treturn conn.getLocator().getReadonlyCopy();\n\t}\n\n\t/**\n\t * Get the default transcoder that's in use.\n\t */\n\tpublic Transcoder<Object> getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tbyte[] keyBytes=KeyUtil.getKeyBytes(key);\n\t\tif(keyBytes.length > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\tif(keyBytes.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Key must contain at least one character.\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(byte b : keyBytes) {\n\t\t\tif(b == ' ' || b == '\\n' || b == '\\r' || b == 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void checkState() {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tvalidateKey(key);\n\t\tcheckState();\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, true);\n\t}\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of);\n\t}\n\n\tprivate <T> Future<Boolean> asyncStore(StoreType storeType, String key,\n\t\t\t\t\t\t   int exp, T value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\treturn asyncStore(storeType, key, exp, value, transcoder);\n\t}\n\n\tprivate <T> Future<Boolean> asyncCat(\n\t\t\tConcatenationType catType, long cas, String key,\n\t\t\tT value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.cat(catType, cas, key, co.getData(),\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\trv.set(val.isSuccess());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> append(long cas, String key, Object val) {\n\t\treturn append(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> append(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.append, cas, key, val, tc);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> prepend(long cas, String key, Object val) {\n\t\treturn prepend(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> prepend(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n\t}\n\n\t/**\n     * Asynchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a future that will indicate the status of the CAS\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return asyncCAS(key, casId, 0, value, tc);\n\t}\n\n\t/**\n\t * Asynchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASResponse> asyncCAS(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASResponse> rv=new OperationFuture<CASResponse>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op=opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n\t\t\t\tco.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\tif(val instanceof CASOperationStatus) {\n\t\t\t\t\t\t\trv.set(((CASOperationStatus)val).getCASResponse());\n\t\t\t\t\t\t} else if(val instanceof CancelledOperationStatus) {\n\t\t\t\t\t\t\t// Cancelled, ignore and let it float up\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\"Unhandled state: \" + val);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asynchronous CAS operation using the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n\t\treturn asyncCAS(key, casId, value, transcoder);\n\t}\n\n\t/**\n     * Perform a synchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a CASResponse\n     * @throws OperationTimeoutException if global operation timeout is\n     *         exceeded\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> CASResponse cas(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return cas(key, casId, 0, value, tc);\n    }\n\n\t/**\n\t * Perform a synchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if global operation timeout is\n\t *         exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASResponse cas(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n\t\t\t\t\tTimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Perform a synchronous CAS operation with the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASResponse cas(String key, long casId, Object value) {\n\t\treturn cas(key, casId, value, transcoder);\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> add(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Add an object to the cache (using the default transcoder)\n\t * iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> set(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Set an object in the cache (using the default transcoder)\n\t * regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> replace(String key, int exp, T o,\n\t\tTranscoder<T> tc) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Replace an object with the given value (transcoded with the default\n\t * transcoder) iff there is already a value for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal GetFuture<T> rv=new GetFuture<T>(latch, operationTimeout);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate Future<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=TranscodeService.getInstance().decode(tc,\n\t\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize()));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the given key asynchronously and decode with the default\n\t * transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\t\treturn asyncGet(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASValue<T>> asyncGets(final String key,\n\t\t\tfinal Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASValue<T>> rv=\n\t\t\tnew OperationFuture<CASValue<T>>(latch, operationTimeout);\n\n\t\tOperation op=opFact.gets(key,\n\t\t\t\tnew GetsOperation.Callback() {\n\t\t\tprivate CASValue<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, long cas, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tassert cas > 0 : \"CAS was less than zero:  \" + cas;\n\t\t\t\tval=new CASValue<T>(cas, tc.decode(\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously and decode using\n\t * the default transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASValue<Object>> asyncGets(final String key) {\n\t\treturn asyncGets(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if global operation timeout is\n\t * \t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGets(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASValue<Object> gets(String key) {\n\t\treturn gets(key, transcoder);\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> T get(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGet(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get with a single key and decode using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Object get(String key) {\n\t\treturn get(key, transcoder);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Collection<String> keys,\n\t\tfinal Transcoder<T> tc) {\n\t\tfinal Map<String, Future<T>> m=new ConcurrentHashMap<String, Future<T>>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, TranscodeService.getInstance().decode(tc,\n\t\t\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\n\t\t// Now that we know how many servers it breaks down into, and the latch\n\t\t// is all set up, convert all of these strings collections to operations\n\t\tfinal Map<MemcachedNode, Operation> mops=\n\t\t\tnew HashMap<MemcachedNode, Operation>();\n\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tOperation op=opFact.get(me.getValue(), cb);\n\t\t\tmops.put(me.getKey(), op);\n\t\t\tops.add(op);\n\t\t}\n\t\tassert mops.size() == chunks.size();\n\t\tcheckState();\n\t\tconn.addOperations(mops);\n\t\treturn new BulkGetFuture<T>(m, ops, latch);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache and decode them\n\t * with the given transcoder.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\treturn asyncGetBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n\t\tString... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Collection<String> keys,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\"Timeout waiting for bulkvalues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\treturn getBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\t/**\n\t * Get a set of stats from all connections.\n\t *\n\t * @param arg which stats to get\n\t * @return a Map of the server SocketAddress to a map of String stat\n\t *\t\t   keys to String stat values.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:\t%s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol\n\t\t\t\t\t\t// Allows us to avoid string processing.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tif (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Mutate operation timed out, unable to modify counter [\"\n\t\t\t\t\t\t+ key + \"]\");\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, exp);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, exp);\n\t}\n\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, exp, String.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, exp);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Timeout waiting to mutate or init value\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Future<Long> asyncMutate(Mutator m, String key, int by, long def,\n\t\t\tint exp) {\n\t\tfinal CountDownLatch latch = new CountDownLatch(1);\n\t\tfinal OperationFuture<Long> rv = new OperationFuture<Long>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op = addOp(key, opFact.mutate(m, key, by, def, exp,\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\trv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}));\n\t\trv.setOperation(op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asychronous increment.\n\t *\n\t * @return a future with the incremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncIncr(String key, int by) {\n\t\treturn asyncMutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Asynchronous decrement.\n\t *\n\t * @return a future with the decremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncDecr(String key, int by) {\n\t\treturn asyncMutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * <p>\n\t * The hold argument specifies the amount of time in seconds (or Unix time\n\t * until which) the client wishes the server to refuse \"add\" and \"replace\"\n\t * commands with this key. For this amount of item, the item is put into a\n\t * delete queue, which means that it won't possible to retrieve it by the\n\t * \"get\" command, but \"add\" and \"replace\" command with this key will also\n\t * fail (the \"set\" command will succeed, however). After the time passes,\n\t * the item is finally deleted from server memory.\n\t * <\/p>\n\t *\n\t * @param key the key to delete\n\t * @param hold how long the key should be unavailable to add commands\n\t *\n\t * @deprecated Hold values are no longer honored.\n\t */\n\t@Deprecated\n\tpublic Future<Boolean> delete(String key, int hold) {\n\t\treturn delete(key);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\toperationTimeout);\n\t\tDeleteOperation op=opFact.delete(key,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult,\n\t\t\t\toperationTimeout) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\tprivate void logRunException(Exception e) {\n\t\tif(shuttingDown) {\n\t\t\t// There are a couple types of errors that occur during the\n\t\t\t// shutdown sequence that are considered OK.  Log at debug.\n\t\t\tgetLogger().debug(\"Exception occurred during shutdown\", e);\n\t\t} else {\n\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(CancelledKeyException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(ClosedSelectorException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(IllegalStateException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\t// Guard against double shutdowns (bug 8).\n\t\tif(shuttingDown) {\n\t\t\tgetLogger().info(\"Suppressing duplicate attempt to shut down\");\n\t\t\treturn false;\n\t\t}\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t *\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, false);\n\t\ttry {\n\t\t\t// XXX:  Perhaps IllegalStateException should be caught here\n\t\t\t// and the check retried.\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return true if the observer was added.\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn conn.addObserver(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed, but no longer does\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn conn.removeObserver(obs);\n\t}\n\n\tstatic class BulkGetFuture<T> implements Future<Map<String, T>> {\n\t\tprivate final Map<String, Future<T>> rvMap;\n\t\tprivate final Collection<Operation> ops;\n\t\tprivate final CountDownLatch latch;\n\t\tprivate boolean cancelled=false;\n\n\t\tpublic BulkGetFuture(Map<String, Future<T>> m,\n\t\t\t\tCollection<Operation> getOps, CountDownLatch l) {\n\t\t\tsuper();\n\t\t\trvMap = m;\n\t\t\tops = getOps;\n\t\t\tlatch=l;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tboolean rv=false;\n\t\t\tfor(Operation op : ops) {\n\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t\tfor (Future<T> v : rvMap.values()) {\n\t\t\t\tv.cancel(ign);\n\t\t\t}\n\t\t\tcancelled=true;\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic Map<String, T> get()\n\t\t\tthrows InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\"Timed out waiting forever\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic Map<String, T> get(long timeout, TimeUnit unit)\n\t\t\tthrows InterruptedException,\n\t\t\tExecutionException, TimeoutException {\n\t\t\tif(!latch.await(timeout, unit)) {\n\t\t\t\tthrow new TimeoutException(\"Operation timed out.\");\n\t\t\t}\n\t\t\tfor(Operation op : ops) {\n\t\t\t\tif(op.isCancelled()) {\n\t\t\t\t\tthrow new ExecutionException(\n\t\t\t\t\t\t\tnew RuntimeException(\"Cancelled\"));\n\t\t\t\t}\n\t\t\t\tif(op.hasErrored()) {\n\t\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t\t}\n\t\t\t}\n\t\t\tMap<String, T> m = new HashMap<String, T>();\n\t\t\tfor (Map.Entry<String, Future<T>> me : rvMap.entrySet()) {\n\t\t\t\tm.put(me.getKey(), me.getValue().get());\n\t\t\t}\n\t\t\treturn m;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn cancelled;\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn latch.getCount() == 0;\n\t\t}\n\t}\n\n\tstatic class OperationFuture<T> implements Future<T> {\n\n\t\tprivate final CountDownLatch latch;\n\t\tprivate final AtomicReference<T> objRef;\n\t\tprivate Operation op;\n\t\tprivate final long globalOperationTimeout;\n\n\t\tpublic OperationFuture(CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis(l, new AtomicReference<T>(null), globalOperationTimeout);\n\t\t}\n\n\t\tpublic OperationFuture(CountDownLatch l, AtomicReference<T> oref,\n\t\t\tlong timeout) {\n\t\t\tsuper();\n\t\t\tlatch=l;\n\t\t\tobjRef=oref;\n\t\t\tglobalOperationTimeout = timeout;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tassert op != null : \"No operation\";\n\t\t\top.cancel();\n\t\t\t// This isn't exactly correct, but it's close enough.  If we're in\n\t\t\t// a writing state, we *probably* haven't started.\n\t\t\treturn op.getState() == OperationState.WRITING;\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(globalOperationTimeout, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Timed out waiting for operation\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\tif(!latch.await(duration, units)) {\n\t\t\t\tthrow new TimeoutException(\"Timed out waiting for operation\");\n\t\t\t}\n\t\t\tif(op != null && op.hasErrored()) {\n\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t}\n\t\t\tif(isCancelled()) {\n\t\t\t\tthrow new ExecutionException(new RuntimeException(\"Cancelled\"));\n\t\t\t}\n\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tvoid set(T o) {\n\t\t\tobjRef.set(o);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\top=to;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn op.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn latch.getCount() == 0 ||\n\t\t\t\top.isCancelled() || op.getState() == OperationState.COMPLETE;\n\t\t}\n\n\t}\n\n\tstatic class GetFuture<T> implements Future<T> {\n\t\tprivate final OperationFuture<Future<T>> rv;\n\n\t\tpublic GetFuture(CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis.rv = new OperationFuture<Future<T>>(l, globalOperationTimeout);\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\treturn rv.cancel(ign);\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\tFuture<T> v = rv.get();\n\t\t\treturn v == null ? null : v.get();\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\tFuture<T> v = rv.get(duration, units);\n\t\t\treturn v == null ? null : v.get();\n\t\t}\n\n\t\tvoid set(Future<T> d) {\n\t\t\trv.set(d);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\trv.setOperation(to);\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn rv.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn rv.isDone();\n\t\t}\n\t}\n}\n","lineNo":1700}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.transcoders.Transcoder;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tnew InetSocketAddress(\"hostname\", portNum));\n *\n *\t// Store a value (async) for one hour\n *\tc.set(\"someKey\", 3600, someObject);\n *\t// Retrieve a value.\n *\tObject myObject=c.get(\"someKey\");\n *\t<\/pre>\n *\n *\t<h2>Advanced Usage<\/h2>\n *\n *\t<p>\n *\t MemcachedClient may be processing a great deal of asynchronous messages or\n *\t possibly dealing with an unreachable memcached, which may delay processing.\n *\t If a memcached is disabled, for example, MemcachedConnection will continue\n *\t to attempt to reconnect and replay pending operations until it comes back\n *\t up.  To prevent this from causing your application to hang, you can use\n *\t one of the asynchronous mechanisms to time out a request and cancel the\n *\t operation to the server.\n *\t<\/p>\n *\n *\t<pre>\n *\t// Get a memcached client connected to several servers\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tAddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *\t// Try to get a value, for up to 5 seconds, and cancel if it doesn't return\n *\tObject myObj=null;\n *\tFuture&lt;Object&gt; f=c.asyncGet(\"someKey\");\n *\ttry {\n *\t\tmyObj=f.get(5, TimeUnit.SECONDS);\n *\t} catch(TimeoutException e) {\n *\t\t// Since we don't need this, go ahead and cancel the operation.  This\n *\t\t// is not strictly necessary, but it'll save some work on the server.\n *\t\tf.cancel();\n *\t\t// Do other timeout related stuff\n *\t}\n * <\/pre>\n */\npublic final class MemcachedClient extends SpyThread implements MemcachedClientIF {\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final long operationTimeout;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tfinal Transcoder<Object> transcoder;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param bufSize read buffer size per connection (in bytes)\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tif(cf == null) {\n\t\t\tthrow new NullPointerException(\"Connection factory required\");\n\t\t}\n\t\tif(addrs == null) {\n\t\t\tthrow new NullPointerException(\"Server list required\");\n\t\t}\n\t\tif(addrs.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"You must have at least one server to connect to\");\n\t\t}\n\t\tif(cf.getOperationTimeout() <= 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Operation timeout must be positive.\");\n\t\t}\n\t\ttranscoder=cf.getDefaultTranscoder();\n\t\topFact=cf.getOperationFactory();\n\t\tassert opFact != null : \"Connection factory failed to make op factory\";\n\t\tconn=cf.createConnection(addrs);\n\t\tassert conn != null : \"Connection factory failed to make a connection\";\n\t\toperationTimeout = cf.getOperationTimeout();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tsetDaemon(cf.isDaemon());\n\t\tstart();\n\t}\n\n\t/**\n\t * Get the addresses of available servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getAvailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the addresses of unavailable servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getUnavailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(!node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get a read-only wrapper around the node locator wrapping this instance.\n\t */\n\tpublic NodeLocator getNodeLocator() {\n\t\treturn conn.getLocator().getReadonlyCopy();\n\t}\n\n\t/**\n\t * Get the default transcoder that's in use.\n\t */\n\tpublic Transcoder<Object> getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tbyte[] keyBytes=KeyUtil.getKeyBytes(key);\n\t\tif(keyBytes.length > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\tif(keyBytes.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Key must contain at least one character.\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(byte b : keyBytes) {\n\t\t\tif(b == ' ' || b == '\\n' || b == '\\r' || b == 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void checkState() {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tvalidateKey(key);\n\t\tcheckState();\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, true);\n\t}\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of);\n\t}\n\n\tprivate <T> Future<Boolean> asyncStore(StoreType storeType, String key,\n\t\t\t\t\t\t   int exp, T value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\treturn asyncStore(storeType, key, exp, value, transcoder);\n\t}\n\n\tprivate <T> Future<Boolean> asyncCat(\n\t\t\tConcatenationType catType, long cas, String key,\n\t\t\tT value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.cat(catType, cas, key, co.getData(),\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\trv.set(val.isSuccess());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> append(long cas, String key, Object val) {\n\t\treturn append(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> append(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.append, cas, key, val, tc);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> prepend(long cas, String key, Object val) {\n\t\treturn prepend(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> prepend(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n\t}\n\n\t/**\n     * Asynchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a future that will indicate the status of the CAS\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return asyncCAS(key, casId, 0, value, tc);\n\t}\n\n\t/**\n\t * Asynchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASResponse> asyncCAS(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASResponse> rv=new OperationFuture<CASResponse>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op=opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n\t\t\t\tco.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\tif(val instanceof CASOperationStatus) {\n\t\t\t\t\t\t\trv.set(((CASOperationStatus)val).getCASResponse());\n\t\t\t\t\t\t} else if(val instanceof CancelledOperationStatus) {\n\t\t\t\t\t\t\t// Cancelled, ignore and let it float up\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\"Unhandled state: \" + val);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asynchronous CAS operation using the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n\t\treturn asyncCAS(key, casId, value, transcoder);\n\t}\n\n\t/**\n     * Perform a synchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a CASResponse\n     * @throws OperationTimeoutException if global operation timeout is\n     *         exceeded\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> CASResponse cas(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return cas(key, casId, 0, value, tc);\n    }\n\n\t/**\n\t * Perform a synchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if global operation timeout is\n\t *         exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASResponse cas(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n\t\t\t\t\tTimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Perform a synchronous CAS operation with the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASResponse cas(String key, long casId, Object value) {\n\t\treturn cas(key, casId, value, transcoder);\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> add(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Add an object to the cache (using the default transcoder)\n\t * iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> set(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Set an object in the cache (using the default transcoder)\n\t * regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> replace(String key, int exp, T o,\n\t\tTranscoder<T> tc) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Replace an object with the given value (transcoded with the default\n\t * transcoder) iff there is already a value for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal GetFuture<T> rv=new GetFuture<T>(tc, latch, operationTimeout);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate CachedData val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=new CachedData(flags, data, tc.getMaxSize());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the given key asynchronously and decode with the default\n\t * transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\t\treturn asyncGet(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASValue<T>> asyncGets(final String key,\n\t\t\tfinal Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASValue<T>> rv=\n\t\t\tnew OperationFuture<CASValue<T>>(latch, operationTimeout);\n\n\t\tOperation op=opFact.gets(key,\n\t\t\t\tnew GetsOperation.Callback() {\n\t\t\tprivate CASValue<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, long cas, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tassert cas > 0 : \"CAS was less than zero:  \" + cas;\n\t\t\t\tval=new CASValue<T>(cas, tc.decode(\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously and decode using\n\t * the default transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASValue<Object>> asyncGets(final String key) {\n\t\treturn asyncGets(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if global operation timeout is\n\t * \t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGets(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASValue<Object> gets(String key) {\n\t\treturn gets(key, transcoder);\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> T get(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGet(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get with a single key and decode using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Object get(String key) {\n\t\treturn get(key, transcoder);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Collection<String> keys,\n\t\tfinal Transcoder<T> tc) {\n\t\tfinal Map<String, CachedData> m=new ConcurrentHashMap<String, CachedData>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, new CachedData(flags, data, tc.getMaxSize()));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\n\t\t// Now that we know how many servers it breaks down into, and the latch\n\t\t// is all set up, convert all of these strings collections to operations\n\t\tfinal Map<MemcachedNode, Operation> mops=\n\t\t\tnew HashMap<MemcachedNode, Operation>();\n\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tOperation op=opFact.get(me.getValue(), cb);\n\t\t\tmops.put(me.getKey(), op);\n\t\t\tops.add(op);\n\t\t}\n\t\tassert mops.size() == chunks.size();\n\t\tcheckState();\n\t\tconn.addOperations(mops);\n\t\treturn new BulkGetFuture<T>(tc, m, ops, latch);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache and decode them\n\t * with the given transcoder.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\treturn asyncGetBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n\t\tString... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Collection<String> keys,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\"Timeout waiting for bulkvalues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\treturn getBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\t/**\n\t * Get a set of stats from all connections.\n\t *\n\t * @param arg which stats to get\n\t * @return a Map of the server SocketAddress to a map of String stat\n\t *\t\t   keys to String stat values.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:\t%s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol\n\t\t\t\t\t\t// Allows us to avoid string processing.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tif (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Mutate operation timed out, unable to modify counter [\"\n\t\t\t\t\t\t+ key + \"]\");\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, exp);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, exp);\n\t}\n\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, exp, String.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, exp);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Timeout waiting to mutate or init value\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Future<Long> asyncMutate(Mutator m, String key, int by, long def,\n\t\t\tint exp) {\n\t\tfinal CountDownLatch latch = new CountDownLatch(1);\n\t\tfinal OperationFuture<Long> rv = new OperationFuture<Long>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op = addOp(key, opFact.mutate(m, key, by, def, exp,\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\trv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}));\n\t\trv.setOperation(op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asychronous increment.\n\t *\n\t * @return a future with the incremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncIncr(String key, int by) {\n\t\treturn asyncMutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Asynchronous decrement.\n\t *\n\t * @return a future with the decremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncDecr(String key, int by) {\n\t\treturn asyncMutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * <p>\n\t * The hold argument specifies the amount of time in seconds (or Unix time\n\t * until which) the client wishes the server to refuse \"add\" and \"replace\"\n\t * commands with this key. For this amount of item, the item is put into a\n\t * delete queue, which means that it won't possible to retrieve it by the\n\t * \"get\" command, but \"add\" and \"replace\" command with this key will also\n\t * fail (the \"set\" command will succeed, however). After the time passes,\n\t * the item is finally deleted from server memory.\n\t * <\/p>\n\t *\n\t * @param key the key to delete\n\t * @param hold how long the key should be unavailable to add commands\n\t *\n\t * @deprecated Hold values are no longer honored.\n\t */\n\t@Deprecated\n\tpublic Future<Boolean> delete(String key, int hold) {\n\t\treturn delete(key);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\toperationTimeout);\n\t\tDeleteOperation op=opFact.delete(key,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult,\n\t\t\t\toperationTimeout) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\tprivate void logRunException(Exception e) {\n\t\tif(shuttingDown) {\n\t\t\t// There are a couple types of errors that occur during the\n\t\t\t// shutdown sequence that are considered OK.  Log at debug.\n\t\t\tgetLogger().debug(\"Exception occurred during shutdown\", e);\n\t\t} else {\n\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(CancelledKeyException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(ClosedSelectorException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(IllegalStateException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\t// Guard against double shutdowns (bug 8).\n\t\tif(shuttingDown) {\n\t\t\tgetLogger().info(\"Suppressing duplicate attempt to shut down\");\n\t\t\treturn false;\n\t\t}\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t *\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, false);\n\t\ttry {\n\t\t\t// XXX:  Perhaps IllegalStateException should be caught here\n\t\t\t// and the check retried.\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return true if the observer was added.\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn conn.addObserver(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed, but no longer does\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn conn.removeObserver(obs);\n\t}\n\n\tstatic class BulkGetFuture<T> implements Future<Map<String, T>> {\n\t\tprivate final Transcoder<T> tc;\n\t\tprivate final Map<String, CachedData> rvMap;\n\t\tprivate final Collection<Operation> ops;\n\t\tprivate final CountDownLatch latch;\n\t\tprivate boolean cancelled=false;\n\n\t\tpublic BulkGetFuture(Transcoder<T> tc, Map<String, CachedData> m,\n\t\t\t\tCollection<Operation> getOps, CountDownLatch l) {\n\t\t\tsuper();\n\t\t\tthis.tc = tc;\n\t\t\trvMap = m;\n\t\t\tops = getOps;\n\t\t\tlatch=l;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tboolean rv=false;\n\t\t\tfor(Operation op : ops) {\n\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t\tcancelled=true;\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic Map<String, T> get()\n\t\t\tthrows InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\"Timed out waiting forever\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic Map<String, T> get(long timeout, TimeUnit unit)\n\t\t\tthrows InterruptedException,\n\t\t\tExecutionException, TimeoutException {\n\t\t\tif(!latch.await(timeout, unit)) {\n\t\t\t\tthrow new TimeoutException(\"Operation timed out.\");\n\t\t\t}\n\t\t\tfor(Operation op : ops) {\n\t\t\t\tif(op.isCancelled()) {\n\t\t\t\t\tthrow new ExecutionException(\n\t\t\t\t\t\t\tnew RuntimeException(\"Cancelled\"));\n\t\t\t\t}\n\t\t\t\tif(op.hasErrored()) {\n\t\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t\t}\n\t\t\t}\n\t\t\tMap<String, T> m = new HashMap<String, T>();\n\t\t\tfor (Map.Entry<String, CachedData> me : rvMap.entrySet()) {\n\t\t\t\tT val = tc.decode(me.getValue());\n\t\t\t\t// val may be null if the transcoder did not understand\n\t\t\t\t// the value.\n\t\t\t\tif(val != null) {\n\t\t\t\t\tm.put(me.getKey(), val);\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn m;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn cancelled;\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn latch.getCount() == 0;\n\t\t}\n\t}\n\n\tstatic class OperationFuture<T> implements Future<T> {\n\n\t\tprivate final CountDownLatch latch;\n\t\tprivate final AtomicReference<T> objRef;\n\t\tprivate Operation op;\n\t\tprivate final long globalOperationTimeout;\n\n\t\tpublic OperationFuture(CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis(l, new AtomicReference<T>(null), globalOperationTimeout);\n\t\t}\n\n\t\tpublic OperationFuture(CountDownLatch l, AtomicReference<T> oref,\n\t\t\tlong timeout) {\n\t\t\tsuper();\n\t\t\tlatch=l;\n\t\t\tobjRef=oref;\n\t\t\tglobalOperationTimeout = timeout;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tassert op != null : \"No operation\";\n\t\t\top.cancel();\n\t\t\t// This isn't exactly correct, but it's close enough.  If we're in\n\t\t\t// a writing state, we *probably* haven't started.\n\t\t\treturn op.getState() == OperationState.WRITING;\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(globalOperationTimeout, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Timed out waiting for operation\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\tif(!latch.await(duration, units)) {\n\t\t\t\tthrow new TimeoutException(\"Timed out waiting for operation\");\n\t\t\t}\n\t\t\tif(op != null && op.hasErrored()) {\n\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t}\n\t\t\tif(isCancelled()) {\n\t\t\t\tthrow new ExecutionException(new RuntimeException(\"Cancelled\"));\n\t\t\t}\n\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tvoid set(T o) {\n\t\t\tobjRef.set(o);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\top=to;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn op.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn latch.getCount() == 0 ||\n\t\t\t\top.isCancelled() || op.getState() == OperationState.COMPLETE;\n\t\t}\n\n\t}\n\n\tstatic class GetFuture<T> implements Future<T> {\n\t\tprivate final Transcoder<T> tc;\n\t\tprivate final OperationFuture<CachedData> rv;\n\n\t\tpublic GetFuture(Transcoder<T> tc, CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis.tc = tc;\n\t\t\tthis.rv = new OperationFuture<CachedData>(l, globalOperationTimeout);\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\treturn rv.cancel(ign);\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\treturn decode(rv.get());\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\treturn decode(rv.get(duration, units));\n\t\t}\n\n\t\tprivate T decode(CachedData d) {\n\t\t\treturn d == null ? null : tc.decode(d);\n\t\t}\n\n\t\tvoid set(CachedData d) {\n\t\t\trv.set(d);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\trv.setOperation(to);\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn rv.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn rv.isDone();\n\t\t}\n\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.channels.CancelledKeyException;\nimport java.nio.channels.ClosedSelectorException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.memcached.compat.SpyThread;\nimport net.spy.memcached.ops.CASOperationStatus;\nimport net.spy.memcached.ops.CancelledOperationStatus;\nimport net.spy.memcached.ops.ConcatenationType;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.GetsOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\nimport net.spy.memcached.transcoders.TranscodeService;\nimport net.spy.memcached.transcoders.Transcoder;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tnew InetSocketAddress(\"hostname\", portNum));\n *\n *\t// Store a value (async) for one hour\n *\tc.set(\"someKey\", 3600, someObject);\n *\t// Retrieve a value.\n *\tObject myObject=c.get(\"someKey\");\n *\t<\/pre>\n *\n *\t<h2>Advanced Usage<\/h2>\n *\n *\t<p>\n *\t MemcachedClient may be processing a great deal of asynchronous messages or\n *\t possibly dealing with an unreachable memcached, which may delay processing.\n *\t If a memcached is disabled, for example, MemcachedConnection will continue\n *\t to attempt to reconnect and replay pending operations until it comes back\n *\t up.  To prevent this from causing your application to hang, you can use\n *\t one of the asynchronous mechanisms to time out a request and cancel the\n *\t operation to the server.\n *\t<\/p>\n *\n *\t<pre>\n *\t// Get a memcached client connected to several servers\n *\tMemcachedClient c=new MemcachedClient(\n *\t\tAddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *\t// Try to get a value, for up to 5 seconds, and cancel if it doesn't return\n *\tObject myObj=null;\n *\tFuture&lt;Object&gt; f=c.asyncGet(\"someKey\");\n *\ttry {\n *\t\tmyObj=f.get(5, TimeUnit.SECONDS);\n *\t} catch(TimeoutException e) {\n *\t\t// Since we don't need this, go ahead and cancel the operation.  This\n *\t\t// is not strictly necessary, but it'll save some work on the server.\n *\t\tf.cancel();\n *\t\t// Do other timeout related stuff\n *\t}\n * <\/pre>\n */\npublic final class MemcachedClient extends SpyThread implements MemcachedClientIF {\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final long operationTimeout;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tfinal Transcoder<Object> transcoder;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param bufSize read buffer size per connection (in bytes)\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tif(cf == null) {\n\t\t\tthrow new NullPointerException(\"Connection factory required\");\n\t\t}\n\t\tif(addrs == null) {\n\t\t\tthrow new NullPointerException(\"Server list required\");\n\t\t}\n\t\tif(addrs.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"You must have at least one server to connect to\");\n\t\t}\n\t\tif(cf.getOperationTimeout() <= 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Operation timeout must be positive.\");\n\t\t}\n\t\ttranscoder=cf.getDefaultTranscoder();\n\t\topFact=cf.getOperationFactory();\n\t\tassert opFact != null : \"Connection factory failed to make op factory\";\n\t\tconn=cf.createConnection(addrs);\n\t\tassert conn != null : \"Connection factory failed to make a connection\";\n\t\toperationTimeout = cf.getOperationTimeout();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tsetDaemon(cf.isDaemon());\n\t\tstart();\n\t}\n\n\t/**\n\t * Get the addresses of available servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getAvailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the addresses of unavailable servers.\n\t *\n\t * <p>\n\t * This is based on a snapshot in time so shouldn't be considered\n\t * completely accurate, but is a useful for getting a feel for what's\n\t * working and what's not working.\n\t * <\/p>\n\t */\n\tpublic Collection<SocketAddress> getUnavailableServers() {\n\t\tCollection<SocketAddress> rv=new ArrayList<SocketAddress>();\n\t\tfor(MemcachedNode node : conn.getLocator().getAll()) {\n\t\t\tif(!node.isActive()) {\n\t\t\t\trv.add(node.getSocketAddress());\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get a read-only wrapper around the node locator wrapping this instance.\n\t */\n\tpublic NodeLocator getNodeLocator() {\n\t\treturn conn.getLocator().getReadonlyCopy();\n\t}\n\n\t/**\n\t * Get the default transcoder that's in use.\n\t */\n\tpublic Transcoder<Object> getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tbyte[] keyBytes=KeyUtil.getKeyBytes(key);\n\t\tif(keyBytes.length > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\tif(keyBytes.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\"Key must contain at least one character.\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(byte b : keyBytes) {\n\t\t\tif(b == ' ' || b == '\\n' || b == '\\r' || b == 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void checkState() {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tvalidateKey(key);\n\t\tcheckState();\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, true);\n\t}\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of);\n\t}\n\n\tprivate <T> Future<Boolean> asyncStore(StoreType storeType, String key,\n\t\t\t\t\t\t   int exp, T value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\treturn asyncStore(storeType, key, exp, value, transcoder);\n\t}\n\n\tprivate <T> Future<Boolean> asyncCat(\n\t\t\tConcatenationType catType, long cas, String key,\n\t\t\tT value, Transcoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\t\toperationTimeout);\n\t\tOperation op=opFact.cat(catType, cas, key, co.getData(),\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\trv.set(val.isSuccess());\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> append(long cas, String key, Object val) {\n\t\treturn append(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Append to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be appended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> append(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.append, cas, key, val, tc);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> prepend(long cas, String key, Object val) {\n\t\treturn prepend(cas, key, val, transcoder);\n\t}\n\n\t/**\n\t * Prepend to an existing value in the cache.\n\t *\n\t * @param cas cas identifier (ignored in the ascii protocol)\n\t * @param key the key to whose value will be prepended\n\t * @param val the value to append\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future indicating success\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> prepend(long cas, String key, T val,\n\t\t\tTranscoder<T> tc) {\n\t\treturn asyncCat(ConcatenationType.prepend, cas, key, val, tc);\n\t}\n\n\t/**\n     * Asynchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a future that will indicate the status of the CAS\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> Future<CASResponse> asyncCAS(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return asyncCAS(key, casId, 0, value, tc);\n\t}\n\n\t/**\n\t * Asynchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASResponse> asyncCAS(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\tCachedData co=tc.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASResponse> rv=new OperationFuture<CASResponse>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op=opFact.cas(StoreType.set, key, casId, co.getFlags(), exp,\n\t\t\t\tco.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\tif(val instanceof CASOperationStatus) {\n\t\t\t\t\t\t\trv.set(((CASOperationStatus)val).getCASResponse());\n\t\t\t\t\t\t} else if(val instanceof CancelledOperationStatus) {\n\t\t\t\t\t\t\t// Cancelled, ignore and let it float up\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\"Unhandled state: \" + val);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asynchronous CAS operation using the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a future that will indicate the status of the CAS\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASResponse> asyncCAS(String key, long casId, Object value) {\n\t\treturn asyncCAS(key, casId, value, transcoder);\n\t}\n\n\t/**\n     * Perform a synchronous CAS operation.\n     *\n     * @param key the key\n     * @param casId the CAS identifier (from a gets operation)\n     * @param value the new value\n     * @param tc the transcoder to serialize and unserialize the value\n     * @return a CASResponse\n     * @throws OperationTimeoutException if global operation timeout is\n     *         exceeded\n     * @throws IllegalStateException in the rare circumstance where queue\n     *         is too full to accept any more requests\n     */\n    public <T> CASResponse cas(String key, long casId, T value,\n            Transcoder<T> tc) {\n        return cas(key, casId, 0, value, tc);\n    }\n\n\t/**\n\t * Perform a synchronous CAS operation.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param exp the expiration of this object\n\t * @param value the new value\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if global operation timeout is\n\t *         exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASResponse cas(String key, long casId, int exp, T value,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncCAS(key, casId, exp, value, tc).get(operationTimeout,\n\t\t\t\t\tTimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Perform a synchronous CAS operation with the default transcoder.\n\t *\n\t * @param key the key\n\t * @param casId the CAS identifier (from a gets operation)\n\t * @param value the new value\n\t * @return a CASResponse\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASResponse cas(String key, long casId, Object value) {\n\t\treturn cas(key, casId, value, transcoder);\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> add(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Add an object to the cache (using the default transcoder)\n\t * iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> set(String key, int exp, T o, Transcoder<T> tc) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Set an object in the cache (using the default transcoder)\n\t * regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @param tc the transcoder to serialize and unserialize the value\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Boolean> replace(String key, int exp, T o,\n\t\tTranscoder<T> tc) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, tc);\n\t}\n\n\t/**\n\t * Replace an object with the given value (transcoded with the default\n\t * transcoder) iff there is already a value for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o, transcoder);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<T> asyncGet(final String key, final Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal GetFuture<T> rv=new GetFuture<T>(latch, operationTimeout);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate Future<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=TranscodeService.getInstance().decode(tc,\n\t\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize()));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get the given key asynchronously and decode with the default\n\t * transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\t\treturn asyncGet(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<CASValue<T>> asyncGets(final String key,\n\t\t\tfinal Transcoder<T> tc) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<CASValue<T>> rv=\n\t\t\tnew OperationFuture<CASValue<T>>(latch, operationTimeout);\n\n\t\tOperation op=opFact.gets(key,\n\t\t\t\tnew GetsOperation.Callback() {\n\t\t\tprivate CASValue<T> val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, long cas, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tassert cas > 0 : \"CAS was less than zero:  \" + cas;\n\t\t\t\tval=new CASValue<T>(cas, tc.decode(\n\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Gets (with CAS support) the given key asynchronously and decode using\n\t * the default transcoder.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<CASValue<Object>> asyncGets(final String key) {\n\t\treturn asyncGets(key, transcoder);\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if global operation timeout is\n\t * \t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> CASValue<T> gets(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGets(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Gets (with CAS support) with a single key using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache and CAS id (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic CASValue<Object> gets(String key) {\n\t\treturn gets(key, transcoder);\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> T get(String key, Transcoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGet(key, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\"Timeout waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get with a single key and decode using the default transcoder.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Object get(String key) {\n\t\treturn get(key, transcoder);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Collection<String> keys,\n\t\tfinal Transcoder<T> tc) {\n\t\tfinal Map<String, Future<T>> m=new ConcurrentHashMap<String, Future<T>>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, TranscodeService.getInstance().decode(tc,\n\t\t\t\t\t\t\tnew CachedData(flags, data, tc.getMaxSize())));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\n\t\t// Now that we know how many servers it breaks down into, and the latch\n\t\t// is all set up, convert all of these strings collections to operations\n\t\tfinal Map<MemcachedNode, Operation> mops=\n\t\t\tnew HashMap<MemcachedNode, Operation>();\n\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tOperation op=opFact.get(me.getValue(), cb);\n\t\t\tmops.put(me.getKey(), op);\n\t\t\tops.add(op);\n\t\t}\n\t\tassert mops.size() == chunks.size();\n\t\tcheckState();\n\t\tconn.addOperations(mops);\n\t\treturn new BulkGetFuture<T>(m, ops, latch);\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache and decode them\n\t * with the given transcoder.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\treturn asyncGetBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Future<Map<String, T>> asyncGetBulk(Transcoder<T> tc,\n\t\tString... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets with the default transcoder.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Collection<String> keys,\n\t\t\tTranscoder<T> tc) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys, tc).get(\n\t\t\t\toperationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t} catch (TimeoutException e) {\n\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\"Timeout waiting for bulkvalues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\treturn getBulk(keys, transcoder);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param tc the transcoder to serialize and unserialize value\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic <T> Map<String, T> getBulk(Transcoder<T> tc, String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), tc);\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys), transcoder);\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\t/**\n\t * Get a set of stats from all connections.\n\t *\n\t * @param arg which stats to get\n\t * @return a Map of the server SocketAddress to a map of String stat\n\t *\t\t   keys to String stat values.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:\t%s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await(operationTimeout, TimeUnit.MILLISECONDS);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol\n\t\t\t\t\t\t// Allows us to avoid string processing.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tif (!latch.await(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Mutate operation timed out, unable to modify counter [\"\n\t\t\t\t\t\t+ key + \"]\");\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, exp);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @param exp the expiration of this object\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def, int exp) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, exp);\n\t}\n\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, exp, String.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get(operationTimeout, TimeUnit.MILLISECONDS)) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, exp);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new OperationTimeoutException(\n\t\t\t\t\t\"Timeout waiting to mutate or init value\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Future<Long> asyncMutate(Mutator m, String key, int by, long def,\n\t\t\tint exp) {\n\t\tfinal CountDownLatch latch = new CountDownLatch(1);\n\t\tfinal OperationFuture<Long> rv = new OperationFuture<Long>(\n\t\t\t\tlatch, operationTimeout);\n\t\tOperation op = addOp(key, opFact.mutate(m, key, by, def, exp,\n\t\t\t\tnew OperationCallback() {\n\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\trv.set(new Long(s.isSuccess() ? s.getMessage() : \"-1\"));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}));\n\t\trv.setOperation(op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Asychronous increment.\n\t *\n\t * @return a future with the incremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncIncr(String key, int by) {\n\t\treturn asyncMutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Asynchronous decrement.\n\t *\n\t * @return a future with the decremented value, or -1 if the\n\t *\t\t   increment failed.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Long> asyncDecr(String key, int by) {\n\t\treturn asyncMutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long incr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t * @throws OperationTimeoutException if the global operation timeout is\n\t *\t\t   exceeded\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * <p>\n\t * The hold argument specifies the amount of time in seconds (or Unix time\n\t * until which) the client wishes the server to refuse \"add\" and \"replace\"\n\t * commands with this key. For this amount of item, the item is put into a\n\t * delete queue, which means that it won't possible to retrieve it by the\n\t * \"get\" command, but \"add\" and \"replace\" command with this key will also\n\t * fail (the \"set\" command will succeed, however). After the time passes,\n\t * the item is finally deleted from server memory.\n\t * <\/p>\n\t *\n\t * @param key the key to delete\n\t * @param hold how long the key should be unavailable to add commands\n\t *\n\t * @deprecated Hold values are no longer honored.\n\t */\n\t@Deprecated\n\tpublic Future<Boolean> delete(String key, int hold) {\n\t\treturn delete(key);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch,\n\t\t\toperationTimeout);\n\t\tDeleteOperation op=opFact.delete(key,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult,\n\t\t\t\toperationTimeout) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\tprivate void logRunException(Exception e) {\n\t\tif(shuttingDown) {\n\t\t\t// There are a couple types of errors that occur during the\n\t\t\t// shutdown sequence that are considered OK.  Log at debug.\n\t\t\tgetLogger().debug(\"Exception occurred during shutdown\", e);\n\t\t} else {\n\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(CancelledKeyException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(ClosedSelectorException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t} catch(IllegalStateException e) {\n\t\t\t\tlogRunException(e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\t// Guard against double shutdowns (bug 8).\n\t\tif(shuttingDown) {\n\t\t\tgetLogger().info(\"Suppressing duplicate attempt to shut down\");\n\t\t\treturn false;\n\t\t}\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t *\n\t * @throws IllegalStateException in the rare circumstance where queue\n\t *         is too full to accept any more requests\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, false);\n\t\ttry {\n\t\t\t// XXX:  Perhaps IllegalStateException should be caught here\n\t\t\t// and the check retried.\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return true if the observer was added.\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn conn.addObserver(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed, but no longer does\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn conn.removeObserver(obs);\n\t}\n\n\tstatic class BulkGetFuture<T> implements Future<Map<String, T>> {\n\t\tprivate final Map<String, Future<T>> rvMap;\n\t\tprivate final Collection<Operation> ops;\n\t\tprivate final CountDownLatch latch;\n\t\tprivate boolean cancelled=false;\n\n\t\tpublic BulkGetFuture(Map<String, Future<T>> m,\n\t\t\t\tCollection<Operation> getOps, CountDownLatch l) {\n\t\t\tsuper();\n\t\t\trvMap = m;\n\t\t\tops = getOps;\n\t\t\tlatch=l;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tboolean rv=false;\n\t\t\tfor(Operation op : ops) {\n\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t\tfor (Future<T> v : rvMap.values()) {\n\t\t\t\tv.cancel(ign);\n\t\t\t}\n\t\t\tcancelled=true;\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic Map<String, T> get()\n\t\t\tthrows InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\"Timed out waiting forever\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic Map<String, T> get(long timeout, TimeUnit unit)\n\t\t\tthrows InterruptedException,\n\t\t\tExecutionException, TimeoutException {\n\t\t\tif(!latch.await(timeout, unit)) {\n\t\t\t\tthrow new TimeoutException(\"Operation timed out.\");\n\t\t\t}\n\t\t\tfor(Operation op : ops) {\n\t\t\t\tif(op.isCancelled()) {\n\t\t\t\t\tthrow new ExecutionException(\n\t\t\t\t\t\t\tnew RuntimeException(\"Cancelled\"));\n\t\t\t\t}\n\t\t\t\tif(op.hasErrored()) {\n\t\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t\t}\n\t\t\t}\n\t\t\tMap<String, T> m = new HashMap<String, T>();\n\t\t\tfor (Map.Entry<String, Future<T>> me : rvMap.entrySet()) {\n\t\t\t\tm.put(me.getKey(), me.getValue().get());\n\t\t\t}\n\t\t\treturn m;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn cancelled;\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn latch.getCount() == 0;\n\t\t}\n\t}\n\n\tstatic class OperationFuture<T> implements Future<T> {\n\n\t\tprivate final CountDownLatch latch;\n\t\tprivate final AtomicReference<T> objRef;\n\t\tprivate Operation op;\n\t\tprivate final long globalOperationTimeout;\n\n\t\tpublic OperationFuture(CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis(l, new AtomicReference<T>(null), globalOperationTimeout);\n\t\t}\n\n\t\tpublic OperationFuture(CountDownLatch l, AtomicReference<T> oref,\n\t\t\tlong timeout) {\n\t\t\tsuper();\n\t\t\tlatch=l;\n\t\t\tobjRef=oref;\n\t\t\tglobalOperationTimeout = timeout;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tassert op != null : \"No operation\";\n\t\t\top.cancel();\n\t\t\t// This isn't exactly correct, but it's close enough.  If we're in\n\t\t\t// a writing state, we *probably* haven't started.\n\t\t\treturn op.getState() == OperationState.WRITING;\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(globalOperationTimeout, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Timed out waiting for operation\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\tif(!latch.await(duration, units)) {\n\t\t\t\tthrow new TimeoutException(\"Timed out waiting for operation\");\n\t\t\t}\n\t\t\tif(op != null && op.hasErrored()) {\n\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t}\n\t\t\tif(isCancelled()) {\n\t\t\t\tthrow new ExecutionException(new RuntimeException(\"Cancelled\"));\n\t\t\t}\n\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tvoid set(T o) {\n\t\t\tobjRef.set(o);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\top=to;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn op.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn latch.getCount() == 0 ||\n\t\t\t\top.isCancelled() || op.getState() == OperationState.COMPLETE;\n\t\t}\n\n\t}\n\n\tstatic class GetFuture<T> implements Future<T> {\n\t\tprivate final OperationFuture<Future<T>> rv;\n\n\t\tpublic GetFuture(CountDownLatch l, long globalOperationTimeout) {\n\t\t\tthis.rv = new OperationFuture<Future<T>>(l, globalOperationTimeout);\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\treturn rv.cancel(ign);\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\tFuture<T> v = rv.get();\n\t\t\treturn v == null ? null : v.get();\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException, ExecutionException {\n\t\t\tFuture<T> v = rv.get(duration, units);\n\t\t\treturn v == null ? null : v.get();\n\t\t}\n\n\t\tvoid set(Future<T> d) {\n\t\t\trv.set(d);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\trv.setOperation(to);\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn rv.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn rv.isDone();\n\t\t}\n\t}\n}\n","lineNo":1706}
{"Smelly Sample":"package net.spy.memcached.protocol;\n\nimport java.io.IOException;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.concurrent.BlockingQueue;\n\nimport net.spy.memcached.MemcachedNode;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\n\n/**\n * Represents a node with the memcached cluster, along with buffering and\n * operation queues.\n */\npublic abstract class TCPMemcachedNodeImpl extends SpyObject\n\timplements MemcachedNode {\n\n\tprivate final SocketAddress socketAddress;\n\tprivate final ByteBuffer rbuf;\n\tprivate final ByteBuffer wbuf;\n\tprotected final BlockingQueue<Operation> writeQ;\n\tprivate final BlockingQueue<Operation> readQ;\n\tprivate final BlockingQueue<Operation> inputQueue;\n\t// This has been declared volatile so it can be used as an availability\n\t// indicator.\n\tprivate volatile int reconnectAttempt=1;\n\tprivate SocketChannel channel;\n\tprivate int toWrite=0;\n\tprotected GetOperation getOp=null;\n\tprivate SelectionKey sk=null;\n\n\tpublic TCPMemcachedNodeImpl(SocketAddress sa, SocketChannel c,\n\t\t\tint bufSize, BlockingQueue<Operation> rq,\n\t\t\tBlockingQueue<Operation> wq, BlockingQueue<Operation> iq) {\n\t\tsuper();\n\t\tassert sa != null : \"No SocketAddress\";\n\t\tassert c != null : \"No SocketChannel\";\n\t\tassert bufSize > 0 : \"Invalid buffer size: \" + bufSize;\n\t\tassert rq != null : \"No operation read queue\";\n\t\tassert wq != null : \"No operation write queue\";\n\t\tassert iq != null : \"No input queue\";\n\t\tsocketAddress=sa;\n\t\tsetChannel(c);\n\t\trbuf=ByteBuffer.allocate(bufSize);\n\t\twbuf=ByteBuffer.allocate(bufSize);\n\t\tgetWbuf().clear();\n\t\treadQ=rq;\n\t\twriteQ=wq;\n\t\tinputQueue=iq;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#copyInputQueue()\n\t */\n\tpublic final void copyInputQueue() {\n\t\tCollection<Operation> tmp=new ArrayList<Operation>();\n\n\t\t// don't drain more than we have space to place\n\t\tinputQueue.drainTo(tmp, writeQ.remainingCapacity());\n\n\t\twriteQ.addAll(tmp);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#destroyInputQueue()\n\t */\n\tpublic Collection<Operation> destroyInputQueue() {\n\t\tCollection<Operation> rv=new ArrayList<Operation>();\n\t\tinputQueue.drainTo(rv);\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setupResend()\n\t */\n\tpublic final void setupResend() {\n\t\t// First, reset the current write op.\n\t\tOperation op=getCurrentWriteOp();\n\t\tif(op != null) {\n\t\t\top.getBuffer().reset();\n\t\t}\n\t\t// Now cancel all the pending read operations.  Might be better to\n\t\t// to requeue them.\n\t\twhile(hasReadOp()) {\n\t\t\top=removeCurrentReadOp();\n\t\t\tgetLogger().warn(\"Discarding partially completed op: %s\", op);\n\t\t\top.cancel();\n\t\t}\n\n\t\tgetWbuf().clear();\n\t\tgetRbuf().clear();\n\t\ttoWrite=0;\n\t}\n\n\t// Prepare the pending operations.  Return true if there are any pending\n\t// ops\n\tprivate boolean preparePending() {\n\t\t// Copy the input queue into the write queue.\n\t\tcopyInputQueue();\n\n\t\t// Now check the ops\n\t\tOperation nextOp=getCurrentWriteOp();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\", nextOp);\n\t\t\tremoveCurrentWriteOp();\n\t\t\tnextOp=getCurrentWriteOp();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#fillWriteBuffer(boolean)\n\t */\n\tpublic final void fillWriteBuffer(boolean optimizeGets) {\n\t\tif(toWrite == 0 && readQ.remainingCapacity() > 0) {\n\t\t\tgetWbuf().clear();\n\t\t\tOperation o=getCurrentWriteOp();\n\t\t\twhile(o != null && toWrite < getWbuf().capacity()) {\n\t\t\t\tassert o.getState() == OperationState.WRITING;\n\t\t\t\tByteBuffer obuf=o.getBuffer();\n\t\t\t\tint bytesToCopy=Math.min(getWbuf().remaining(),\n\t\t\t\t\t\tobuf.remaining());\n\t\t\t\tbyte b[]=new byte[bytesToCopy];\n\t\t\t\tobuf.get(b);\n\t\t\t\tgetWbuf().put(b);\n\t\t\t\tgetLogger().debug(\"After copying stuff from %s: %s\",\n\t\t\t\t\t\to, getWbuf());\n\t\t\t\tif(!o.getBuffer().hasRemaining()) {\n\t\t\t\t\to.writeComplete();\n\t\t\t\t\ttransitionWriteItem();\n\n\t\t\t\t\tpreparePending();\n\t\t\t\t\tif(optimizeGets) {\n\t\t\t\t\t\toptimize();\n\t\t\t\t\t}\n\n\t\t\t\t\to=getCurrentWriteOp();\n\t\t\t\t}\n\t\t\t\ttoWrite += bytesToCopy;\n\t\t\t}\n\t\t\tgetWbuf().flip();\n\t\t\tassert toWrite <= getWbuf().capacity()\n\t\t\t\t: \"toWrite exceeded capacity: \" + this;\n\t\t\tassert toWrite == getWbuf().remaining()\n\t\t\t\t: \"Expected \" + toWrite + \" remaining, got \"\n\t\t\t\t+ getWbuf().remaining();\n\t\t} else {\n\t\t\tgetLogger().debug(\"Buffer is full, skipping\");\n\t\t}\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#transitionWriteItem()\n\t */\n\tpublic final void transitionWriteItem() {\n\t\tOperation op=removeCurrentWriteOp();\n\t\tassert op != null : \"There is no write item to transition\";\n\t\tgetLogger().debug(\"Transitioning %s to read\", op);\n\t\treadQ.add(op);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#optimize()\n\t */\n\tprotected abstract void optimize();\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentReadOp()\n\t */\n\tpublic final Operation getCurrentReadOp() {\n\t\treturn readQ.peek();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentReadOp()\n\t */\n\tpublic final Operation removeCurrentReadOp() {\n\t\treturn readQ.remove();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentWriteOp()\n\t */\n\tpublic final Operation getCurrentWriteOp() {\n\t\treturn getOp == null ? writeQ.peek() : getOp;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentWriteOp()\n\t */\n\tpublic final Operation removeCurrentWriteOp() {\n\t\tOperation rv=getOp;\n\t\tif(rv == null) {\n\t\t\trv=writeQ.remove();\n\t\t} else {\n\t\t\tgetOp=null;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasReadOp()\n\t */\n\tpublic final boolean hasReadOp() {\n\t\treturn !readQ.isEmpty();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasWriteOp()\n\t */\n\tpublic final boolean hasWriteOp() {\n\t\treturn !(getOp == null && writeQ.isEmpty());\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#addOp(net.spy.memcached.ops.Operation)\n\t */\n\tpublic final void addOp(Operation op) {\n\t\tboolean added=inputQueue.add(op);\n\t\tassert added; // documented to throw an IllegalStateException\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSelectionOps()\n\t */\n\tpublic final int getSelectionOps() {\n\t\tint rv=0;\n\t\tif(getChannel().isConnected()) {\n\t\t\tif(hasReadOp()) {\n\t\t\t\trv |= SelectionKey.OP_READ;\n\t\t\t}\n\t\t\tif(toWrite > 0 || hasWriteOp()) {\n\t\t\t\trv |= SelectionKey.OP_WRITE;\n\t\t\t}\n\t\t} else {\n\t\t\trv = SelectionKey.OP_CONNECT;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getRbuf()\n\t */\n\tpublic final ByteBuffer getRbuf() {\n\t\treturn rbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getWbuf()\n\t */\n\tpublic final ByteBuffer getWbuf() {\n\t\treturn wbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSocketAddress()\n\t */\n\tpublic final SocketAddress getSocketAddress() {\n\t\treturn socketAddress;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#isActive()\n\t */\n\tpublic final boolean isActive() {\n\t\treturn reconnectAttempt == 0\n\t\t\t&& getChannel() != null && getChannel().isConnected();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#reconnecting()\n\t */\n\tpublic final void reconnecting() {\n\t\treconnectAttempt++;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#connected()\n\t */\n\tpublic final void connected() {\n\t\treconnectAttempt=0;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getReconnectCount()\n\t */\n\tpublic final int getReconnectCount() {\n\t\treturn reconnectAttempt;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#toString()\n\t */\n\t@Override\n\tpublic final String toString() {\n\t\tint sops=0;\n\t\tif(getSk()!= null && getSk().isValid()) {\n\t\t\tsops=getSk().interestOps();\n\t\t}\n\t\tint rsize=readQ.size() + (getOp == null ? 0 : 1);\n\t\tint wsize=writeQ.size();\n\t\tint isize=inputQueue.size();\n\t\treturn \"{QA sa=\" + getSocketAddress() + \", #Rops=\" + rsize\n\t\t\t+ \", #Wops=\" + wsize\n\t\t\t+ \", #iq=\" + isize\n\t\t\t+ \", topRop=\" + getCurrentReadOp()\n\t\t\t+ \", topWop=\" + getCurrentWriteOp()\n\t\t\t+ \", toWrite=\" + toWrite\n\t\t\t+ \", interested=\" + sops + \"}\";\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#registerChannel(java.nio.channels.SocketChannel, java.nio.channels.SelectionKey)\n\t */\n\tpublic final void registerChannel(SocketChannel ch, SelectionKey skey) {\n\t\tsetChannel(ch);\n\t\tsetSk(skey);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setChannel(java.nio.channels.SocketChannel)\n\t */\n\tpublic final void setChannel(SocketChannel to) {\n\t\tassert channel == null || !channel.isOpen()\n\t\t\t: \"Attempting to overwrite channel\";\n\t\tchannel = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getChannel()\n\t */\n\tpublic final SocketChannel getChannel() {\n\t\treturn channel;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setSk(java.nio.channels.SelectionKey)\n\t */\n\tpublic final void setSk(SelectionKey to) {\n\t\tsk = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSk()\n\t */\n\tpublic final SelectionKey getSk() {\n\t\treturn sk;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getBytesRemainingInBuffer()\n\t */\n\tpublic final int getBytesRemainingToWrite() {\n\t\treturn toWrite;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#writeSome()\n\t */\n\tpublic final int writeSome() throws IOException {\n\t\tint wrote=channel.write(wbuf);\n\t\tassert wrote >= 0 : \"Wrote negative bytes?\";\n\t\ttoWrite -= wrote;\n\t\tassert toWrite >= 0\n\t\t\t: \"toWrite went negative after writing \" + wrote\n\t\t\t\t+ \" bytes for \" + this;\n\t\tgetLogger().debug(\"Wrote %d bytes\", wrote);\n\t\treturn wrote;\n\t}\n\n\n\tpublic final void fixupOps() {\n\t\tif(sk != null && sk.isValid()) {\n\t\t\tint iops=getSelectionOps();\n\t\t\tgetLogger().debug(\"Setting interested opts to %d\", iops);\n\t\t\tsk.interestOps(iops);\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selection key is not valid.\");\n\t\t}\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached.protocol;\n\nimport java.io.IOException;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.concurrent.BlockingQueue;\n\nimport net.spy.memcached.MemcachedNode;\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\n\n/**\n * Represents a node with the memcached cluster, along with buffering and\n * operation queues.\n */\npublic abstract class TCPMemcachedNodeImpl extends SpyObject\n\timplements MemcachedNode {\n\n\tprivate final SocketAddress socketAddress;\n\tprivate final ByteBuffer rbuf;\n\tprivate final ByteBuffer wbuf;\n\tprotected final BlockingQueue<Operation> writeQ;\n\tprivate final BlockingQueue<Operation> readQ;\n\tprivate final BlockingQueue<Operation> inputQueue;\n\t// This has been declared volatile so it can be used as an availability\n\t// indicator.\n\tprivate volatile int reconnectAttempt=1;\n\tprivate SocketChannel channel;\n\tprivate int toWrite=0;\n\tprotected GetOperation getOp=null;\n\tprivate volatile SelectionKey sk=null;\n\n\tpublic TCPMemcachedNodeImpl(SocketAddress sa, SocketChannel c,\n\t\t\tint bufSize, BlockingQueue<Operation> rq,\n\t\t\tBlockingQueue<Operation> wq, BlockingQueue<Operation> iq) {\n\t\tsuper();\n\t\tassert sa != null : \"No SocketAddress\";\n\t\tassert c != null : \"No SocketChannel\";\n\t\tassert bufSize > 0 : \"Invalid buffer size: \" + bufSize;\n\t\tassert rq != null : \"No operation read queue\";\n\t\tassert wq != null : \"No operation write queue\";\n\t\tassert iq != null : \"No input queue\";\n\t\tsocketAddress=sa;\n\t\tsetChannel(c);\n\t\trbuf=ByteBuffer.allocate(bufSize);\n\t\twbuf=ByteBuffer.allocate(bufSize);\n\t\tgetWbuf().clear();\n\t\treadQ=rq;\n\t\twriteQ=wq;\n\t\tinputQueue=iq;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#copyInputQueue()\n\t */\n\tpublic final void copyInputQueue() {\n\t\tCollection<Operation> tmp=new ArrayList<Operation>();\n\n\t\t// don't drain more than we have space to place\n\t\tinputQueue.drainTo(tmp, writeQ.remainingCapacity());\n\n\t\twriteQ.addAll(tmp);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#destroyInputQueue()\n\t */\n\tpublic Collection<Operation> destroyInputQueue() {\n\t\tCollection<Operation> rv=new ArrayList<Operation>();\n\t\tinputQueue.drainTo(rv);\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setupResend()\n\t */\n\tpublic final void setupResend() {\n\t\t// First, reset the current write op.\n\t\tOperation op=getCurrentWriteOp();\n\t\tif(op != null) {\n\t\t\top.getBuffer().reset();\n\t\t}\n\t\t// Now cancel all the pending read operations.  Might be better to\n\t\t// to requeue them.\n\t\twhile(hasReadOp()) {\n\t\t\top=removeCurrentReadOp();\n\t\t\tgetLogger().warn(\"Discarding partially completed op: %s\", op);\n\t\t\top.cancel();\n\t\t}\n\n\t\tgetWbuf().clear();\n\t\tgetRbuf().clear();\n\t\ttoWrite=0;\n\t}\n\n\t// Prepare the pending operations.  Return true if there are any pending\n\t// ops\n\tprivate boolean preparePending() {\n\t\t// Copy the input queue into the write queue.\n\t\tcopyInputQueue();\n\n\t\t// Now check the ops\n\t\tOperation nextOp=getCurrentWriteOp();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\", nextOp);\n\t\t\tremoveCurrentWriteOp();\n\t\t\tnextOp=getCurrentWriteOp();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#fillWriteBuffer(boolean)\n\t */\n\tpublic final void fillWriteBuffer(boolean optimizeGets) {\n\t\tif(toWrite == 0 && readQ.remainingCapacity() > 0) {\n\t\t\tgetWbuf().clear();\n\t\t\tOperation o=getCurrentWriteOp();\n\t\t\twhile(o != null && toWrite < getWbuf().capacity()) {\n\t\t\t\tassert o.getState() == OperationState.WRITING;\n\t\t\t\tByteBuffer obuf=o.getBuffer();\n\t\t\t\tint bytesToCopy=Math.min(getWbuf().remaining(),\n\t\t\t\t\t\tobuf.remaining());\n\t\t\t\tbyte b[]=new byte[bytesToCopy];\n\t\t\t\tobuf.get(b);\n\t\t\t\tgetWbuf().put(b);\n\t\t\t\tgetLogger().debug(\"After copying stuff from %s: %s\",\n\t\t\t\t\t\to, getWbuf());\n\t\t\t\tif(!o.getBuffer().hasRemaining()) {\n\t\t\t\t\to.writeComplete();\n\t\t\t\t\ttransitionWriteItem();\n\n\t\t\t\t\tpreparePending();\n\t\t\t\t\tif(optimizeGets) {\n\t\t\t\t\t\toptimize();\n\t\t\t\t\t}\n\n\t\t\t\t\to=getCurrentWriteOp();\n\t\t\t\t}\n\t\t\t\ttoWrite += bytesToCopy;\n\t\t\t}\n\t\t\tgetWbuf().flip();\n\t\t\tassert toWrite <= getWbuf().capacity()\n\t\t\t\t: \"toWrite exceeded capacity: \" + this;\n\t\t\tassert toWrite == getWbuf().remaining()\n\t\t\t\t: \"Expected \" + toWrite + \" remaining, got \"\n\t\t\t\t+ getWbuf().remaining();\n\t\t} else {\n\t\t\tgetLogger().debug(\"Buffer is full, skipping\");\n\t\t}\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#transitionWriteItem()\n\t */\n\tpublic final void transitionWriteItem() {\n\t\tOperation op=removeCurrentWriteOp();\n\t\tassert op != null : \"There is no write item to transition\";\n\t\tgetLogger().debug(\"Transitioning %s to read\", op);\n\t\treadQ.add(op);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#optimize()\n\t */\n\tprotected abstract void optimize();\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentReadOp()\n\t */\n\tpublic final Operation getCurrentReadOp() {\n\t\treturn readQ.peek();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentReadOp()\n\t */\n\tpublic final Operation removeCurrentReadOp() {\n\t\treturn readQ.remove();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getCurrentWriteOp()\n\t */\n\tpublic final Operation getCurrentWriteOp() {\n\t\treturn getOp == null ? writeQ.peek() : getOp;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#removeCurrentWriteOp()\n\t */\n\tpublic final Operation removeCurrentWriteOp() {\n\t\tOperation rv=getOp;\n\t\tif(rv == null) {\n\t\t\trv=writeQ.remove();\n\t\t} else {\n\t\t\tgetOp=null;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasReadOp()\n\t */\n\tpublic final boolean hasReadOp() {\n\t\treturn !readQ.isEmpty();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#hasWriteOp()\n\t */\n\tpublic final boolean hasWriteOp() {\n\t\treturn !(getOp == null && writeQ.isEmpty());\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#addOp(net.spy.memcached.ops.Operation)\n\t */\n\tpublic final void addOp(Operation op) {\n\t\tboolean added=inputQueue.add(op);\n\t\tassert added; // documented to throw an IllegalStateException\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSelectionOps()\n\t */\n\tpublic final int getSelectionOps() {\n\t\tint rv=0;\n\t\tif(getChannel().isConnected()) {\n\t\t\tif(hasReadOp()) {\n\t\t\t\trv |= SelectionKey.OP_READ;\n\t\t\t}\n\t\t\tif(toWrite > 0 || hasWriteOp()) {\n\t\t\t\trv |= SelectionKey.OP_WRITE;\n\t\t\t}\n\t\t} else {\n\t\t\trv = SelectionKey.OP_CONNECT;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getRbuf()\n\t */\n\tpublic final ByteBuffer getRbuf() {\n\t\treturn rbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getWbuf()\n\t */\n\tpublic final ByteBuffer getWbuf() {\n\t\treturn wbuf;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSocketAddress()\n\t */\n\tpublic final SocketAddress getSocketAddress() {\n\t\treturn socketAddress;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#isActive()\n\t */\n\tpublic final boolean isActive() {\n\t\treturn reconnectAttempt == 0\n\t\t\t&& getChannel() != null && getChannel().isConnected();\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#reconnecting()\n\t */\n\tpublic final void reconnecting() {\n\t\treconnectAttempt++;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#connected()\n\t */\n\tpublic final void connected() {\n\t\treconnectAttempt=0;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getReconnectCount()\n\t */\n\tpublic final int getReconnectCount() {\n\t\treturn reconnectAttempt;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#toString()\n\t */\n\t@Override\n\tpublic final String toString() {\n\t\tint sops=0;\n\t\tif(getSk()!= null && getSk().isValid()) {\n\t\t\tsops=getSk().interestOps();\n\t\t}\n\t\tint rsize=readQ.size() + (getOp == null ? 0 : 1);\n\t\tint wsize=writeQ.size();\n\t\tint isize=inputQueue.size();\n\t\treturn \"{QA sa=\" + getSocketAddress() + \", #Rops=\" + rsize\n\t\t\t+ \", #Wops=\" + wsize\n\t\t\t+ \", #iq=\" + isize\n\t\t\t+ \", topRop=\" + getCurrentReadOp()\n\t\t\t+ \", topWop=\" + getCurrentWriteOp()\n\t\t\t+ \", toWrite=\" + toWrite\n\t\t\t+ \", interested=\" + sops + \"}\";\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#registerChannel(java.nio.channels.SocketChannel, java.nio.channels.SelectionKey)\n\t */\n\tpublic final void registerChannel(SocketChannel ch, SelectionKey skey) {\n\t\tsetChannel(ch);\n\t\tsetSk(skey);\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setChannel(java.nio.channels.SocketChannel)\n\t */\n\tpublic final void setChannel(SocketChannel to) {\n\t\tassert channel == null || !channel.isOpen()\n\t\t\t: \"Attempting to overwrite channel\";\n\t\tchannel = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getChannel()\n\t */\n\tpublic final SocketChannel getChannel() {\n\t\treturn channel;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#setSk(java.nio.channels.SelectionKey)\n\t */\n\tpublic final void setSk(SelectionKey to) {\n\t\tsk = to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getSk()\n\t */\n\tpublic final SelectionKey getSk() {\n\t\treturn sk;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#getBytesRemainingInBuffer()\n\t */\n\tpublic final int getBytesRemainingToWrite() {\n\t\treturn toWrite;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.MemcachedNode#writeSome()\n\t */\n\tpublic final int writeSome() throws IOException {\n\t\tint wrote=channel.write(wbuf);\n\t\tassert wrote >= 0 : \"Wrote negative bytes?\";\n\t\ttoWrite -= wrote;\n\t\tassert toWrite >= 0\n\t\t\t: \"toWrite went negative after writing \" + wrote\n\t\t\t\t+ \" bytes for \" + this;\n\t\tgetLogger().debug(\"Wrote %d bytes\", wrote);\n\t\treturn wrote;\n\t}\n\n\n\tpublic final void fixupOps() {\n\t\t// As the selection key can be changed at any point due to node\n\t\t// failure, we'll grab the current volatile value and configure it.\n\t\tSelectionKey s = sk;\n\t\tif(s != null && s.isValid()) {\n\t\t\tint iops=getSelectionOps();\n\t\t\tgetLogger().debug(\"Setting interested opts to %d\", iops);\n\t\t\ts.interestOps(iops);\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selection key is not valid.\");\n\t\t}\n\t}\n}\n","lineNo":382}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\n\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic final class MemcachedConnection extends SpyObject {\n\n\t// The number of empty selects we'll allow before assuming we may have\n\t// missed one and should check the current selectors.  This generally\n\t// indicates a bug, but we'll check it nonetheless.\n\tprivate static final int DOUBLE_CHECK_EMPTY = 256;\n\t// The number of empty selects we'll allow before blowing up.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 0x1000000;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, get optimization will collapse multiple sequential get ops\n\tprivate boolean optimizeGets=true;\n\tprivate Selector selector=null;\n\tprivate final NodeLocator locator;\n\tprivate final FailureMode failureMode;\n\tprivate int emptySelects=0;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n\tprivate final Collection<ConnectionObserver> connObservers =\n\t\tnew ConcurrentLinkedQueue<ConnectionObserver>();\n\tprivate final OperationFactory opFact;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n\t\t\tFailureMode fm, OperationFactory opfactory)\n\t\tthrows IOException {\n\t\tconnObservers.addAll(obs);\n\t\treconnectQueue=new TreeMap<Long, MemcachedNode>();\n\t\taddedQueue=new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tfailureMode = fm;\n\t\topFact = opfactory;\n\t\tselector=Selector.open();\n\t\tList<MemcachedNode> connections=new ArrayList<MemcachedNode>(a.size());\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tMemcachedNode qa=f.createMemcachedNode(sa, ch, bufSize);\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tconnected(qa);\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.setSk(ch.register(selector, ops, qa));\n\t\t\tassert ch.isConnected()\n\t\t\t\t|| qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\tconnections.add(qa);\n\t\t}\n\t\tlocator=f.createLocator(connections);\n\t}\n\n\t/**\n\t * Enable or disable get optimization.\n\t *\n\t * When enabled (default), multiple sequential gets are collapsed into one.\n\t */\n\tpublic void setGetOptimization(boolean to) {\n\t\toptimizeGets=to;\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getSk() != null && qa.getSk().isValid()) {\n\t\t\t\tif(qa.getChannel().isConnected()) {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty() && !shutDown) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > DOUBLE_CHECK_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlostConnection((MemcachedNode)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\t\tif(!shutDown && !reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<MemcachedNode> toAdd=new HashSet<MemcachedNode>();\n\t\t\t// Transfer the queue into a hashset.  There are very likely more\n\t\t\t// additions than there are nodes.\n\t\t\tCollection<MemcachedNode> todo=new HashSet<MemcachedNode>();\n\t\t\ttry {\n\t\t\t\tMemcachedNode qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\ttodo.add(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// Found everything\n\t\t\t}\n\n\t\t\t// Now process the queue.\n\t\t\tfor(MemcachedNode qa : todo) {\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\tif(qa.isActive()) {\n\t\t\t\t\tif(qa.getCurrentWriteOp() != null) {\n\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t}\n\t\t\t\tqa.copyInputQueue();\n\t\t\t\tif(readyForIO) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\t\thandleWrites(qa.getSk(), qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\tlostConnection(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tqa.fixupOps();\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return whether the observer was successfully added\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn connObservers.add(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed and now doesn't\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn connObservers.remove(obs);\n\t}\n\n\tprivate void connected(MemcachedNode qa) {\n\t\tassert qa.getChannel().isConnected() : \"Not connected.\";\n\t\tint rt = qa.getReconnectCount();\n\t\tqa.connected();\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionEstablished(qa.getSocketAddress(), rt);\n\t\t}\n\t}\n\n\tprivate void lostConnection(MemcachedNode qa) {\n\t\tqueueReconnect(qa);\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionLost(qa.getSocketAddress());\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tMemcachedNode qa=(MemcachedNode)sk.attachment();\n\t\ttry {\n\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\tif(sk.isConnectable()) {\n\t\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\t\tfinal SocketChannel channel=qa.getChannel();\n\t\t\t\tif(channel.finishConnect()) {\n\t\t\t\t\tconnected(qa);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t}\n\t\t\t\tif(sk.isWritable()) {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(ClosedChannelException e) {\n\t\t\tif(!shutDown) {\n\t\t\t\tgetLogger().info(\"Closed channel and not shutting down.  \"\n\t\t\t\t\t+ \"Queueing reconnect on %s\", qa, e);\n\t\t\t\tlostConnection(qa);\n\t\t\t}\n\t\t} catch(ConnectException e) {\n\t\t\t// Failures to establish a connection should attempt a reconnect\n\t\t\t// without signaling the observers.\n\t\t\tgetLogger().info(\"Reconnecting due to failure to connect to %s\",\n\t\t\t\t\tqa, e);\n\t\t\tqueueReconnect(qa);\n\t\t} catch(Exception e) {\n\t\t\t// Various errors occur on Linux that wind up here.  However, any\n\t\t\t// particular error processing an item should simply cause us to\n\t\t\t// reconnect to the server.\n\t\t\tgetLogger().info(\"Reconnecting due to exception on %s\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t}\n\t\tqa.fixupOps();\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(optimizeGets);\n\t\tboolean canWriteMore=qa.getBytesRemainingToWrite() > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.writeSome();\n\t\t\tqa.fillWriteBuffer(optimizeGets);\n\t\t\tcanWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tByteBuffer rbuf=qa.getRbuf();\n\t\tfinal SocketChannel channel = qa.getChannel();\n\t\tint read=channel.read(rbuf);\n\t\tif (read < 0) {\n\t\t    // GRUMBLE.\n\t\t    throw new IOException(\"Disconnected\");\n\t\t}\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\trbuf.flip();\n\t\t\twhile(rbuf.remaining() > 0) {\n\t\t\t\tif(currentOp == null) {\n\t\t\t\t\tthrow new IllegalStateException(\"No read operation.\");\n\t\t\t\t}\n\t\t\t\tcurrentOp.readFromBuffer(rbuf);\n\t\t\t\tif(currentOp.getState() == OperationState.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t}\n\t\t\t}\n\t\t\trbuf.clear();\n\t\t\tread=channel.read(rbuf);\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(MemcachedNode qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.getReconnectCount());\n\t\t\tif(qa.getSk() != null) {\n\t\t\t\tqa.getSk().cancel();\n\t\t\t\tassert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnecting();\n\t\t\ttry {\n\t\t\t\tif(qa.getChannel() != null && qa.getChannel().socket() != null) {\n\t\t\t\t\tqa.getChannel().socket().close();\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"The channel or socket was null for %s\",\n\t\t\t\t\t\tqa);\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.setChannel(null);\n\n\t\t\tlong delay=Math.min((100*qa.getReconnectCount()) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\n\t\t\tif(failureMode == FailureMode.Redistribute) {\n\t\t\t\tredistributeOperations(qa.destroyInputQueue());\n\t\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\t\tcancelOperations(qa.destroyInputQueue());\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void cancelOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\top.cancel();\n\t\t}\n\t}\n\n\tprivate void redistributeOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\tif(op instanceof KeyedOperation) {\n\t\t\t\tKeyedOperation ko = (KeyedOperation)op;\n\t\t\t\tint added = 0;\n\t\t\t\tfor(String k : ko.getKeys()) {\n\t\t\t\t\tfor(Operation newop : opFact.clone(ko)) {\n\t\t\t\t\t\taddOperation(k, newop);\n\t\t\t\t\t\tadded++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert added > 0\n\t\t\t\t\t: \"Didn't add any new operations when redistributing\";\n\t\t\t} else {\n\t\t\t\t// Cancel things that don't have definite targets.\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tfinal long now=System.currentTimeMillis();\n\t\tfinal Map<MemcachedNode, Boolean> seen=\n\t\t\tnew IdentityHashMap<MemcachedNode, Boolean>();\n\t\tfinal List<MemcachedNode> rereQueue=new ArrayList<MemcachedNode>();\n\t\tfor(Iterator<MemcachedNode> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tfinal MemcachedNode qa=i.next();\n\t\t\ti.remove();\n\t\t\ttry {\n\t\t\t\tif(!seen.containsKey(qa)) {\n\t\t\t\t\tseen.put(qa, Boolean.TRUE);\n\t\t\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\t\t\tfinal SocketChannel ch=SocketChannel.open();\n\t\t\t\t\tch.configureBlocking(false);\n\t\t\t\t\tint ops=0;\n\t\t\t\t\tif(ch.connect(qa.getSocketAddress())) {\n\t\t\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\t\t\tassert ch.isConnected();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t\t}\n\t\t\t\t\tqa.registerChannel(ch, ch.register(selector, ops, qa));\n\t\t\t\t\tassert qa.getChannel() == ch : \"Channel was lost.\";\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Skipping duplicate reconnect request for %s\", qa);\n\t\t\t\t}\n\t\t\t} catch(ConnectException e) {\n\t\t\t\tgetLogger().warn(\"Error on reconnect\", e);\n\t\t\t\trereQueue.add(qa);\n\t\t\t}\n\t\t}\n\t\t// Requeue any fast-failed connects.\n\t\tfor(MemcachedNode n : rereQueue) {\n\t\t\tqueueReconnect(n);\n\t\t}\n\t}\n\n\t/**\n\t * Get the node locator used by this connection.\n\t */\n\tNodeLocator getLocator() {\n\t\treturn locator;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t *\n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\tpublic void addOperation(final String key, final Operation o) {\n\t\tMemcachedNode placeIn=null;\n\t\tMemcachedNode primary = locator.getPrimary(key);\n\t\tif(primary.isActive() || failureMode == FailureMode.Retry) {\n\t\t\tplaceIn=primary;\n\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\to.cancel();\n\t\t} else {\n\t\t\t// Look for another node in sequence that is ready.\n\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\tplaceIn == null && i.hasNext(); ) {\n\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\tif(n.isActive()) {\n\t\t\t\t\tplaceIn=n;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we didn't find an active node, queue it in the primary node\n\t\t\t// and wait for it to come back online.\n\t\t\tif(placeIn == null) {\n\t\t\t\tplaceIn = primary;\n\t\t\t}\n\t\t}\n\n\t\tassert o.isCancelled() || placeIn != null\n\t\t\t: \"No node found for key \" + key;\n\t\tif(placeIn != null) {\n\t\t\taddOperation(placeIn, o);\n\t\t} else {\n\t\t\tassert o.isCancelled() : \"No not found for \"\n\t\t\t\t+ key + \" (and not immediately cancelled)\";\n\t\t}\n\t}\n\n\tpublic void addOperation(final MemcachedNode node, final Operation o) {\n\t\to.initialize();\n\t\tnode.addOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n\t\tfor(Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n\t\t\tfinal MemcachedNode node=me.getKey();\n\t\t\tOperation o=me.getValue();\n\t\t\to.initialize();\n\t\t\tnode.addOp(o);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t}\n\n\t/**\n\t * Broadcast an operation to all nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(locator.getAll().size());\n\t\tfor(MemcachedNode node : locator.getAll()) {\n\t\t\tOperation op = of.newOp(node, latch);\n\t\t\top.initialize();\n\t\t\tnode.addOp(op);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\treturn latch;\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tshutDown=true;\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getChannel() != null) {\n\t\t\t\tqa.getChannel().close();\n\t\t\t\tqa.setSk(null);\n\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.getBytesRemainingToWrite());\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.getChannel());\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.getSocketAddress());\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.ClosedChannelException;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\n\nimport net.spy.memcached.compat.SpyObject;\nimport net.spy.memcached.ops.KeyedOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic final class MemcachedConnection extends SpyObject {\n\n\t// The number of empty selects we'll allow before assuming we may have\n\t// missed one and should check the current selectors.  This generally\n\t// indicates a bug, but we'll check it nonetheless.\n\tprivate static final int DOUBLE_CHECK_EMPTY = 256;\n\t// The number of empty selects we'll allow before blowing up.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 0x1000000;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, get optimization will collapse multiple sequential get ops\n\tprivate boolean optimizeGets=true;\n\tprivate Selector selector=null;\n\tprivate final NodeLocator locator;\n\tprivate final FailureMode failureMode;\n\tprivate int emptySelects=0;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<MemcachedNode> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, MemcachedNode> reconnectQueue;\n\n\tprivate final Collection<ConnectionObserver> connObservers =\n\t\tnew ConcurrentLinkedQueue<ConnectionObserver>();\n\tprivate final OperationFactory opFact;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a, Collection<ConnectionObserver> obs,\n\t\t\tFailureMode fm, OperationFactory opfactory)\n\t\tthrows IOException {\n\t\tconnObservers.addAll(obs);\n\t\treconnectQueue=new TreeMap<Long, MemcachedNode>();\n\t\taddedQueue=new ConcurrentLinkedQueue<MemcachedNode>();\n\t\tfailureMode = fm;\n\t\topFact = opfactory;\n\t\tselector=Selector.open();\n\t\tList<MemcachedNode> connections=new ArrayList<MemcachedNode>(a.size());\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tMemcachedNode qa=f.createMemcachedNode(sa, ch, bufSize);\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tconnected(qa);\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.setSk(ch.register(selector, ops, qa));\n\t\t\tassert ch.isConnected()\n\t\t\t\t|| qa.getSk().interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\tconnections.add(qa);\n\t\t}\n\t\tlocator=f.createLocator(connections);\n\t}\n\n\t/**\n\t * Enable or disable get optimization.\n\t *\n\t * When enabled (default), multiple sequential gets are collapsed into one.\n\t */\n\tpublic void setGetOptimization(boolean to) {\n\t\toptimizeGets=to;\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getSk() != null && qa.getSk().isValid()) {\n\t\t\t\tif(qa.getChannel().isConnected()) {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.getSk().interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty() && !shutDown) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > DOUBLE_CHECK_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlostConnection((MemcachedNode)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\t\tif(!shutDown && !reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<MemcachedNode> toAdd=new HashSet<MemcachedNode>();\n\t\t\t// Transfer the queue into a hashset.  There are very likely more\n\t\t\t// additions than there are nodes.\n\t\t\tCollection<MemcachedNode> todo=new HashSet<MemcachedNode>();\n\t\t\ttry {\n\t\t\t\tMemcachedNode qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\ttodo.add(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// Found everything\n\t\t\t}\n\n\t\t\t// Now process the queue.\n\t\t\tfor(MemcachedNode qa : todo) {\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\tif(qa.isActive()) {\n\t\t\t\t\tif(qa.getCurrentWriteOp() != null) {\n\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t}\n\t\t\t\tqa.copyInputQueue();\n\t\t\t\tif(readyForIO) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\t\thandleWrites(qa.getSk(), qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\tlostConnection(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tqa.fixupOps();\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t/**\n\t * Add a connection observer.\n\t *\n\t * @return whether the observer was successfully added\n\t */\n\tpublic boolean addObserver(ConnectionObserver obs) {\n\t\treturn connObservers.add(obs);\n\t}\n\n\t/**\n\t * Remove a connection observer.\n\t *\n\t * @return true if the observer existed and now doesn't\n\t */\n\tpublic boolean removeObserver(ConnectionObserver obs) {\n\t\treturn connObservers.remove(obs);\n\t}\n\n\tprivate void connected(MemcachedNode qa) {\n\t\tassert qa.getChannel().isConnected() : \"Not connected.\";\n\t\tint rt = qa.getReconnectCount();\n\t\tqa.connected();\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionEstablished(qa.getSocketAddress(), rt);\n\t\t}\n\t}\n\n\tprivate void lostConnection(MemcachedNode qa) {\n\t\tqueueReconnect(qa);\n\t\tfor(ConnectionObserver observer : connObservers) {\n\t\t\tobserver.connectionLost(qa.getSocketAddress());\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tMemcachedNode qa=(MemcachedNode)sk.attachment();\n\t\ttry {\n\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Handling IO for:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\tif(sk.isConnectable()) {\n\t\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\t\tfinal SocketChannel channel=qa.getChannel();\n\t\t\t\tif(channel.finishConnect()) {\n\t\t\t\t\tconnected(qa);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.getWbuf().hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t}\n\t\t\t\tif(sk.isWritable()) {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(ClosedChannelException e) {\n\t\t\tif(!shutDown) {\n\t\t\t\tgetLogger().info(\"Closed channel and not shutting down.  \"\n\t\t\t\t\t+ \"Queueing reconnect on %s\", qa, e);\n\t\t\t\tlostConnection(qa);\n\t\t\t}\n\t\t} catch(ConnectException e) {\n\t\t\t// Failures to establish a connection should attempt a reconnect\n\t\t\t// without signaling the observers.\n\t\t\tgetLogger().info(\"Reconnecting due to failure to connect to %s\",\n\t\t\t\t\tqa, e);\n\t\t\tqueueReconnect(qa);\n\t\t} catch(Exception e) {\n\t\t\t// Various errors occur on Linux that wind up here.  However, any\n\t\t\t// particular error processing an item should simply cause us to\n\t\t\t// reconnect to the server.\n\t\t\tgetLogger().info(\"Reconnecting due to exception on %s\", qa, e);\n\t\t\tlostConnection(qa);\n\t\t}\n\t\tqa.fixupOps();\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(optimizeGets);\n\t\tboolean canWriteMore=qa.getBytesRemainingToWrite() > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.writeSome();\n\t\t\tqa.fillWriteBuffer(optimizeGets);\n\t\t\tcanWriteMore = wrote > 0 && qa.getBytesRemainingToWrite() > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, MemcachedNode qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tByteBuffer rbuf=qa.getRbuf();\n\t\tfinal SocketChannel channel = qa.getChannel();\n\t\tint read=channel.read(rbuf);\n\t\tif (read < 0) {\n\t\t    // GRUMBLE.\n\t\t    throw new IOException(\"Disconnected\");\n\t\t}\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\trbuf.flip();\n\t\t\twhile(rbuf.remaining() > 0) {\n\t\t\t\tif(currentOp == null) {\n\t\t\t\t\tthrow new IllegalStateException(\"No read operation.\");\n\t\t\t\t}\n\t\t\t\tcurrentOp.readFromBuffer(rbuf);\n\t\t\t\tif(currentOp.getState() == OperationState.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t}\n\t\t\t}\n\t\t\trbuf.clear();\n\t\t\tread=channel.read(rbuf);\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(MemcachedNode qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.getReconnectCount());\n\t\t\tif(qa.getSk() != null) {\n\t\t\t\tqa.getSk().cancel();\n\t\t\t\tassert !qa.getSk().isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnecting();\n\t\t\ttry {\n\t\t\t\tif(qa.getChannel() != null && qa.getChannel().socket() != null) {\n\t\t\t\t\tqa.getChannel().socket().close();\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\"The channel or socket was null for %s\",\n\t\t\t\t\t\tqa);\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.setChannel(null);\n\n\t\t\tlong delay=Math.min((100*qa.getReconnectCount()) ^ 2, MAX_DELAY);\n\t\t\tlong reconTime = System.currentTimeMillis() + delay;\n\n\t\t\t// Avoid potential condition where two connections are scheduled\n\t\t\t// for reconnect at the exact same time.  This is expected to be\n\t\t\t// a rare situation.\n\t\t\twhile(reconnectQueue.containsKey(reconTime)) {\n\t\t\t\treconTime++;\n\t\t\t}\n\n\t\t\treconnectQueue.put(reconTime, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\n\t\t\tif(failureMode == FailureMode.Redistribute) {\n\t\t\t\tredistributeOperations(qa.destroyInputQueue());\n\t\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\t\tcancelOperations(qa.destroyInputQueue());\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void cancelOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\top.cancel();\n\t\t}\n\t}\n\n\tprivate void redistributeOperations(Collection<Operation> ops) {\n\t\tfor(Operation op : ops) {\n\t\t\tif(op instanceof KeyedOperation) {\n\t\t\t\tKeyedOperation ko = (KeyedOperation)op;\n\t\t\t\tint added = 0;\n\t\t\t\tfor(String k : ko.getKeys()) {\n\t\t\t\t\tfor(Operation newop : opFact.clone(ko)) {\n\t\t\t\t\t\taddOperation(k, newop);\n\t\t\t\t\t\tadded++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert added > 0\n\t\t\t\t\t: \"Didn't add any new operations when redistributing\";\n\t\t\t} else {\n\t\t\t\t// Cancel things that don't have definite targets.\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tfinal long now=System.currentTimeMillis();\n\t\tfinal Map<MemcachedNode, Boolean> seen=\n\t\t\tnew IdentityHashMap<MemcachedNode, Boolean>();\n\t\tfinal List<MemcachedNode> rereQueue=new ArrayList<MemcachedNode>();\n\t\tfor(Iterator<MemcachedNode> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tfinal MemcachedNode qa=i.next();\n\t\t\ti.remove();\n\t\t\ttry {\n\t\t\t\tif(!seen.containsKey(qa)) {\n\t\t\t\t\tseen.put(qa, Boolean.TRUE);\n\t\t\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\t\t\tfinal SocketChannel ch=SocketChannel.open();\n\t\t\t\t\tch.configureBlocking(false);\n\t\t\t\t\tint ops=0;\n\t\t\t\t\tif(ch.connect(qa.getSocketAddress())) {\n\t\t\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\t\t\tassert ch.isConnected();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t\t\t}\n\t\t\t\t\tqa.registerChannel(ch, ch.register(selector, ops, qa));\n\t\t\t\t\tassert qa.getChannel() == ch : \"Channel was lost.\";\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Skipping duplicate reconnect request for %s\", qa);\n\t\t\t\t}\n\t\t\t} catch(ConnectException e) {\n\t\t\t\tgetLogger().warn(\"Error on reconnect\", e);\n\t\t\t\trereQueue.add(qa);\n\t\t\t}\n\t\t}\n\t\t// Requeue any fast-failed connects.\n\t\tfor(MemcachedNode n : rereQueue) {\n\t\t\tqueueReconnect(n);\n\t\t}\n\t}\n\n\t/**\n\t * Get the node locator used by this connection.\n\t */\n\tNodeLocator getLocator() {\n\t\treturn locator;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t *\n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\tpublic void addOperation(final String key, final Operation o) {\n\t\tMemcachedNode placeIn=null;\n\t\tMemcachedNode primary = locator.getPrimary(key);\n\t\tif(primary.isActive() || failureMode == FailureMode.Retry) {\n\t\t\tplaceIn=primary;\n\t\t} else if(failureMode == FailureMode.Cancel) {\n\t\t\to.cancel();\n\t\t} else {\n\t\t\t// Look for another node in sequence that is ready.\n\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\tplaceIn == null && i.hasNext(); ) {\n\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\tif(n.isActive()) {\n\t\t\t\t\tplaceIn=n;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we didn't find an active node, queue it in the primary node\n\t\t\t// and wait for it to come back online.\n\t\t\tif(placeIn == null) {\n\t\t\t\tplaceIn = primary;\n\t\t\t}\n\t\t}\n\n\t\tassert o.isCancelled() || placeIn != null\n\t\t\t: \"No node found for key \" + key;\n\t\tif(placeIn != null) {\n\t\t\taddOperation(placeIn, o);\n\t\t} else {\n\t\t\tassert o.isCancelled() : \"No not found for \"\n\t\t\t\t+ key + \" (and not immediately cancelled)\";\n\t\t}\n\t}\n\n\tpublic void addOperation(final MemcachedNode node, final Operation o) {\n\t\to.initialize();\n\t\tnode.addOp(o);\n\t\taddedQueue.offer(node);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %s\", o, node);\n\t}\n\n\tpublic void addOperations(final Map<MemcachedNode, Operation> ops) {\n\n\t\tfor(Map.Entry<MemcachedNode, Operation> me : ops.entrySet()) {\n\t\t\tfinal MemcachedNode node=me.getKey();\n\t\t\tOperation o=me.getValue();\n\t\t\to.initialize();\n\t\t\tnode.addOp(o);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t}\n\n\t/**\n\t * Broadcast an operation to all nodes.\n\t */\n\tpublic CountDownLatch broadcastOperation(final BroadcastOpFactory of) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(locator.getAll().size());\n\t\tfor(MemcachedNode node : locator.getAll()) {\n\t\t\tOperation op = of.newOp(node, latch);\n\t\t\top.initialize();\n\t\t\tnode.addOp(op);\n\t\t\taddedQueue.offer(node);\n\t\t}\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\treturn latch;\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tshutDown=true;\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tif(qa.getChannel() != null) {\n\t\t\t\tqa.getChannel().close();\n\t\t\t\tqa.setSk(null);\n\t\t\t\tif(qa.getBytesRemainingToWrite() > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.getBytesRemainingToWrite());\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.getChannel());\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(MemcachedNode qa : locator.getAll()) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.getSocketAddress());\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n}\n","lineNo":415}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached.transcoders;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\nimport java.io.UnsupportedEncodingException;\nimport java.util.Date;\nimport java.util.zip.GZIPInputStream;\nimport java.util.zip.GZIPOutputStream;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.CachedData;\nimport net.spy.util.CloseUtil;\n\n/**\n * Transcoder that serializes and compresses objects.\n */\npublic final class SerializingTranscoder extends SpyObject\n\timplements Transcoder<Object> {\n\n\t/**\n\t * Default compression threshold value.\n\t */\n\tpublic static final int DEFAULT_COMPRESSION_THRESHOLD = 16384;\n\n\t// General flags\n\tstatic final int SERIALIZED=1;\n\tstatic final int COMPRESSED=2;\n\n\t// Special flags for specially handled types.\n\tprivate static final int SPECIAL_MASK=0xff00;\n\tstatic final int SPECIAL_BOOLEAN=(1<<8);\n\tstatic final int SPECIAL_INT=(2<<8);\n\tstatic final int SPECIAL_LONG=(3<<8);\n\tstatic final int SPECIAL_DATE=(4<<8);\n\tstatic final int SPECIAL_BYTE=(5<<8);\n\tstatic final int SPECIAL_FLOAT=(6<<8);\n\tstatic final int SPECIAL_DOUBLE=(7<<8);\n\tstatic final int SPECIAL_BYTEARRAY=(8<<8);\n\n\tprivate static final String DEFAULT_CHARSET = \"UTF-8\";\n\n\tprivate int compressionThreshold=DEFAULT_COMPRESSION_THRESHOLD;\n\tprivate String charset=DEFAULT_CHARSET;\n\n\t/**\n\t * Set the compression threshold to the given number of bytes.  This\n\t * transcoder will attempt to compress any data being stored that's larger\n\t * than this.\n\t *\n\t * @param to the number of bytes\n\t */\n\tpublic void setCompressionThreshold(int to) {\n\t\tcompressionThreshold=to;\n\t}\n\n\t/**\n\t * Set the character set for string value transcoding (defaults to UTF-8).\n\t */\n\tpublic void setCharset(String to) {\n\t\t// Validate the character set.\n\t\ttry {\n\t\t\tnew String(new byte[97], to);\n\t\t} catch (UnsupportedEncodingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t\tcharset=to;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.Transcoder#decode(net.spy.memcached.CachedData)\n\t */\n\tpublic Object decode(CachedData d) {\n\t\tbyte[] data=d.getData();\n\t\tObject rv=null;\n\t\tif((d.getFlags() & COMPRESSED) != 0) {\n\t\t\tdata=decompress(d.getData());\n\t\t}\n\t\tif((d.getFlags() & SERIALIZED) != 0) {\n\t\t\trv=deserialize(data);\n\t\t} else if((d.getFlags() & SPECIAL_MASK) != 0) {\n\t\t\tswitch(d.getFlags() & SPECIAL_MASK) {\n\t\t\t\tcase SPECIAL_BOOLEAN:\n\t\t\t\t\trv=Boolean.valueOf(TranscoderUtils.decodeBoolean(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_INT:\n\t\t\t\t\trv=new Integer(TranscoderUtils.decodeInt(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_LONG:\n\t\t\t\t\trv=new Long(TranscoderUtils.decodeLong(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_DATE:\n\t\t\t\t\trv=new Date(TranscoderUtils.decodeLong(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_BYTE:\n\t\t\t\t\trv=new Byte(TranscoderUtils.decodeByte(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_FLOAT:\n\t\t\t\t\trv=new Float(Float.intBitsToFloat(TranscoderUtils.decodeInt(data)));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_DOUBLE:\n\t\t\t\t\trv=new Double(Double.longBitsToDouble(TranscoderUtils.decodeLong(data)));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_BYTEARRAY:\n\t\t\t\t\trv=data;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault: assert false;\n\t\t\t}\n\t\t} else {\n\t\t\ttry {\n\t\t\t\trv=new String(data, charset);\n\t\t\t} catch (UnsupportedEncodingException e) {\n\t\t\t\tthrow new RuntimeException(e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.Transcoder#encode(java.lang.Object)\n\t */\n\tpublic CachedData encode(Object o) {\n\t\tCachedData rv=null;\n\t\tbyte[] b=null;\n\t\tint flags=0;\n\t\tif(o instanceof String) {\n\t\t\ttry {\n\t\t\t\tb=((String)o).getBytes(charset);\n\t\t\t} catch (UnsupportedEncodingException e) {\n\t\t\t\tthrow new RuntimeException(e);\n\t\t\t}\n\t\t} else if(o instanceof Long) {\n\t\t\tb=TranscoderUtils.encodeLong((Long)o);\n\t\t\tflags |= SPECIAL_LONG;\n\t\t} else if(o instanceof Integer) {\n\t\t\tb=TranscoderUtils.encodeInt((Integer)o);\n\t\t\tflags |= SPECIAL_INT;\n\t\t} else if(o instanceof Boolean) {\n\t\t\tb=TranscoderUtils.encodeBoolean((Boolean)o);\n\t\t\tflags |= SPECIAL_BOOLEAN;\n\t\t} else if(o instanceof Date) {\n\t\t\tb=TranscoderUtils.encodeLong(((Date)o).getTime());\n\t\t\tflags |= SPECIAL_DATE;\n\t\t} else if(o instanceof Byte) {\n\t\t\tb=TranscoderUtils.encodeByte((Byte)o);\n\t\t\tflags |= SPECIAL_BYTE;\n\t\t} else if(o instanceof Float) {\n\t\t\tb=TranscoderUtils.encodeInt(Float.floatToRawIntBits((Float)o));\n\t\t\tflags |= SPECIAL_FLOAT;\n\t\t} else if(o instanceof Double) {\n\t\t\tb=TranscoderUtils.encodeLong(Double.doubleToRawLongBits((Double)o));\n\t\t\tflags |= SPECIAL_DOUBLE;\n\t\t} else if(o instanceof byte[]) {\n\t\t\tb=(byte[])o;\n\t\t\tflags |= SPECIAL_BYTEARRAY;\n\t\t} else {\n\t\t\tb=serialize(o);\n\t\t\tflags |= SERIALIZED;\n\t\t}\n\t\tif(b != null) {\n\t\t\tif(b.length > compressionThreshold) {\n\t\t\t\tbyte[] compressed=compress(b);\n\t\t\t\tif(compressed.length < b.length) {\n\t\t\t\t\tgetLogger().info(\"Compressed %s from %d to %d\",\n\t\t\t\t\t\to.getClass().getName(), b.length, compressed.length);\n\t\t\t\t\tb=compressed;\n\t\t\t\t\tflags |= COMPRESSED;\n\t\t\t\t} else {\n\t\t\t\t\tgetLogger().info(\n\t\t\t\t\t\t\"Compression increased the size of %s from %d to %d\",\n\t\t\t\t\t\to.getClass().getName(), b.length, compressed.length);\n\t\t\t\t}\n\t\t\t}\n\t\t\trv=new CachedData(flags, b);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate byte[] serialize(Object o) {\n\t\tassert o != null;\n\t\tbyte[] rv=null;\n\t\ttry {\n\t\t\tByteArrayOutputStream bos=new ByteArrayOutputStream();\n\t\t\tObjectOutputStream os=new ObjectOutputStream(bos);\n\t\t\tos.writeObject(o);\n\t\t\tos.close();\n\t\t\tbos.close();\n\t\t\trv=bos.toByteArray();\n\t\t} catch(IOException e) {\n\t\t\tthrow new IllegalArgumentException(\"Non-serializable object\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate Object deserialize(byte[] in) {\n\t\tObject rv=null;\n\t\tassert in != null;\n\t\ttry {\n\t\t\tByteArrayInputStream bis=new ByteArrayInputStream(in);\n\t\t\tObjectInputStream is=new ObjectInputStream(bis);\n\t\t\trv=is.readObject();\n\t\t\tis.close();\n\t\t\tbis.close();\n\t\t} catch(IOException e) {\n\t\t\tgetLogger().warn(\"Caught IOException decoding %d bytes of data\",\n\t\t\t\t\tin.length, e);\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tgetLogger().warn(\"Caught CNFE decoding %d bytes of data\",\n\t\t\t\t\tin.length, e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate byte[] compress(byte[] in) {\n\t\tassert in != null;\n\t\tByteArrayOutputStream bos=new ByteArrayOutputStream();\n\t\tGZIPOutputStream gz=null;\n\t\ttry {\n\t\t\tgz = new GZIPOutputStream(bos);\n\t\t\tgz.write(in);\n\t\t} catch (IOException e) {\n\t\t\tthrow new RuntimeException(\"IO exception compressing data\", e);\n\t\t} finally {\n\t\t\tCloseUtil.close(gz);\n\t\t\tCloseUtil.close(bos);\n\t\t}\n\t\tbyte[] rv=bos.toByteArray();\n\t\tgetLogger().debug(\"Compressed %d bytes to %d\", in.length, rv.length);\n\t\treturn rv;\n\t}\n\n\tprivate byte[] decompress(byte[] in) {\n\t\tassert in != null;\n\t\tByteArrayInputStream bis=new ByteArrayInputStream(in);\n\t\tByteArrayOutputStream bos=new ByteArrayOutputStream();\n\t\tGZIPInputStream gis;\n\t\ttry {\n\t\t\tgis = new GZIPInputStream(bis);\n\n\t\t\tbyte[] buf=new byte[8192];\n\t\t\tint r=-1;\n\t\t\twhile((r=gis.read(buf)) > 0) {\n\t\t\t\tbos.write(buf, 0, r);\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new RuntimeException(\"Error decompressing data\", e);\n\t\t}\n\t\treturn bos.toByteArray();\n\t}\n\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached.transcoders;\n\nimport java.util.Date;\n\nimport net.spy.memcached.CachedData;\n\n/**\n * Transcoder that serializes and compresses objects.\n */\npublic class SerializingTranscoder extends BaseSerializingTranscoder\n\timplements Transcoder<Object> {\n\n\t// General flags\n\tstatic final int SERIALIZED=1;\n\tstatic final int COMPRESSED=2;\n\n\t// Special flags for specially handled types.\n\tprivate static final int SPECIAL_MASK=0xff00;\n\tstatic final int SPECIAL_BOOLEAN=(1<<8);\n\tstatic final int SPECIAL_INT=(2<<8);\n\tstatic final int SPECIAL_LONG=(3<<8);\n\tstatic final int SPECIAL_DATE=(4<<8);\n\tstatic final int SPECIAL_BYTE=(5<<8);\n\tstatic final int SPECIAL_FLOAT=(6<<8);\n\tstatic final int SPECIAL_DOUBLE=(7<<8);\n\tstatic final int SPECIAL_BYTEARRAY=(8<<8);\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.Transcoder#decode(net.spy.memcached.CachedData)\n\t */\n\tpublic Object decode(CachedData d) {\n\t\tbyte[] data=d.getData();\n\t\tObject rv=null;\n\t\tif((d.getFlags() & COMPRESSED) != 0) {\n\t\t\tdata=decompress(d.getData());\n\t\t}\n\t\tint flags=d.getFlags() & SPECIAL_MASK;\n\t\tif((d.getFlags() & SERIALIZED) != 0 && data != null) {\n\t\t\trv=deserialize(data);\n\t\t} else if(flags != 0 && data != null) {\n\t\t\tswitch(flags) {\n\t\t\t\tcase SPECIAL_BOOLEAN:\n\t\t\t\t\trv=Boolean.valueOf(TranscoderUtils.decodeBoolean(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_INT:\n\t\t\t\t\trv=new Integer(TranscoderUtils.decodeInt(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_LONG:\n\t\t\t\t\trv=new Long(TranscoderUtils.decodeLong(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_DATE:\n\t\t\t\t\trv=new Date(TranscoderUtils.decodeLong(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_BYTE:\n\t\t\t\t\trv=new Byte(TranscoderUtils.decodeByte(data));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_FLOAT:\n\t\t\t\t\trv=new Float(Float.intBitsToFloat(\n\t\t\t\t\t\t\tTranscoderUtils.decodeInt(data)));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_DOUBLE:\n\t\t\t\t\trv=new Double(Double.longBitsToDouble(\n\t\t\t\t\t\t\tTranscoderUtils.decodeLong(data)));\n\t\t\t\t\tbreak;\n\t\t\t\tcase SPECIAL_BYTEARRAY:\n\t\t\t\t\trv=data;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tgetLogger().warn(\"Undecodeable with flags %x\", flags);\n\t\t\t}\n\t\t} else {\n\t\t\trv=decodeString(data);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/* (non-Javadoc)\n\t * @see net.spy.memcached.Transcoder#encode(java.lang.Object)\n\t */\n\tpublic CachedData encode(Object o) {\n\t\tbyte[] b=null;\n\t\tint flags=0;\n\t\tif(o instanceof String) {\n\t\t\tb=encodeString((String)o);\n\t\t} else if(o instanceof Long) {\n\t\t\tb=TranscoderUtils.encodeLong((Long)o);\n\t\t\tflags |= SPECIAL_LONG;\n\t\t} else if(o instanceof Integer) {\n\t\t\tb=TranscoderUtils.encodeInt((Integer)o);\n\t\t\tflags |= SPECIAL_INT;\n\t\t} else if(o instanceof Boolean) {\n\t\t\tb=TranscoderUtils.encodeBoolean((Boolean)o);\n\t\t\tflags |= SPECIAL_BOOLEAN;\n\t\t} else if(o instanceof Date) {\n\t\t\tb=TranscoderUtils.encodeLong(((Date)o).getTime());\n\t\t\tflags |= SPECIAL_DATE;\n\t\t} else if(o instanceof Byte) {\n\t\t\tb=TranscoderUtils.encodeByte((Byte)o);\n\t\t\tflags |= SPECIAL_BYTE;\n\t\t} else if(o instanceof Float) {\n\t\t\tb=TranscoderUtils.encodeInt(Float.floatToRawIntBits((Float)o));\n\t\t\tflags |= SPECIAL_FLOAT;\n\t\t} else if(o instanceof Double) {\n\t\t\tb=TranscoderUtils.encodeLong(Double.doubleToRawLongBits((Double)o));\n\t\t\tflags |= SPECIAL_DOUBLE;\n\t\t} else if(o instanceof byte[]) {\n\t\t\tb=(byte[])o;\n\t\t\tflags |= SPECIAL_BYTEARRAY;\n\t\t} else {\n\t\t\tb=serialize(o);\n\t\t\tflags |= SERIALIZED;\n\t\t}\n\t\tassert b != null;\n\t\tif(b.length > compressionThreshold) {\n\t\t\tbyte[] compressed=compress(b);\n\t\t\tif(compressed.length < b.length) {\n\t\t\t\tgetLogger().info(\"Compressed %s from %d to %d\",\n\t\t\t\t\to.getClass().getName(), b.length, compressed.length);\n\t\t\t\tb=compressed;\n\t\t\t\tflags |= COMPRESSED;\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\n\t\t\t\t\t\"Compression increased the size of %s from %d to %d\",\n\t\t\t\t\to.getClass().getName(), b.length, compressed.length);\n\t\t\t}\n\t\t}\n\t\treturn new CachedData(flags, b);\n\t}\n\n}\n","lineNo":39}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached.protocol.ascii;\n\nimport java.nio.ByteBuffer;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashSet;\n\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\n\n/**\n * Operation for retrieving data.\n */\nclass GetOperationImpl extends OperationImpl implements GetOperation {\n\n\tprivate static final OperationStatus END=\n\t\tnew OperationStatus(true, \"END\");\n\n\tprivate final Collection<String> keys;\n\tprivate String currentKey=null;\n\tprivate int currentFlags=0;\n\tprivate byte[] data=null;\n\tprivate int readOffset=0;\n\t// Character we're looking for after a data read\n\tprivate byte lookingFor='\\0';\n\n\tprivate GetOperation.Callback cb=null;\n\n\tpublic GetOperationImpl(String key, GetOperation.Callback c) {\n\t\tsuper(c);\n\t\tkeys=Collections.singleton(key);\n\t\tcb=c;\n\t}\n\n\tpublic GetOperationImpl(Collection<String> k, GetOperation.Callback c) {\n\t\tsuper(c);\n\t\tkeys=new HashSet<String>(k);\n\t\tcb=c;\n\t}\n\n\t/**\n\t * Get the keys this GetOperation is looking for.\n\t */\n\tpublic final Collection<String> getKeys() {\n\t\treturn keys;\n\t}\n\n\t@Override\n\tpublic final void handleLine(String line) {\n\t\tif(line.equals(\"END\")) {\n\t\t\tgetLogger().debug(\"Get complete!\");\n\t\t\tcb.receivedStatus(END);\n\t\t\ttransitionState(OperationState.COMPLETE);\n\t\t\tdata=null;\n\t\t} else if(line.startsWith(\"VALUE \")) {\n\t\t\tgetLogger().debug(\"Got line %s\", line);\n\t\t\tString[] stuff=line.split(\" \");\n\t\t\tassert stuff[0].equals(\"VALUE\");\n\t\t\tcurrentKey=stuff[1];\n\t\t\tcurrentFlags=Integer.parseInt(stuff[2]);\n\t\t\tdata=new byte[Integer.parseInt(stuff[3])];\n\t\t\treadOffset=0;\n\t\t\tgetLogger().debug(\"Set read type to data\");\n\t\t\tsetReadType(OperationReadType.DATA);\n\t\t} else {\n\t\t\tassert false : \"Unknown line type: \" + line;\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void handleRead(ByteBuffer b) {\n\t\tassert currentKey != null;\n\t\tassert data != null;\n\t\t// This will be the case, because we'll clear them when it's not.\n\t\tassert readOffset <= data.length\n\t\t\t: \"readOffset is \" + readOffset + \" data.length is \" + data.length;\n\n\t\tgetLogger().debug(\"readOffset: %d, length: %d\",\n\t\t\t\treadOffset, data.length);\n\t\t// If we're not looking for termination, we're still looking for data\n\t\tif(lookingFor == '\\0') {\n\t\t\tint toRead=data.length - readOffset;\n\t\t\tint available=b.remaining();\n\t\t\ttoRead=Math.min(toRead, available);\n\t\t\tgetLogger().debug(\"Reading %d bytes\", toRead);\n\t\t\tb.get(data, readOffset, toRead);\n\t\t\treadOffset+=toRead;\n\t\t}\n\t\t// Transition us into a ``looking for \\r\\n'' kind of state if we've\n\t\t// read enough and are still in a data state.\n\t\tif(readOffset == data.length && lookingFor == '\\0') {\n\t\t\tcb.gotData(currentKey, currentFlags, data);\n\t\t\tlookingFor='\\r';\n\t\t}\n\t\t// If we're looking for an ending byte, let's go find it.\n\t\tif(lookingFor != '\\0' && b.hasRemaining()) {\n\t\t\tdo {\n\t\t\t\tbyte tmp=b.get();\n\t\t\t\tassert tmp == lookingFor : \"Expecting \" + lookingFor + \", got \"\n\t\t\t\t\t+ (char)tmp;\n\t\t\t\tswitch(lookingFor) {\n\t\t\t\t\tcase '\\r': lookingFor='\\n'; break;\n\t\t\t\t\tcase '\\n': lookingFor='\\0'; break;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tassert false: \"Looking for unexpected char: \"\n\t\t\t\t\t\t\t+ (char)lookingFor;\n\t\t\t\t}\n\t\t\t} while(lookingFor != '\\0' && b.hasRemaining());\n\t\t\t// Completed the read, reset stuff.\n\t\t\tif(lookingFor == '\\0') {\n\t\t\t\tcurrentKey=null;\n\t\t\t\tdata=null;\n\t\t\t\treadOffset=0;\n\t\t\t\tcurrentFlags=0;\n\t\t\t\tgetLogger().debug(\"Setting read type back to line.\");\n\t\t\t\tsetReadType(OperationReadType.LINE);\n\t\t\t}\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void initialize() {\n\t\t// Figure out the length of the request\n\t\tint size=\"get\\r\\n\".length();\n\t\tfor(String s : keys) {\n\t\t\tsize+=s.length();\n\t\t\tsize++;\n\t\t}\n\t\tByteBuffer b=ByteBuffer.allocate(size);\n\t\tb.put(\"get\".getBytes());\n\t\tfor(String s : keys) {\n\t\t\tb.put((byte)' ');\n\t\t\tb.put(s.getBytes());\n\t\t}\n\t\tb.put(\"\\r\\n\".getBytes());\n\t\tb.flip();\n\t\tsetBuffer(b);\n\t}\n\n\t@Override\n\tprotected final void wasCancelled() {\n\t\tcb.receivedStatus(CANCELLED);\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached.protocol.ascii;\n\nimport java.nio.ByteBuffer;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashSet;\n\nimport net.spy.memcached.KeyUtil;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\n\n/**\n * Operation for retrieving data.\n */\nclass GetOperationImpl extends OperationImpl implements GetOperation {\n\n\tprivate static final OperationStatus END=\n\t\tnew OperationStatus(true, \"END\");\n\n\tprivate final Collection<String> keys;\n\tprivate String currentKey=null;\n\tprivate int currentFlags=0;\n\tprivate byte[] data=null;\n\tprivate int readOffset=0;\n\t// Character we're looking for after a data read\n\tprivate byte lookingFor='\\0';\n\n\tprivate GetOperation.Callback cb=null;\n\n\tpublic GetOperationImpl(String key, GetOperation.Callback c) {\n\t\tsuper(c);\n\t\tkeys=Collections.singleton(key);\n\t\tcb=c;\n\t}\n\n\tpublic GetOperationImpl(Collection<String> k, GetOperation.Callback c) {\n\t\tsuper(c);\n\t\tkeys=new HashSet<String>(k);\n\t\tcb=c;\n\t}\n\n\t/**\n\t * Get the keys this GetOperation is looking for.\n\t */\n\tpublic final Collection<String> getKeys() {\n\t\treturn keys;\n\t}\n\n\t@Override\n\tpublic final void handleLine(String line) {\n\t\tif(line.equals(\"END\")) {\n\t\t\tgetLogger().debug(\"Get complete!\");\n\t\t\tcb.receivedStatus(END);\n\t\t\ttransitionState(OperationState.COMPLETE);\n\t\t\tdata=null;\n\t\t} else if(line.startsWith(\"VALUE \")) {\n\t\t\tgetLogger().debug(\"Got line %s\", line);\n\t\t\tString[] stuff=line.split(\" \");\n\t\t\tassert stuff[0].equals(\"VALUE\");\n\t\t\tcurrentKey=stuff[1];\n\t\t\tcurrentFlags=Integer.parseInt(stuff[2]);\n\t\t\tdata=new byte[Integer.parseInt(stuff[3])];\n\t\t\treadOffset=0;\n\t\t\tgetLogger().debug(\"Set read type to data\");\n\t\t\tsetReadType(OperationReadType.DATA);\n\t\t} else {\n\t\t\tassert false : \"Unknown line type: \" + line;\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void handleRead(ByteBuffer b) {\n\t\tassert currentKey != null;\n\t\tassert data != null;\n\t\t// This will be the case, because we'll clear them when it's not.\n\t\tassert readOffset <= data.length\n\t\t\t: \"readOffset is \" + readOffset + \" data.length is \" + data.length;\n\n\t\tgetLogger().debug(\"readOffset: %d, length: %d\",\n\t\t\t\treadOffset, data.length);\n\t\t// If we're not looking for termination, we're still looking for data\n\t\tif(lookingFor == '\\0') {\n\t\t\tint toRead=data.length - readOffset;\n\t\t\tint available=b.remaining();\n\t\t\ttoRead=Math.min(toRead, available);\n\t\t\tgetLogger().debug(\"Reading %d bytes\", toRead);\n\t\t\tb.get(data, readOffset, toRead);\n\t\t\treadOffset+=toRead;\n\t\t}\n\t\t// Transition us into a ``looking for \\r\\n'' kind of state if we've\n\t\t// read enough and are still in a data state.\n\t\tif(readOffset == data.length && lookingFor == '\\0') {\n\t\t\tcb.gotData(currentKey, currentFlags, data);\n\t\t\tlookingFor='\\r';\n\t\t}\n\t\t// If we're looking for an ending byte, let's go find it.\n\t\tif(lookingFor != '\\0' && b.hasRemaining()) {\n\t\t\tdo {\n\t\t\t\tbyte tmp=b.get();\n\t\t\t\tassert tmp == lookingFor : \"Expecting \" + lookingFor + \", got \"\n\t\t\t\t\t+ (char)tmp;\n\t\t\t\tswitch(lookingFor) {\n\t\t\t\t\tcase '\\r': lookingFor='\\n'; break;\n\t\t\t\t\tcase '\\n': lookingFor='\\0'; break;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tassert false: \"Looking for unexpected char: \"\n\t\t\t\t\t\t\t+ (char)lookingFor;\n\t\t\t\t}\n\t\t\t} while(lookingFor != '\\0' && b.hasRemaining());\n\t\t\t// Completed the read, reset stuff.\n\t\t\tif(lookingFor == '\\0') {\n\t\t\t\tcurrentKey=null;\n\t\t\t\tdata=null;\n\t\t\t\treadOffset=0;\n\t\t\t\tcurrentFlags=0;\n\t\t\t\tgetLogger().debug(\"Setting read type back to line.\");\n\t\t\t\tsetReadType(OperationReadType.LINE);\n\t\t\t}\n\t\t}\n\t}\n\n\t@Override\n\tpublic final void initialize() {\n\t\t// Figure out the length of the request\n\t\tint size=\"get\\r\\n\".length();\n\t\tCollection<byte[]> keyBytes=KeyUtil.getKeyBytes(keys);\n\t\tfor(byte[] k : keyBytes) {\n\t\t\tsize+=k.length;\n\t\t\tsize++;\n\t\t}\n\t\tByteBuffer b=ByteBuffer.allocate(size);\n\t\tb.put(\"get\".getBytes());\n\t\tfor(byte[] k : keyBytes) {\n\t\t\tb.put((byte)' ');\n\t\t\tb.put(k);\n\t\t}\n\t\tb.put(\"\\r\\n\".getBytes());\n\t\tb.flip();\n\t\tsetBuffer(b);\n\t}\n\n\t@Override\n\tprotected final void wasCancelled() {\n\t\tcb.receivedStatus(CANCELLED);\n\t}\n}\n","lineNo":129}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.SpyThread;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *  MemcachedClient c=new MemcachedClient(\n *      new InetSocketAddress(\"hostname\", portNum));\n *\n *  // Store a value (async) for one hour\n *  c.set(\"someKey\", 3600, someObject);\n *  // Retrieve a value.\n *  Object myObject=c.get(\"someKey\");\n *  <\/pre>\n *\n *  <h2>Advanced Usage<\/h2>\n *\n *  <p>\n *   MemcachedClient may be processing a great deal of asynchronous messages or\n *   possibly dealing with an unreachable memcached, which may delay processing.\n *   If a memcached is disabled, for example, MemcachedConnection will continue\n *   to attempt to reconnect and replay pending operations until it comes back\n *   up.  To prevent this from causing your application to hang, you can use\n *   one of the asynchronous mechanisms to time out a request and cancel the\n *   operation to the server.\n *  <\/p>\n *\n *  <pre>\n *  // Get a memcached client connected to several servers\n *  MemcachedClient c=new MemcachedClient(\n *      AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *  // Try to get a value, for up to 5 seconds, and cancel if it doesn't return\n *  Object myObj=null;\n *  Future&lt;Object&gt; f=c.asyncGet(\"someKey\");\n *  try {\n *      myObj=f.get(5, TimeUnit.SECONDS);\n *  } catch(TimeoutException e) {\n *      // Since we don't need this, go ahead and cancel the operation.  This\n *      // is not strictly necessary, but it'll save some work on the server.\n *      f.cancel();\n *      // Do other timeout related stuff\n *  }\n * <\/pre>\n */\npublic final class MemcachedClient extends SpyThread {\n\n\tprivate static final int MAX_KEY_LENGTH = 250;\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tprivate HashAlgorithm hashAlg=HashAlgorithm.NATIVE_HASH;\n\n\tTranscoder transcoder=null;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param bufSize read buffer size per connection (in bytes)\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\ttranscoder=new SerializingTranscoder();\n\t\tconn=cf.createConnection(addrs);\n\t\topFact=cf.getOperationFactory();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tstart();\n\t}\n\n\t/**\n\t * Set the hash algorithm.\n\t */\n\tpublic HashAlgorithm getHashAlgorithm() {\n\t\treturn hashAlg;\n\t}\n\n\t/**\n\t * Set the hash algorithm for computing which server should receive\n\t * requests for a given key.\n\t */\n\tpublic void setHashAlgorithm(HashAlgorithm to) {\n\t\tif(to == null) {\n\t\t\tthrow new NullPointerException(\"Null hash algorithm not allowed\");\n\t\t}\n\t\thashAlg=to;\n\t}\n\n\t/**\n\t * Set the transcoder for managing the cache representations of objects\n\t * going in and out of the cache.\n\t */\n\tpublic void setTranscoder(Transcoder to) {\n\t\tif(to == null) {\n\t\t\tthrow new NullPointerException(\"Can't use a null transcoder\");\n\t\t}\n\t\ttranscoder=to;\n\t}\n\n\t/**\n\t * Get the current transcoder that's in use.\n\t */\n\tpublic Transcoder getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tif(key.length() > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(char c : key.toCharArray()) {\n\t\t\tif(Character.isWhitespace(c) || Character.isISOControl(c)) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tvalidateKey(key);\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tOperation addOp(final MemcachedNode node, final Operation op) {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t\tconn.addOperation(node, op);\n\t\treturn op;\n\t}\n\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, true);\n\t}\n\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of);\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\tCachedData co=transcoder.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Object> rv=new OperationFuture<Object>(latch);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate Object val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=transcoder.decode(new CachedData(flags, data));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t */\n\tpublic Object get(String key) {\n\t\ttry {\n\t\t\treturn asyncGet(key).get();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\tfinal Map<String, Object> m=new ConcurrentHashMap<String, Object>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tm.put(k, transcoder.decode(new CachedData(flags, data)));\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tops.add(addOp(me.getKey(), opFact.get(me.getValue(), cb)));\n\t\t}\n\t\treturn new BulkGetFuture(m, ops, latch);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys));\n\t}\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys).get();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys));\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\tprivate Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:  %s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol is\n\t\t\t\t\t\t// yet undefined.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tlatch.await();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, 0,\tString.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get()) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, 0);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t */\n\tpublic long incr(String key, int by, int def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @param when when the deletion should take effect\n\t */\n\tpublic Future<Boolean> delete(String key, int when) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch);\n\t\tDeleteOperation op=opFact.delete(key, when,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Shortcut to delete that will immediately delete the item from the cache.\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\treturn delete(key, 0);\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, false);\n\t\ttry {\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\tstatic class BulkGetFuture implements Future<Map<String, Object>> {\n\t\tprivate final Map<String, Object> rvMap;\n\t\tprivate final Collection<Operation> ops;\n\t\tprivate final CountDownLatch latch;\n\t\tprivate boolean cancelled=false;\n\n\t\tpublic BulkGetFuture(Map<String, Object> m,\n\t\t\t\tCollection<Operation> getOps, CountDownLatch l) {\n\t\t\tsuper();\n\t\t\trvMap = m;\n\t\t\tops = getOps;\n\t\t\tlatch=l;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tboolean rv=false;\n\t\t\tfor(Operation op : ops) {\n\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t\tcancelled=true;\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic Map<String, Object> get()\n\t\t\tthrows InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\"Timed out waiting forever\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic Map<String, Object> get(long timeout, TimeUnit unit)\n\t\t\tthrows InterruptedException,\n\t\t\tExecutionException, TimeoutException {\n\t\t\tlatch.await(timeout, unit);\n\t\t\tfor(Operation op : ops) {\n\t\t\t\tif(op.isCancelled()) {\n\t\t\t\t\tthrow new ExecutionException(\n\t\t\t\t\t\t\tnew RuntimeException(\"Cancelled\"));\n\t\t\t\t}\n\t\t\t\tif(op.hasErrored()) {\n\t\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn rvMap;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn cancelled;\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn latch.getCount() == 0;\n\t\t}\n\t}\n\n\tstatic class OperationFuture<T> implements Future<T> {\n\n\t\tprivate final CountDownLatch latch;\n\t\tprivate final AtomicReference<T> objRef;\n\t\tprivate Operation op;\n\n\t\tpublic OperationFuture(CountDownLatch l) {\n\t\t\tthis(l, new AtomicReference<T>(null));\n\t\t}\n\n\t\tpublic OperationFuture(CountDownLatch l, AtomicReference<T> oref) {\n\t\t\tsuper();\n\t\t\tlatch=l;\n\t\t\tobjRef=oref;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tassert op != null : \"No operation\";\n\t\t\top.cancel();\n\t\t\t// This isn't exactly correct, but it's close enough.  If we're in\n\t\t\t// a writing state, we *probably* haven't started.\n\t\t\treturn op.getState() == OperationState.WRITING;\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\tlatch.await();\n\t\t\tassert isDone() : \"Latch released, but operation wasn't done.\";\n\t\t\tif(op != null && op.hasErrored()) {\n\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t}\n\t\t\tif(isCancelled()) {\n\t\t\t\tthrow new ExecutionException(new RuntimeException(\"Cancelled\"));\n\t\t\t}\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException {\n\t\t\tlatch.await(duration, units);\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tvoid set(T o) {\n\t\t\tobjRef.set(o);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\top=to;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn op.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn latch.getCount() == 0 ||\n\t\t\t\top.isCancelled() || op.getState() == OperationState.COMPLETE;\n\t\t}\n\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport net.spy.SpyThread;\nimport net.spy.memcached.ops.DeleteOperation;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Mutator;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationCallback;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.ops.OperationStatus;\nimport net.spy.memcached.ops.StatsOperation;\nimport net.spy.memcached.ops.StoreType;\n\n/**\n * Client to a memcached server.\n *\n * <h2>Basic usage<\/h2>\n *\n * <pre>\n *  MemcachedClient c=new MemcachedClient(\n *      new InetSocketAddress(\"hostname\", portNum));\n *\n *  // Store a value (async) for one hour\n *  c.set(\"someKey\", 3600, someObject);\n *  // Retrieve a value.\n *  Object myObject=c.get(\"someKey\");\n *  <\/pre>\n *\n *  <h2>Advanced Usage<\/h2>\n *\n *  <p>\n *   MemcachedClient may be processing a great deal of asynchronous messages or\n *   possibly dealing with an unreachable memcached, which may delay processing.\n *   If a memcached is disabled, for example, MemcachedConnection will continue\n *   to attempt to reconnect and replay pending operations until it comes back\n *   up.  To prevent this from causing your application to hang, you can use\n *   one of the asynchronous mechanisms to time out a request and cancel the\n *   operation to the server.\n *  <\/p>\n *\n *  <pre>\n *  // Get a memcached client connected to several servers\n *  MemcachedClient c=new MemcachedClient(\n *      AddrUtil.getAddresses(\"server1:11211 server2:11211\"));\n *\n *  // Try to get a value, for up to 5 seconds, and cancel if it doesn't return\n *  Object myObj=null;\n *  Future&lt;Object&gt; f=c.asyncGet(\"someKey\");\n *  try {\n *      myObj=f.get(5, TimeUnit.SECONDS);\n *  } catch(TimeoutException e) {\n *      // Since we don't need this, go ahead and cancel the operation.  This\n *      // is not strictly necessary, but it'll save some work on the server.\n *      f.cancel();\n *      // Do other timeout related stuff\n *  }\n * <\/pre>\n */\npublic final class MemcachedClient extends SpyThread {\n\n\tprivate static final int MAX_KEY_LENGTH = 250;\n\n\tprivate volatile boolean running=true;\n\tprivate volatile boolean shuttingDown=false;\n\n\tprivate final MemcachedConnection conn;\n\tfinal OperationFactory opFact;\n\n\tprivate HashAlgorithm hashAlg=HashAlgorithm.NATIVE_HASH;\n\n\tTranscoder transcoder=null;\n\n\t/**\n\t * Get a memcache client operating on the specified memcached locations.\n\t *\n\t * @param ia the memcached locations\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(InetSocketAddress... ia) throws IOException {\n\t\tthis(new DefaultConnectionFactory(), Arrays.asList(ia));\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param addrs the socket addrs\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\tthis(new DefaultConnectionFactory(), addrs);\n\t}\n\n\t/**\n\t * Get a memcache client over the specified memcached locations.\n\t *\n\t * @param bufSize read buffer size per connection (in bytes)\n\t * @param addrs the socket addresses\n\t * @throws IOException if connections cannot be established\n\t */\n\tpublic MemcachedClient(ConnectionFactory cf, List<InetSocketAddress> addrs)\n\t\tthrows IOException {\n\t\ttranscoder=new SerializingTranscoder();\n\t\tconn=cf.createConnection(addrs);\n\t\topFact=cf.getOperationFactory();\n\t\tsetName(\"Memcached IO over \" + conn);\n\t\tstart();\n\t}\n\n\t/**\n\t * Set the hash algorithm.\n\t */\n\tpublic HashAlgorithm getHashAlgorithm() {\n\t\treturn hashAlg;\n\t}\n\n\t/**\n\t * Set the hash algorithm for computing which server should receive\n\t * requests for a given key.\n\t */\n\tpublic void setHashAlgorithm(HashAlgorithm to) {\n\t\tif(to == null) {\n\t\t\tthrow new NullPointerException(\"Null hash algorithm not allowed\");\n\t\t}\n\t\thashAlg=to;\n\t}\n\n\t/**\n\t * Set the transcoder for managing the cache representations of objects\n\t * going in and out of the cache.\n\t */\n\tpublic void setTranscoder(Transcoder to) {\n\t\tif(to == null) {\n\t\t\tthrow new NullPointerException(\"Can't use a null transcoder\");\n\t\t}\n\t\ttranscoder=to;\n\t}\n\n\t/**\n\t * Get the current transcoder that's in use.\n\t */\n\tpublic Transcoder getTranscoder() {\n\t\treturn transcoder;\n\t}\n\n\tprivate void validateKey(String key) {\n\t\tif(key.length() > MAX_KEY_LENGTH) {\n\t\t\tthrow new IllegalArgumentException(\"Key is too long (maxlen = \"\n\t\t\t\t\t+ MAX_KEY_LENGTH + \")\");\n\t\t}\n\t\t// Validate the key\n\t\tfor(char c : key.toCharArray()) {\n\t\t\tif(Character.isWhitespace(c) || Character.isISOControl(c)) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"Key contains invalid characters:  ``\" + key + \"''\");\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * (internal use) Add a raw operation to a numbered connection.\n\t * This method is exposed for testing.\n\t *\n\t * @param which server number\n\t * @param op the operation to perform\n\t * @return the Operation\n\t */\n\tOperation addOp(final String key, final Operation op) {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tvalidateKey(key);\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t\tconn.addOperation(key, op);\n\t\treturn op;\n\t}\n\n\tOperation addOp(final MemcachedNode node, final Operation op) {\n\t\tif(shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\tassert isAlive() : \"IO Thread is not running.\";\n\t\tconn.addOperation(node, op);\n\t\treturn op;\n\t}\n\n\n\tCountDownLatch broadcastOp(final BroadcastOpFactory of) {\n\t\treturn broadcastOp(of, true);\n\t}\n\n\n\tprivate CountDownLatch broadcastOp(BroadcastOpFactory of,\n\t\t\tboolean checkShuttingDown) {\n\t\tif(checkShuttingDown && shuttingDown) {\n\t\t\tthrow new IllegalStateException(\"Shutting down\");\n\t\t}\n\t\treturn conn.broadcastOperation(of);\n\t}\n\n\tprivate Future<Boolean> asyncStore(StoreType storeType,\n\t\t\tString key, int exp, Object value) {\n\t\tCachedData co=transcoder.encode(value);\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch);\n\t\tOperation op=opFact.store(storeType, key, co.getFlags(),\n\t\t\t\texp, co.getData(), new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus val) {\n\t\t\t\t\t\trv.set(val.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Add an object to the cache iff it does not exist already.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t */\n\tpublic Future<Boolean> add(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.add, key, exp, o);\n\t}\n\n\t/**\n\t * Set an object in the cache regardless of any existing value.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t */\n\tpublic Future<Boolean> set(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.set, key, exp, o);\n\t}\n\n\t/**\n\t * Replace an object with the given value iff there is already a value\n\t * for the given key.\n\t *\n\t * <p>\n\t * The <code>exp<\/code> value is passed along to memcached exactly as\n\t * given, and will be processed per the memcached protocol specification:\n\t * <\/p>\n\t *\n\t * <blockquote>\n\t * <p>\n\t * The actual value sent may either be\n\t * Unix time (number of seconds since January 1, 1970, as a 32-bit\n\t * value), or a number of seconds starting from current time. In the\n\t * latter case, this number of seconds may not exceed 60*60*24*30 (number\n\t * of seconds in 30 days); if the number sent by a client is larger than\n\t * that, the server will consider it to be real Unix time value rather\n\t * than an offset from current time.\n\t * <\/p>\n\t * <\/blockquote>\n\t *\n\t * @param key the key under which this object should be added.\n\t * @param exp the expiration of this object\n\t * @param o the object to store\n\t * @return a future representing the processing of this operation\n\t */\n\tpublic Future<Boolean> replace(String key, int exp, Object o) {\n\t\treturn asyncStore(StoreType.replace, key, exp, o);\n\t}\n\n\t/**\n\t * Get the given key asynchronously.\n\t *\n\t * @param key the key to fetch\n\t * @return a future that will hold the return value of the fetch\n\t */\n\tpublic Future<Object> asyncGet(final String key) {\n\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Object> rv=new OperationFuture<Object>(latch);\n\n\t\tOperation op=opFact.get(key,\n\t\t\t\tnew GetOperation.Callback() {\n\t\t\tprivate Object val=null;\n\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\trv.set(val);\n\t\t\t}\n\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\tassert key.equals(k) : \"Wrong key returned\";\n\t\t\t\tval=transcoder.decode(new CachedData(flags, data));\n\t\t\t}\n\t\t\tpublic void complete() {\n\t\t\t\tlatch.countDown();\n\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get with a single key.\n\t *\n\t * @param key the key to get\n\t * @return the result from the cache (null if there is none)\n\t */\n\tpublic Object get(String key) {\n\t\ttry {\n\t\t\treturn asyncGet(key).get();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for value\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Exception waiting for value\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Asynchronously get a bunch of objects from the cache.\n\t *\n\t * @param keys the keys to request\n\t * @return a Future result of that fetch\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(Collection<String> keys) {\n\t\tfinal Map<String, Object> m=new ConcurrentHashMap<String, Object>();\n\t\t// Break the gets down into groups by key\n\t\tfinal Map<MemcachedNode, Collection<String>> chunks\n\t\t\t=new HashMap<MemcachedNode, Collection<String>>();\n\t\tfinal NodeLocator locator=conn.getLocator();\n\t\tfor(String key : keys) {\n\t\t\tvalidateKey(key);\n\t\t\tfinal MemcachedNode primaryNode=locator.getPrimary(key);\n\t\t\tMemcachedNode node=null;\n\t\t\tif(primaryNode.isActive()) {\n\t\t\t\tnode=primaryNode;\n\t\t\t} else {\n\t\t\t\tfor(Iterator<MemcachedNode> i=locator.getSequence(key);\n\t\t\t\t\tnode == null && i.hasNext();) {\n\t\t\t\t\tMemcachedNode n=i.next();\n\t\t\t\t\tif(n.isActive()) {\n\t\t\t\t\t\tnode=n;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(node == null) {\n\t\t\t\t\tnode=primaryNode;\n\t\t\t\t}\n\t\t\t}\n\t\t\tassert node != null : \"Didn't find a node for \" + key;\n\t\t\tCollection<String> ks=chunks.get(node);\n\t\t\tif(ks == null) {\n\t\t\t\tks=new ArrayList<String>();\n\t\t\t\tchunks.put(node, ks);\n\t\t\t}\n\t\t\tks.add(key);\n\t\t}\n\t\tfinal CountDownLatch latch=new CountDownLatch(chunks.size());\n\t\tfinal Collection<Operation> ops=new ArrayList<Operation>();\n\n\t\tGetOperation.Callback cb=new GetOperation.Callback() {\n\t\t\t\t@SuppressWarnings(\"synthetic-access\")\n\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful get:  %s\", status);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void gotData(String k, int flags, byte[] data) {\n\t\t\t\t\tObject val = transcoder.decode(new CachedData(flags, data));\n\t\t\t\t\t// val may be null if the transcoder did not understand\n\t\t\t\t\t// the value.\n\t\t\t\t\tif(val != null) {\n\t\t\t\t\t\tm.put(k, val);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpublic void complete() {\n\t\t\t\t\tlatch.countDown();\n\t\t\t\t}\n\t\t};\n\t\tfor(Map.Entry<MemcachedNode, Collection<String>> me\n\t\t\t\t: chunks.entrySet()) {\n\t\t\tops.add(addOp(me.getKey(), opFact.get(me.getValue(), cb)));\n\t\t}\n\t\treturn new BulkGetFuture(m, ops, latch);\n\t}\n\n\t/**\n\t * Varargs wrapper for asynchronous bulk gets.\n\t *\n\t * @param keys one more more keys to get\n\t * @return the future values of those keys\n\t */\n\tpublic Future<Map<String, Object>> asyncGetBulk(String... keys) {\n\t\treturn asyncGetBulk(Arrays.asList(keys));\n\t}\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t */\n\tpublic Map<String, Object> getBulk(Collection<String> keys) {\n\t\ttry {\n\t\t\treturn asyncGetBulk(keys).get();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted getting bulk values\", e);\n\t\t} catch (ExecutionException e) {\n\t\t\tthrow new RuntimeException(\"Failed getting bulk values\", e);\n\t\t}\n\t}\n\n\t/**\n\t * Get the values for multiple keys from the cache.\n\t *\n\t * @param keys the keys\n\t * @return a map of the values (for each value that exists)\n\t */\n\tpublic Map<String, Object> getBulk(String... keys) {\n\t\treturn getBulk(Arrays.asList(keys));\n\t}\n\n\t/**\n\t * Get the versions of all of the connected memcacheds.\n\t */\n\tpublic Map<SocketAddress, String> getVersions() {\n\t\tfinal Map<SocketAddress, String>rv=\n\t\t\tnew ConcurrentHashMap<SocketAddress, String>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\treturn opFact.version(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\trv.put(sa, s.getMessage());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for versions\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Get all of the stats from all of the connections.\n\t */\n\tpublic Map<SocketAddress, Map<String, String>> getStats() {\n\t\treturn getStats(null);\n\t}\n\n\tprivate Map<SocketAddress, Map<String, String>> getStats(final String arg) {\n\t\tfinal Map<SocketAddress, Map<String, String>> rv\n\t\t\t=new HashMap<SocketAddress, Map<String, String>>();\n\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tfinal SocketAddress sa=n.getSocketAddress();\n\t\t\t\trv.put(sa, new HashMap<String, String>());\n\t\t\t\treturn opFact.stats(arg,\n\t\t\t\t\t\tnew StatsOperation.Callback() {\n\t\t\t\t\tpublic void gotStat(String name, String val) {\n\t\t\t\t\t\trv.get(sa).put(name, val);\n\t\t\t\t\t}\n\t\t\t\t\t@SuppressWarnings(\"synthetic-access\") // getLogger()\n\t\t\t\t\tpublic void receivedStatus(OperationStatus status) {\n\t\t\t\t\t\tif(!status.isSuccess()) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Unsuccessful stat fetch:  %s\",\n\t\t\t\t\t\t\t\t\tstatus);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t}});\n\t\ttry {\n\t\t\tblatch.await();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for stats\", e);\n\t\t}\n\t\treturn rv;\n\t}\n\n\tprivate long mutate(Mutator m, String key, int by, long def, int exp) {\n\t\tfinal AtomicLong rv=new AtomicLong();\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\taddOp(key, opFact.mutate(m, key, by, def, exp, new OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t// XXX:  Potential abstraction leak.\n\t\t\t\t\t\t// The handling of incr/decr in the binary protocol is\n\t\t\t\t\t\t// yet undefined.\n\t\t\t\t\t\trv.set(new Long(s.isSuccess()?s.getMessage():\"-1\"));\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}}));\n\t\ttry {\n\t\t\tlatch.await();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted\", e);\n\t\t}\n\t\tgetLogger().debug(\"Mutation returned %s\", rv);\n\t\treturn rv.get();\n\t}\n\n\t/**\n\t * Increment the given key by the given amount.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @return the new value (-1 if the key doesn't exist)\n\t */\n\tpublic long incr(String key, int by) {\n\t\treturn mutate(Mutator.incr, key, by, 0, -1);\n\t}\n\n\t/**\n\t * Decrement the given key by the given value.\n\t *\n\t * @param key the key\n\t * @param by the value\n\t * @return the new value (-1 if the key doesn't exist)\n\t */\n\tpublic long decr(String key, int by) {\n\t\treturn mutate(Mutator.decr, key, by, 0, -1);\n\t}\n\n\tprivate long mutateWithDefault(Mutator t, String key,\n\t\t\tint by, long def, int exp) {\n\t\tlong rv=mutate(t, key, by, def, exp);\n\t\t// The ascii protocol doesn't support defaults, so I added them\n\t\t// manually here.\n\t\tif(rv == -1) {\n\t\t\tFuture<Boolean> f=asyncStore(StoreType.add,\n\t\t\t\t\tkey, 0,\tString.valueOf(def));\n\t\t\ttry {\n\t\t\t\tif(f.get()) {\n\t\t\t\t\trv=def;\n\t\t\t\t} else {\n\t\t\t\t\trv=mutate(t, key, by, 0, 0);\n\t\t\t\t\tassert rv != -1 : \"Failed to mutate or init value\";\n\t\t\t\t}\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Interrupted waiting for store\", e);\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tthrow new RuntimeException(\"Failed waiting for store\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Increment the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to increment\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to increment or add\n\t */\n\tpublic long incr(String key, int by, int def) {\n\t\treturn mutateWithDefault(Mutator.incr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Decrement the given counter, returning the new value.\n\t *\n\t * @param key the key\n\t * @param by the amount to decrement\n\t * @param def the default value (if the counter does not exist)\n\t * @return the new value, or -1 if we were unable to decrement or add\n\t */\n\tpublic long decr(String key, int by, long def) {\n\t\treturn mutateWithDefault(Mutator.decr, key, by, def, 0);\n\t}\n\n\t/**\n\t * Delete the given key from the cache.\n\t *\n\t * @param key the key to delete\n\t * @param when when the deletion should take effect\n\t */\n\tpublic Future<Boolean> delete(String key, int when) {\n\t\tfinal CountDownLatch latch=new CountDownLatch(1);\n\t\tfinal OperationFuture<Boolean> rv=new OperationFuture<Boolean>(latch);\n\t\tDeleteOperation op=opFact.delete(key, when,\n\t\t\t\tnew OperationCallback() {\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\trv.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\trv.setOperation(op);\n\t\taddOp(key, op);\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Shortcut to delete that will immediately delete the item from the cache.\n\t */\n\tpublic Future<Boolean> delete(String key) {\n\t\treturn delete(key, 0);\n\t}\n\n\t/**\n\t * Flush all caches from all servers with a delay of application.\n\t */\n\tpublic Future<Boolean> flush(final int delay) {\n\t\tfinal AtomicReference<Boolean> flushResult=\n\t\t\tnew AtomicReference<Boolean>(null);\n\t\tfinal ConcurrentLinkedQueue<Operation> ops=\n\t\t\tnew ConcurrentLinkedQueue<Operation>();\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\tOperation op=opFact.flush(delay, new OperationCallback(){\n\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\tflushResult.set(s.isSuccess());\n\t\t\t\t\t}\n\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t}});\n\t\t\t\tops.add(op);\n\t\t\t\treturn op;\n\t\t\t}});\n\t\treturn new OperationFuture<Boolean>(blatch, flushResult) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean ign) {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\top.cancel();\n\t\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isCancelled() {\n\t\t\t\tboolean rv=false;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv |= op.isCancelled();\n\t\t\t\t}\n\t\t\t\treturn rv;\n\t\t\t}\n\t\t\t@Override\n\t\t\tpublic boolean isDone() {\n\t\t\t\tboolean rv=true;\n\t\t\t\tfor(Operation op : ops) {\n\t\t\t\t\trv &= op.getState() == OperationState.COMPLETE;\n\t\t\t\t}\n\t\t\t\treturn rv || isCancelled();\n\t\t\t}\n\t\t};\n\t}\n\n\t/**\n\t * Flush all caches from all servers immediately.\n\t */\n\tpublic Future<Boolean> flush() {\n\t\treturn flush(-1);\n\t}\n\n\t/**\n\t * Infinitely loop processing IO.\n\t */\n\t@Override\n\tpublic void run() {\n\t\twhile(running) {\n\t\t\ttry {\n\t\t\t\tconn.handleIO();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling memcached IO\", e);\n\t\t\t}\n\t\t}\n\t\tgetLogger().info(\"Shut down memcached client\");\n\t}\n\n\t/**\n\t * Shut down immediately.\n\t */\n\tpublic void shutdown() {\n\t\tshutdown(-1, TimeUnit.MILLISECONDS);\n\t}\n\n\t/**\n\t * Shut down this client gracefully.\n\t */\n\tpublic boolean shutdown(long timeout, TimeUnit unit) {\n\t\tshuttingDown=true;\n\t\tString baseName=getName();\n\t\tsetName(baseName + \" - SHUTTING DOWN\");\n\t\tboolean rv=false;\n\t\ttry {\n\t\t\t// Conditionally wait\n\t\t\tif(timeout > 0) {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (waiting)\");\n\t\t\t\trv=waitForQueues(timeout, unit);\n\t\t\t}\n\t\t} finally {\n\t\t\t// But always begin the shutdown sequence\n\t\t\ttry {\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (telling client)\");\n\t\t\t\trunning=false;\n\t\t\t\tconn.shutdown();\n\t\t\t\tsetName(baseName + \" - SHUTTING DOWN (informed client)\");\n\t\t\t} catch (IOException e) {\n\t\t\t\tgetLogger().warn(\"exception while shutting down\", e);\n\t\t\t}\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Wait for the queues to die down.\n\t */\n\tpublic boolean waitForQueues(long timeout, TimeUnit unit) {\n\t\tCountDownLatch blatch = broadcastOp(new BroadcastOpFactory(){\n\t\t\tpublic Operation newOp(final MemcachedNode n,\n\t\t\t\t\tfinal CountDownLatch latch) {\n\t\t\t\treturn opFact.noop(\n\t\t\t\t\t\tnew OperationCallback() {\n\t\t\t\t\t\t\tpublic void complete() {\n\t\t\t\t\t\t\t\tlatch.countDown();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpublic void receivedStatus(OperationStatus s) {\n\t\t\t\t\t\t\t\t// Nothing special when receiving status, only\n\t\t\t\t\t\t\t\t// necessary to complete the interface\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\t\t\t}}, false);\n\t\ttry {\n\t\t\treturn blatch.await(timeout, unit);\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(\"Interrupted waiting for queues\", e);\n\t\t}\n\t}\n\n\tstatic class BulkGetFuture implements Future<Map<String, Object>> {\n\t\tprivate final Map<String, Object> rvMap;\n\t\tprivate final Collection<Operation> ops;\n\t\tprivate final CountDownLatch latch;\n\t\tprivate boolean cancelled=false;\n\n\t\tpublic BulkGetFuture(Map<String, Object> m,\n\t\t\t\tCollection<Operation> getOps, CountDownLatch l) {\n\t\t\tsuper();\n\t\t\trvMap = m;\n\t\t\tops = getOps;\n\t\t\tlatch=l;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tboolean rv=false;\n\t\t\tfor(Operation op : ops) {\n\t\t\t\trv |= op.getState() == OperationState.WRITING;\n\t\t\t\top.cancel();\n\t\t\t}\n\t\t\tcancelled=true;\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic Map<String, Object> get()\n\t\t\tthrows InterruptedException, ExecutionException {\n\t\t\ttry {\n\t\t\t\treturn get(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\tthrow new RuntimeException(\"Timed out waiting forever\", e);\n\t\t\t}\n\t\t}\n\n\t\tpublic Map<String, Object> get(long timeout, TimeUnit unit)\n\t\t\tthrows InterruptedException,\n\t\t\tExecutionException, TimeoutException {\n\t\t\tlatch.await(timeout, unit);\n\t\t\tfor(Operation op : ops) {\n\t\t\t\tif(op.isCancelled()) {\n\t\t\t\t\tthrow new ExecutionException(\n\t\t\t\t\t\t\tnew RuntimeException(\"Cancelled\"));\n\t\t\t\t}\n\t\t\t\tif(op.hasErrored()) {\n\t\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn rvMap;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\treturn cancelled;\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\treturn latch.getCount() == 0;\n\t\t}\n\t}\n\n\tstatic class OperationFuture<T> implements Future<T> {\n\n\t\tprivate final CountDownLatch latch;\n\t\tprivate final AtomicReference<T> objRef;\n\t\tprivate Operation op;\n\n\t\tpublic OperationFuture(CountDownLatch l) {\n\t\t\tthis(l, new AtomicReference<T>(null));\n\t\t}\n\n\t\tpublic OperationFuture(CountDownLatch l, AtomicReference<T> oref) {\n\t\t\tsuper();\n\t\t\tlatch=l;\n\t\t\tobjRef=oref;\n\t\t}\n\n\t\tpublic boolean cancel(boolean ign) {\n\t\t\tassert op != null : \"No operation\";\n\t\t\top.cancel();\n\t\t\t// This isn't exactly correct, but it's close enough.  If we're in\n\t\t\t// a writing state, we *probably* haven't started.\n\t\t\treturn op.getState() == OperationState.WRITING;\n\t\t}\n\n\t\tpublic T get() throws InterruptedException, ExecutionException {\n\t\t\tlatch.await();\n\t\t\tassert isDone() : \"Latch released, but operation wasn't done.\";\n\t\t\tif(op != null && op.hasErrored()) {\n\t\t\t\tthrow new ExecutionException(op.getException());\n\t\t\t}\n\t\t\tif(isCancelled()) {\n\t\t\t\tthrow new ExecutionException(new RuntimeException(\"Cancelled\"));\n\t\t\t}\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tpublic T get(long duration, TimeUnit units)\n\t\t\tthrows InterruptedException, TimeoutException {\n\t\t\tlatch.await(duration, units);\n\t\t\treturn objRef.get();\n\t\t}\n\n\t\tvoid set(T o) {\n\t\t\tobjRef.set(o);\n\t\t}\n\n\t\tvoid setOperation(Operation to) {\n\t\t\top=to;\n\t\t}\n\n\t\tpublic boolean isCancelled() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn op.isCancelled();\n\t\t}\n\n\t\tpublic boolean isDone() {\n\t\t\tassert op != null : \"No operation\";\n\t\t\treturn latch.getCount() == 0 ||\n\t\t\t\top.isCancelled() || op.getState() == OperationState.COMPLETE;\n\t\t}\n\n\t}\n}\n","lineNo":423}
{"Smelly Sample":"package net.spy.memcached.protocol.ascii;\n\nimport java.net.SocketAddress;\nimport java.nio.channels.SocketChannel;\nimport java.util.concurrent.BlockingQueue;\n\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.protocol.TCPMemcachedNodeImpl;\n\n/**\n * Memcached node for the ASCII protocol.\n */\npublic final class AsciiMemcachedNodeImpl extends TCPMemcachedNodeImpl {\n\n\tpublic AsciiMemcachedNodeImpl(SocketAddress sa, SocketChannel c,\n\t\t\tint bufSize, BlockingQueue<Operation> rq,\n\t\t\tBlockingQueue<Operation> wq, BlockingQueue<Operation> iq) {\n\t\tsuper(sa, c, bufSize, rq, wq, iq);\n\t}\n\n\t@Override\n\tprotected void optimize() {\n\t\t// make sure there are at least two get operations in a row before\n\t\t// attempting to optimize them.\n\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\tgetOp=(GetOperationImpl)writeQ.remove();\n\t\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\t\tOptimizedGetImpl og=new OptimizedGetImpl(getOp);\n\t\t\t\tgetOp=og;\n\n\t\t\t\twhile(writeQ.peek() instanceof GetOperation) {\n\t\t\t\t\tGetOperationImpl o=(GetOperationImpl) writeQ.remove();\n\t\t\t\t\tif(!o.isCancelled()) {\n\t\t\t\t\t\tog.addOperation(o);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Initialize the new mega get\n\t\t\t\tgetOp.initialize();\n\t\t\t\tassert getOp.getState() == OperationState.WRITING;\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\"Set up %s with %s keys and %s callbacks\",\n\t\t\t\t\tthis, og.numKeys(), og.numCallbacks());\n\t\t\t}\n\t\t}\n\t}\n\n}\n","Method after Refactoring":"package net.spy.memcached.protocol.ascii;\n\nimport java.net.SocketAddress;\nimport java.nio.channels.SocketChannel;\nimport java.util.concurrent.BlockingQueue;\n\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationState;\nimport net.spy.memcached.protocol.ProxyCallback;\nimport net.spy.memcached.protocol.TCPMemcachedNodeImpl;\n\n/**\n * Memcached node for the ASCII protocol.\n */\npublic final class AsciiMemcachedNodeImpl extends TCPMemcachedNodeImpl {\n\n\tpublic AsciiMemcachedNodeImpl(SocketAddress sa, SocketChannel c,\n\t\t\tint bufSize, BlockingQueue<Operation> rq,\n\t\t\tBlockingQueue<Operation> wq, BlockingQueue<Operation> iq) {\n\t\tsuper(sa, c, bufSize, rq, wq, iq);\n\t}\n\n\t@Override\n\tprotected void optimize() {\n\t\t// make sure there are at least two get operations in a row before\n\t\t// attempting to optimize them.\n\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\tgetOp=(GetOperationImpl)writeQ.remove();\n\t\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\t\tOptimizedGetImpl og=new OptimizedGetImpl(getOp);\n\t\t\t\tgetOp=og;\n\n\t\t\t\twhile(writeQ.peek() instanceof GetOperation) {\n\t\t\t\t\tGetOperationImpl o=(GetOperationImpl) writeQ.remove();\n\t\t\t\t\tif(!o.isCancelled()) {\n\t\t\t\t\t\tog.addOperation(o);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Initialize the new mega get\n\t\t\t\tgetOp.initialize();\n\t\t\t\tassert getOp.getState() == OperationState.WRITING;\n\t\t\t\tif(getLogger().isDebugEnabled()) {\n\t\t\t\t\tProxyCallback pcb=(ProxyCallback) og.getCallback();\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Set up %s with %s keys and %s callbacks\",\n\t\t\t\t\t\t\tthis, pcb.numKeys(), pcb.numCallbacks());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n}\n","lineNo":45}
{"Smelly Sample":"package net.spy.memcached;\n\nimport java.util.Collection;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\n\nimport net.spy.SpyObject;\n\n/**\n * This is an implementation of the Ketama consistent hash strategy from\n * last.fm.  This implementation may not be compatible with libketama as\n * hashing is considered separate from node location.\n * \n * Note that this implementation does not currently supported weighted nodes.\n * \n * @see http://www.last.fm/user/RJ/journal/2007/04/10/392555/\n */\npublic class KetamaNodeLocator extends SpyObject implements NodeLocator {\n\n\tstatic final int NUM_REPS = 100;\n\n\tfinal SortedMap<Long, MemcachedNode> ketamaNodes=\n\t\tnew TreeMap<Long, MemcachedNode>();\n\tfinal Collection<MemcachedNode> allNodes;\n\n\tfinal HashAlgorithm hashAlg;\n\n\tpublic KetamaNodeLocator(List<MemcachedNode> nodes, HashAlgorithm alg) {\n\t\tsuper();\n\t\tallNodes = nodes;\n\t\thashAlg = alg;\n\n\t\tfor(MemcachedNode node : nodes) {\n\t\t\t// XXX:  Replace getSocketAddress() with something more precise\n\t\t\tfor(int i=0; i<NUM_REPS; i++) {\n\t\t\t\tlong hash = hashAlg.hash(node.getSocketAddress() + \"-\" + i);\n\t\t\t\tketamaNodes.put(hash, node);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic Collection<MemcachedNode> getAll() {\n\t\treturn allNodes;\n\t}\n\n\tpublic MemcachedNode getPrimary(final String k) {\n\t\tMemcachedNode rv=getNodeForKey(hashAlg.hash(k));\n\t\tassert rv != null : \"Found no node for key \" + k;\n\t\treturn rv;\n\t}\n\n\tMemcachedNode getNodeForKey(long hash) {\n\t\tfinal MemcachedNode rv;\n\t\tif(!ketamaNodes.containsKey(hash)) {\n\t\t\t// Java 1.6 adds a ceilingKey method, but I'm still stuck in 1.5\n\t\t\t// in a lot of places, so I'm doing this myself.\n\t\t\tSortedMap<Long, MemcachedNode> tailMap=ketamaNodes.tailMap(hash);\n\t\t\tif(tailMap.isEmpty()) {\n\t\t\t\thash=ketamaNodes.firstKey();\n\t\t\t} else {\n\t\t\t\thash=tailMap.firstKey();\n\t\t\t}\n\t\t}\n\t\trv=ketamaNodes.get(hash);\n\t\treturn rv;\n\t}\n\n\tpublic Iterator<MemcachedNode> getSequence(String k) {\n\t\treturn new KetamaIterator(k, allNodes.size());\n\t}\n\n\n\tclass KetamaIterator implements Iterator<MemcachedNode> {\n\n\t\tfinal String key;\n\t\tlong hashVal;\n\t\tint remainingTries;\n\t\tint numTries=0;\n\n\t\tpublic KetamaIterator(final String k, final int t) {\n\t\t\tsuper();\n\t\t\thashVal=hashAlg.hash(k);\n\t\t\tremainingTries=t;\n\t\t\tkey=k;\n\t\t}\n\n\t\tprivate void nextHash() {\n\t\t\t// this.calculateHash(Integer.toString(tries)+key).hashCode();\n\t\t\tlong tmpKey=hashAlg.hash(numTries + key);\n\t\t\t// This echos the implementation of Long.hashCode()\n\t\t\thashVal += (int)(tmpKey ^ (tmpKey >>> 32));\n\t\t\tremainingTries--;\n\t\t}\n\n\t\tpublic boolean hasNext() {\n\t\t\treturn remainingTries > 0;\n\t\t}\n\n\t\tpublic MemcachedNode next() {\n\t\t\ttry {\n\t\t\t\treturn getNodeForKey(hashVal);\n\t\t\t} finally {\n\t\t\t\tnextHash();\n\t\t\t}\n\t\t}\n\n\t\tpublic void remove() {\n\t\t\tthrow new UnsupportedOperationException(\"remove not supported\");\n\t\t}\n\n\t}\n}\n","Method after Refactoring":"package net.spy.memcached;\n\nimport java.util.Collection;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\n\nimport net.spy.SpyObject;\n\n/**\n * This is an implementation of the Ketama consistent hash strategy from\n * last.fm.  This implementation may not be compatible with libketama as\n * hashing is considered separate from node location.\n * \n * Note that this implementation does not currently supported weighted nodes.\n * \n * @see http://www.last.fm/user/RJ/journal/2007/04/10/392555/\n */\npublic class KetamaNodeLocator extends SpyObject implements NodeLocator {\n\n\tstatic final int NUM_REPS = 100;\n\n\tfinal SortedMap<Long, MemcachedNode> ketamaNodes=\n\t\tnew TreeMap<Long, MemcachedNode>();\n\tfinal Collection<MemcachedNode> allNodes;\n\n\tfinal HashAlgorithm hashAlg;\n\n\tpublic KetamaNodeLocator(List<MemcachedNode> nodes, HashAlgorithm alg) {\n\t\tsuper();\n\t\tallNodes = nodes;\n\t\thashAlg = alg;\n\n\t\tfor(MemcachedNode node : nodes) {\n\t\t\t// XXX:  Replace getSocketAddress() with something more precise\n\t\t\tString sockStr=String.valueOf(node.getSocketAddress());\n\t\t\tfor(int i=0; i<NUM_REPS; i++) {\n\t\t\t\tlong hash = hashAlg.hash(sockStr + \"-\" + i);\n\t\t\t\tketamaNodes.put(hash, node);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic Collection<MemcachedNode> getAll() {\n\t\treturn allNodes;\n\t}\n\n\tpublic MemcachedNode getPrimary(final String k) {\n\t\tMemcachedNode rv=getNodeForKey(hashAlg.hash(k));\n\t\tassert rv != null : \"Found no node for key \" + k;\n\t\treturn rv;\n\t}\n\n\tMemcachedNode getNodeForKey(long hash) {\n\t\tfinal MemcachedNode rv;\n\t\tif(!ketamaNodes.containsKey(hash)) {\n\t\t\t// Java 1.6 adds a ceilingKey method, but I'm still stuck in 1.5\n\t\t\t// in a lot of places, so I'm doing this myself.\n\t\t\tSortedMap<Long, MemcachedNode> tailMap=ketamaNodes.tailMap(hash);\n\t\t\tif(tailMap.isEmpty()) {\n\t\t\t\thash=ketamaNodes.firstKey();\n\t\t\t} else {\n\t\t\t\thash=tailMap.firstKey();\n\t\t\t}\n\t\t}\n\t\trv=ketamaNodes.get(hash);\n\t\treturn rv;\n\t}\n\n\tpublic Iterator<MemcachedNode> getSequence(String k) {\n\t\treturn new KetamaIterator(k, allNodes.size());\n\t}\n\n\n\tclass KetamaIterator implements Iterator<MemcachedNode> {\n\n\t\tfinal String key;\n\t\tlong hashVal;\n\t\tint remainingTries;\n\t\tint numTries=0;\n\n\t\tpublic KetamaIterator(final String k, final int t) {\n\t\t\tsuper();\n\t\t\thashVal=hashAlg.hash(k);\n\t\t\tremainingTries=t;\n\t\t\tkey=k;\n\t\t}\n\n\t\tprivate void nextHash() {\n\t\t\t// this.calculateHash(Integer.toString(tries)+key).hashCode();\n\t\t\tlong tmpKey=hashAlg.hash(numTries + key);\n\t\t\t// This echos the implementation of Long.hashCode()\n\t\t\thashVal += (int)(tmpKey ^ (tmpKey >>> 32));\n\t\t\tremainingTries--;\n\t\t}\n\n\t\tpublic boolean hasNext() {\n\t\t\treturn remainingTries > 0;\n\t\t}\n\n\t\tpublic MemcachedNode next() {\n\t\t\ttry {\n\t\t\t\treturn getNodeForKey(hashVal);\n\t\t\t} finally {\n\t\t\t\tnextHash();\n\t\t\t}\n\t\t}\n\n\t\tpublic void remove() {\n\t\t\tthrow new UnsupportedOperationException(\"remove not supported\");\n\t\t}\n\n\t}\n}\n","lineNo":37}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OptimizedGet;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyObject {\n\t// The number of empty selects we'll allow before taking action.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 100;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\t// Maximum number of sequential errors before reconnecting.\n\tprivate static final int EXCESSIVE_ERRORS = 1;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, get optimization will collapse multiple sequential get ops\n\tprivate boolean optimizeGets=true;\n\tprivate Selector selector=null;\n\tprivate QueueAttachment[] connections=null;\n\tprivate int emptySelects=0;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<QueueAttachment> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, QueueAttachment> reconnectQueue;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\treconnectQueue=new TreeMap<Long, QueueAttachment>();\n\t\taddedQueue=new ConcurrentLinkedQueue<QueueAttachment>();\n\t\tselector=Selector.open();\n\t\tconnections=new QueueAttachment[a.size()];\n\t\tint cons=0;\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tQueueAttachment qa=new QueueAttachment(sa, ch, bufSize,\n\t\t\t\tf.createOperationQueue(), f.createOperationQueue(),\n\t\t\t\tf.createOperationQueue());\n\t\t\tqa.which=cons;\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t\tassert ch.isConnected()\n\t\t\t\t|| qa.sk.interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\tconnections[cons++]=qa;\n\t\t}\n\t}\n\n\t/**\n\t * Enable or disable get optimization.\n\t *\n\t * When enabled (default), multiple sequential gets are collapsed into one.\n\t */\n\tpublic void setGetOptimization(boolean to) {\n\t\toptimizeGets=to;\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tif(qa.sk.isValid()) {\n\t\t\t\tif(qa.channel.isConnected()) {\n\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.toWrite > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty()) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > EXCESSIVE_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueueReconnect((QueueAttachment)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY + 10\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Got selection key:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<QueueAttachment> toAdd=new HashSet<QueueAttachment>();\n\t\t\ttry {\n\t\t\t\tQueueAttachment qa=null;\n\t\t\t\tboolean readyForIO=false;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\tif(qa.channel != null && qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.getCurrentWriteOp();\n\t\t\t\t\t\tif(op != null) {\n\t\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t\t}\n\t\t\t\t\tqa.copyInputQueue();\n\t\t\t\t\tif(readyForIO) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tif(qa.wbuf.hasRemaining()) {\n\t\t\t\t\t\t\t\thandleWrites(qa.sk, qa);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfixupOps(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// out of stuff.\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tassert !sk.isAcceptable() : \"We don't do accepting here.\";\n\t\tQueueAttachment qa=(QueueAttachment)sk.attachment();\n\t\tif(sk.isConnectable()) {\n\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\ttry {\n\t\t\t\tif(qa.channel.finishConnect()) {\n\t\t\t\t\tassert qa.channel.isConnected() : \"Not connected.\";\n\t\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.wbuf.hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !qa.channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t} else {\n\t\t\tif(sk.isWritable()) {\n\t\t\t\ttry {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tgetLogger().info(\"IOException handling %s, reconnecting\",\n\t\t\t\t\t\t\tqa.getCurrentWriteOp(), e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(sk.isReadable()) {\n\t\t\t\ttry {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t\tqa.protocolErrors=0;\n\t\t\t\t} catch (OperationException e) {\n\t\t\t\t\tif(++qa.protocolErrors >= EXCESSIVE_ERRORS) {\n\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tgetLogger().info(\"IOException handling %s, reconnecting\",\n\t\t\t\t\t\t\tqa.getCurrentReadOp(), e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfixupOps(qa);\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, QueueAttachment qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(optimizeGets);\n\t\tboolean canWriteMore=qa.toWrite > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.channel.write(qa.wbuf);\n\t\t\tassert wrote >= 0 : \"Wrote negative bytes?\";\n\t\t\tqa.toWrite -= wrote;\n\t\t\tassert qa.toWrite >= 0\n\t\t\t\t: \"toWrite went negative after writing \" + wrote\n\t\t\t\t\t+ \" bytes for \" + qa;\n\t\t\tgetLogger().debug(\"Wrote %d bytes\", wrote);\n\t\t\tqa.fillWriteBuffer(optimizeGets);\n\t\t\tcanWriteMore = wrote > 0 && qa.toWrite > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, QueueAttachment qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tint read=qa.channel.read(qa.rbuf);\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\tqa.rbuf.flip();\n\t\t\twhile(qa.rbuf.remaining() > 0) {\n\t\t\t\tassert currentOp != null : \"No read operation\";\n\t\t\t\tcurrentOp.readFromBuffer(qa.rbuf);\n\t\t\t\tif(currentOp.getState() == Operation.State.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, qa.rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t}\n\t\t\t}\n\t\t\tqa.rbuf.clear();\n\t\t\tread=qa.channel.read(qa.rbuf);\n\t\t}\n\t}\n\n\tprivate void fixupOps(QueueAttachment qa) {\n\t\tif(qa.sk.isValid()) {\n\t\t\tint iops=qa.getSelectionOps();\n\t\t\tgetLogger().debug(\"Setting interested opts to %d\", iops);\n\t\t\tqa.sk.interestOps(iops);\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selection key is not valid.\");\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(QueueAttachment qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.reconnectAttempt);\n\t\t\tif(qa.sk != null) {\n\t\t\t\tqa.sk.cancel();\n\t\t\t\tassert !qa.sk.isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnectAttempt++;\n\t\t\ttry {\n\t\t\t\tqa.channel.socket().close();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.channel=null;\n\n\t\t\tlong delay=Math.min((100*qa.reconnectAttempt) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tlong now=System.currentTimeMillis();\n\t\tfor(Iterator<QueueAttachment> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tQueueAttachment qa=i.next();\n\t\t\ti.remove();\n\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(qa.socketAddress)) {\n\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.channel=ch;\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t}\n\t}\n\n\t/**\n\t * Get the number of connections currently handled.\n\t */\n\tpublic int getNumConnections() {\n\t\treturn connections.length;\n\t}\n\n\t/**\n\t * Get the remote address of the socket with the given ID.\n\t * \n\t * @param which which id\n\t * @return the rmeote address\n\t */\n\tpublic SocketAddress getAddressOf(int which) {\n\t\treturn connections[which].socketAddress;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t * \n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void addOperation(int which, Operation o) {\n\t\tQueueAttachment qa=connections[which];\n\t\to.initialize();\n\t\tqa.addOp(o);\n\t\taddedQueue.offer(qa);\n\t\tSelector s=selector.wakeup();\n\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\tgetLogger().debug(\"Added %s to %d\", o, which);\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tqa.channel.close();\n\t\t\tqa.sk=null;\n\t\t\tif(qa.toWrite > 0) {\n\t\t\t\tgetLogger().warn(\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\tqa.toWrite);\n\t\t\t}\n\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.channel);\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.socketAddress);\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n\tstatic class QueueAttachment extends SpyObject {\n\t\tpublic final SocketAddress socketAddress;\n\t\tpublic final ByteBuffer rbuf;\n\t\tpublic final ByteBuffer wbuf;\n\t\tprivate final BlockingQueue<Operation> writeQ;\n\t\tprivate final BlockingQueue<Operation> readQ;\n\t\tprivate final BlockingQueue<Operation> inputQueue;\n\t\tpublic int which=0;\n\t\tpublic int reconnectAttempt=1;\n\t\tpublic SocketChannel channel;\n\t\tpublic int toWrite=0;\n\t\tprivate GetOperation getOp=null;\n\t\tpublic SelectionKey sk=null;\n\n\t\t// Count sequential protocol errors.\n\t\tpublic int protocolErrors=0;\n\n\t\tpublic QueueAttachment(SocketAddress sa, SocketChannel c, int bufSize,\n\t\t\t\tBlockingQueue<Operation> rq, BlockingQueue<Operation> wq,\n\t\t\t\tBlockingQueue<Operation> iq) {\n\t\t\tsuper();\n\t\t\tassert sa != null : \"No SocketAddress\";\n\t\t\tassert c != null : \"No SocketChannel\";\n\t\t\tassert bufSize > 0 : \"Invalid buffer size: \" + bufSize;\n\t\t\tassert rq != null : \"No operation read queue\";\n\t\t\tassert wq != null : \"No operation write queue\";\n\t\t\tassert iq != null : \"No input queue\";\n\t\t\tsocketAddress=sa;\n\t\t\tchannel=c;\n\t\t\trbuf=ByteBuffer.allocate(bufSize);\n\t\t\twbuf=ByteBuffer.allocate(bufSize);\n\t\t\twbuf.clear();\n\t\t\treadQ=rq;\n\t\t\twriteQ=wq;\n\t\t\tinputQueue=iq;\n\t\t}\n\n\t\tpublic void copyInputQueue() {\n\t\t\tCollection<Operation> tmp=new ArrayList<Operation>();\n\t\t\tinputQueue.drainTo(tmp);\n\t\t\twriteQ.addAll(tmp);\n\t\t}\n\n\n\t\tpublic void setupResend() {\n\t\t\t// First, reset the current write op.\n\t\t\tOperation op=getCurrentWriteOp();\n\t\t\tif(op != null) {\n\t\t\t\top.getBuffer().reset();\n\t\t\t}\n\t\t\t// Now cancel all the pending read operations.  Might be better to\n\t\t\t// to requeue them.\n\t\t\twhile(hasReadOp()) {\n\t\t\t\top=removeCurrentReadOp();\n\t\t\t\tgetLogger().warn(\"Discarding partially completed op: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t}\n\n\t\t\twbuf.clear();\n\t\t\trbuf.clear();\n\t\t\ttoWrite=0;\n\t\t\tprotocolErrors=0;\n\t\t}\n\n\t\t// Prepare the pending operations.  Return true if there are any pending\n\t\t// ops\n\t\tprivate boolean preparePending() {\n\t\t\t// Copy the input queue into the write queue.\n\t\t\tcopyInputQueue();\n\n\t\t\t// Now check the ops\n\t\t\tOperation nextOp=getCurrentWriteOp();\n\t\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\t\tgetLogger().info(\"Removing cancelled operation: %s\", nextOp);\n\t\t\t\tremoveCurrentWriteOp();\n\t\t\t\tnextOp=getCurrentWriteOp();\n\t\t\t}\n\t\t\treturn nextOp != null;\n\t\t}\n\n\t\tpublic void fillWriteBuffer(boolean optimizeGets) {\n\t\t\tif(toWrite == 0) {\n\t\t\t\twbuf.clear();\n\t\t\t\tOperation o=getCurrentWriteOp();\n\t\t\t\twhile(o != null && toWrite < wbuf.capacity()) {\n\t\t\t\t\tassert o.getState() == Operation.State.WRITING;\n\t\t\t\t\tByteBuffer obuf=o.getBuffer();\n\t\t\t\t\tint bytesToCopy=Math.min(wbuf.remaining(),\n\t\t\t\t\t\t\tobuf.remaining());\n\t\t\t\t\tbyte b[]=new byte[bytesToCopy];\n\t\t\t\t\tobuf.get(b);\n\t\t\t\t\twbuf.put(b);\n\t\t\t\t\tgetLogger().debug(\"After copying stuff from %s: %s\",\n\t\t\t\t\t\t\to, wbuf);\n\t\t\t\t\tif(!o.getBuffer().hasRemaining()) {\n\t\t\t\t\t\to.writeComplete();\n\t\t\t\t\t\ttransitionWriteItem();\n\n\t\t\t\t\t\tpreparePending();\n\t\t\t\t\t\tif(optimizeGets) {\n\t\t\t\t\t\t\toptimize();\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\to=getCurrentWriteOp();\n\t\t\t\t\t}\n\t\t\t\t\ttoWrite += bytesToCopy;\n\t\t\t\t}\n\t\t\t\twbuf.flip();\n\t\t\t\tassert toWrite <= wbuf.capacity()\n\t\t\t\t\t: \"toWrite exceeded capacity: \" + this;\n\t\t\t\tassert toWrite == wbuf.remaining()\n\t\t\t\t\t: \"Expected \" + toWrite + \" remaining, got \"\n\t\t\t\t\t+ wbuf.remaining();\n\t\t\t} else {\n\t\t\t\tgetLogger().debug(\"Buffer is full, skipping\");\n\t\t\t}\n\t\t}\n\n\t\tpublic void transitionWriteItem() {\n\t\t\tOperation op=removeCurrentWriteOp();\n\t\t\tassert op != null : \"There is no write item to transition\";\n\t\t\tassert op.getState() == Operation.State.READING;\n\t\t\tgetLogger().debug(\"Transitioning %s to read\", op);\n\t\t\treadQ.add(op);\n\t\t}\n\n\t\tpublic void optimize() {\n\t\t\t// make sure there are at least two get operations in a row before\n\t\t\t// attempting to optimize them.\n\t\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\t\tgetOp=(GetOperation)writeQ.remove();\n\t\t\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\t\t\tOptimizedGet og=new OptimizedGet(getOp);\n\t\t\t\t\tgetOp=og;\n\n\t\t\t\t\twhile(writeQ.peek() instanceof GetOperation) {\n\t\t\t\t\t\tGetOperation o=(GetOperation) writeQ.remove();\n\t\t\t\t\t\tif(!o.isCancelled()) {\n\t\t\t\t\t\t\tog.addOperation(o);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Initialize the new mega get\n\t\t\t\t\tgetOp.initialize();\n\t\t\t\t\tassert getOp.getState() == Operation.State.WRITING;\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Set up %s with %s keys and %s callbacks\",\n\t\t\t\t\t\tthis, og.numKeys(), og.numCallbacks());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tpublic Operation getCurrentReadOp() {\n\t\t\treturn readQ.peek();\n\t\t}\n\n\t\tpublic Operation removeCurrentReadOp() {\n\t\t\treturn readQ.remove();\n\t\t}\n\n\t\tpublic Operation getCurrentWriteOp() {\n\t\t\treturn getOp == null ? writeQ.peek() : getOp;\n\t\t}\n\n\t\tpublic Operation removeCurrentWriteOp() {\n\t\t\tOperation rv=getOp;\n\t\t\tif(rv == null) {\n\t\t\t\trv=writeQ.remove();\n\t\t\t} else {\n\t\t\t\tgetOp=null;\n\t\t\t}\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic boolean hasReadOp() {\n\t\t\treturn !readQ.isEmpty();\n\t\t}\n\n\t\tpublic boolean hasWriteOp() {\n\t\t\treturn !(getOp == null && writeQ.isEmpty());\n\t\t}\n\n\t\tpublic void addOp(Operation op) {\n\t\t\tboolean added=inputQueue.add(op);\n\t\t\tassert added; // documented to throw an IllegalStateException\n\t\t}\n\n\t\tpublic int getSelectionOps() {\n\t\t\tint rv=0;\n\t\t\tif(channel.isConnected()) {\n\t\t\t\tif(hasReadOp()) {\n\t\t\t\t\trv |= SelectionKey.OP_READ;\n\t\t\t\t}\n\t\t\t\tif(toWrite > 0 || hasWriteOp()) {\n\t\t\t\t\trv |= SelectionKey.OP_WRITE;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trv = SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\treturn rv;\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\tint sops=0;\n\t\t\tif(sk!= null && sk.isValid()) {\n\t\t\t\tsops=sk.interestOps();\n\t\t\t}\n\t\t\tint rsize=readQ.size() + (getOp == null ? 0 : 1);\n\t\t\tint wsize=writeQ.size();\n\t\t\tint isize=inputQueue.size();\n\t\t\treturn \"{QA sa=\" + socketAddress + \", #Rops=\" + rsize\n\t\t\t\t+ \", #Wops=\" + wsize\n\t\t\t\t+ \", #iq=\" + isize\n\t\t\t\t+ \", topRop=\" + getCurrentReadOp()\n\t\t\t\t+ \", topWop=\" + getCurrentWriteOp()\n\t\t\t\t+ \", toWrite=\" + toWrite\n\t\t\t\t+ \", interested=\" + sops + \"}\";\n\t\t}\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.ops.GetOperation;\nimport net.spy.memcached.ops.Operation;\nimport net.spy.memcached.ops.OperationException;\nimport net.spy.memcached.ops.OptimizedGet;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyObject {\n\t// The number of empty selects we'll allow before taking action.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 100;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\t// Maximum number of sequential errors before reconnecting.\n\tprivate static final int EXCESSIVE_ERRORS = 1;\n\n\tprivate volatile boolean shutDown=false;\n\t// If true, get optimization will collapse multiple sequential get ops\n\tprivate boolean optimizeGets=true;\n\tprivate Selector selector=null;\n\tprivate QueueAttachment[] connections=null;\n\tprivate int emptySelects=0;\n\t// AddedQueue is used to track the QueueAttachments for which operations\n\t// have recently been queued.\n\tprivate final ConcurrentLinkedQueue<QueueAttachment> addedQueue;\n\t// reconnectQueue contains the attachments that need to be reconnected\n\t// The key is the time at which they are eligible for reconnect\n\tprivate final SortedMap<Long, QueueAttachment> reconnectQueue;\n\n\t/**\n\t * Construct a memcached connection.\n\t *\n\t * @param bufSize the size of the buffer used for reading from the server\n\t * @param f the factory that will provide an operation queue\n\t * @param a the addresses of the servers to connect to\n\t *\n\t * @throws IOException if a connection attempt fails early\n\t */\n\tpublic MemcachedConnection(int bufSize, ConnectionFactory f,\n\t\t\tList<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\treconnectQueue=new TreeMap<Long, QueueAttachment>();\n\t\taddedQueue=new ConcurrentLinkedQueue<QueueAttachment>();\n\t\tselector=Selector.open();\n\t\tconnections=new QueueAttachment[a.size()];\n\t\tint cons=0;\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tQueueAttachment qa=new QueueAttachment(sa, ch, bufSize,\n\t\t\t\tf.createOperationQueue(), f.createOperationQueue(),\n\t\t\t\tf.createOperationQueue());\n\t\t\tqa.which=cons;\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t\tassert ch.isConnected()\n\t\t\t\t|| qa.sk.interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\tconnections[cons++]=qa;\n\t\t}\n\t}\n\n\t/**\n\t * Enable or disable get optimization.\n\t *\n\t * When enabled (default), multiple sequential gets are collapsed into one.\n\t */\n\tpublic void setGetOptimization(boolean to) {\n\t\toptimizeGets=to;\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tif(qa.sk.isValid()) {\n\t\t\t\tif(qa.channel.isConnected()) {\n\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\tint expected=0;\n\t\t\t\t\tif(qa.hasReadOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_READ;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.hasWriteOp()) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tif(qa.toWrite > 0) {\n\t\t\t\t\t\texpected |= SelectionKey.OP_WRITE;\n\t\t\t\t\t}\n\t\t\t\t\tassert sops == expected : \"Invalid ops:  \"\n\t\t\t\t\t\t+ qa + \", expected \" + expected + \", got \" + sops;\n\t\t\t\t} else {\n\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t+ sops;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t/**\n\t * MemcachedClient calls this method to handle IO over the connections.\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void handleIO() throws IOException {\n\t\tif(shutDown) {\n\t\t\tthrow new IOException(\"No IO while shut down\");\n\t\t}\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\n\t\tif(selectedKeys.isEmpty()) {\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > EXCESSIVE_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueueReconnect((QueueAttachment)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY + 10\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Got selection key:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t}\n\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\t// Handle any requests that have been made against the client.\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<QueueAttachment> toAdd=new HashSet<QueueAttachment>();\n\t\t\ttry {\n\t\t\t\tQueueAttachment qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\tboolean readyForIO=false;\n\t\t\t\t\tif(qa.channel != null && qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.getCurrentWriteOp();\n\t\t\t\t\t\tif(op != null) {\n\t\t\t\t\t\t\treadyForIO=true;\n\t\t\t\t\t\t\tgetLogger().debug(\"Handling queued write %s\", qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t\t}\n\t\t\t\t\tqa.copyInputQueue();\n\t\t\t\t\tif(readyForIO) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tif(qa.wbuf.hasRemaining()) {\n\t\t\t\t\t\t\t\thandleWrites(qa.sk, qa);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\t\tgetLogger().warn(\"Exception handling write\", e);\n\t\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfixupOps(qa);\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// out of stuff.\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tassert !sk.isAcceptable() : \"We don't do accepting here.\";\n\t\tQueueAttachment qa=(QueueAttachment)sk.attachment();\n\t\tif(sk.isConnectable()) {\n\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\ttry {\n\t\t\t\tif(qa.channel.finishConnect()) {\n\t\t\t\t\tassert qa.channel.isConnected() : \"Not connected.\";\n\t\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\tif(qa.wbuf.hasRemaining()) {\n\t\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert !qa.channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t} else {\n\t\t\tif(sk.isWritable()) {\n\t\t\t\ttry {\n\t\t\t\t\thandleWrites(sk, qa);\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tgetLogger().info(\"IOException handling %s, reconnecting\",\n\t\t\t\t\t\t\tqa.getCurrentWriteOp(), e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(sk.isReadable()) {\n\t\t\t\ttry {\n\t\t\t\t\thandleReads(sk, qa);\n\t\t\t\t\tqa.protocolErrors=0;\n\t\t\t\t} catch (OperationException e) {\n\t\t\t\t\tif(++qa.protocolErrors >= EXCESSIVE_ERRORS) {\n\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tgetLogger().info(\"IOException handling %s, reconnecting\",\n\t\t\t\t\t\t\tqa.getCurrentReadOp(), e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfixupOps(qa);\n\t}\n\n\tprivate void handleWrites(SelectionKey sk, QueueAttachment qa)\n\t\tthrows IOException {\n\t\tqa.fillWriteBuffer(optimizeGets);\n\t\tboolean canWriteMore=qa.toWrite > 0;\n\t\twhile(canWriteMore) {\n\t\t\tint wrote=qa.channel.write(qa.wbuf);\n\t\t\tassert wrote >= 0 : \"Wrote negative bytes?\";\n\t\t\tqa.toWrite -= wrote;\n\t\t\tassert qa.toWrite >= 0\n\t\t\t\t: \"toWrite went negative after writing \" + wrote\n\t\t\t\t\t+ \" bytes for \" + qa;\n\t\t\tgetLogger().debug(\"Wrote %d bytes\", wrote);\n\t\t\tqa.fillWriteBuffer(optimizeGets);\n\t\t\tcanWriteMore = wrote > 0 && qa.toWrite > 0;\n\t\t}\n\t}\n\n\tprivate void handleReads(SelectionKey sk, QueueAttachment qa)\n\t\tthrows IOException {\n\t\tOperation currentOp = qa.getCurrentReadOp();\n\t\tint read=qa.channel.read(qa.rbuf);\n\t\twhile(read > 0) {\n\t\t\tgetLogger().debug(\"Read %d bytes\", read);\n\t\t\tqa.rbuf.flip();\n\t\t\twhile(qa.rbuf.remaining() > 0) {\n\t\t\t\tassert currentOp != null : \"No read operation\";\n\t\t\t\tcurrentOp.readFromBuffer(qa.rbuf);\n\t\t\t\tif(currentOp.getState() == Operation.State.COMPLETE) {\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\"Completed read op: %s and giving the next %d bytes\",\n\t\t\t\t\t\t\tcurrentOp, qa.rbuf.remaining());\n\t\t\t\t\tOperation op=qa.removeCurrentReadOp();\n\t\t\t\t\tassert op == currentOp\n\t\t\t\t\t: \"Expected to pop \" + currentOp + \" got \" + op;\n\t\t\t\t\tcurrentOp=qa.getCurrentReadOp();\n\t\t\t\t}\n\t\t\t}\n\t\t\tqa.rbuf.clear();\n\t\t\tread=qa.channel.read(qa.rbuf);\n\t\t}\n\t}\n\n\tprivate void fixupOps(QueueAttachment qa) {\n\t\tif(qa.sk.isValid()) {\n\t\t\tint iops=qa.getSelectionOps();\n\t\t\tgetLogger().debug(\"Setting interested opts to %d\", iops);\n\t\t\tqa.sk.interestOps(iops);\n\t\t} else {\n\t\t\tgetLogger().debug(\"Selection key is not valid.\");\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\tprivate void queueReconnect(QueueAttachment qa) {\n\t\tif(!shutDown) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.reconnectAttempt);\n\t\t\tif(qa.sk != null) {\n\t\t\t\tqa.sk.cancel();\n\t\t\t\tassert !qa.sk.isValid() : \"Cancelled selection key is valid\";\n\t\t\t}\n\t\t\tqa.reconnectAttempt++;\n\t\t\ttry {\n\t\t\t\tqa.channel.socket().close();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.channel=null;\n\n\t\t\tlong delay=Math.min((100*qa.reconnectAttempt) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tqa.setupResend();\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tlong now=System.currentTimeMillis();\n\t\tfor(Iterator<QueueAttachment> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tQueueAttachment qa=i.next();\n\t\t\ti.remove();\n\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(qa.socketAddress)) {\n\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.channel=ch;\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t}\n\t}\n\n\t/**\n\t * Get the number of connections currently handled.\n\t */\n\tpublic int getNumConnections() {\n\t\treturn connections.length;\n\t}\n\n\t/**\n\t * Get the remote address of the socket with the given ID.\n\t * \n\t * @param which which id\n\t * @return the rmeote address\n\t */\n\tpublic SocketAddress getAddressOf(int which) {\n\t\treturn connections[which].socketAddress;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t * \n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\tpublic void addOperation(int which, Operation o) {\n\t\tboolean placed=false;\n\t\tint pos=which;\n\t\tint loops=0;\n\t\twhile(!placed) {\n\t\t\tassert loops < 3 : \"Too many loops!\";\n\t\t\tQueueAttachment qa=connections[pos];\n\t\t\tif(which == pos) {\n\t\t\t\tloops++;\n\t\t\t}\n\t\t\tif(qa.reconnectAttempt == 0 || loops > 1) {\n\t\t\t\to.initialize();\n\t\t\t\tqa.addOp(o);\n\t\t\t\taddedQueue.offer(qa);\n\t\t\t\tSelector s=selector.wakeup();\n\t\t\t\tassert s == selector : \"Wakeup returned the wrong selector.\";\n\t\t\t\tgetLogger().debug(\"Added %s to %d\", o, which);\n\t\t\t\tplaced=true;\n\t\t\t} else {\n\t\t\t\tif(++pos >= connections.length) {\n\t\t\t\t\tpos=0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tif(qa.channel != null) {\n\t\t\t\tqa.channel.close();\n\t\t\t\tqa.sk=null;\n\t\t\t\tif(qa.toWrite > 0) {\n\t\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Shut down with %d bytes remaining to write\",\n\t\t\t\t\t\t\tqa.toWrite);\n\t\t\t\t}\n\t\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.channel);\n\t\t\t}\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.socketAddress);\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n\tstatic class QueueAttachment extends SpyObject {\n\t\tpublic final SocketAddress socketAddress;\n\t\tpublic final ByteBuffer rbuf;\n\t\tpublic final ByteBuffer wbuf;\n\t\tprivate final BlockingQueue<Operation> writeQ;\n\t\tprivate final BlockingQueue<Operation> readQ;\n\t\tprivate final BlockingQueue<Operation> inputQueue;\n\t\tpublic int which=0;\n\t\t// This has been declared volatile so it can be used as an availability\n\t\t// indicator.\n\t\tpublic volatile int reconnectAttempt=1;\n\t\tpublic SocketChannel channel;\n\t\tpublic int toWrite=0;\n\t\tprivate GetOperation getOp=null;\n\t\tpublic SelectionKey sk=null;\n\n\t\t// Count sequential protocol errors.\n\t\tpublic int protocolErrors=0;\n\n\t\tpublic QueueAttachment(SocketAddress sa, SocketChannel c, int bufSize,\n\t\t\t\tBlockingQueue<Operation> rq, BlockingQueue<Operation> wq,\n\t\t\t\tBlockingQueue<Operation> iq) {\n\t\t\tsuper();\n\t\t\tassert sa != null : \"No SocketAddress\";\n\t\t\tassert c != null : \"No SocketChannel\";\n\t\t\tassert bufSize > 0 : \"Invalid buffer size: \" + bufSize;\n\t\t\tassert rq != null : \"No operation read queue\";\n\t\t\tassert wq != null : \"No operation write queue\";\n\t\t\tassert iq != null : \"No input queue\";\n\t\t\tsocketAddress=sa;\n\t\t\tchannel=c;\n\t\t\trbuf=ByteBuffer.allocate(bufSize);\n\t\t\twbuf=ByteBuffer.allocate(bufSize);\n\t\t\twbuf.clear();\n\t\t\treadQ=rq;\n\t\t\twriteQ=wq;\n\t\t\tinputQueue=iq;\n\t\t}\n\n\t\tpublic void copyInputQueue() {\n\t\t\tCollection<Operation> tmp=new ArrayList<Operation>();\n\t\t\tinputQueue.drainTo(tmp);\n\t\t\twriteQ.addAll(tmp);\n\t\t}\n\n\n\t\tpublic void setupResend() {\n\t\t\t// First, reset the current write op.\n\t\t\tOperation op=getCurrentWriteOp();\n\t\t\tif(op != null) {\n\t\t\t\top.getBuffer().reset();\n\t\t\t}\n\t\t\t// Now cancel all the pending read operations.  Might be better to\n\t\t\t// to requeue them.\n\t\t\twhile(hasReadOp()) {\n\t\t\t\top=removeCurrentReadOp();\n\t\t\t\tgetLogger().warn(\"Discarding partially completed op: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t}\n\n\t\t\twbuf.clear();\n\t\t\trbuf.clear();\n\t\t\ttoWrite=0;\n\t\t\tprotocolErrors=0;\n\t\t}\n\n\t\t// Prepare the pending operations.  Return true if there are any pending\n\t\t// ops\n\t\tprivate boolean preparePending() {\n\t\t\t// Copy the input queue into the write queue.\n\t\t\tcopyInputQueue();\n\n\t\t\t// Now check the ops\n\t\t\tOperation nextOp=getCurrentWriteOp();\n\t\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\t\tgetLogger().info(\"Removing cancelled operation: %s\", nextOp);\n\t\t\t\tremoveCurrentWriteOp();\n\t\t\t\tnextOp=getCurrentWriteOp();\n\t\t\t}\n\t\t\treturn nextOp != null;\n\t\t}\n\n\t\tpublic void fillWriteBuffer(boolean optimizeGets) {\n\t\t\tif(toWrite == 0) {\n\t\t\t\twbuf.clear();\n\t\t\t\tOperation o=getCurrentWriteOp();\n\t\t\t\twhile(o != null && toWrite < wbuf.capacity()) {\n\t\t\t\t\tassert o.getState() == Operation.State.WRITING;\n\t\t\t\t\tByteBuffer obuf=o.getBuffer();\n\t\t\t\t\tint bytesToCopy=Math.min(wbuf.remaining(),\n\t\t\t\t\t\t\tobuf.remaining());\n\t\t\t\t\tbyte b[]=new byte[bytesToCopy];\n\t\t\t\t\tobuf.get(b);\n\t\t\t\t\twbuf.put(b);\n\t\t\t\t\tgetLogger().debug(\"After copying stuff from %s: %s\",\n\t\t\t\t\t\t\to, wbuf);\n\t\t\t\t\tif(!o.getBuffer().hasRemaining()) {\n\t\t\t\t\t\to.writeComplete();\n\t\t\t\t\t\ttransitionWriteItem();\n\n\t\t\t\t\t\tpreparePending();\n\t\t\t\t\t\tif(optimizeGets) {\n\t\t\t\t\t\t\toptimize();\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\to=getCurrentWriteOp();\n\t\t\t\t\t}\n\t\t\t\t\ttoWrite += bytesToCopy;\n\t\t\t\t}\n\t\t\t\twbuf.flip();\n\t\t\t\tassert toWrite <= wbuf.capacity()\n\t\t\t\t\t: \"toWrite exceeded capacity: \" + this;\n\t\t\t\tassert toWrite == wbuf.remaining()\n\t\t\t\t\t: \"Expected \" + toWrite + \" remaining, got \"\n\t\t\t\t\t+ wbuf.remaining();\n\t\t\t} else {\n\t\t\t\tgetLogger().debug(\"Buffer is full, skipping\");\n\t\t\t}\n\t\t}\n\n\t\tpublic void transitionWriteItem() {\n\t\t\tOperation op=removeCurrentWriteOp();\n\t\t\tassert op != null : \"There is no write item to transition\";\n\t\t\tassert op.getState() == Operation.State.READING;\n\t\t\tgetLogger().debug(\"Transitioning %s to read\", op);\n\t\t\treadQ.add(op);\n\t\t}\n\n\t\tpublic void optimize() {\n\t\t\t// make sure there are at least two get operations in a row before\n\t\t\t// attempting to optimize them.\n\t\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\t\tgetOp=(GetOperation)writeQ.remove();\n\t\t\t\tif(writeQ.peek() instanceof GetOperation) {\n\t\t\t\t\tOptimizedGet og=new OptimizedGet(getOp);\n\t\t\t\t\tgetOp=og;\n\n\t\t\t\t\twhile(writeQ.peek() instanceof GetOperation) {\n\t\t\t\t\t\tGetOperation o=(GetOperation) writeQ.remove();\n\t\t\t\t\t\tif(!o.isCancelled()) {\n\t\t\t\t\t\t\tog.addOperation(o);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Initialize the new mega get\n\t\t\t\t\tgetOp.initialize();\n\t\t\t\t\tassert getOp.getState() == Operation.State.WRITING;\n\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Set up %s with %s keys and %s callbacks\",\n\t\t\t\t\t\tthis, og.numKeys(), og.numCallbacks());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tpublic Operation getCurrentReadOp() {\n\t\t\treturn readQ.peek();\n\t\t}\n\n\t\tpublic Operation removeCurrentReadOp() {\n\t\t\treturn readQ.remove();\n\t\t}\n\n\t\tpublic Operation getCurrentWriteOp() {\n\t\t\treturn getOp == null ? writeQ.peek() : getOp;\n\t\t}\n\n\t\tpublic Operation removeCurrentWriteOp() {\n\t\t\tOperation rv=getOp;\n\t\t\tif(rv == null) {\n\t\t\t\trv=writeQ.remove();\n\t\t\t} else {\n\t\t\t\tgetOp=null;\n\t\t\t}\n\t\t\treturn rv;\n\t\t}\n\n\t\tpublic boolean hasReadOp() {\n\t\t\treturn !readQ.isEmpty();\n\t\t}\n\n\t\tpublic boolean hasWriteOp() {\n\t\t\treturn !(getOp == null && writeQ.isEmpty());\n\t\t}\n\n\t\tpublic void addOp(Operation op) {\n\t\t\tboolean added=inputQueue.add(op);\n\t\t\tassert added; // documented to throw an IllegalStateException\n\t\t}\n\n\t\tpublic int getSelectionOps() {\n\t\t\tint rv=0;\n\t\t\tif(channel.isConnected()) {\n\t\t\t\tif(hasReadOp()) {\n\t\t\t\t\trv |= SelectionKey.OP_READ;\n\t\t\t\t}\n\t\t\t\tif(toWrite > 0 || hasWriteOp()) {\n\t\t\t\t\trv |= SelectionKey.OP_WRITE;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trv = SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\treturn rv;\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\tint sops=0;\n\t\t\tif(sk!= null && sk.isValid()) {\n\t\t\t\tsops=sk.interestOps();\n\t\t\t}\n\t\t\tint rsize=readQ.size() + (getOp == null ? 0 : 1);\n\t\t\tint wsize=writeQ.size();\n\t\t\tint isize=inputQueue.size();\n\t\t\treturn \"{QA sa=\" + socketAddress + \", #Rops=\" + rsize\n\t\t\t\t+ \", #Wops=\" + wsize\n\t\t\t\t+ \", #iq=\" + isize\n\t\t\t\t+ \", topRop=\" + getCurrentReadOp()\n\t\t\t\t+ \", topWop=\" + getCurrentWriteOp()\n\t\t\t\t+ \", toWrite=\" + toWrite\n\t\t\t\t+ \", interested=\" + sops + \"}\";\n\t\t}\n\t}\n}\n","lineNo":424}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached.ops;\n\nimport java.nio.ByteBuffer;\n\nimport net.spy.SpyObject;\n\n/**\n * Operations on a memcached connection.\n */\npublic abstract class Operation extends SpyObject {\n\t/**\n\t * State of this operation.\n\t */\n\tpublic enum State {\n\t\t/**\n\t\t * State indicating this operation is writing data to the server.\n\t\t */\n\t\tWRITING,\n\t\t/**\n\t\t * State indicating this operation is reading data from the server.\n\t\t */\n\t\tREADING,\n\t\t/**\n\t\t * State indicating this operation is complete.\n\t\t */\n\t\tCOMPLETE\n\t}\n\t/**\n\t * Data read types.\n\t */\n\tpublic enum ReadType {\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read lines.\n\t\t */\n\t\tLINE,\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read raw data.\n\t\t */\n\t\tDATA\n\t}\n\t\n\tprivate State state=State.WRITING;\n\tprivate ReadType readType=ReadType.LINE;\n\tprivate ByteBuffer cmd=null;\n\tprivate StringBuilder currentLine=new StringBuilder();\n\tprivate boolean cancelled=false;\n\tprivate boolean foundCr=false;\n\n\t/**\n\t * Has this operation been cancelled?\n\t */\n\tpublic boolean isCancelled() {\n\t\treturn cancelled;\n\t}\n\n\t/**\n\t * Cancel this operation.\n\t */\n\tpublic void cancel() {\n\t\tcancelled=true;\n\t\twasCancelled();\n\t}\n\n\t/**\n\t * This is called on each subclass whenever an operation was cancelled.\n\t */\n\tprotected abstract void wasCancelled();\n\n\t/**\n\t * Get the current state of this operation.\n\t */\n\tpublic State getState() {\n\t\treturn state;\n\t}\n\n\t/**\n\t * Get the write buffer for this operation.\n\t */\n\tpublic ByteBuffer getBuffer() {\n\t\tassert cmd != null : \"No output buffer.\";\n\t\treturn cmd;\n\t}\n\n\t/**\n\t * Set the write buffer for this operation.\n\t */\n\tprotected void setBuffer(ByteBuffer to) {\n\t\tassert to != null : \"Trying to set buffer to null\";\n\t\tcmd=to;\n\t\tcmd.mark();\n\t}\n\n\t/**\n\t * Transition the state of this operation to the given state.\n\t */\n\tprotected void transitionState(State newState) {\n\t\tgetLogger().debug(\"Transitioned state from %s to %s\", state, newState);\n\t\tstate=newState;\n\t\t// Discard our buffer when we no longer need it.\n\t\tif(state != State.WRITING) {\n\t\t\tcmd=null;\n\t\t}\n\t}\n\n\t/**\n\t * Invoked after having written all of the bytes from the supplied output\n\t * buffer.\n\t */\n\tpublic void writeComplete() {\n\t\ttransitionState(State.READING);\n\t}\n\n\t/**\n\t * Get the current read type of this operation.\n\t */\n\tpublic ReadType getReadType() {\n\t\treturn readType;\n\t}\n\n\t/**\n\t * Set the read type of this operation.\n\t */\n\tprotected void setReadType(ReadType to) {\n\t\treadType=to;\n\t}\n\n\t/**\n\t * Set some arguments for an operation into the given byte buffer.\n\t */\n\tprotected void setArguments(ByteBuffer bb, Object... args) {\n\t\tboolean wasFirst=true;\n\t\tfor(Object o : args) {\n\t\t\tif(wasFirst) {\n\t\t\t\twasFirst=false;\n\t\t\t} else {\n\t\t\t\tbb.put((byte)' ');\n\t\t\t}\n\t\t\tbb.put(String.valueOf(o).getBytes());\n\t\t}\n\t\tbb.put(\"\\r\\n\".getBytes());\n\t}\n\n\t/**\n\t * Initialize this operation.  This is used to prepare output byte buffers\n\t * and stuff.\n\t */\n\tpublic abstract void initialize();\n\n\t/**\n\t * Read data from the given byte buffer and dispatch to the appropriate\n\t * read mechanism.\n\t */\n\tpublic final void readFromBuffer(ByteBuffer data) {\n\t\t// Loop while there's data remaining to get it all drained.\n\t\twhile(state != State.COMPLETE && data.remaining() > 0) {\n\t\t\tif(readType == ReadType.DATA) {\n\t\t\t\thandleRead(data);\n\t\t\t} else {\n\t\t\t\tint offset=-1;\n\t\t\t\tfor(int i=0; data.remaining() > 0; i++) {\n\t\t\t\t\tbyte b=data.get();\n\t\t\t\t\tif(b == '\\r') {\n\t\t\t\t\t\tfoundCr=true;\n\t\t\t\t\t} else if(b == '\\n') {\n\t\t\t\t\t\tassert foundCr: \"got a \\\\n without a \\\\r\";\n\t\t\t\t\t\toffset=i;\n\t\t\t\t\t\tfoundCr=false;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tassert !foundCr : \"got a \\\\r without a \\\\n\";\n\t\t\t\t\t\tcurrentLine.append((char)b);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(offset >= 0) {\n\t\t\t\t\thandleLine(currentLine.toString());\n\t\t\t\t\tcurrentLine.delete(0, currentLine.length());\n\t\t\t\t\tassert currentLine.length() == 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Handle a raw data read.\n\t */\n\tpublic void handleRead(ByteBuffer data) {\n\t\tassert false;\n\t}\n\n\t/**\n\t * Handle a textual read.\n\t */\n\tpublic abstract void handleLine(String line);\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n\npackage net.spy.memcached.ops;\n\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\n\nimport net.spy.SpyObject;\n\n/**\n * Operations on a memcached connection.\n */\npublic abstract class Operation extends SpyObject {\n\t/**\n\t * State of this operation.\n\t */\n\tpublic enum State {\n\t\t/**\n\t\t * State indicating this operation is writing data to the server.\n\t\t */\n\t\tWRITING,\n\t\t/**\n\t\t * State indicating this operation is reading data from the server.\n\t\t */\n\t\tREADING,\n\t\t/**\n\t\t * State indicating this operation is complete.\n\t\t */\n\t\tCOMPLETE\n\t}\n\n\t/**\n\t * Error classification.\n\t */\n\tpublic enum ErrorType {\n\t\t/**\n\t\t * General error.\n\t\t */\n\t\tGENERAL(0),\n\t\t/**\n\t\t * Error that occurred because the client did something stupid.\n\t\t */\n\t\tCLIENT(\"CLIENT_ERROR \".length()),\n\t\t/**\n\t\t * Error that occurred because the server did something stupid.\n\t\t */\n\t\tSERVER(\"SERVER_ERROR \".length());\n\n\t\tprivate final int size;\n\n\t\tErrorType(int s) {\n\t\t\tsize=s;\n\t\t}\n\n\t\tpublic int getSize() {\n\t\t\treturn size;\n\t\t}\n\t}\n\n\t/**\n\t * Data read types.\n\t */\n\tpublic enum ReadType {\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read lines.\n\t\t */\n\t\tLINE,\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read raw data.\n\t\t */\n\t\tDATA\n\t}\n\t\n\tprivate State state=State.WRITING;\n\tprivate ReadType readType=ReadType.LINE;\n\tprivate ByteBuffer cmd=null;\n\tprivate StringBuilder currentLine=new StringBuilder();\n\tprivate boolean cancelled=false;\n\tprivate boolean foundCr=false;\n\tprivate OperationException exception=null;\n\tprivate OperationCallback callback=null;\n\n\tprotected Operation() {\n\t\tsuper();\n\t}\n\n\tprotected Operation(OperationCallback cb) {\n\t\tsuper();\n\t\tcallback=cb;\n\t}\n\n\t/**\n\t * Get the operation callback associated with this operation.\n\t */\n\tprotected OperationCallback getCallback() {\n\t\treturn callback;\n\t}\n\n\n\t/**\n\t * Set the callback for this instance.\n\t */\n\tprotected void setCallback(OperationCallback to) {\n\t\tcallback=to;\n\t}\n\n\t/**\n\t * Has this operation been cancelled?\n\t */\n\tpublic boolean isCancelled() {\n\t\treturn cancelled;\n\t}\n\n\t/**\n\t * True if an error occurred while processing this operation.\n\t */\n\tpublic boolean hasErrored() {\n\t\treturn exception != null;\n\t}\n\n\t/**\n\t * Get the exception that occurred (or null if no exception occurred).\n\t */\n\tpublic OperationException getException() {\n\t\treturn exception;\n\t}\n\n\t/**\n\t * Cancel this operation.\n\t */\n\tpublic void cancel() {\n\t\tcancelled=true;\n\t\twasCancelled();\n\t\tcallback.complete();\n\t}\n\n\t/**\n\t * This is called on each subclass whenever an operation was cancelled.\n\t */\n\tprotected abstract void wasCancelled();\n\n\t/**\n\t * Get the current state of this operation.\n\t */\n\tpublic State getState() {\n\t\treturn state;\n\t}\n\n\t/**\n\t * Get the write buffer for this operation.\n\t */\n\tpublic ByteBuffer getBuffer() {\n\t\tassert cmd != null : \"No output buffer.\";\n\t\treturn cmd;\n\t}\n\n\t/**\n\t * Set the write buffer for this operation.\n\t */\n\tprotected void setBuffer(ByteBuffer to) {\n\t\tassert to != null : \"Trying to set buffer to null\";\n\t\tcmd=to;\n\t\tcmd.mark();\n\t}\n\n\t/**\n\t * Transition the state of this operation to the given state.\n\t */\n\tprotected void transitionState(State newState) {\n\t\tgetLogger().debug(\"Transitioned state from %s to %s\", state, newState);\n\t\tstate=newState;\n\t\t// Discard our buffer when we no longer need it.\n\t\tif(state != State.WRITING) {\n\t\t\tcmd=null;\n\t\t}\n\t\tif(state == State.COMPLETE) {\n\t\t\tcallback.complete();\n\t\t}\n\t}\n\n\t/**\n\t * Invoked after having written all of the bytes from the supplied output\n\t * buffer.\n\t */\n\tpublic void writeComplete() {\n\t\ttransitionState(State.READING);\n\t}\n\n\t/**\n\t * Get the current read type of this operation.\n\t */\n\tpublic ReadType getReadType() {\n\t\treturn readType;\n\t}\n\n\t/**\n\t * Set the read type of this operation.\n\t */\n\tprotected void setReadType(ReadType to) {\n\t\treadType=to;\n\t}\n\n\t/**\n\t * Set some arguments for an operation into the given byte buffer.\n\t */\n\tprotected void setArguments(ByteBuffer bb, Object... args) {\n\t\tboolean wasFirst=true;\n\t\tfor(Object o : args) {\n\t\t\tif(wasFirst) {\n\t\t\t\twasFirst=false;\n\t\t\t} else {\n\t\t\t\tbb.put((byte)' ');\n\t\t\t}\n\t\t\tbb.put(String.valueOf(o).getBytes());\n\t\t}\n\t\tbb.put(\"\\r\\n\".getBytes());\n\t}\n\n\t/**\n\t * Initialize this operation.  This is used to prepare output byte buffers\n\t * and stuff.\n\t */\n\tpublic abstract void initialize();\n\n\t/**\n\t * Read data from the given byte buffer and dispatch to the appropriate\n\t * read mechanism.\n\t */\n\tpublic final void readFromBuffer(ByteBuffer data) throws IOException {\n\t\t// Loop while there's data remaining to get it all drained.\n\t\twhile(state != State.COMPLETE && data.remaining() > 0) {\n\t\t\tif(readType == ReadType.DATA) {\n\t\t\t\thandleRead(data);\n\t\t\t} else {\n\t\t\t\tint offset=-1;\n\t\t\t\tfor(int i=0; data.remaining() > 0; i++) {\n\t\t\t\t\tbyte b=data.get();\n\t\t\t\t\tif(b == '\\r') {\n\t\t\t\t\t\tfoundCr=true;\n\t\t\t\t\t} else if(b == '\\n') {\n\t\t\t\t\t\tassert foundCr: \"got a \\\\n without a \\\\r\";\n\t\t\t\t\t\toffset=i;\n\t\t\t\t\t\tfoundCr=false;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tassert !foundCr : \"got a \\\\r without a \\\\n\";\n\t\t\t\t\t\tcurrentLine.append((char)b);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(offset >= 0) {\n\t\t\t\t\tString line=currentLine.toString();\n\t\t\t\t\tcurrentLine.delete(0, currentLine.length());\n\t\t\t\t\tassert currentLine.length() == 0;\n\t\t\t\t\tErrorType eType=classifyError(line);\n\t\t\t\t\tif(eType != null) {\n\t\t\t\t\t\thandleError(eType, line);\n\t\t\t\t\t} else {\n\t\t\t\t\t\thandleLine(line);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void handleError(ErrorType eType, String line) throws IOException {\n\t\tgetLogger().error(\"Error:  %s\", line);\n\t\tswitch(eType) {\n\t\t\tcase GENERAL:\n\t\t\t\texception=new OperationException();\n\t\t\t\tbreak;\n\t\t\tcase SERVER:\n\t\t\t\texception=new OperationException(eType, line);\n\t\t\t\tbreak;\n\t\t\tcase CLIENT:\n\t\t\t\texception=new OperationException(eType, line);\n\t\t\t\tbreak;\n\t\t\tdefault: assert false;\n\t\t}\n\t\ttransitionState(State.COMPLETE);\n\t\tthrow exception;\n\t}\n\n\tprivate ErrorType classifyError(String line) {\n\t\tErrorType rv=null;\n\t\tif(line.startsWith(\"ERROR\")) {\n\t\t\trv=ErrorType.GENERAL;\n\t\t} else if(line.startsWith(\"CLIENT_ERROR\")) {\n\t\t\trv=ErrorType.CLIENT;\n\t\t} else if(line.startsWith(\"SERVER_ERROR\")) {\n\t\t\trv=ErrorType.SERVER;\n\t\t}\n\t\treturn rv;\n\t}\n\n\t/**\n\t * Handle a raw data read.\n\t */\n\tpublic void handleRead(ByteBuffer data) {\n\t\tassert false;\n\t}\n\n\t/**\n\t * Handle a textual read.\n\t */\n\tpublic abstract void handleLine(String line);\n}\n","lineNo":251}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n// arch-tag: FFBD1BC9-52AD-4B4C-A339-A26092C55A9F\n\npackage net.spy.memcached.ops;\n\nimport java.nio.ByteBuffer;\n\nimport net.spy.SpyObject;\n\n/**\n * Operations on a memcached connection.\n */\npublic abstract class Operation extends SpyObject {\n\t/**\n\t * State of this operation.\n\t */\n\tpublic enum State {\n\t\t/**\n\t\t * State indicating this operation is writing data to the server.\n\t\t */\n\t\tWRITING,\n\t\t/**\n\t\t * State indicating this operation is reading data from the server.\n\t\t */\n\t\tREADING,\n\t\t/**\n\t\t * State indicating this operation is complete.\n\t\t */\n\t\tCOMPLETE\n\t}\n\t/**\n\t * Data read types.\n\t */\n\tpublic enum ReadType {\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read lines.\n\t\t */\n\t\tLINE,\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read raw data.\n\t\t */\n\t\tDATA\n\t}\n\t\n\tprivate State state=State.WRITING;\n\tprivate ReadType readType=ReadType.LINE;\n\tprivate ByteBuffer cmd=null;\n\tprivate StringBuffer currentLine=new StringBuffer();\n\tprivate boolean cancelled=false;\n\n\tprotected Operation() {\n\t\tsuper();\n\t}\n\n\t/**\n\t * Has this operation been cancelled?\n\t */\n\tpublic boolean isCancelled() {\n\t\treturn cancelled;\n\t}\n\n\t/**\n\t * Cancel this operation.\n\t */\n\tpublic void cancel() {\n\t\tcancelled=true;\n\t\twasCancelled();\n\t}\n\n\t/**\n\t * This is called on each subclass whenever an operation was cancelled.\n\t */\n\tprotected abstract void wasCancelled();\n\n\t/**\n\t * Get the current state of this operation.\n\t */\n\tpublic State getState() {\n\t\treturn state;\n\t}\n\n\t/**\n\t * Get the write buffer for this operation.\n\t */\n\tpublic ByteBuffer getBuffer() {\n\t\tassert cmd != null : \"No output buffer.\";\n\t\treturn cmd;\n\t}\n\n\t/**\n\t * Set the write buffer for this operation.\n\t */\n\tprotected void setBuffer(ByteBuffer to) {\n\t\tassert to != null : \"Trying to set buffer to null\";\n\t\tcmd=to;\n\t\tcmd.mark();\n\t}\n\n\t/**\n\t * Transition the state of this operation to the given state.\n\t */\n\tprotected void transitionState(State newState) {\n\t\tgetLogger().debug(\"Transitioned state from %s to %s\", state, newState);\n\t\tstate=newState;\n\t\t// Discard our buffer when we no longer need it.\n\t\tif(state != State.WRITING) {\n\t\t\tcmd=null;\n\t\t}\n\t}\n\n\t/**\n\t * Invoked after having written all of the bytes from the supplied output\n\t * buffer.\n\t */\n\tpublic void writeComplete() {\n\t\ttransitionState(State.READING);\n\t}\n\n\t/**\n\t * Get the current read type of this operation.\n\t */\n\tpublic ReadType getReadType() {\n\t\treturn readType;\n\t}\n\n\t/**\n\t * Set the read type of this operation.\n\t */\n\tprotected void setReadType(ReadType to) {\n\t\treadType=to;\n\t}\n\n\t/**\n\t * Set some arguments for an operation into the given byte buffer.\n\t */\n\tprotected void setArguments(ByteBuffer bb, Object... args) {\n\t\tboolean wasFirst=true;\n\t\tfor(Object o : args) {\n\t\t\tif(wasFirst) {\n\t\t\t\twasFirst=false;\n\t\t\t} else {\n\t\t\t\tbb.put((byte)' ');\n\t\t\t}\n\t\t\tbb.put(String.valueOf(o).getBytes());\n\t\t}\n\t\tbb.put(\"\\r\\n\".getBytes());\n\t}\n\n\t/**\n\t * Initialize this operation.\n\t */\n\tpublic abstract void initialize();\n\n\t/**\n\t * Read data from the given byte buffer and dispatch to the appropriate\n\t * read mechanism.\n\t */\n\tpublic final void readFromBuffer(ByteBuffer data) {\n\t\t// Loop while there's data remaining to get it all drained.\n\t\twhile(data.remaining() > 0) {\n\t\t\tif(readType == ReadType.DATA) {\n\t\t\t\thandleRead(data);\n\t\t\t} else {\n\t\t\t\tint offset=-1;\n\t\t\t\tfor(int i=0; data.remaining() > 0; i++) {\n\t\t\t\t\tbyte b=data.get();\n\t\t\t\t\tif(b == '\\r') {\n\t\t\t\t\t\tassert data.get() == '\\n' : \"got a \\\\r without a \\\\n\";\n\t\t\t\t\t\toffset=i;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tcurrentLine.append((char)b);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(offset >= 0) {\n\t\t\t\t\thandleLine(currentLine.toString());\n\t\t\t\t\tcurrentLine.delete(0, currentLine.length());\n\t\t\t\t\tassert currentLine.length() == 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Handle a raw data read.\n\t */\n\tpublic void handleRead(ByteBuffer data) {\n\t\tassert false;\n\t}\n\n\t/**\n\t * Handle a textual read.\n\t */\n\tpublic abstract void handleLine(String line);\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n// arch-tag: FFBD1BC9-52AD-4B4C-A339-A26092C55A9F\n\npackage net.spy.memcached.ops;\n\nimport java.nio.ByteBuffer;\n\nimport net.spy.SpyObject;\n\n/**\n * Operations on a memcached connection.\n */\npublic abstract class Operation extends SpyObject {\n\t/**\n\t * State of this operation.\n\t */\n\tpublic enum State {\n\t\t/**\n\t\t * State indicating this operation is writing data to the server.\n\t\t */\n\t\tWRITING,\n\t\t/**\n\t\t * State indicating this operation is reading data from the server.\n\t\t */\n\t\tREADING,\n\t\t/**\n\t\t * State indicating this operation is complete.\n\t\t */\n\t\tCOMPLETE\n\t}\n\t/**\n\t * Data read types.\n\t */\n\tpublic enum ReadType {\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read lines.\n\t\t */\n\t\tLINE,\n\t\t/**\n\t\t * Read type indicating an operation currently wants to read raw data.\n\t\t */\n\t\tDATA\n\t}\n\t\n\tprivate State state=State.WRITING;\n\tprivate ReadType readType=ReadType.LINE;\n\tprivate ByteBuffer cmd=null;\n\tprivate StringBuffer currentLine=new StringBuffer();\n\tprivate boolean cancelled=false;\n\n\tprotected Operation() {\n\t\tsuper();\n\t}\n\n\t/**\n\t * Has this operation been cancelled?\n\t */\n\tpublic boolean isCancelled() {\n\t\treturn cancelled;\n\t}\n\n\t/**\n\t * Cancel this operation.\n\t */\n\tpublic void cancel() {\n\t\tcancelled=true;\n\t\twasCancelled();\n\t}\n\n\t/**\n\t * This is called on each subclass whenever an operation was cancelled.\n\t */\n\tprotected abstract void wasCancelled();\n\n\t/**\n\t * Get the current state of this operation.\n\t */\n\tpublic State getState() {\n\t\treturn state;\n\t}\n\n\t/**\n\t * Get the write buffer for this operation.\n\t */\n\tpublic ByteBuffer getBuffer() {\n\t\tassert cmd != null : \"No output buffer.\";\n\t\treturn cmd;\n\t}\n\n\t/**\n\t * Set the write buffer for this operation.\n\t */\n\tprotected void setBuffer(ByteBuffer to) {\n\t\tassert to != null : \"Trying to set buffer to null\";\n\t\tcmd=to;\n\t\tcmd.mark();\n\t}\n\n\t/**\n\t * Transition the state of this operation to the given state.\n\t */\n\tprotected void transitionState(State newState) {\n\t\tgetLogger().debug(\"Transitioned state from %s to %s\", state, newState);\n\t\tstate=newState;\n\t\t// Discard our buffer when we no longer need it.\n\t\tif(state != State.WRITING) {\n\t\t\tcmd=null;\n\t\t}\n\t}\n\n\t/**\n\t * Invoked after having written all of the bytes from the supplied output\n\t * buffer.\n\t */\n\tpublic void writeComplete() {\n\t\ttransitionState(State.READING);\n\t}\n\n\t/**\n\t * Get the current read type of this operation.\n\t */\n\tpublic ReadType getReadType() {\n\t\treturn readType;\n\t}\n\n\t/**\n\t * Set the read type of this operation.\n\t */\n\tprotected void setReadType(ReadType to) {\n\t\treadType=to;\n\t}\n\n\t/**\n\t * Set some arguments for an operation into the given byte buffer.\n\t */\n\tprotected void setArguments(ByteBuffer bb, Object... args) {\n\t\tboolean wasFirst=true;\n\t\tfor(Object o : args) {\n\t\t\tif(wasFirst) {\n\t\t\t\twasFirst=false;\n\t\t\t} else {\n\t\t\t\tbb.put((byte)' ');\n\t\t\t}\n\t\t\tbb.put(String.valueOf(o).getBytes());\n\t\t}\n\t\tbb.put(\"\\r\\n\".getBytes());\n\t}\n\n\t/**\n\t * Initialize this operation.\n\t */\n\tpublic abstract void initialize();\n\n\t/**\n\t * Read data from the given byte buffer and dispatch to the appropriate\n\t * read mechanism.\n\t */\n\tpublic final void readFromBuffer(ByteBuffer data) {\n\t\t// Loop while there's data remaining to get it all drained.\n\t\twhile(data.remaining() > 0) {\n\t\t\tif(readType == ReadType.DATA) {\n\t\t\t\thandleRead(data);\n\t\t\t} else {\n\t\t\t\tint offset=-1;\n\t\t\t\tfor(int i=0; data.remaining() > 0; i++) {\n\t\t\t\t\tbyte b=data.get();\n\t\t\t\t\tif(b == '\\r') {\n\t\t\t\t\t\tbyte newline=data.get();\n\t\t\t\t\t\tassert newline == '\\n' : \"got a \\\\r without a \\\\n\";\n\t\t\t\t\t\toffset=i;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tcurrentLine.append((char)b);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(offset >= 0) {\n\t\t\t\t\thandleLine(currentLine.toString());\n\t\t\t\t\tcurrentLine.delete(0, currentLine.length());\n\t\t\t\t\tassert currentLine.length() == 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Handle a raw data read.\n\t */\n\tpublic void handleRead(ByteBuffer data) {\n\t\tassert false;\n\t}\n\n\t/**\n\t * Handle a textual read.\n\t */\n\tpublic abstract void handleLine(String line);\n}\n","lineNo":168}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n// arch-tag: 30573332-B549-4E6F-AD59-04C6D0928419\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.ops.Operation;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyObject {\n\t// The number of empty selects we'll allow before taking action.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 100;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\tprivate static final int MAX_OPS_QUEUE_LEN = 8192;\n\n\tprivate Selector selector=null;\n\tprivate QueueAttachment[] connections=null;\n\tprivate int emptySelects=0;\n\tprivate ConcurrentLinkedQueue<QueueAttachment> addedQueue=null;\n\tprivate SortedMap<Long, QueueAttachment> reconnectQueue=null;\n\n\tpublic MemcachedConnection(int bufSize, List<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\treconnectQueue=new TreeMap<Long, QueueAttachment>();\n\t\taddedQueue=new ConcurrentLinkedQueue<QueueAttachment>();\n\t\tselector=Selector.open();\n\t\tconnections=new QueueAttachment[a.size()];\n\t\tint cons=0;\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tQueueAttachment qa=new QueueAttachment(sa, ch, bufSize);\n\t\t\tqa.which=cons;\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t} else {\n\t\t\t\tgetLogger().debug(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t\tconnections[cons++]=qa;\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tif(qa.sk.isValid()) {\n\t\t\t\tif(qa.channel.isConnected()) {\n\t\t\t\t\tOperation op=null;\n\t\t\t\t\tint sops=0;\n\t\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t\top=qa.ops.peek();\n\t\t\t\t\t\tsops=qa.sk.interestOps();\n\t\t\t\t\t}\n\t\t\t\t\tif(op == null) {\n\t\t\t\t\t\tassert sops == 0 : \"Invalid ops: \" + qa;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tswitch(op.getState()) {\n\t\t\t\t\t\t\tcase READING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_READ) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase WRITING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_WRITE) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase COMPLETE:\n\t\t\t\t\t\t\t\tassert false : \"Completed item in queue\";\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert qa.sk.interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t\t\t: \"Not connected, and not watching for connect.\";\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void handleIO() throws IOException {\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tif(selected > 0) {\n\t\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\t\t\tassert selected == selectedKeys.size();\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Got selection key:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t} else {\n\t\t\t// It's very easy in NIO to write a bug such that your selector\n\t\t\t// spins madly.  This will catch that and let it break.\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > EXCESSIVE_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueueReconnect((QueueAttachment)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY + 10\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t}\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<QueueAttachment> toAdd=new HashSet<QueueAttachment>();\n\t\t\ttry {\n\t\t\t\tQueueAttachment qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\tif(qa.channel != null && qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.ops.peek();\n\t\t\t\t\t\tif(op != null\n\t\t\t\t\t\t\t\t&& op.getState() == Operation.State.WRITING) {\n\t\t\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\t\t\"Handling queued write on %s\", qa);\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\thandleOperation(op, qa.sk, qa);\n\t\t\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\t\t\tgetLogger().warn(\"Exception handling %s\",\n\t\t\t\t\t\t\t\t\t\top, e);\n\t\t\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// out of stuff.\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tQueueAttachment qa=(QueueAttachment)sk.attachment();\n\t\tif(sk.isConnectable()) {\n\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\ttry {\n\t\t\t\tif(qa.channel.finishConnect()) {\n\t\t\t\t\tassert qa.channel.isConnected() : \"Not connected.\";\n\t\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\t\t}\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert !qa.channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t} else {\n\t\t\tOperation currentOp=qa.ops.peek();\n\t\t\tif(currentOp != null) {\n\t\t\t\ttry {\n\t\t\t\t\thandleOperation(currentOp, sk, qa);\n\t\t\t\t} catch(IOException e) {\n\t\t\t\t\tgetLogger().warn(\"Exception handling %s, reconnecting\",\n\t\t\t\t\t\t\tcurrentOp, e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(1);\n\t\t\t\t\ttry {\n\t\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\t\tassert read == -1\n\t\t\t\t\t\t\t: \"expected to read -1 bytes, read \" + read;\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"IOException reading while not\"\n\t\t\t\t\t\t\t\t+ \" expecting a readable channel\", e);\n\t\t\t\t\t}\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"No current operations, but selectors ready\";\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tprivate String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\t// Handle IO for an operation.\n\tprivate void handleOperation(Operation currentOp, SelectionKey sk,\n\t\t\tQueueAttachment qa) throws IOException {\n\t\tgetLogger().debug(\"Current operation: %s\", currentOp);\n\t\t// First switch is for IO.\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tassert !sk.isWritable() : \"While reading, came up writable\";\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tint read=qa.channel.read(qa.buf);\n\t\t\t\t\tif(read < 0) {\n\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqa.buf.flip();\n\t\t\t\t\t\tcurrentOp.readFromBuffer(qa.buf);\n\t\t\t\t\t\tqa.buf.clear();\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"While reading, came up not readable.\";\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tboolean mustReinint=false;\n\t\t\t\tif(sk.isValid() && sk.isReadable()) {\n\t\t\t\t\tgetLogger().debug(\"Readable in write mode.\");\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(512);\n\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\tgetLogger().debug(\"Read %d bytes in write mode\", read);\n\t\t\t\t\tif(read > 0) {\n\t\t\t\t\t\tb.flip();\n\t\t\t\t\t\tgetLogger().error(\n\t\t\t\t\t\t\t\"Read %d bytes in write mode (%s) -- reconnecting\",\n\t\t\t\t\t\t\tread, dbgBuffer(b, read));\n\t\t\t\t\t\tmustReinint=true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(mustReinint) {\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tByteBuffer b=currentOp.getBuffer();\n\t\t\t\t\tint wrote=qa.channel.write(b);\n\t\t\t\t\tgetLogger().debug(\"Wrote %d bytes for %s\",\n\t\t\t\t\t\t\twrote, currentOp);\n\t\t\t\t\tif(b.remaining() == 0) {\n\t\t\t\t\t\tcurrentOp.writeComplete();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tassert false : \"Current op is in complete state\";\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t\t// Second switch is for post-IO examination and state transition\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_READ);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tgetLogger().debug(\"Operation is still writing (%d remaining).\",\n\t\t\t\t\tcurrentOp.getBuffer().remaining());\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tqa.ops.remove();\n\t\t\t\t// If there are more operations in the queue, tell\n\t\t\t\t// it we want to write\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t}\n\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t// After removing the cancelled operations, if there's\n\t\t\t\t\t// another operation waiting to go, wait for write\n\t\t\t\t\tif(hasPendingOperations(qa) && sk.isValid()) {\n\t\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t}\n\n\tprivate boolean hasPendingOperations(QueueAttachment qa) {\n\t\tassert Thread.holdsLock(qa) : \"Not locking qa\";\n\t\tOperation nextOp=qa.ops.peek();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\",\n\t\t\t\t\tnextOp);\n\t\t\tqa.ops.remove();\n\t\t\tnextOp=qa.ops.peek();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\tprivate void queueReconnect(QueueAttachment qa) {\n\t\tsynchronized(qa) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.reconnectAttempt);\n\t\t\tqa.sk.cancel();\n\t\t\tassert !qa.sk.isValid() : \"Cancelled selection key is valid\";\n\t\t\tqa.reconnectAttempt++;\n\t\t\ttry {\n\t\t\t\tqa.channel.socket().close();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.channel=null;\n\n\t\t\tlong delay=Math.min((100*qa.reconnectAttempt) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tsetupResend(qa);\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tlong now=System.currentTimeMillis();\n\t\tfor(Iterator<QueueAttachment> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tQueueAttachment qa=i.next();\n\t\t\ti.remove();\n\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tch.connect(qa.socketAddress);\n\t\t\tqa.channel=ch;\n\t\t\tqa.sk=ch.register(selector, 0, qa);\n\t\t\tqa.sk.interestOps(SelectionKey.OP_CONNECT);\n\t\t}\n\t}\n\n\tprivate void setupResend(QueueAttachment qa) {\n\t\tOperation op=qa.ops.peek();\n\t\tif(op != null) {\n\t\t\tif(op.getState() == Operation.State.WRITING) {\n\t\t\t\tgetLogger().warn(\"Resetting write state of op: %s\", op);\n\t\t\t\top.getBuffer().reset();\n\t\t\t\taddedQueue.offer(qa);\n\t\t\t} else {\n\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Discarding partially completed operation: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t\tqa.ops.remove();\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Get the number of connections currently handled.\n\t */\n\tpublic int getNumConnections() {\n\t\treturn connections.length;\n\t}\n\n\t/**\n\t * Get the remote address of the socket with the given ID.\n\t * \n\t * @param which which id\n\t * @return the rmeote address\n\t */\n\tpublic SocketAddress getAddressOf(int which) {\n\t\treturn connections[which].socketAddress;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t * \n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void addOperation(int which, Operation o) {\n\t\tQueueAttachment qa=connections[which];\n\t\to.initialize();\n\t\tsynchronized(qa) {\n\t\t\tqa.ops.add(o);\n\t\t\tif(qa.ops.size() == 1 && qa.sk.isValid()) {\n\t\t\t\tqa.sk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t}\n\t\t}\n\t\taddedQueue.offer(qa);\n\t\tselector.wakeup();\n\t\tgetLogger().debug(\"Added %s to %d\", o, which);\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tqa.channel.close();\n\t\t\tqa.sk=null;\n\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.channel);\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.socketAddress);\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n\tprivate static class QueueAttachment {\n\t\tpublic int which=0;\n\t\tpublic int reconnectAttempt=1;\n\t\tpublic SocketAddress socketAddress=null;\n\t\tpublic SocketChannel channel=null;\n\t\tpublic ByteBuffer buf=null;\n\t\tpublic BlockingQueue<Operation> ops=null;\n\t\tpublic SelectionKey sk=null;\n\t\tpublic QueueAttachment(SocketAddress sa, SocketChannel c, int bufSize) {\n\t\t\tsuper();\n\t\t\tsocketAddress=sa;\n\t\t\tchannel=c;\n\t\t\tbuf=ByteBuffer.allocate(bufSize);\n\t\t\tops=new ArrayBlockingQueue<Operation>(MAX_OPS_QUEUE_LEN);\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\tint sops=0;\n\t\t\tif(sk!= null && sk.isValid()) {\n\t\t\t\tsops=sk.interestOps();\n\t\t\t}\n\t\t\treturn \"{QA sa=\" + socketAddress + \", #ops=\" + ops.size()\n\t\t\t\t+ \", topop=\" + ops.peek() + \", interested=\" + sops + \"}\";\n\t\t}\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n// arch-tag: 30573332-B549-4E6F-AD59-04C6D0928419\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.ops.Operation;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyObject {\n\t// The number of empty selects we'll allow before taking action.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 100;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\tprivate static final int MAX_OPS_QUEUE_LEN = 8192;\n\n\tprivate Selector selector=null;\n\tprivate QueueAttachment[] connections=null;\n\tprivate int emptySelects=0;\n\tprivate ConcurrentLinkedQueue<QueueAttachment> addedQueue=null;\n\tprivate SortedMap<Long, QueueAttachment> reconnectQueue=null;\n\n\tpublic MemcachedConnection(int bufSize, List<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\treconnectQueue=new TreeMap<Long, QueueAttachment>();\n\t\taddedQueue=new ConcurrentLinkedQueue<QueueAttachment>();\n\t\tselector=Selector.open();\n\t\tconnections=new QueueAttachment[a.size()];\n\t\tint cons=0;\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tQueueAttachment qa=new QueueAttachment(sa, ch, bufSize);\n\t\t\tqa.which=cons;\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t\tassert ch.isConnected()\n\t\t\t\t|| qa.sk.interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\tconnections[cons++]=qa;\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsynchronized(qa) {\n\t\t\t\tif(qa.sk.isValid()) {\n\t\t\t\t\tif(qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.ops.peek();\n\t\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\t\tif(op == null) {\n\t\t\t\t\t\t\tassert sops == 0 : \"Invalid ops: \" + qa;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tswitch(op.getState()) {\n\t\t\t\t\t\t\tcase READING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_READ) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase WRITING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_WRITE) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase COMPLETE:\n\t\t\t\t\t\t\t\tassert false : \"Completed item in queue\";\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t\t\t+ sops;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void handleIO() throws IOException {\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tif(selected > 0) {\n\t\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\t\t\tassert selected == selectedKeys.size();\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Got selection key:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t} else {\n\t\t\t// It's very easy in NIO to write a bug such that your selector\n\t\t\t// spins madly.  This will catch that and let it break.\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > EXCESSIVE_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueueReconnect((QueueAttachment)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY + 10\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t}\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<QueueAttachment> toAdd=new HashSet<QueueAttachment>();\n\t\t\ttry {\n\t\t\t\tQueueAttachment qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\tif(qa.channel != null && qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.ops.peek();\n\t\t\t\t\t\tif(op != null\n\t\t\t\t\t\t\t\t&& op.getState() == Operation.State.WRITING) {\n\t\t\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\t\t\"Handling queued write on %s\", qa);\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\thandleOperation(op, qa.sk, qa);\n\t\t\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\t\t\tgetLogger().warn(\"Exception handling %s\",\n\t\t\t\t\t\t\t\t\t\top, e);\n\t\t\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// out of stuff.\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tQueueAttachment qa=(QueueAttachment)sk.attachment();\n\t\tif(sk.isConnectable()) {\n\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\ttry {\n\t\t\t\tif(qa.channel.finishConnect()) {\n\t\t\t\t\tassert qa.channel.isConnected() : \"Not connected.\";\n\t\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\t\t}\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert !qa.channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t} else {\n\t\t\tOperation currentOp=qa.ops.peek();\n\t\t\tif(currentOp != null) {\n\t\t\t\ttry {\n\t\t\t\t\thandleOperation(currentOp, sk, qa);\n\t\t\t\t} catch(IOException e) {\n\t\t\t\t\tgetLogger().warn(\"Exception handling %s, reconnecting\",\n\t\t\t\t\t\t\tcurrentOp, e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(1);\n\t\t\t\t\ttry {\n\t\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\t\tassert read == -1\n\t\t\t\t\t\t\t: \"expected to read -1 bytes, read \" + read;\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"IOException reading while not\"\n\t\t\t\t\t\t\t\t+ \" expecting a readable channel\", e);\n\t\t\t\t\t}\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"No current operations, but selectors ready\";\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\t// Handle IO for an operation.\n\tprivate void handleOperation(Operation currentOp, SelectionKey sk,\n\t\t\tQueueAttachment qa) throws IOException {\n\t\tgetLogger().debug(\"Current operation: %s\", currentOp);\n\t\t// First switch is for IO.\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tassert !sk.isWritable() : \"While reading, came up writable\";\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tint read=qa.channel.read(qa.buf);\n\t\t\t\t\tif(read < 0) {\n\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqa.buf.flip();\n\t\t\t\t\t\tcurrentOp.readFromBuffer(qa.buf);\n\t\t\t\t\t\tqa.buf.clear();\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"While reading, came up not readable.\";\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tboolean mustReinint=false;\n\t\t\t\tif(sk.isValid() && sk.isReadable()) {\n\t\t\t\t\tgetLogger().debug(\"Readable in write mode.\");\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(512);\n\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\tgetLogger().debug(\"Read %d bytes in write mode\", read);\n\t\t\t\t\tif(read > 0) {\n\t\t\t\t\t\tb.flip();\n\t\t\t\t\t\tgetLogger().error(\n\t\t\t\t\t\t\t\"Read %d bytes in write mode (%s) -- reconnecting\",\n\t\t\t\t\t\t\tread, dbgBuffer(b, read));\n\t\t\t\t\t\tmustReinint=true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(mustReinint) {\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tByteBuffer b=currentOp.getBuffer();\n\t\t\t\t\tint wrote=qa.channel.write(b);\n\t\t\t\t\tgetLogger().debug(\"Wrote %d bytes for %s\",\n\t\t\t\t\t\t\twrote, currentOp);\n\t\t\t\t\tif(b.remaining() == 0) {\n\t\t\t\t\t\tcurrentOp.writeComplete();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tassert false : \"Current op is in complete state\";\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t\t// Second switch is for post-IO examination and state transition\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_READ);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tgetLogger().debug(\"Operation is still writing (%d remaining).\",\n\t\t\t\t\tcurrentOp.getBuffer().remaining());\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tqa.ops.remove();\n\t\t\t\t// If there are more operations in the queue, tell\n\t\t\t\t// it we want to write\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t}\n\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t// After removing the cancelled operations, if there's\n\t\t\t\t\t// another operation waiting to go, wait for write\n\t\t\t\t\tif(hasPendingOperations(qa) && sk.isValid()) {\n\t\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t}\n\n\tprivate boolean hasPendingOperations(QueueAttachment qa) {\n\t\tassert Thread.holdsLock(qa) : \"Not locking qa\";\n\t\tOperation nextOp=qa.ops.peek();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\",\n\t\t\t\t\tnextOp);\n\t\t\tqa.ops.remove();\n\t\t\tnextOp=qa.ops.peek();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\tprivate void queueReconnect(QueueAttachment qa) {\n\t\tsynchronized(qa) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.reconnectAttempt);\n\t\t\tqa.sk.cancel();\n\t\t\tassert !qa.sk.isValid() : \"Cancelled selection key is valid\";\n\t\t\tqa.reconnectAttempt++;\n\t\t\ttry {\n\t\t\t\tqa.channel.socket().close();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.channel=null;\n\n\t\t\tlong delay=Math.min((100*qa.reconnectAttempt) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tsetupResend(qa);\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tlong now=System.currentTimeMillis();\n\t\tfor(Iterator<QueueAttachment> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tQueueAttachment qa=i.next();\n\t\t\ti.remove();\n\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(qa.socketAddress)) {\n\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.channel=ch;\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t}\n\t}\n\n\tprivate void setupResend(QueueAttachment qa) {\n\t\tOperation op=qa.ops.peek();\n\t\tif(op != null) {\n\t\t\tif(op.getState() == Operation.State.WRITING) {\n\t\t\t\tgetLogger().warn(\"Resetting write state of op: %s\", op);\n\t\t\t\top.getBuffer().reset();\n\t\t\t\taddedQueue.offer(qa);\n\t\t\t} else {\n\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Discarding partially completed operation: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t\tqa.ops.remove();\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Get the number of connections currently handled.\n\t */\n\tpublic int getNumConnections() {\n\t\treturn connections.length;\n\t}\n\n\t/**\n\t * Get the remote address of the socket with the given ID.\n\t * \n\t * @param which which id\n\t * @return the rmeote address\n\t */\n\tpublic SocketAddress getAddressOf(int which) {\n\t\treturn connections[which].socketAddress;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t * \n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void addOperation(int which, Operation o) {\n\t\tQueueAttachment qa=connections[which];\n\t\to.initialize();\n\t\tsynchronized(qa) {\n\t\t\tqa.ops.add(o);\n\t\t\tif(qa.ops.size() == 1 && qa.sk.isValid()\n\t\t\t\t\t&& qa.channel.isConnected()) {\n\t\t\t\tqa.sk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t}\n\t\t}\n\t\taddedQueue.offer(qa);\n\t\tselector.wakeup();\n\t\tgetLogger().debug(\"Added %s to %d\", o, which);\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tqa.channel.close();\n\t\t\tqa.sk=null;\n\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.channel);\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.socketAddress);\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n\tprivate static class QueueAttachment {\n\t\tpublic int which=0;\n\t\tpublic int reconnectAttempt=1;\n\t\tpublic SocketAddress socketAddress=null;\n\t\tpublic SocketChannel channel=null;\n\t\tpublic ByteBuffer buf=null;\n\t\tpublic BlockingQueue<Operation> ops=null;\n\t\tpublic SelectionKey sk=null;\n\t\tpublic QueueAttachment(SocketAddress sa, SocketChannel c, int bufSize) {\n\t\t\tsuper();\n\t\t\tsocketAddress=sa;\n\t\t\tchannel=c;\n\t\t\tbuf=ByteBuffer.allocate(bufSize);\n\t\t\tops=new ArrayBlockingQueue<Operation>(MAX_OPS_QUEUE_LEN);\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\tint sops=0;\n\t\t\tif(sk!= null && sk.isValid()) {\n\t\t\t\tsops=sk.interestOps();\n\t\t\t}\n\t\t\treturn \"{QA sa=\" + socketAddress + \", #ops=\" + ops.size()\n\t\t\t\t+ \", topop=\" + ops.peek() + \", interested=\" + sops + \"}\";\n\t\t}\n\t}\n}\n","lineNo":99}
{"Smelly Sample":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n// arch-tag: 30573332-B549-4E6F-AD59-04C6D0928419\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.ops.Operation;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyObject {\n\t// The number of empty selects we'll allow before taking action.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 100;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\tprivate static final int MAX_OPS_QUEUE_LEN = 8192;\n\n\tprivate Selector selector=null;\n\tprivate QueueAttachment[] connections=null;\n\tprivate int emptySelects=0;\n\tprivate ConcurrentLinkedQueue<QueueAttachment> addedQueue=null;\n\tprivate SortedMap<Long, QueueAttachment> reconnectQueue=null;\n\n\tpublic MemcachedConnection(int bufSize, List<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\treconnectQueue=new TreeMap<Long, QueueAttachment>();\n\t\taddedQueue=new ConcurrentLinkedQueue<QueueAttachment>();\n\t\tselector=Selector.open();\n\t\tconnections=new QueueAttachment[a.size()];\n\t\tint cons=0;\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tQueueAttachment qa=new QueueAttachment(sa, ch, bufSize);\n\t\t\tqa.which=cons;\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t} else {\n\t\t\t\tgetLogger().debug(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t\tconnections[cons++]=qa;\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tif(qa.sk.isValid()) {\n\t\t\t\tif(qa.channel.isConnected()) {\n\t\t\t\t\tOperation op=null;\n\t\t\t\t\tint sops=0;\n\t\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t\top=qa.ops.peek();\n\t\t\t\t\t\tsops=qa.sk.interestOps();\n\t\t\t\t\t}\n\t\t\t\t\tif(op == null) {\n\t\t\t\t\t\tassert sops == 0 : \"Invalid ops: \" + qa;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tswitch(op.getState()) {\n\t\t\t\t\t\t\tcase READING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_READ) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase WRITING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_WRITE) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase COMPLETE:\n\t\t\t\t\t\t\t\tassert false : \"Completed item in queue\";\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert qa.sk.interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t\t\t: \"Not connected, and not watching for connect.\";\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void handleIO() throws IOException {\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tif(selected > 0) {\n\t\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\t\t\tassert selected == selectedKeys.size();\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Got selection key:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t} else {\n\t\t\t// It's very easy in NIO to write a bug such that your selector\n\t\t\t// spins madly.  This will catch that and let it break.\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > EXCESSIVE_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueueReconnect((QueueAttachment)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY + 10\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t}\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<QueueAttachment> toAdd=new HashSet<QueueAttachment>();\n\t\t\ttry {\n\t\t\t\tQueueAttachment qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\tif(qa.channel != null && qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.ops.peek();\n\t\t\t\t\t\tif(op != null\n\t\t\t\t\t\t\t\t&& op.getState() == Operation.State.WRITING) {\n\t\t\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\t\t\"Handling queued write on %s\", qa);\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\thandleOperation(op, qa.sk, qa);\n\t\t\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\t\t\tgetLogger().warn(\"Exception handling %s\",\n\t\t\t\t\t\t\t\t\t\top, e);\n\t\t\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// out of stuff.\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tQueueAttachment qa=(QueueAttachment)sk.attachment();\n\t\tif(sk.isConnectable()) {\n\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\ttry {\n\t\t\t\tif(qa.channel.finishConnect()) {\n\t\t\t\t\tassert qa.channel.isConnected() : \"Not connected.\";\n\t\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\t\t}\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert !qa.channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t} else {\n\t\t\tOperation currentOp=qa.ops.peek();\n\t\t\tif(currentOp != null) {\n\t\t\t\ttry {\n\t\t\t\t\thandleOperation(currentOp, sk, qa);\n\t\t\t\t} catch(IOException e) {\n\t\t\t\t\tgetLogger().warn(\"Exception handling %s, reconnecting\",\n\t\t\t\t\t\t\tcurrentOp, e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(1);\n\t\t\t\t\ttry {\n\t\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\t\tassert read == -1\n\t\t\t\t\t\t\t: \"expected to read -1 bytes, read \" + read;\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"IOException reading while not\"\n\t\t\t\t\t\t\t\t+ \" expecting a readable channel\", e);\n\t\t\t\t\t}\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"No current operations, but selectors ready\";\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tprivate String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\t// Handle IO for an operation.\n\tprivate void handleOperation(Operation currentOp, SelectionKey sk,\n\t\t\tQueueAttachment qa) throws IOException {\n\t\tgetLogger().debug(\"Current operation: %s\", currentOp);\n\t\t// First switch is for IO.\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tassert !sk.isWritable() : \"While reading, came up writable\";\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tint read=qa.channel.read(qa.buf);\n\t\t\t\t\tif(read < 0) {\n\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqa.buf.flip();\n\t\t\t\t\t\tcurrentOp.readFromBuffer(qa.buf);\n\t\t\t\t\t\tqa.buf.clear();\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"While reading, came up not readable.\";\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tboolean mustReinint=false;\n\t\t\t\tif(sk.isValid() && sk.isReadable()) {\n\t\t\t\t\tgetLogger().debug(\"Readable in write mode.\");\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(512);\n\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\tgetLogger().debug(\"Read %d bytes in write mode\", read);\n\t\t\t\t\tif(read > 0) {\n\t\t\t\t\t\tb.flip();\n\t\t\t\t\t\tgetLogger().error(\n\t\t\t\t\t\t\t\"Read %d bytes in write mode (%s) -- reconnecting\",\n\t\t\t\t\t\t\tread, dbgBuffer(b, read));\n\t\t\t\t\t\tmustReinint=true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(mustReinint) {\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tByteBuffer b=currentOp.getBuffer();\n\t\t\t\t\tint wrote=qa.channel.write(b);\n\t\t\t\t\tgetLogger().debug(\"Wrote %d bytes for %s\",\n\t\t\t\t\t\t\twrote, currentOp);\n\t\t\t\t\tif(b.remaining() == 0) {\n\t\t\t\t\t\tcurrentOp.writeComplete();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tassert false : \"Current op is in complete state\";\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t\t// Second switch is for post-IO examination and state transition\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_READ);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tgetLogger().debug(\"Operation is still writing (%d remaining).\",\n\t\t\t\t\tcurrentOp.getBuffer().remaining());\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tqa.ops.remove();\n\t\t\t\t// If there are more operations in the queue, tell\n\t\t\t\t// it we want to write\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t}\n\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t// After removing the cancelled operations, if there's\n\t\t\t\t\t// another operation waiting to go, wait for write\n\t\t\t\t\tif(hasPendingOperations(qa) && sk.isValid()) {\n\t\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t}\n\n\tprivate boolean hasPendingOperations(QueueAttachment qa) {\n\t\tassert Thread.holdsLock(qa) : \"Not locking qa\";\n\t\tOperation nextOp=qa.ops.peek();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\",\n\t\t\t\t\tnextOp);\n\t\t\tqa.ops.remove();\n\t\t\tnextOp=qa.ops.peek();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\tprivate void queueReconnect(QueueAttachment qa) {\n\t\tsynchronized(qa) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.reconnectAttempt);\n\t\t\tqa.sk.cancel();\n\t\t\tassert !qa.sk.isValid() : \"Cancelled selection key is valid\";\n\t\t\tqa.reconnectAttempt++;\n\t\t\ttry {\n\t\t\t\tqa.channel.socket().close();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.channel=null;\n\n\t\t\tlong delay=Math.min((100*qa.reconnectAttempt) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tsetupResend(qa);\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tlong now=System.currentTimeMillis();\n\t\tfor(Iterator<QueueAttachment> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tQueueAttachment qa=i.next();\n\t\t\ti.remove();\n\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tch.connect(qa.socketAddress);\n\t\t\tqa.channel=ch;\n\t\t\tqa.sk=ch.register(selector, 0, qa);\n\t\t\tqa.sk.interestOps(SelectionKey.OP_CONNECT);\n\t\t}\n\t}\n\n\tprivate void setupResend(QueueAttachment qa) {\n\t\tOperation op=qa.ops.peek();\n\t\tif(op != null) {\n\t\t\tif(op.getState() == Operation.State.WRITING) {\n\t\t\t\tgetLogger().warn(\"Resetting write state of op: %s\", op);\n\t\t\t\top.getBuffer().reset();\n\t\t\t\taddedQueue.offer(qa);\n\t\t\t} else {\n\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Discarding partially completed operation: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t\tqa.ops.remove();\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Get the number of connections currently handled.\n\t */\n\tpublic int getNumConnections() {\n\t\treturn connections.length;\n\t}\n\n\t/**\n\t * Get the remote address of the socket with the given ID.\n\t * \n\t * @param which which id\n\t * @return the rmeote address\n\t */\n\tpublic SocketAddress getAddressOf(int which) {\n\t\treturn connections[which].socketAddress;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t * \n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void addOperation(int which, Operation o) {\n\t\tQueueAttachment qa=connections[which];\n\t\to.initialize();\n\t\tsynchronized(qa) {\n\t\t\tqa.ops.add(o);\n\t\t\tif(qa.ops.size() == 1 && qa.sk.isValid()) {\n\t\t\t\tqa.sk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t}\n\t\t}\n\t\taddedQueue.offer(qa);\n\t\tselector.wakeup();\n\t\tgetLogger().debug(\"Added %s to %d\", o, which);\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tqa.channel.close();\n\t\t\tqa.sk=null;\n\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.channel);\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.socketAddress);\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n\tprivate static class QueueAttachment {\n\t\tpublic int which=0;\n\t\tpublic int reconnectAttempt=1;\n\t\tpublic SocketAddress socketAddress=null;\n\t\tpublic SocketChannel channel=null;\n\t\tpublic ByteBuffer buf=null;\n\t\tpublic BlockingQueue<Operation> ops=null;\n\t\tpublic SelectionKey sk=null;\n\t\tpublic QueueAttachment(SocketAddress sa, SocketChannel c, int bufSize) {\n\t\t\tsuper();\n\t\t\tsocketAddress=sa;\n\t\t\tchannel=c;\n\t\t\tbuf=ByteBuffer.allocate(bufSize);\n\t\t\tops=new ArrayBlockingQueue<Operation>(MAX_OPS_QUEUE_LEN);\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\tint sops=0;\n\t\t\tif(sk!= null && sk.isValid()) {\n\t\t\t\tsops=sk.interestOps();\n\t\t\t}\n\t\t\treturn \"{QA sa=\" + socketAddress + \", #ops=\" + ops.size()\n\t\t\t\t+ \", topop=\" + ops.peek() + \", interested=\" + sops + \"}\";\n\t\t}\n\t}\n}\n","Method after Refactoring":"// Copyright (c) 2006  Dustin Sallings <dustin@spy.net>\n// arch-tag: 30573332-B549-4E6F-AD59-04C6D0928419\n\npackage net.spy.memcached;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.SocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.SocketChannel;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\nimport net.spy.SpyObject;\nimport net.spy.memcached.ops.Operation;\n\n/**\n * Connection to a cluster of memcached servers.\n */\npublic class MemcachedConnection extends SpyObject {\n\t// The number of empty selects we'll allow before taking action.  It's too\n\t// easy to write a bug that causes it to loop uncontrollably.  This helps\n\t// find those bugs and often works around them.\n\tprivate static final int EXCESSIVE_EMPTY = 100;\n\t// maximum amount of time to wait between reconnect attempts\n\tprivate static final long MAX_DELAY = 30000;\n\tprivate static final int MAX_OPS_QUEUE_LEN = 8192;\n\n\tprivate Selector selector=null;\n\tprivate QueueAttachment[] connections=null;\n\tprivate int emptySelects=0;\n\tprivate ConcurrentLinkedQueue<QueueAttachment> addedQueue=null;\n\tprivate SortedMap<Long, QueueAttachment> reconnectQueue=null;\n\n\tpublic MemcachedConnection(int bufSize, List<InetSocketAddress> a)\n\t\tthrows IOException {\n\t\treconnectQueue=new TreeMap<Long, QueueAttachment>();\n\t\taddedQueue=new ConcurrentLinkedQueue<QueueAttachment>();\n\t\tselector=Selector.open();\n\t\tconnections=new QueueAttachment[a.size()];\n\t\tint cons=0;\n\t\tfor(SocketAddress sa : a) {\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tQueueAttachment qa=new QueueAttachment(sa, ch, bufSize);\n\t\t\tqa.which=cons;\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(sa)) {\n\t\t\t\tgetLogger().info(\"Connected to %s immediately\", qa);\n\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tgetLogger().info(\"Added %s to connect queue\", qa);\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t\tassert ch.isConnected()\n\t\t\t\t|| qa.sk.interestOps() == SelectionKey.OP_CONNECT\n\t\t\t\t: \"Not connected, and not wanting to connect\";\n\t\t\tconnections[cons++]=qa;\n\t\t}\n\t}\n\n\tprivate boolean selectorsMakeSense() {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsynchronized(qa) {\n\t\t\t\tif(qa.sk.isValid()) {\n\t\t\t\t\tif(qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.ops.peek();\n\t\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\t\tif(op == null) {\n\t\t\t\t\t\t\tassert sops == 0 : \"Invalid ops: \" + qa;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tswitch(op.getState()) {\n\t\t\t\t\t\t\tcase READING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_READ) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase WRITING:\n\t\t\t\t\t\t\t\tassert (sops & SelectionKey.OP_WRITE) != 0\n\t\t\t\t\t\t\t\t\t: \"Invalid ops: \" + qa;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase COMPLETE:\n\t\t\t\t\t\t\t\tassert false : \"Completed item in queue\";\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tint sops=qa.sk.interestOps();\n\t\t\t\t\t\tassert sops == SelectionKey.OP_CONNECT\n\t\t\t\t\t\t\t: \"Not connected, and not watching for connect: \"\n\t\t\t\t\t\t\t\t+ sops;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgetLogger().debug(\"Checked the selectors.\");\n\t\treturn true;\n\t}\n\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void handleIO() throws IOException {\n\n\t\t// Deal with all of the stuff that's been added, but may not be marked\n\t\t// writable.\n\t\thandleInputQueue();\n\t\tgetLogger().debug(\"Done dealing with queue.\");\n\n\t\tlong delay=0;\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tlong now=System.currentTimeMillis();\n\t\t\tlong then=reconnectQueue.firstKey();\n\t\t\tdelay=Math.max(then-now, 1);\n\t\t}\n\t\tgetLogger().debug(\"Selecting with delay of %sms\", delay);\n\t\tassert selectorsMakeSense() : \"Selectors don't make sense.\";\n\t\tint selected=selector.select(delay);\n\t\tif(selected > 0) {\n\t\t\tSet<SelectionKey> selectedKeys=selector.selectedKeys();\n\t\t\tassert selected == selectedKeys.size();\n\t\t\tgetLogger().debug(\"Selected %d, selected %d keys\",\n\t\t\t\t\tselected, selectedKeys.size());\n\t\t\temptySelects=0;\n\t\t\tfor(SelectionKey sk : selectedKeys) {\n\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\"Got selection key:  %s (r=%s, w=%s, c=%s, op=%s)\",\n\t\t\t\t\t\tsk, sk.isReadable(), sk.isWritable(),\n\t\t\t\t\t\tsk.isConnectable(), sk.attachment());\n\t\t\t\thandleIO(sk);\n\t\t\t} // for each selector\n\t\t\tselectedKeys.clear();\n\t\t} else {\n\t\t\t// It's very easy in NIO to write a bug such that your selector\n\t\t\t// spins madly.  This will catch that and let it break.\n\t\t\tgetLogger().debug(\"No selectors ready, interrupted: \"\n\t\t\t\t\t+ Thread.interrupted());\n\t\t\tif(++emptySelects > EXCESSIVE_EMPTY) {\n\t\t\t\tfor(SelectionKey sk : selector.keys()) {\n\t\t\t\t\tgetLogger().info(\"%s has %s, interested in %s\",\n\t\t\t\t\t\t\tsk, sk.readyOps(), sk.interestOps());\n\t\t\t\t\tif(sk.readyOps() != 0) {\n\t\t\t\t\t\tgetLogger().info(\"%s has a ready op, handling IO\", sk);\n\t\t\t\t\t\thandleIO(sk);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueueReconnect((QueueAttachment)sk.attachment());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassert emptySelects < EXCESSIVE_EMPTY + 10\n\t\t\t\t\t: \"Too many empty selects\";\n\t\t\t}\n\t\t}\n\t\tif(!reconnectQueue.isEmpty()) {\n\t\t\tattemptReconnects();\n\t\t}\n\t}\n\n\tprivate void handleInputQueue() {\n\t\tif(!addedQueue.isEmpty()) {\n\t\t\tgetLogger().debug(\"Handling queue\");\n\t\t\t// If there's stuff in the added queue.  Try to process it.\n\t\t\tCollection<QueueAttachment> toAdd=new HashSet<QueueAttachment>();\n\t\t\ttry {\n\t\t\t\tQueueAttachment qa=null;\n\t\t\t\twhile((qa=addedQueue.remove()) != null) {\n\t\t\t\t\tif(qa.channel != null && qa.channel.isConnected()) {\n\t\t\t\t\t\tOperation op=qa.ops.peek();\n\t\t\t\t\t\tif(op != null\n\t\t\t\t\t\t\t\t&& op.getState() == Operation.State.WRITING) {\n\t\t\t\t\t\t\tgetLogger().debug(\n\t\t\t\t\t\t\t\t\t\"Handling queued write on %s\", qa);\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\thandleOperation(op, qa.sk, qa);\n\t\t\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\t\t\tgetLogger().warn(\"Exception handling %s\",\n\t\t\t\t\t\t\t\t\t\top, e);\n\t\t\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttoAdd.add(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch(NoSuchElementException e) {\n\t\t\t\t// out of stuff.\n\t\t\t}\n\t\t\taddedQueue.addAll(toAdd);\n\t\t}\n\t}\n\n\t// Handle IO for a specific selector.  Any IOException will cause a\n\t// reconnect\n\tprivate void handleIO(SelectionKey sk) {\n\t\tQueueAttachment qa=(QueueAttachment)sk.attachment();\n\t\tif(sk.isConnectable()) {\n\t\t\tgetLogger().info(\"Connection state changed for %s\", sk);\n\t\t\ttry {\n\t\t\t\tif(qa.channel.finishConnect()) {\n\t\t\t\t\tassert qa.channel.isConnected() : \"Not connected.\";\n\t\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t\tqa.reconnectAttempt=0;\n\t\t\t\t\t}\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert !qa.channel.isConnected() : \"connected\";\n\t\t\t\t}\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"Problem handling connect\", e);\n\t\t\t\tqueueReconnect(qa);\n\t\t\t}\n\t\t} else {\n\t\t\tOperation currentOp=qa.ops.peek();\n\t\t\tif(currentOp != null) {\n\t\t\t\ttry {\n\t\t\t\t\thandleOperation(currentOp, sk, qa);\n\t\t\t\t} catch(IOException e) {\n\t\t\t\t\tgetLogger().warn(\"Exception handling %s, reconnecting\",\n\t\t\t\t\t\t\tcurrentOp, e);\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(1);\n\t\t\t\t\ttry {\n\t\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\t\tassert read == -1\n\t\t\t\t\t\t\t: \"expected to read -1 bytes, read \" + read;\n\t\t\t\t\t} catch(IOException e) {\n\t\t\t\t\t\tgetLogger().warn(\"IOException reading while not\"\n\t\t\t\t\t\t\t\t+ \" expecting a readable channel\", e);\n\t\t\t\t\t}\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"No current operations, but selectors ready\";\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Make a debug string out of the given buffer's values\n\tstatic String dbgBuffer(ByteBuffer b, int size) {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tbyte[] bytes=b.array();\n\t\tfor(int i=0; i<size; i++) {\n\t\t\tchar ch=(char)bytes[i];\n\t\t\tif(Character.isWhitespace(ch) || Character.isLetterOrDigit(ch)) {\n\t\t\t\tsb.append(ch);\n\t\t\t} else {\n\t\t\t\tsb.append(\"\\\\x\");\n\t\t\t\tsb.append(Integer.toHexString(bytes[i] & 0xff));\n\t\t\t}\n\t\t}\n\t\treturn sb.toString();\n\t}\n\n\t// Handle IO for an operation.\n\tprivate void handleOperation(Operation currentOp, SelectionKey sk,\n\t\t\tQueueAttachment qa) throws IOException {\n\t\tgetLogger().debug(\"Current operation: %s\", currentOp);\n\t\t// First switch is for IO.\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tassert !sk.isWritable() : \"While reading, came up writable\";\n\t\t\t\tif(sk.isReadable()) {\n\t\t\t\t\tint read=qa.channel.read(qa.buf);\n\t\t\t\t\tif(read < 0) {\n\t\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqa.buf.flip();\n\t\t\t\t\t\tcurrentOp.readFromBuffer(qa.buf);\n\t\t\t\t\t\tqa.buf.clear();\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tassert false : \"While reading, came up not readable.\";\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tboolean mustReinint=false;\n\t\t\t\tif(sk.isValid() && sk.isReadable()) {\n\t\t\t\t\tgetLogger().debug(\"Readable in write mode.\");\n\t\t\t\t\tByteBuffer b=ByteBuffer.allocate(512);\n\t\t\t\t\tint read=qa.channel.read(b);\n\t\t\t\t\tgetLogger().debug(\"Read %d bytes in write mode\", read);\n\t\t\t\t\tif(read > 0) {\n\t\t\t\t\t\tb.flip();\n\t\t\t\t\t\tgetLogger().error(\n\t\t\t\t\t\t\t\"Read %d bytes in write mode (%s) -- reconnecting\",\n\t\t\t\t\t\t\tread, dbgBuffer(b, read));\n\t\t\t\t\t\tmustReinint=true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(mustReinint) {\n\t\t\t\t\tqueueReconnect(qa);\n\t\t\t\t} else {\n\t\t\t\t\tByteBuffer b=currentOp.getBuffer();\n\t\t\t\t\tint wrote=qa.channel.write(b);\n\t\t\t\t\tgetLogger().debug(\"Wrote %d bytes for %s\",\n\t\t\t\t\t\t\twrote, currentOp);\n\t\t\t\t\tif(b.remaining() == 0) {\n\t\t\t\t\t\tcurrentOp.writeComplete();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tassert false : \"Current op is in complete state\";\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t\t// Second switch is for post-IO examination and state transition\n\t\tswitch(currentOp.getState()) {\n\t\t\tcase READING:\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_READ);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase WRITING:\n\t\t\t\tgetLogger().debug(\"Operation is still writing (%d remaining).\",\n\t\t\t\t\tcurrentOp.getBuffer().remaining());\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase COMPLETE:\n\t\t\t\tqa.ops.remove();\n\t\t\t\t// If there are more operations in the queue, tell\n\t\t\t\t// it we want to write\n\t\t\t\tif(sk.isValid()) {\n\t\t\t\t\tsk.interestOps(0);\n\t\t\t\t}\n\t\t\t\tsynchronized(qa) {\n\t\t\t\t\t// After removing the cancelled operations, if there's\n\t\t\t\t\t// another operation waiting to go, wait for write\n\t\t\t\t\tif(hasPendingOperations(qa) && sk.isValid()) {\n\t\t\t\t\t\tsk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t\t\t\taddedQueue.offer(qa);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tassert false;\n\t\t}\n\t}\n\n\tprivate boolean hasPendingOperations(QueueAttachment qa) {\n\t\tassert Thread.holdsLock(qa) : \"Not locking qa\";\n\t\tOperation nextOp=qa.ops.peek();\n\t\twhile(nextOp != null && nextOp.isCancelled()) {\n\t\t\tgetLogger().info(\"Removing cancelled operation: %s\",\n\t\t\t\t\tnextOp);\n\t\t\tqa.ops.remove();\n\t\t\tnextOp=qa.ops.peek();\n\t\t}\n\t\treturn nextOp != null;\n\t}\n\n\tprivate void queueReconnect(QueueAttachment qa) {\n\t\tsynchronized(qa) {\n\t\t\tgetLogger().warn(\"Closing, and reopening %s, attempt %d.\",\n\t\t\t\t\tqa, qa.reconnectAttempt);\n\t\t\tqa.sk.cancel();\n\t\t\tassert !qa.sk.isValid() : \"Cancelled selection key is valid\";\n\t\t\tqa.reconnectAttempt++;\n\t\t\ttry {\n\t\t\t\tqa.channel.socket().close();\n\t\t\t} catch(IOException e) {\n\t\t\t\tgetLogger().warn(\"IOException trying to close a socket\", e);\n\t\t\t}\n\t\t\tqa.channel=null;\n\n\t\t\tlong delay=Math.min((100*qa.reconnectAttempt) ^ 2, MAX_DELAY);\n\n\t\t\treconnectQueue.put(System.currentTimeMillis() + delay, qa);\n\n\t\t\t// Need to do a little queue management.\n\t\t\tsetupResend(qa);\n\t\t}\n\t}\n\n\tprivate void attemptReconnects() throws IOException {\n\t\tlong now=System.currentTimeMillis();\n\t\tfor(Iterator<QueueAttachment> i=\n\t\t\t\treconnectQueue.headMap(now).values().iterator(); i.hasNext();) {\n\t\t\tQueueAttachment qa=i.next();\n\t\t\ti.remove();\n\t\t\tgetLogger().info(\"Reconnecting %s\", qa);\n\t\t\tSocketChannel ch=SocketChannel.open();\n\t\t\tch.configureBlocking(false);\n\t\t\tint ops=0;\n\t\t\tif(ch.connect(qa.socketAddress)) {\n\t\t\t\tgetLogger().info(\"Immediately reconnected to %s\", qa);\n\t\t\t\tassert ch.isConnected();\n\t\t\t} else {\n\t\t\t\tops=SelectionKey.OP_CONNECT;\n\t\t\t}\n\t\t\tqa.channel=ch;\n\t\t\tqa.sk=ch.register(selector, ops, qa);\n\t\t}\n\t}\n\n\tprivate void setupResend(QueueAttachment qa) {\n\t\tOperation op=qa.ops.peek();\n\t\tif(op != null) {\n\t\t\tif(op.getState() == Operation.State.WRITING) {\n\t\t\t\tgetLogger().warn(\"Resetting write state of op: %s\", op);\n\t\t\t\top.getBuffer().reset();\n\t\t\t\taddedQueue.offer(qa);\n\t\t\t} else {\n\t\t\t\tgetLogger().warn(\n\t\t\t\t\t\t\"Discarding partially completed operation: %s\", op);\n\t\t\t\top.cancel();\n\t\t\t\tqa.ops.remove();\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Get the number of connections currently handled.\n\t */\n\tpublic int getNumConnections() {\n\t\treturn connections.length;\n\t}\n\n\t/**\n\t * Get the remote address of the socket with the given ID.\n\t * \n\t * @param which which id\n\t * @return the rmeote address\n\t */\n\tpublic SocketAddress getAddressOf(int which) {\n\t\treturn connections[which].socketAddress;\n\t}\n\n\t/**\n\t * Add an operation to the given connection.\n\t * \n\t * @param which the connection offset\n\t * @param o the operation\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void addOperation(int which, Operation o) {\n\t\tQueueAttachment qa=connections[which];\n\t\to.initialize();\n\t\tsynchronized(qa) {\n\t\t\tqa.ops.add(o);\n\t\t\tif(qa.ops.size() == 1 && qa.sk.isValid()\n\t\t\t\t\t&& qa.channel.isConnected()) {\n\t\t\t\tqa.sk.interestOps(SelectionKey.OP_WRITE);\n\t\t\t}\n\t\t}\n\t\taddedQueue.offer(qa);\n\t\tselector.wakeup();\n\t\tgetLogger().debug(\"Added %s to %d\", o, which);\n\t}\n\n\t/**\n\t * Shut down all of the connections.\n\t */\n\tpublic void shutdown() throws IOException {\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tqa.channel.close();\n\t\t\tqa.sk=null;\n\t\t\tgetLogger().debug(\"Shut down channel %s\", qa.channel);\n\t\t}\n\t\tselector.close();\n\t\tgetLogger().debug(\"Shut down selector %s\", selector);\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder sb=new StringBuilder();\n\t\tsb.append(\"{MemcachedConnection to\");\n\t\tfor(QueueAttachment qa : connections) {\n\t\t\tsb.append(\" \");\n\t\t\tsb.append(qa.socketAddress);\n\t\t}\n\t\tsb.append(\"}\");\n\t\treturn sb.toString();\n\t}\n\n\tprivate static class QueueAttachment {\n\t\tpublic int which=0;\n\t\tpublic int reconnectAttempt=1;\n\t\tpublic SocketAddress socketAddress=null;\n\t\tpublic SocketChannel channel=null;\n\t\tpublic ByteBuffer buf=null;\n\t\tpublic BlockingQueue<Operation> ops=null;\n\t\tpublic SelectionKey sk=null;\n\t\tpublic QueueAttachment(SocketAddress sa, SocketChannel c, int bufSize) {\n\t\t\tsuper();\n\t\t\tsocketAddress=sa;\n\t\t\tchannel=c;\n\t\t\tbuf=ByteBuffer.allocate(bufSize);\n\t\t\tops=new ArrayBlockingQueue<Operation>(MAX_OPS_QUEUE_LEN);\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\tint sops=0;\n\t\t\tif(sk!= null && sk.isValid()) {\n\t\t\t\tsops=sk.interestOps();\n\t\t\t}\n\t\t\treturn \"{QA sa=\" + socketAddress + \", #ops=\" + ops.size()\n\t\t\t\t+ \", topop=\" + ops.peek() + \", interested=\" + sops + \"}\";\n\t\t}\n\t}\n}\n","lineNo":399}
