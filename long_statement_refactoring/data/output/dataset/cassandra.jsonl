{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.repair;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.StandardCharsets;\nimport java.sql.Timestamp;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.function.BiFunction;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.LongSupplier;\nimport java.util.function.Supplier;\nimport javax.annotation.Nullable;\n\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Maps;\nimport org.apache.cassandra.config.UnitConfigOverride;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\n\nimport accord.utils.DefaultRandom;\nimport accord.utils.Gen;\nimport accord.utils.Gens;\nimport accord.utils.RandomSource;\nimport org.agrona.collections.Long2ObjectHashMap;\nimport org.agrona.collections.LongHashSet;\nimport org.apache.cassandra.concurrent.ExecutorBuilder;\nimport org.apache.cassandra.concurrent.ExecutorBuilderFactory;\nimport org.apache.cassandra.concurrent.ExecutorFactory;\nimport org.apache.cassandra.concurrent.ExecutorPlus;\nimport org.apache.cassandra.concurrent.InfiniteLoopExecutor;\nimport org.apache.cassandra.concurrent.Interruptible;\nimport org.apache.cassandra.concurrent.ScheduledExecutorPlus;\nimport org.apache.cassandra.concurrent.SequentialExecutorPlus;\nimport org.apache.cassandra.concurrent.SimulatedExecutorFactory;\nimport org.apache.cassandra.concurrent.Stage;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.CQLTester;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Digest;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.compaction.ICompactionManager;\nimport org.apache.cassandra.db.marshal.EmptyType;\nimport org.apache.cassandra.db.repair.CassandraTableRepairManager;\nimport org.apache.cassandra.db.repair.PendingAntiCompaction;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.dht.Range;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.exceptions.RequestFailureReason;\nimport org.apache.cassandra.gms.ApplicationState;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.HeartBeatState;\nimport org.apache.cassandra.gms.IEndpointStateChangeSubscriber;\nimport org.apache.cassandra.gms.IFailureDetector;\nimport org.apache.cassandra.gms.IGossiper;\nimport org.apache.cassandra.gms.VersionedValue;\nimport org.apache.cassandra.io.util.DataInputBuffer;\nimport org.apache.cassandra.io.util.DataOutputBuffer;\nimport org.apache.cassandra.locator.IEndpointSnitch;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.LocalStrategy;\nimport org.apache.cassandra.locator.RangesAtEndpoint;\nimport org.apache.cassandra.locator.TokenMetadata;\nimport org.apache.cassandra.net.ConnectionType;\nimport org.apache.cassandra.net.IVerbHandler;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.net.MessageDelivery;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.net.RequestCallback;\nimport org.apache.cassandra.repair.messages.RepairMessage;\nimport org.apache.cassandra.repair.messages.RepairOption;\nimport org.apache.cassandra.repair.messages.ValidationResponse;\nimport org.apache.cassandra.repair.state.Completable;\nimport org.apache.cassandra.repair.state.CoordinatorState;\nimport org.apache.cassandra.repair.state.JobState;\nimport org.apache.cassandra.repair.state.SessionState;\nimport org.apache.cassandra.repair.state.ValidationState;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.SystemDistributedKeyspace;\nimport org.apache.cassandra.schema.TableId;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.ActiveRepairService;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.streaming.StreamEventHandler;\nimport org.apache.cassandra.streaming.StreamReceiveException;\nimport org.apache.cassandra.streaming.StreamSession;\nimport org.apache.cassandra.streaming.StreamState;\nimport org.apache.cassandra.streaming.StreamingChannel;\nimport org.apache.cassandra.streaming.StreamingDataInputPlus;\nimport org.apache.cassandra.tools.nodetool.Repair;\nimport org.apache.cassandra.utils.AbstractTypeGenerators;\nimport org.apache.cassandra.utils.CassandraGenerators;\nimport org.apache.cassandra.utils.Clock;\nimport org.apache.cassandra.utils.Closeable;\nimport org.apache.cassandra.utils.FailingBiConsumer;\nimport org.apache.cassandra.utils.Generators;\nimport org.apache.cassandra.utils.MBeanWrapper;\nimport org.apache.cassandra.utils.MerkleTree;\nimport org.apache.cassandra.utils.MerkleTrees;\nimport org.apache.cassandra.utils.NoSpamLogger;\nimport org.apache.cassandra.utils.concurrent.AsyncPromise;\nimport org.apache.cassandra.utils.concurrent.Future;\nimport org.apache.cassandra.utils.concurrent.ImmediateFuture;\nimport org.apache.cassandra.utils.progress.ProgressEventType;\nimport org.assertj.core.api.Assertions;\nimport org.mockito.Mockito;\nimport org.quicktheories.impl.JavaRandom;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.CLOCK_GLOBAL;\nimport static org.apache.cassandra.config.CassandraRelevantProperties.ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION;\n\npublic abstract class FuzzTestBase extends CQLTester.InMemory\n{\n    private static final int MISMATCH_NUM_PARTITIONS = 1;\n    private static final Gen<String> IDENTIFIER_GEN = fromQT(Generators.IDENTIFIER_GEN);\n    private static final Gen<String> KEYSPACE_NAME_GEN = fromQT(CassandraGenerators.KEYSPACE_NAME_GEN);\n    private static final Gen<TableId> TABLE_ID_GEN = fromQT(CassandraGenerators.TABLE_ID_GEN);\n    private static final Gen<InetAddressAndPort> ADDRESS_W_PORT = fromQT(CassandraGenerators.INET_ADDRESS_AND_PORT_GEN);\n\n    private static boolean SETUP_SCHEMA = false;\n    static String KEYSPACE;\n    static List<String> TABLES;\n\n    @BeforeClass\n    public static void setUpClass()\n    {\n        ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION.setBoolean(true);\n        CLOCK_GLOBAL.setString(ClockAccess.class.getName());\n        // when running in CI an external actor will replace the test configs based off the test type (such as trie, cdc, etc.), this could then have failing tests\n        // that do not repo with the same seed!  To fix that, go to UnitConfigOverride and update the config type to match the one that failed in CI, this should then\n        // use the same config, so the seed should not reproduce.\n        UnitConfigOverride.maybeOverrideConfig();\n\n        DatabaseDescriptor.daemonInitialization();\n        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance); // TOOD (coverage): random select\n        DatabaseDescriptor.setLocalDataCenter(\"test\");\n        StreamingChannel.Factory.Global.unsafeSet(new StreamingChannel.Factory()\n        {\n            private final AtomicInteger counter = new AtomicInteger();\n\n            @Override\n            public StreamingChannel create(InetSocketAddress to, int messagingVersion, StreamingChannel.Kind kind) throws IOException\n            {\n                StreamingChannel mock = Mockito.mock(StreamingChannel.class);\n                int id = counter.incrementAndGet();\n                StreamSession session = Mockito.mock(StreamSession.class);\n                StreamReceiveException access = new StreamReceiveException(session, \"mock access rejected\");\n                StreamingDataInputPlus input = Mockito.mock(StreamingDataInputPlus.class, invocationOnMock -> {\n                    throw access;\n                });\n                Mockito.doNothing().when(input).close();\n                Mockito.when(mock.in()).thenReturn(input);\n                Mockito.when(mock.id()).thenReturn(id);\n                Mockito.when(mock.peer()).thenReturn(to);\n                Mockito.when(mock.connectedTo()).thenReturn(to);\n                Mockito.when(mock.send(Mockito.any())).thenReturn(ImmediateFuture.success(null));\n                Mockito.when(mock.close()).thenReturn(ImmediateFuture.success(null));\n                return mock;\n            }\n        });\n        ExecutorFactory delegate = ExecutorFactory.Global.executorFactory();\n        ExecutorFactory.Global.unsafeSet(new ExecutorFactory()\n        {\n            @Override\n            public LocalAwareSubFactory localAware()\n            {\n                return delegate.localAware();\n            }\n\n            @Override\n            public ScheduledExecutorPlus scheduled(boolean executeOnShutdown, String name, int priority, SimulatorSemantics simulatorSemantics)\n            {\n                return delegate.scheduled(executeOnShutdown, name, priority, simulatorSemantics);\n            }\n\n            private boolean shouldMock()\n            {\n                return StackWalker.getInstance().walk(frame -> {\n                    StackWalker.StackFrame caller = frame.skip(3).findFirst().get();\n                    return caller.getClassName().startsWith(\"org.apache.cassandra.streaming.\");\n                });\n            }\n\n            @Override\n            public Thread startThread(String name, Runnable runnable, InfiniteLoopExecutor.Daemon daemon)\n            {\n                if (shouldMock()) return new Thread();\n                return delegate.startThread(name, runnable, daemon);\n            }\n\n            @Override\n            public Interruptible infiniteLoop(String name, Interruptible.Task task, InfiniteLoopExecutor.SimulatorSafe simulatorSafe, InfiniteLoopExecutor.Daemon daemon, InfiniteLoopExecutor.Interrupts interrupts)\n            {\n                return delegate.infiniteLoop(name, task, simulatorSafe, daemon, interrupts);\n            }\n\n            @Override\n            public ThreadGroup newThreadGroup(String name)\n            {\n                return delegate.newThreadGroup(name);\n            }\n\n            @Override\n            public ExecutorBuilderFactory<ExecutorPlus, SequentialExecutorPlus> withJmx(String jmxPath)\n            {\n                return delegate.withJmx(jmxPath);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends SequentialExecutorPlus> configureSequential(String name)\n            {\n                return delegate.configureSequential(name);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends ExecutorPlus> configurePooled(String name, int threads)\n            {\n                return delegate.configurePooled(name, threads);\n            }\n        });\n\n        // will both make sure this is loaded and used\n        if (!(Clock.Global.clock() instanceof ClockAccess)) throw new IllegalStateException(\"Unable to override clock\");\n\n        // set the repair rcp timeout high so we don't hit it... this class is mostly testing repair reaching success\n        // so don't want to deal with unlucky histories...\n        DatabaseDescriptor.setRepairRpcTimeout(TimeUnit.DAYS.toMillis(1));\n\n\n        InMemory.setUpClass();\n    }\n\n    @Before\n    public void setupSchema()\n    {\n        if (SETUP_SCHEMA) return;\n        SETUP_SCHEMA = true;\n        // StorageService can not be mocked out, nor can ColumnFamilyStores, so make sure that the keyspace is a \"local\" keyspace to avoid replication as the peers don't actually exist for replication\n        schemaChange(String.format(\"CREATE KEYSPACE %s WITH REPLICATION = {'class': '%s'}\", SchemaConstants.DISTRIBUTED_KEYSPACE_NAME, HackStrat.class.getName()));\n        for (TableMetadata table : SystemDistributedKeyspace.metadata().tables)\n            schemaChange(table.toCqlString(false, false));\n\n        createSchema();\n    }\n\n    protected void cleanupRepairTables()\n    {\n        for (String table : Arrays.asList(SystemKeyspace.REPAIRS))\n            execute(String.format(\"TRUNCATE %s.%s\", SchemaConstants.SYSTEM_KEYSPACE_NAME, table));\n    }\n\n    private void createSchema()\n    {\n        // The main reason to use random here with a fixed seed is just to have a set of tables that are not hard coded.\n        // The tables will have diversity to them that most likely doesn't matter to repair (hence why the tables are shared), but\n        // is useful just in case some assumptions change.\n        RandomSource rs = new DefaultRandom(42);\n        String ks = KEYSPACE_NAME_GEN.next(rs);\n        List<String> tableNames = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(10, 100).next(rs);\n        JavaRandom qt = new JavaRandom(rs.asJdkRandom());\n        Tables.Builder tableBuilder = Tables.builder();\n        List<TableId> ids = Gens.lists(TABLE_ID_GEN).unique().ofSize(tableNames.size()).next(rs);\n        for (int i = 0; i < tableNames.size(); i++)\n        {\n            String name = tableNames.get(i);\n            TableId id = ids.get(i);\n            TableMetadata tableMetadata = new CassandraGenerators.TableMetadataBuilder().withKeyspaceName(ks).withTableName(name).withTableId(id).withTableKinds(TableMetadata.Kind.REGULAR)\n                                                                                        // shouldn't matter, just wanted to avoid UDT as that needs more setup\n                                                                                        .withDefaultTypeGen(AbstractTypeGenerators.builder().withTypeKinds(AbstractTypeGenerators.TypeKind.PRIMITIVE).withoutPrimitive(EmptyType.instance).build()).build().generate(qt);\n            tableBuilder.add(tableMetadata);\n        }\n        KeyspaceParams params = KeyspaceParams.simple(3);\n        KeyspaceMetadata metadata = KeyspaceMetadata.create(ks, params, tableBuilder.build());\n\n        // create\n        schemaChange(metadata.toCqlString(false, false));\n        KEYSPACE = ks;\n        for (TableMetadata table : metadata.tables)\n            schemaChange(table.toCqlString(false, false));\n        TABLES = tableNames;\n    }\n\n    static void enableMessageFaults(Cluster cluster)\n    {\n        cluster.allowedMessageFaults(new BiFunction<>()\n        {\n            private final LongHashSet noFaults = new LongHashSet();\n            private final LongHashSet allowDrop = new LongHashSet();\n\n            @Override\n            public Set<Faults> apply(Cluster.Node node, Message<?> message)\n            {\n                if (RepairMessage.ALLOWS_RETRY.contains(message.verb()))\n                {\n                    allowDrop.add(message.id());\n                    return Faults.DROPPED;\n                }\n                switch (message.verb())\n                {\n                    // these messages are not resilent to ephemeral issues\n                    case STATUS_REQ:\n                    case STATUS_RSP:\n                        noFaults.add(message.id());\n                        return Faults.NONE;\n                    default:\n                        if (noFaults.contains(message.id())) return Faults.NONE;\n                        if (allowDrop.contains(message.id())) return Faults.DROPPED;\n                        // was a new message added and the test not updated?\n                        IllegalStateException e = new IllegalStateException(\"Verb: \" + message.verb());\n                        cluster.failures.add(e);\n                        throw e;\n                }\n            }\n        });\n    }\n\n    static void runAndAssertSuccess(Cluster cluster, int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        cluster.processAll();\n        assertSuccess(example, shouldSync, repair);\n    }\n\n    static void assertSuccess(int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        Completable.Result result = repair.state.getResult();\n        Assertions.assertThat(result)\n                  .describedAs(\"Expected repair to have completed with success, but is still running... %s; example %d\", repair.state, example).isNotNull()\n                  .describedAs(\"Unexpected state: %s -> %s; example %d\", repair.state, result, example).isEqualTo(Completable.Result.success(repairSuccessMessage(repair)));\n        Assertions.assertThat(repair.state.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(CoordinatorState.State.class));\n        Assertions.assertThat(repair.state.getSessions()).isNotEmpty();\n        boolean shouldSnapshot = repair.state.options.getParallelism() != RepairParallelism.PARALLEL\n                                 && (!repair.state.options.isIncremental() || repair.state.options.isPreview());\n        for (SessionState session : repair.state.getSessions())\n        {\n            Assertions.assertThat(session.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(SessionState.State.class));\n            Assertions.assertThat(session.getJobs()).isNotEmpty();\n            for (JobState job : session.getJobs())\n            {\n                EnumSet<JobState.State> expected = EnumSet.allOf(JobState.State.class);\n                if (!shouldSnapshot)\n                {\n                    expected.remove(JobState.State.SNAPSHOT_START);\n                    expected.remove(JobState.State.SNAPSHOT_COMPLETE);\n                }\n                if (!shouldSync)\n                {\n                    expected.remove(JobState.State.STREAM_START);\n                }\n                Set<JobState.State> actual = job.getStateTimesMillis().keySet();\n                Assertions.assertThat(actual).isEqualTo(expected);\n            }\n        }\n    }\n\n    static String repairSuccessMessage(RepairCoordinator repair)\n    {\n        RepairOption options = repair.state.options;\n        if (options.isPreview())\n        {\n            String suffix;\n            switch (options.getPreviewKind())\n            {\n                case UNREPAIRED:\n                case ALL:\n                    suffix = \"Previewed data was in sync\";\n                    break;\n                case REPAIRED:\n                    suffix = \"Repaired data is in sync\";\n                    break;\n                default:\n                    throw new IllegalArgumentException(\"Unexpected preview repair kind: \" + options.getPreviewKind());\n            }\n            return \"Repair preview completed successfully; \" + suffix;\n        }\n        return \"Repair completed successfully\";\n    }\n\n    InetAddressAndPort pickParticipant(RandomSource rs, Cluster.Node coordinator, RepairCoordinator repair)\n    {\n        if (repair.state.isComplete())\n            throw new IllegalStateException(\"Repair is completed! \" + repair.state.getResult());\n        List<InetAddressAndPort> participaents = new ArrayList<>(repair.state.getNeighborsAndRanges().participants.size() + 1);\n        if (rs.nextBoolean()) participaents.add(coordinator.broadcastAddressAndPort());\n        participaents.addAll(repair.state.getNeighborsAndRanges().participants);\n        participaents.sort(Comparator.naturalOrder());\n\n        InetAddressAndPort selected = rs.pick(participaents);\n        return selected;\n    }\n\n    static void addMismatch(RandomSource rs, ColumnFamilyStore cfs, Validator validator)\n    {\n        ValidationState state = validator.state;\n        int maxDepth = DatabaseDescriptor.getRepairSessionMaxTreeDepth();\n        state.phase.start(MISMATCH_NUM_PARTITIONS, 1024);\n\n        MerkleTrees trees = new MerkleTrees(cfs.getPartitioner());\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            int depth = (int) Math.min(Math.ceil(Math.log(MISMATCH_NUM_PARTITIONS) / Math.log(2)), maxDepth);\n            trees.addMerkleTree((int) Math.pow(2, depth), range);\n        }\n        Set<Token> allTokens = new HashSet<>();\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            Gen<Token> gen = fromQT(CassandraGenerators.tokensInRange(range));\n            Set<Token> tokens = new LinkedHashSet<>();\n            for (int i = 0, size = rs.nextInt(1, 10); i < size; i++)\n            {\n                for (int attempt = 0; !tokens.add(gen.next(rs)) && attempt < 5; attempt++)\n                {\n                }\n            }\n            // tokens may or may not be of the expected size; this depends on how wide the range is\n            for (Token token : tokens)\n                trees.split(token);\n            allTokens.addAll(tokens);\n        }\n        for (Token token : allTokens)\n        {\n            findCorrectRange(trees, token, range -> {\n                Digest digest = Digest.forValidator();\n                digest.update(ByteBuffer.wrap(token.toString().getBytes(StandardCharsets.UTF_8)));\n                range.addHash(new MerkleTree.RowHash(token, digest.digest(), 1));\n            });\n        }\n        state.partitionsProcessed++;\n        state.bytesRead = 1024;\n        state.phase.sendingTrees();\n        Stage.ANTI_ENTROPY.execute(() -> {\n            state.phase.success();\n            validator.respond(new ValidationResponse(validator.desc, trees));\n        });\n    }\n\n    private static void findCorrectRange(MerkleTrees trees, Token token, Consumer<MerkleTree.TreeRange> fn)\n    {\n        MerkleTrees.TreeRangeIterator it = trees.rangeIterator();\n        while (it.hasNext())\n        {\n            MerkleTree.TreeRange next = it.next();\n            if (next.contains(token))\n            {\n                fn.accept(next);\n                return;\n            }\n        }\n    }\n\n    private enum RepairType\n    {FULL, IR}\n\n    private enum PreviewType\n    {NONE, REPAIRED, UNREPAIRED}\n\n    static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, List<String> tableNames)\n    {\n        return repairOption(rs, coordinator, ks, Gens.lists(Gens.pick(tableNames)).ofSizeBetween(1, tableNames.size()), Gens.enums().all(RepairType.class), Gens.enums().all(PreviewType.class), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption irOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.IR), Gens.constant(PreviewType.NONE), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption previewOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.FULL), Gens.constant(PreviewType.REPAIRED), Gens.enums().all(RepairParallelism.class));\n    }\n\n    private static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen, Gen<RepairType> repairTypeGen, Gen<PreviewType> previewTypeGen, Gen<RepairParallelism> repairParallelismGen)\n    {\n        List<String> args = new ArrayList<>();\n        args.add(ks);\n        args.addAll(tablesGen.next(rs));\n        args.add(\"-pr\");\n        RepairType type = repairTypeGen.next(rs);\n        switch (type)\n        {\n            case IR:\n                // default\n                break;\n            case FULL:\n                args.add(\"--full\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported repair type: \" + type);\n        }\n        PreviewType previewType = previewTypeGen.next(rs);\n        switch (previewType)\n        {\n            case NONE:\n                break;\n            case REPAIRED:\n                args.add(\"--validate\");\n                break;\n            case UNREPAIRED:\n                args.add(\"--preview\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported preview type: \" + previewType);\n        }\n        RepairParallelism parallelism = repairParallelismGen.next(rs);\n        switch (parallelism)\n        {\n            case SEQUENTIAL:\n                args.add(\"--sequential\");\n                break;\n            case PARALLEL:\n                // default\n                break;\n            case DATACENTER_AWARE:\n                args.add(\"--dc-parallel\");\n                break;\n            default:\n                throw new AssertionError(\"Unknown parallelism: \" + parallelism);\n        }\n        if (rs.nextBoolean()) args.add(\"--optimise-streams\");\n        RepairOption options = RepairOption.parse(Repair.parseOptionMap(() -> \"test\", args), DatabaseDescriptor.getPartitioner());\n        if (options.getRanges().isEmpty())\n        {\n            if (options.isPrimaryRange())\n            {\n                // when repairing only primary range, neither dataCenters nor hosts can be set\n                if (options.getDataCenters().isEmpty() && options.getHosts().isEmpty())\n                    options.getRanges().addAll(coordinator.getPrimaryRanges(ks));\n                    // except dataCenters only contain local DC (i.e. -local)\n                else if (options.isInLocalDCOnly())\n                    options.getRanges().addAll(coordinator.getPrimaryRangesWithinDC(ks));\n                else\n                    throw new IllegalArgumentException(\"You need to run primary range repair on all nodes in the cluster.\");\n            }\n            else\n            {\n                Iterables.addAll(options.getRanges(), coordinator.getLocalReplicas(ks).onlyFull().ranges());\n            }\n        }\n        return options;\n    }\n\n    enum Faults\n    {\n        DELAY, DROP;\n\n        public static final Set<Faults> NONE = Collections.emptySet();\n        public static final Set<Faults> DROPPED = EnumSet.of(DELAY, DROP);\n    }\n\n    private static class Connection\n    {\n        final InetAddressAndPort from, to;\n\n        private Connection(InetAddressAndPort from, InetAddressAndPort to)\n        {\n            this.from = from;\n            this.to = to;\n        }\n\n        @Override\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Connection that = (Connection) o;\n            return from.equals(that.from) && to.equals(that.to);\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(from, to);\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"Connection{\" + \"from=\" + from + \", to=\" + to + '}';\n        }\n    }\n\n    interface MessageListener\n    {\n        default void preHandle(Cluster.Node node, Message<?> msg) {}\n    }\n\n    static class Cluster\n    {\n        private static final FailingBiConsumer<ColumnFamilyStore, Validator> DEFAULT_VALIDATION = ValidationManager::doValidation;\n\n        final Map<InetAddressAndPort, Node> nodes;\n        private final IFailureDetector failureDetector = Mockito.mock(IFailureDetector.class);\n        private final IEndpointSnitch snitch = Mockito.mock(IEndpointSnitch.class);\n        private final SimulatedExecutorFactory globalExecutor;\n        final ScheduledExecutorPlus unorderedScheduled;\n        final ExecutorPlus orderedExecutor;\n        private final Gossip gossiper = new Gossip();\n        private final MBeanWrapper mbean = Mockito.mock(MBeanWrapper.class);\n        private final List<Throwable> failures = new ArrayList<>();\n        private final List<MessageListener> listeners = new ArrayList<>();\n        private final RandomSource rs;\n        private BiFunction<Node, Message<?>, Set<Faults>> allowedMessageFaults = (a, b) -> Collections.emptySet();\n\n        private final Map<Connection, LongSupplier> networkLatencies = new HashMap<>();\n        private final Map<Connection, Supplier<Boolean>> networkDrops = new HashMap<>();\n\n        Cluster(RandomSource rs)\n        {\n            ClockAccess.includeThreadAsOwner();\n            this.rs = rs;\n            globalExecutor = new SimulatedExecutorFactory(rs, fromQT(Generators.TIMESTAMP_GEN.map(Timestamp::getTime)).mapToLong(TimeUnit.MILLISECONDS::toNanos).next(rs));\n            orderedExecutor = globalExecutor.configureSequential(\"ignore\").build();\n            unorderedScheduled = globalExecutor.scheduled(\"ignored\");\n\n\n\n            // We run tests in an isolated JVM per class, so not cleaing up is safe... but if that assumption ever changes, will need to cleanup\n            Stage.ANTI_ENTROPY.unsafeSetExecutor(orderedExecutor);\n            Stage.INTERNAL_RESPONSE.unsafeSetExecutor(unorderedScheduled);\n            Mockito.when(failureDetector.isAlive(Mockito.any())).thenReturn(true);\n            Thread expectedThread = Thread.currentThread();\n            NoSpamLogger.unsafeSetClock(() -> {\n                if (Thread.currentThread() != expectedThread)\n                    throw new AssertionError(\"NoSpamLogger.Clock accessed outside of fuzzing...\");\n                return globalExecutor.nanoTime();\n            });\n\n            int numNodes = rs.nextInt(3, 10);\n            List<String> dcs = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(1, Math.min(10, numNodes)).next(rs);\n            Map<InetAddressAndPort, Node> nodes = Maps.newHashMapWithExpectedSize(numNodes);\n            Gen<Token> tokenGen = fromQT(CassandraGenerators.token(DatabaseDescriptor.getPartitioner()));\n            Gen<UUID> hostIdGen = fromQT(Generators.UUID_RANDOM_GEN);\n            Set<Token> tokens = new HashSet<>();\n            Set<UUID> hostIds = new HashSet<>();\n            for (int i = 0; i < numNodes; i++)\n            {\n                InetAddressAndPort addressAndPort = ADDRESS_W_PORT.next(rs);\n                while (nodes.containsKey(addressAndPort)) addressAndPort = ADDRESS_W_PORT.next(rs);\n                Token token;\n                while (!tokens.add(token = tokenGen.next(rs)))\n                {\n                }\n                UUID hostId;\n                while (!hostIds.add(hostId = hostIdGen.next(rs)))\n                {\n                }\n\n                String dc = rs.pick(dcs);\n                String rack = \"rack\";\n                Mockito.when(snitch.getDatacenter(Mockito.eq(addressAndPort))).thenReturn(dc);\n                Mockito.when(snitch.getRack(Mockito.eq(addressAndPort))).thenReturn(rack);\n\n                VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(DatabaseDescriptor.getPartitioner());\n                EndpointState state = new EndpointState(new HeartBeatState(42, 42));\n                state.addApplicationState(ApplicationState.STATUS, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.STATUS_WITH_PORT, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.HOST_ID, valueFactory.hostId(hostId));\n                state.addApplicationState(ApplicationState.TOKENS, valueFactory.tokens(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.DC, valueFactory.datacenter(dc));\n                state.addApplicationState(ApplicationState.RACK, valueFactory.rack(rack));\n                state.addApplicationState(ApplicationState.RELEASE_VERSION, valueFactory.releaseVersion());\n\n                gossiper.endpoints.put(addressAndPort, state);\n\n                Node node = new Node(hostId, addressAndPort, Collections.singletonList(token), new Messaging(addressAndPort));\n                nodes.put(addressAndPort, node);\n            }\n            this.nodes = nodes;\n\n            TokenMetadata tm = StorageService.instance.getTokenMetadata();\n            tm.clearUnsafe();\n            for (Node inst : nodes.values())\n            {\n                tm.updateHostId(inst.hostId(), inst.broadcastAddressAndPort());\n                for (Token token : inst.tokens())\n                    tm.updateNormalToken(token, inst.broadcastAddressAndPort());\n            }\n        }\n\n        public Closeable addListener(MessageListener listener)\n        {\n            listeners.add(listener);\n            return () -> removeListener(listener);\n        }\n\n        public void removeListener(MessageListener listener)\n        {\n            listeners.remove(listener);\n        }\n\n        public void allowedMessageFaults(BiFunction<Node, Message<?>, Set<Faults>> fn)\n        {\n            this.allowedMessageFaults = fn;\n        }\n\n        public void checkFailures()\n        {\n            if (Thread.interrupted())\n                failures.add(new InterruptedException());\n            if (failures.isEmpty()) return;\n            AssertionError error = new AssertionError(\"Unexpected exceptions found\");\n            failures.forEach(error::addSuppressed);\n            failures.clear();\n            throw error;\n        }\n\n        public boolean processOne()\n        {\n            boolean result = globalExecutor.processOne();\n            checkFailures();\n            return result;\n        }\n\n        public void processAll()\n        {\n            while (processOne())\n            {\n            }\n        }\n\n        private class CallbackContext\n        {\n            final RequestCallback callback;\n\n            private CallbackContext(RequestCallback callback)\n            {\n                this.callback = Objects.requireNonNull(callback);\n            }\n\n            public void onResponse(Message msg)\n            {\n                callback.onResponse(msg);\n            }\n\n            public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n            {\n                if (callback.invokeOnFailure()) callback.onFailure(from, failureReason);\n            }\n        }\n\n        private class Messaging implements MessageDelivery\n        {\n            final InetAddressAndPort broadcastAddressAndPort;\n            final Long2ObjectHashMap<CallbackContext> callbacks = new Long2ObjectHashMap<>();\n\n            private Messaging(InetAddressAndPort broadcastAddressAndPort)\n            {\n                this.broadcastAddressAndPort = broadcastAddressAndPort;\n            }\n\n            @Override\n            public <REQ> void send(Message<REQ> message, InetAddressAndPort to)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, null);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb, ConnectionType specifyConnection)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            private <REQ, RSP> void maybeEnqueue(Message<REQ> message, InetAddressAndPort to, @Nullable RequestCallback<RSP> callback)\n            {\n                CallbackContext cb;\n                if (callback != null)\n                {\n                    if (callbacks.containsKey(message.id()))\n                        throw new AssertionError(\"Message id \" + message.id() + \" already has a callback\");\n                    cb = new CallbackContext(callback);\n                    callbacks.put(message.id(), cb);\n                }\n                else\n                {\n                    cb = null;\n                }\n                boolean toSelf = this.broadcastAddressAndPort.equals(to);\n                Node node = nodes.get(to);\n                Set<Faults> allowedFaults = allowedMessageFaults.apply(node, message);\n                if (allowedFaults.isEmpty())\n                {\n                    // enqueue so stack overflow doesn't happen with the inlining\n                    unorderedScheduled.submit(() -> node.handle(message));\n                }\n                else\n                {\n                    Runnable enqueue = () -> {\n                        if (!allowedFaults.contains(Faults.DELAY))\n                        {\n                            unorderedScheduled.submit(() -> node.handle(message));\n                        }\n                        else\n                        {\n                            if (toSelf) unorderedScheduled.submit(() -> node.handle(message));\n                            else\n                                unorderedScheduled.schedule(() -> node.handle(message), networkJitterNanos(to), TimeUnit.NANOSECONDS);\n                        }\n                    };\n\n                    if (!allowedFaults.contains(Faults.DROP)) enqueue.run();\n                    else\n                    {\n                        if (!toSelf && networkDrops(to))\n                        {\n//                            logger.warn(\"Dropped message {}\", message);\n                            // drop\n                        }\n                        else\n                        {\n                            enqueue.run();\n                        }\n                    }\n\n                    if (cb != null)\n                    {\n                        unorderedScheduled.schedule(() -> {\n                            CallbackContext ctx = callbacks.remove(message.id());\n                            if (ctx != null)\n                            {\n                                assert ctx == cb;\n                                try\n                                {\n                                    ctx.onFailure(to, RequestFailureReason.TIMEOUT);\n                                }\n                                catch (Throwable t)\n                                {\n                                    failures.add(t);\n                                }\n                            }\n                        }, message.verb().expiresAfterNanos(), TimeUnit.NANOSECONDS);\n                    }\n                }\n            }\n\n            private long networkJitterNanos(InetAddressAndPort to)\n            {\n                return networkLatencies.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> {\n                    long min = TimeUnit.MICROSECONDS.toNanos(500);\n                    long maxSmall = TimeUnit.MILLISECONDS.toNanos(5);\n                    long max = TimeUnit.SECONDS.toNanos(5);\n                    LongSupplier small = () -> rs.nextLong(min, maxSmall);\n                    LongSupplier large = () -> rs.nextLong(maxSmall, max);\n                    return Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).mapToLong(b -> b ? large.getAsLong() : small.getAsLong()).asLongSupplier(rs);\n                }).getAsLong();\n            }\n\n            private boolean networkDrops(InetAddressAndPort to)\n            {\n                return networkDrops.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).asSupplier(rs)).get();\n            }\n\n            @Override\n            public <REQ, RSP> Future<Message<RSP>> sendWithResult(Message<REQ> message, InetAddressAndPort to)\n            {\n                AsyncPromise<Message<RSP>> promise = new AsyncPromise<>();\n                sendWithCallback(message, to, new RequestCallback<RSP>()\n                {\n                    @Override\n                    public void onResponse(Message<RSP> msg)\n                    {\n                        promise.trySuccess(msg);\n                    }\n\n                    @Override\n                    public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n                    {\n                        promise.tryFailure(new MessagingService.FailureResponseException(from, failureReason));\n                    }\n\n                    @Override\n                    public boolean invokeOnFailure()\n                    {\n                        return true;\n                    }\n                });\n                return promise;\n            }\n\n            @Override\n            public <V> void respond(V response, Message<?> message)\n            {\n                send(message.responseWith(response), message.respondTo());\n            }\n        }\n\n        private class Gossip implements IGossiper\n        {\n            private final Map<InetAddressAndPort, EndpointState> endpoints = new HashMap<>();\n\n            @Override\n            public void register(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Override\n            public void unregister(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Nullable\n            @Override\n            public EndpointState getEndpointStateForEndpoint(InetAddressAndPort ep)\n            {\n                return endpoints.get(ep);\n            }\n        }\n\n        class Node implements SharedContext\n        {\n            private final ICompactionManager compactionManager = Mockito.mock(ICompactionManager.class);\n            final UUID hostId;\n            final InetAddressAndPort addressAndPort;\n            final Collection<Token> tokens;\n            final ActiveRepairService activeRepairService;\n            final IVerbHandler verbHandler;\n            final Messaging messaging;\n            final IValidationManager validationManager;\n            private FailingBiConsumer<ColumnFamilyStore, Validator> doValidation = DEFAULT_VALIDATION;\n            private final StreamExecutor defaultStreamExecutor = plan -> {\n                long delayNanos = rs.nextLong(TimeUnit.SECONDS.toNanos(5), TimeUnit.MINUTES.toNanos(10));\n                unorderedScheduled.schedule(() -> {\n                    StreamState success = new StreamState(plan.planId(), plan.streamOperation(), Collections.emptySet());\n                    for (StreamEventHandler handler : plan.handlers())\n                        handler.onSuccess(success);\n                }, delayNanos, TimeUnit.NANOSECONDS);\n                return null;\n            };\n            private StreamExecutor streamExecutor = defaultStreamExecutor;\n\n            private Node(UUID hostId, InetAddressAndPort addressAndPort, Collection<Token> tokens, Messaging messaging)\n            {\n                this.hostId = hostId;\n                this.addressAndPort = addressAndPort;\n                this.tokens = tokens;\n                this.messaging = messaging;\n                this.activeRepairService = new ActiveRepairService(this);\n                this.validationManager = (cfs, validator) -> unorderedScheduled.submit(() -> {\n                    try\n                    {\n                        doValidation.acceptOrFail(cfs, validator);\n                    }\n                    catch (Throwable e)\n                    {\n                        validator.fail(e);\n                    }\n                });\n                this.verbHandler = new RepairMessageVerbHandler(this);\n\n                activeRepairService.start();\n            }\n\n            public Closeable doValidation(FailingBiConsumer<ColumnFamilyStore, Validator> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                if (previous != DEFAULT_VALIDATION)\n                    throw new IllegalStateException(\"Attemptted to override validation, but was already overridden\");\n                this.doValidation = fn;\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doValidation(Function<FailingBiConsumer<ColumnFamilyStore, Validator>, FailingBiConsumer<ColumnFamilyStore, Validator>> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                this.doValidation = fn.apply(previous);\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doSync(StreamExecutor streamExecutor)\n            {\n                StreamExecutor previous = this.streamExecutor;\n                if (previous != defaultStreamExecutor)\n                    throw new IllegalStateException(\"Attemptted to override sync, but was already overridden\");\n                this.streamExecutor = streamExecutor;\n                return () -> this.streamExecutor = previous;\n            }\n\n            void handle(Message msg)\n            {\n                msg = serde(msg);\n                if (msg == null)\n                {\n                    logger.warn(\"Got a message that failed to serialize/deserialize\");\n                    return;\n                }\n                for (MessageListener l : listeners)\n                    l.preHandle(this, msg);\n                if (msg.verb().isResponse())\n                {\n                    // handle callbacks\n                    if (messaging.callbacks.containsKey(msg.id()))\n                    {\n                        CallbackContext callback = messaging.callbacks.remove(msg.id());\n                        if (callback == null) return;\n                        try\n                        {\n                            if (msg.isFailureResponse())\n                                callback.onFailure(msg.from(), (RequestFailureReason) msg.payload);\n                            else callback.onResponse(msg);\n                        }\n                        catch (Throwable t)\n                        {\n                            failures.add(t);\n                        }\n                    }\n                }\n                else\n                {\n                    try\n                    {\n                        verbHandler.doVerb(msg);\n                    }\n                    catch (Throwable e)\n                    {\n                        failures.add(e);\n                    }\n                }\n            }\n\n            public UUID hostId()\n            {\n                return hostId;\n            }\n\n            @Override\n            public InetAddressAndPort broadcastAddressAndPort()\n            {\n                return addressAndPort;\n            }\n\n            public Collection<Token> tokens()\n            {\n                return tokens;\n            }\n\n            public IFailureDetector failureDetector()\n            {\n                return failureDetector;\n            }\n\n            @Override\n            public IEndpointSnitch snitch()\n            {\n                return snitch;\n            }\n\n            @Override\n            public IGossiper gossiper()\n            {\n                return gossiper;\n            }\n\n            @Override\n            public ICompactionManager compactionManager()\n            {\n                return compactionManager;\n            }\n\n            public ExecutorFactory executorFactory()\n            {\n                return globalExecutor;\n            }\n\n            public ScheduledExecutorPlus optionalTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public Supplier<Random> random()\n            {\n                return () -> rs.fork().asJdkRandom();\n            }\n\n            public Clock clock()\n            {\n                return globalExecutor;\n            }\n\n            public MessageDelivery messaging()\n            {\n                return messaging;\n            }\n\n            public MBeanWrapper mbean()\n            {\n                return mbean;\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options)\n            {\n                return repair(ks, options, true);\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options, boolean addFailureOnErrorNotification)\n            {\n                RepairCoordinator repair = new RepairCoordinator(this, (name, tables) -> StorageService.instance.getValidColumnFamilies(false, false, name, tables), name -> StorageService.instance.getReplicas(name, broadcastAddressAndPort()), 42, options, ks);\n                if (addFailureOnErrorNotification)\n                {\n                    repair.addProgressListener((tag, event) -> {\n                        if (event.getType() == ProgressEventType.ERROR)\n                            failures.add(new AssertionError(event.getMessage()));\n                    });\n                }\n                return repair;\n            }\n\n            public RangesAtEndpoint getLocalReplicas(String ks)\n            {\n                return StorageService.instance.getReplicas(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRanges(String ks)\n            {\n                return StorageService.instance.getPrimaryRangesForEndpoint(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRangesWithinDC(String ks)\n            {\n                return StorageService.instance.getPrimaryRangeForEndpointWithinDC(ks, broadcastAddressAndPort());\n            }\n\n            @Override\n            public ActiveRepairService repair()\n            {\n                return activeRepairService;\n            }\n\n            @Override\n            public IValidationManager validationManager()\n            {\n                return validationManager;\n            }\n\n            @Override\n            public TableRepairManager repairManager(ColumnFamilyStore store)\n            {\n                return new CassandraTableRepairManager(store, this)\n                {\n                    @Override\n                    public void snapshot(String name, Collection<Range<Token>> ranges, boolean force)\n                    {\n                        // no-op\n                    }\n                };\n            }\n\n            @Override\n            public StreamExecutor streamExecutor()\n            {\n                return streamExecutor;\n            }\n        }\n\n        private Message serde(Message msg)\n        {\n            try (DataOutputBuffer b = DataOutputBuffer.scratchBuffer.get())\n            {\n                int messagingVersion = MessagingService.current_version;\n                Message.serializer.serialize(msg, b, messagingVersion);\n                DataInputBuffer in = new DataInputBuffer(b.unsafeGetBufferAndFlip(), false);\n                return Message.serializer.deserialize(in, msg.from(), messagingVersion);\n            }\n            catch (Throwable e)\n            {\n                failures.add(e);\n                return null;\n            }\n        }\n    }\n\n    private static <T> Gen<T> fromQT(org.quicktheories.core.Gen<T> qt)\n    {\n        return rs -> {\n            JavaRandom r = new JavaRandom(rs.asJdkRandom());\n            return qt.generate(r);\n        };\n    }\n\n    public static class HackStrat extends LocalStrategy\n    {\n        public HackStrat(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)\n        {\n            super(keyspaceName, tokenMetadata, snitch, configOptions);\n        }\n    }\n\n    /**\n     * Since the clock is accessable via {@link Global#currentTimeMillis()} and {@link Global#nanoTime()}, and only repair subsystem has the requirement not to touch that, this class\n     * acts as a safty check that validates that repair does not touch these methods but allows the other subsystems to do so.\n     */\n    public static class ClockAccess implements Clock\n    {\n        private static final Set<Thread> OWNERS = Collections.synchronizedSet(new HashSet<>());\n        private final Clock delegate = new Default();\n\n        public static void includeThreadAsOwner()\n        {\n            OWNERS.add(Thread.currentThread());\n        }\n\n        @Override\n        public long nanoTime()\n        {\n            checkAccess();\n            return delegate.nanoTime();\n        }\n\n        @Override\n        public long currentTimeMillis()\n        {\n            checkAccess();\n            return delegate.currentTimeMillis();\n        }\n\n        private enum Access\n        {MAIN_THREAD_ONLY, REJECT, IGNORE}\n\n        private void checkAccess()\n        {\n            Access access = StackWalker.getInstance().walk(frames -> {\n                Iterator<StackWalker.StackFrame> it = frames.iterator();\n                boolean topLevel = false;\n                while (it.hasNext())\n                {\n                    StackWalker.StackFrame next = it.next();\n                    if (!topLevel)\n                    {\n                        // need to find the top level!\n                        while (!Clock.Global.class.getName().equals(next.getClassName()))\n                        {\n                            assert it.hasNext();\n                            next = it.next();\n                        }\n                        topLevel = true;\n                        assert it.hasNext();\n                        next = it.next();\n                    }\n                    if (FuzzTestBase.class.getName().equals(next.getClassName())) return Access.MAIN_THREAD_ONLY;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.db.\") || next.getClassName().startsWith(\"org.apache.cassandra.gms.\") || next.getClassName().startsWith(\"org.apache.cassandra.cql3.\") || next.getClassName().startsWith(\"org.apache.cassandra.metrics.\") || next.getClassName().startsWith(\"org.apache.cassandra.utils.concurrent.\")\n                        || next.getClassName().startsWith(\"org.apache.cassandra.utils.TimeUUID\") // this would be good to solve\n                        || next.getClassName().startsWith(PendingAntiCompaction.class.getName()))\n                        return Access.IGNORE;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.repair\") || ActiveRepairService.class.getName().startsWith(next.getClassName()))\n                        return Access.REJECT;\n                }\n                return Access.IGNORE;\n            });\n            Thread current = Thread.currentThread();\n            switch (access)\n            {\n                case IGNORE:\n                    return;\n                case REJECT:\n                    throw new IllegalStateException(\"Rejecting access\");\n                case MAIN_THREAD_ONLY:\n                    if (!OWNERS.contains(current)) throw new IllegalStateException(\"Accessed in wrong thread: \" + current);\n                    break;\n            }\n        }\n    }\n\n    static class SimulatedFault extends RuntimeException\n    {\n        SimulatedFault(String message)\n        {\n            super(message);\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.repair;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.StandardCharsets;\nimport java.sql.Timestamp;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.function.BiFunction;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.LongSupplier;\nimport java.util.function.Supplier;\nimport javax.annotation.Nullable;\n\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Maps;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\n\nimport accord.utils.DefaultRandom;\nimport accord.utils.Gen;\nimport accord.utils.Gens;\nimport accord.utils.RandomSource;\nimport org.agrona.collections.LongHashSet;\nimport org.apache.cassandra.concurrent.ExecutorBuilder;\nimport org.apache.cassandra.concurrent.ExecutorBuilderFactory;\nimport org.apache.cassandra.concurrent.ExecutorFactory;\nimport org.apache.cassandra.concurrent.ExecutorPlus;\nimport org.apache.cassandra.concurrent.InfiniteLoopExecutor;\nimport org.apache.cassandra.concurrent.Interruptible;\nimport org.apache.cassandra.concurrent.ScheduledExecutorPlus;\nimport org.apache.cassandra.concurrent.SequentialExecutorPlus;\nimport org.apache.cassandra.concurrent.SimulatedExecutorFactory;\nimport org.apache.cassandra.concurrent.Stage;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.config.UnitConfigOverride;\nimport org.apache.cassandra.cql3.CQLTester;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Digest;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.compaction.ICompactionManager;\nimport org.apache.cassandra.db.marshal.EmptyType;\nimport org.apache.cassandra.db.repair.CassandraTableRepairManager;\nimport org.apache.cassandra.db.repair.PendingAntiCompaction;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.dht.Range;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.exceptions.RequestFailureReason;\nimport org.apache.cassandra.gms.ApplicationState;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.HeartBeatState;\nimport org.apache.cassandra.gms.IEndpointStateChangeSubscriber;\nimport org.apache.cassandra.gms.IFailureDetector;\nimport org.apache.cassandra.gms.IGossiper;\nimport org.apache.cassandra.gms.VersionedValue;\nimport org.apache.cassandra.io.util.DataInputBuffer;\nimport org.apache.cassandra.io.util.DataOutputBuffer;\nimport org.apache.cassandra.locator.IEndpointSnitch;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.LocalStrategy;\nimport org.apache.cassandra.locator.RangesAtEndpoint;\nimport org.apache.cassandra.locator.TokenMetadata;\nimport org.apache.cassandra.net.ConnectionType;\nimport org.apache.cassandra.net.IVerbHandler;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.net.MessageDelivery;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.net.RequestCallback;\nimport org.apache.cassandra.repair.messages.RepairMessage;\nimport org.apache.cassandra.repair.messages.RepairOption;\nimport org.apache.cassandra.repair.messages.ValidationResponse;\nimport org.apache.cassandra.repair.state.Completable;\nimport org.apache.cassandra.repair.state.CoordinatorState;\nimport org.apache.cassandra.repair.state.JobState;\nimport org.apache.cassandra.repair.state.SessionState;\nimport org.apache.cassandra.repair.state.ValidationState;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.SystemDistributedKeyspace;\nimport org.apache.cassandra.schema.TableId;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.ActiveRepairService;\nimport org.apache.cassandra.service.PendingRangeCalculatorService;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupComplete;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupHistory;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupRequest;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupResponse;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosRepairState;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosFinishPrepareCleanup;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosStartPrepareCleanup;\nimport org.apache.cassandra.streaming.StreamEventHandler;\nimport org.apache.cassandra.streaming.StreamReceiveException;\nimport org.apache.cassandra.streaming.StreamSession;\nimport org.apache.cassandra.streaming.StreamState;\nimport org.apache.cassandra.streaming.StreamingChannel;\nimport org.apache.cassandra.streaming.StreamingDataInputPlus;\nimport org.apache.cassandra.tools.nodetool.Repair;\nimport org.apache.cassandra.utils.AbstractTypeGenerators;\nimport org.apache.cassandra.utils.CassandraGenerators;\nimport org.apache.cassandra.utils.Clock;\nimport org.apache.cassandra.utils.Closeable;\nimport org.apache.cassandra.utils.FailingBiConsumer;\nimport org.apache.cassandra.utils.Generators;\nimport org.apache.cassandra.utils.MBeanWrapper;\nimport org.apache.cassandra.utils.MerkleTree;\nimport org.apache.cassandra.utils.MerkleTrees;\nimport org.apache.cassandra.utils.NoSpamLogger;\nimport org.apache.cassandra.utils.concurrent.AsyncPromise;\nimport org.apache.cassandra.utils.concurrent.Future;\nimport org.apache.cassandra.utils.concurrent.ImmediateFuture;\nimport org.apache.cassandra.utils.progress.ProgressEventType;\nimport org.assertj.core.api.Assertions;\nimport org.mockito.Mockito;\nimport org.quicktheories.impl.JavaRandom;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.CLOCK_GLOBAL;\nimport static org.apache.cassandra.config.CassandraRelevantProperties.ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION;\n\npublic abstract class FuzzTestBase extends CQLTester.InMemory\n{\n    private static final int MISMATCH_NUM_PARTITIONS = 1;\n    private static final Gen<String> IDENTIFIER_GEN = fromQT(Generators.IDENTIFIER_GEN);\n    private static final Gen<String> KEYSPACE_NAME_GEN = fromQT(CassandraGenerators.KEYSPACE_NAME_GEN);\n    private static final Gen<TableId> TABLE_ID_GEN = fromQT(CassandraGenerators.TABLE_ID_GEN);\n    private static final Gen<InetAddressAndPort> ADDRESS_W_PORT = fromQT(CassandraGenerators.INET_ADDRESS_AND_PORT_GEN);\n\n    private static boolean SETUP_SCHEMA = false;\n    static String KEYSPACE;\n    static List<String> TABLES;\n\n    @BeforeClass\n    public static void setUpClass()\n    {\n        ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION.setBoolean(true);\n        CLOCK_GLOBAL.setString(ClockAccess.class.getName());\n        // when running in CI an external actor will replace the test configs based off the test type (such as trie, cdc, etc.), this could then have failing tests\n        // that do not repo with the same seed!  To fix that, go to UnitConfigOverride and update the config type to match the one that failed in CI, this should then\n        // use the same config, so the seed should not reproduce.\n        UnitConfigOverride.maybeOverrideConfig();\n\n        DatabaseDescriptor.daemonInitialization();\n        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance); // TOOD (coverage): random select\n        DatabaseDescriptor.setLocalDataCenter(\"test\");\n        StreamingChannel.Factory.Global.unsafeSet(new StreamingChannel.Factory()\n        {\n            private final AtomicInteger counter = new AtomicInteger();\n\n            @Override\n            public StreamingChannel create(InetSocketAddress to, int messagingVersion, StreamingChannel.Kind kind) throws IOException\n            {\n                StreamingChannel mock = Mockito.mock(StreamingChannel.class);\n                int id = counter.incrementAndGet();\n                StreamSession session = Mockito.mock(StreamSession.class);\n                StreamReceiveException access = new StreamReceiveException(session, \"mock access rejected\");\n                StreamingDataInputPlus input = Mockito.mock(StreamingDataInputPlus.class, invocationOnMock -> {\n                    throw access;\n                });\n                Mockito.doNothing().when(input).close();\n                Mockito.when(mock.in()).thenReturn(input);\n                Mockito.when(mock.id()).thenReturn(id);\n                Mockito.when(mock.peer()).thenReturn(to);\n                Mockito.when(mock.connectedTo()).thenReturn(to);\n                Mockito.when(mock.send(Mockito.any())).thenReturn(ImmediateFuture.success(null));\n                Mockito.when(mock.close()).thenReturn(ImmediateFuture.success(null));\n                return mock;\n            }\n        });\n        ExecutorFactory delegate = ExecutorFactory.Global.executorFactory();\n        ExecutorFactory.Global.unsafeSet(new ExecutorFactory()\n        {\n            @Override\n            public LocalAwareSubFactory localAware()\n            {\n                return delegate.localAware();\n            }\n\n            @Override\n            public ScheduledExecutorPlus scheduled(boolean executeOnShutdown, String name, int priority, SimulatorSemantics simulatorSemantics)\n            {\n                return delegate.scheduled(executeOnShutdown, name, priority, simulatorSemantics);\n            }\n\n            private boolean shouldMock()\n            {\n                return StackWalker.getInstance().walk(frame -> {\n                    StackWalker.StackFrame caller = frame.skip(3).findFirst().get();\n                    return caller.getClassName().startsWith(\"org.apache.cassandra.streaming.\");\n                });\n            }\n\n            @Override\n            public Thread startThread(String name, Runnable runnable, InfiniteLoopExecutor.Daemon daemon)\n            {\n                if (shouldMock()) return new Thread();\n                return delegate.startThread(name, runnable, daemon);\n            }\n\n            @Override\n            public Interruptible infiniteLoop(String name, Interruptible.Task task, InfiniteLoopExecutor.SimulatorSafe simulatorSafe, InfiniteLoopExecutor.Daemon daemon, InfiniteLoopExecutor.Interrupts interrupts)\n            {\n                return delegate.infiniteLoop(name, task, simulatorSafe, daemon, interrupts);\n            }\n\n            @Override\n            public ThreadGroup newThreadGroup(String name)\n            {\n                return delegate.newThreadGroup(name);\n            }\n\n            @Override\n            public ExecutorBuilderFactory<ExecutorPlus, SequentialExecutorPlus> withJmx(String jmxPath)\n            {\n                return delegate.withJmx(jmxPath);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends SequentialExecutorPlus> configureSequential(String name)\n            {\n                return delegate.configureSequential(name);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends ExecutorPlus> configurePooled(String name, int threads)\n            {\n                return delegate.configurePooled(name, threads);\n            }\n        });\n\n        // will both make sure this is loaded and used\n        if (!(Clock.Global.clock() instanceof ClockAccess)) throw new IllegalStateException(\"Unable to override clock\");\n\n        // set the repair rcp timeout high so we don't hit it... this class is mostly testing repair reaching success\n        // so don't want to deal with unlucky histories...\n        DatabaseDescriptor.setRepairRpcTimeout(TimeUnit.DAYS.toMillis(1));\n\n\n        InMemory.setUpClass();\n    }\n\n    @Before\n    public void setupSchema()\n    {\n        if (SETUP_SCHEMA) return;\n        SETUP_SCHEMA = true;\n        // StorageService can not be mocked out, nor can ColumnFamilyStores, so make sure that the keyspace is a \"local\" keyspace to avoid replication as the peers don't actually exist for replication\n        schemaChange(String.format(\"CREATE KEYSPACE %s WITH REPLICATION = {'class': '%s'}\", SchemaConstants.DISTRIBUTED_KEYSPACE_NAME, HackStrat.class.getName()));\n        for (TableMetadata table : SystemDistributedKeyspace.metadata().tables)\n            schemaChange(table.toCqlString(false, false));\n\n        createSchema();\n    }\n\n    protected void cleanupRepairTables()\n    {\n        for (String table : Arrays.asList(SystemKeyspace.REPAIRS))\n            execute(String.format(\"TRUNCATE %s.%s\", SchemaConstants.SYSTEM_KEYSPACE_NAME, table));\n    }\n\n    private void createSchema()\n    {\n        // The main reason to use random here with a fixed seed is just to have a set of tables that are not hard coded.\n        // The tables will have diversity to them that most likely doesn't matter to repair (hence why the tables are shared), but\n        // is useful just in case some assumptions change.\n        RandomSource rs = new DefaultRandom(42);\n        String ks = KEYSPACE_NAME_GEN.next(rs);\n        List<String> tableNames = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(10, 100).next(rs);\n        JavaRandom qt = new JavaRandom(rs.asJdkRandom());\n        Tables.Builder tableBuilder = Tables.builder();\n        List<TableId> ids = Gens.lists(TABLE_ID_GEN).unique().ofSize(tableNames.size()).next(rs);\n        for (int i = 0; i < tableNames.size(); i++)\n        {\n            String name = tableNames.get(i);\n            TableId id = ids.get(i);\n            TableMetadata tableMetadata = new CassandraGenerators.TableMetadataBuilder().withKeyspaceName(ks).withTableName(name).withTableId(id).withTableKinds(TableMetadata.Kind.REGULAR)\n                                                                                        // shouldn't matter, just wanted to avoid UDT as that needs more setup\n                                                                                        .withDefaultTypeGen(AbstractTypeGenerators.builder().withTypeKinds(AbstractTypeGenerators.TypeKind.PRIMITIVE).withoutPrimitive(EmptyType.instance).build()).build().generate(qt);\n            tableBuilder.add(tableMetadata);\n        }\n        KeyspaceParams params = KeyspaceParams.simple(3);\n        KeyspaceMetadata metadata = KeyspaceMetadata.create(ks, params, tableBuilder.build());\n\n        // create\n        schemaChange(metadata.toCqlString(false, false));\n        KEYSPACE = ks;\n        for (TableMetadata table : metadata.tables)\n            schemaChange(table.toCqlString(false, false));\n        TABLES = tableNames;\n    }\n\n    static void enableMessageFaults(Cluster cluster)\n    {\n        cluster.allowedMessageFaults(new BiFunction<>()\n        {\n            private final LongHashSet noFaults = new LongHashSet();\n            private final LongHashSet allowDrop = new LongHashSet();\n\n            @Override\n            public Set<Faults> apply(Cluster.Node node, Message<?> message)\n            {\n                if (RepairMessage.ALLOWS_RETRY.contains(message.verb()))\n                {\n                    allowDrop.add(message.id());\n                    return Faults.DROPPED;\n                }\n                switch (message.verb())\n                {\n                    // these messages are not resilent to ephemeral issues\n                    case STATUS_REQ:\n                    case STATUS_RSP:\n                    // paxos repair does not support faults and will cause a TIMEOUT error, failing the repair\n                    case PAXOS2_CLEANUP_COMPLETE_REQ:\n                    case PAXOS2_CLEANUP_REQ:\n                    case PAXOS2_CLEANUP_RSP2:\n                    case PAXOS2_CLEANUP_START_PREPARE_REQ:\n                    case PAXOS2_CLEANUP_FINISH_PREPARE_REQ:\n                        noFaults.add(message.id());\n                        return Faults.NONE;\n                    default:\n                        if (noFaults.contains(message.id())) return Faults.NONE;\n                        if (allowDrop.contains(message.id())) return Faults.DROPPED;\n                        // was a new message added and the test not updated?\n                        IllegalStateException e = new IllegalStateException(\"Verb: \" + message.verb());\n                        cluster.failures.add(e);\n                        throw e;\n                }\n            }\n        });\n    }\n\n    static void runAndAssertSuccess(Cluster cluster, int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        cluster.processAll();\n        assertSuccess(example, shouldSync, repair);\n    }\n\n    static void assertSuccess(int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        Completable.Result result = repair.state.getResult();\n        Assertions.assertThat(result)\n                  .describedAs(\"Expected repair to have completed with success, but is still running... %s; example %d\", repair.state, example).isNotNull()\n                  .describedAs(\"Unexpected state: %s -> %s; example %d\", repair.state, result, example).isEqualTo(Completable.Result.success(repairSuccessMessage(repair)));\n        Assertions.assertThat(repair.state.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(CoordinatorState.State.class));\n        Assertions.assertThat(repair.state.getSessions()).isNotEmpty();\n        boolean shouldSnapshot = repair.state.options.getParallelism() != RepairParallelism.PARALLEL\n                                 && (!repair.state.options.isIncremental() || repair.state.options.isPreview());\n        for (SessionState session : repair.state.getSessions())\n        {\n            Assertions.assertThat(session.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(SessionState.State.class));\n            Assertions.assertThat(session.getJobs()).isNotEmpty();\n            for (JobState job : session.getJobs())\n            {\n                EnumSet<JobState.State> expected = EnumSet.allOf(JobState.State.class);\n                if (!shouldSnapshot)\n                {\n                    expected.remove(JobState.State.SNAPSHOT_START);\n                    expected.remove(JobState.State.SNAPSHOT_COMPLETE);\n                }\n                if (!shouldSync)\n                {\n                    expected.remove(JobState.State.STREAM_START);\n                }\n                Set<JobState.State> actual = job.getStateTimesMillis().keySet();\n                Assertions.assertThat(actual).isEqualTo(expected);\n            }\n        }\n    }\n\n    static String repairSuccessMessage(RepairCoordinator repair)\n    {\n        RepairOption options = repair.state.options;\n        if (options.isPreview())\n        {\n            String suffix;\n            switch (options.getPreviewKind())\n            {\n                case UNREPAIRED:\n                case ALL:\n                    suffix = \"Previewed data was in sync\";\n                    break;\n                case REPAIRED:\n                    suffix = \"Repaired data is in sync\";\n                    break;\n                default:\n                    throw new IllegalArgumentException(\"Unexpected preview repair kind: \" + options.getPreviewKind());\n            }\n            return \"Repair preview completed successfully; \" + suffix;\n        }\n        return \"Repair completed successfully\";\n    }\n\n    InetAddressAndPort pickParticipant(RandomSource rs, Cluster.Node coordinator, RepairCoordinator repair)\n    {\n        if (repair.state.isComplete())\n            throw new IllegalStateException(\"Repair is completed! \" + repair.state.getResult());\n        List<InetAddressAndPort> participaents = new ArrayList<>(repair.state.getNeighborsAndRanges().participants.size() + 1);\n        if (rs.nextBoolean()) participaents.add(coordinator.broadcastAddressAndPort());\n        participaents.addAll(repair.state.getNeighborsAndRanges().participants);\n        participaents.sort(Comparator.naturalOrder());\n\n        InetAddressAndPort selected = rs.pick(participaents);\n        return selected;\n    }\n\n    static void addMismatch(RandomSource rs, ColumnFamilyStore cfs, Validator validator)\n    {\n        ValidationState state = validator.state;\n        int maxDepth = DatabaseDescriptor.getRepairSessionMaxTreeDepth();\n        state.phase.start(MISMATCH_NUM_PARTITIONS, 1024);\n\n        MerkleTrees trees = new MerkleTrees(cfs.getPartitioner());\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            int depth = (int) Math.min(Math.ceil(Math.log(MISMATCH_NUM_PARTITIONS) / Math.log(2)), maxDepth);\n            trees.addMerkleTree((int) Math.pow(2, depth), range);\n        }\n        Set<Token> allTokens = new HashSet<>();\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            Gen<Token> gen = fromQT(CassandraGenerators.tokensInRange(range));\n            Set<Token> tokens = new LinkedHashSet<>();\n            for (int i = 0, size = rs.nextInt(1, 10); i < size; i++)\n            {\n                for (int attempt = 0; !tokens.add(gen.next(rs)) && attempt < 5; attempt++)\n                {\n                }\n            }\n            // tokens may or may not be of the expected size; this depends on how wide the range is\n            for (Token token : tokens)\n                trees.split(token);\n            allTokens.addAll(tokens);\n        }\n        for (Token token : allTokens)\n        {\n            findCorrectRange(trees, token, range -> {\n                Digest digest = Digest.forValidator();\n                digest.update(ByteBuffer.wrap(token.toString().getBytes(StandardCharsets.UTF_8)));\n                range.addHash(new MerkleTree.RowHash(token, digest.digest(), 1));\n            });\n        }\n        state.partitionsProcessed++;\n        state.bytesRead = 1024;\n        state.phase.sendingTrees();\n        Stage.ANTI_ENTROPY.execute(() -> {\n            state.phase.success();\n            validator.respond(new ValidationResponse(validator.desc, trees));\n        });\n    }\n\n    private static void findCorrectRange(MerkleTrees trees, Token token, Consumer<MerkleTree.TreeRange> fn)\n    {\n        MerkleTrees.TreeRangeIterator it = trees.rangeIterator();\n        while (it.hasNext())\n        {\n            MerkleTree.TreeRange next = it.next();\n            if (next.contains(token))\n            {\n                fn.accept(next);\n                return;\n            }\n        }\n    }\n\n    private enum RepairType\n    {FULL, IR}\n\n    private enum PreviewType\n    {NONE, REPAIRED, UNREPAIRED}\n\n    static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, List<String> tableNames)\n    {\n        return repairOption(rs, coordinator, ks, Gens.lists(Gens.pick(tableNames)).ofSizeBetween(1, tableNames.size()), Gens.enums().all(RepairType.class), Gens.enums().all(PreviewType.class), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption irOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.IR), Gens.constant(PreviewType.NONE), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption previewOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.FULL), Gens.constant(PreviewType.REPAIRED), Gens.enums().all(RepairParallelism.class));\n    }\n\n    private static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen, Gen<RepairType> repairTypeGen, Gen<PreviewType> previewTypeGen, Gen<RepairParallelism> repairParallelismGen)\n    {\n        List<String> args = new ArrayList<>();\n        args.add(ks);\n        args.addAll(tablesGen.next(rs));\n        args.add(\"-pr\");\n        RepairType type = repairTypeGen.next(rs);\n        switch (type)\n        {\n            case IR:\n                // default\n                break;\n            case FULL:\n                args.add(\"--full\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported repair type: \" + type);\n        }\n        PreviewType previewType = previewTypeGen.next(rs);\n        switch (previewType)\n        {\n            case NONE:\n                break;\n            case REPAIRED:\n                args.add(\"--validate\");\n                break;\n            case UNREPAIRED:\n                args.add(\"--preview\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported preview type: \" + previewType);\n        }\n        RepairParallelism parallelism = repairParallelismGen.next(rs);\n        switch (parallelism)\n        {\n            case SEQUENTIAL:\n                args.add(\"--sequential\");\n                break;\n            case PARALLEL:\n                // default\n                break;\n            case DATACENTER_AWARE:\n                args.add(\"--dc-parallel\");\n                break;\n            default:\n                throw new AssertionError(\"Unknown parallelism: \" + parallelism);\n        }\n        if (rs.nextBoolean()) args.add(\"--optimise-streams\");\n        RepairOption options = RepairOption.parse(Repair.parseOptionMap(() -> \"test\", args), DatabaseDescriptor.getPartitioner());\n        if (options.getRanges().isEmpty())\n        {\n            if (options.isPrimaryRange())\n            {\n                // when repairing only primary range, neither dataCenters nor hosts can be set\n                if (options.getDataCenters().isEmpty() && options.getHosts().isEmpty())\n                    options.getRanges().addAll(coordinator.getPrimaryRanges(ks));\n                    // except dataCenters only contain local DC (i.e. -local)\n                else if (options.isInLocalDCOnly())\n                    options.getRanges().addAll(coordinator.getPrimaryRangesWithinDC(ks));\n                else\n                    throw new IllegalArgumentException(\"You need to run primary range repair on all nodes in the cluster.\");\n            }\n            else\n            {\n                Iterables.addAll(options.getRanges(), coordinator.getLocalReplicas(ks).onlyFull().ranges());\n            }\n        }\n        return options;\n    }\n\n    enum Faults\n    {\n        DELAY, DROP;\n\n        public static final Set<Faults> NONE = Collections.emptySet();\n        public static final Set<Faults> DROPPED = EnumSet.of(DELAY, DROP);\n    }\n\n    private static class Connection\n    {\n        final InetAddressAndPort from, to;\n\n        private Connection(InetAddressAndPort from, InetAddressAndPort to)\n        {\n            this.from = from;\n            this.to = to;\n        }\n\n        @Override\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Connection that = (Connection) o;\n            return from.equals(that.from) && to.equals(that.to);\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(from, to);\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"Connection{\" + \"from=\" + from + \", to=\" + to + '}';\n        }\n    }\n\n    interface MessageListener\n    {\n        default void preHandle(Cluster.Node node, Message<?> msg) {}\n    }\n\n    static class Cluster\n    {\n        private static final FailingBiConsumer<ColumnFamilyStore, Validator> DEFAULT_VALIDATION = ValidationManager::doValidation;\n\n        final Map<InetAddressAndPort, Node> nodes;\n        private final IFailureDetector failureDetector = Mockito.mock(IFailureDetector.class);\n        private final IEndpointSnitch snitch = Mockito.mock(IEndpointSnitch.class);\n        private final SimulatedExecutorFactory globalExecutor;\n        final ScheduledExecutorPlus unorderedScheduled;\n        final ExecutorPlus orderedExecutor;\n        private final Gossip gossiper = new Gossip();\n        private final MBeanWrapper mbean = Mockito.mock(MBeanWrapper.class);\n        private final List<Throwable> failures = new ArrayList<>();\n        private final List<MessageListener> listeners = new ArrayList<>();\n        private final RandomSource rs;\n        private BiFunction<Node, Message<?>, Set<Faults>> allowedMessageFaults = (a, b) -> Collections.emptySet();\n\n        private final Map<Connection, LongSupplier> networkLatencies = new HashMap<>();\n        private final Map<Connection, Supplier<Boolean>> networkDrops = new HashMap<>();\n\n        Cluster(RandomSource rs)\n        {\n            ClockAccess.includeThreadAsOwner();\n            this.rs = rs;\n            globalExecutor = new SimulatedExecutorFactory(rs, fromQT(Generators.TIMESTAMP_GEN.map(Timestamp::getTime)).mapToLong(TimeUnit.MILLISECONDS::toNanos).next(rs));\n            orderedExecutor = globalExecutor.configureSequential(\"ignore\").build();\n            unorderedScheduled = globalExecutor.scheduled(\"ignored\");\n\n\n\n            // We run tests in an isolated JVM per class, so not cleaing up is safe... but if that assumption ever changes, will need to cleanup\n            Stage.ANTI_ENTROPY.unsafeSetExecutor(orderedExecutor);\n            Stage.MISC.unsafeSetExecutor(orderedExecutor);\n            Stage.INTERNAL_RESPONSE.unsafeSetExecutor(unorderedScheduled);\n            Mockito.when(failureDetector.isAlive(Mockito.any())).thenReturn(true);\n            Thread expectedThread = Thread.currentThread();\n            NoSpamLogger.unsafeSetClock(() -> {\n                if (Thread.currentThread() != expectedThread)\n                    throw new AssertionError(\"NoSpamLogger.Clock accessed outside of fuzzing...\");\n                return globalExecutor.nanoTime();\n            });\n\n            int numNodes = rs.nextInt(3, 10);\n            List<String> dcs = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(1, Math.min(10, numNodes)).next(rs);\n            Map<InetAddressAndPort, Node> nodes = Maps.newHashMapWithExpectedSize(numNodes);\n            Gen<Token> tokenGen = fromQT(CassandraGenerators.token(DatabaseDescriptor.getPartitioner()));\n            Gen<UUID> hostIdGen = fromQT(Generators.UUID_RANDOM_GEN);\n            Set<Token> tokens = new HashSet<>();\n            Set<UUID> hostIds = new HashSet<>();\n            for (int i = 0; i < numNodes; i++)\n            {\n                InetAddressAndPort addressAndPort = ADDRESS_W_PORT.next(rs);\n                while (nodes.containsKey(addressAndPort)) addressAndPort = ADDRESS_W_PORT.next(rs);\n                Token token;\n                while (!tokens.add(token = tokenGen.next(rs)))\n                {\n                }\n                UUID hostId;\n                while (!hostIds.add(hostId = hostIdGen.next(rs)))\n                {\n                }\n\n                String dc = rs.pick(dcs);\n                String rack = \"rack\";\n                Mockito.when(snitch.getDatacenter(Mockito.eq(addressAndPort))).thenReturn(dc);\n                Mockito.when(snitch.getRack(Mockito.eq(addressAndPort))).thenReturn(rack);\n\n                VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(DatabaseDescriptor.getPartitioner());\n                EndpointState state = new EndpointState(new HeartBeatState(42, 42));\n                state.addApplicationState(ApplicationState.STATUS, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.STATUS_WITH_PORT, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.HOST_ID, valueFactory.hostId(hostId));\n                state.addApplicationState(ApplicationState.TOKENS, valueFactory.tokens(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.DC, valueFactory.datacenter(dc));\n                state.addApplicationState(ApplicationState.RACK, valueFactory.rack(rack));\n                state.addApplicationState(ApplicationState.RELEASE_VERSION, valueFactory.releaseVersion());\n\n                gossiper.endpoints.put(addressAndPort, state);\n\n                Node node = new Node(hostId, addressAndPort, Collections.singletonList(token), new Messaging(addressAndPort));\n                nodes.put(addressAndPort, node);\n            }\n            this.nodes = nodes;\n\n            TokenMetadata tm = StorageService.instance.getTokenMetadata();\n            tm.clearUnsafe();\n            for (Node inst : nodes.values())\n            {\n                tm.updateHostId(inst.hostId(), inst.broadcastAddressAndPort());\n                for (Token token : inst.tokens())\n                    tm.updateNormalToken(token, inst.broadcastAddressAndPort());\n            }\n        }\n\n        public Closeable addListener(MessageListener listener)\n        {\n            listeners.add(listener);\n            return () -> removeListener(listener);\n        }\n\n        public void removeListener(MessageListener listener)\n        {\n            listeners.remove(listener);\n        }\n\n        public void allowedMessageFaults(BiFunction<Node, Message<?>, Set<Faults>> fn)\n        {\n            this.allowedMessageFaults = fn;\n        }\n\n        public void checkFailures()\n        {\n            if (Thread.interrupted())\n                failures.add(new InterruptedException());\n            if (failures.isEmpty()) return;\n            AssertionError error = new AssertionError(\"Unexpected exceptions found\");\n            failures.forEach(error::addSuppressed);\n            failures.clear();\n            throw error;\n        }\n\n        public boolean processOne()\n        {\n            boolean result = globalExecutor.processOne();\n            checkFailures();\n            return result;\n        }\n\n        public void processAll()\n        {\n            while (processOne())\n            {\n            }\n        }\n\n        private class CallbackContext\n        {\n            final RequestCallback callback;\n\n            private CallbackContext(RequestCallback callback)\n            {\n                this.callback = Objects.requireNonNull(callback);\n            }\n\n            public void onResponse(Message msg)\n            {\n                callback.onResponse(msg);\n            }\n\n            public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n            {\n                if (callback.invokeOnFailure()) callback.onFailure(from, failureReason);\n            }\n        }\n\n        private static class CallbackKey\n        {\n            private final long id;\n            private final InetAddressAndPort peer;\n\n            private CallbackKey(long id, InetAddressAndPort peer)\n            {\n                this.id = id;\n                this.peer = peer;\n            }\n\n            @Override\n            public boolean equals(Object o)\n            {\n                if (this == o) return true;\n                if (o == null || getClass() != o.getClass()) return false;\n                CallbackKey that = (CallbackKey) o;\n                return id == that.id && peer.equals(that.peer);\n            }\n\n            @Override\n            public int hashCode()\n            {\n                return Objects.hash(id, peer);\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"CallbackKey{\" +\n                       \"id=\" + id +\n                       \", peer=\" + peer +\n                       '}';\n            }\n        }\n\n        private class Messaging implements MessageDelivery\n        {\n            final InetAddressAndPort broadcastAddressAndPort;\n            final Map<CallbackKey, CallbackContext> callbacks = new HashMap<>();\n\n            private Messaging(InetAddressAndPort broadcastAddressAndPort)\n            {\n                this.broadcastAddressAndPort = broadcastAddressAndPort;\n            }\n\n            @Override\n            public <REQ> void send(Message<REQ> message, InetAddressAndPort to)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, null);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb, ConnectionType specifyConnection)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            private <REQ, RSP> void maybeEnqueue(Message<REQ> message, InetAddressAndPort to, @Nullable RequestCallback<RSP> callback)\n            {\n                CallbackContext cb;\n                if (callback != null)\n                {\n                    CallbackKey key = new CallbackKey(message.id(), to);\n                    if (callbacks.containsKey(key))\n                        throw new AssertionError(\"Message id \" + message.id() + \" to \" + to + \" already has a callback\");\n                    cb = new CallbackContext(callback);\n                    callbacks.put(key, cb);\n                }\n                else\n                {\n                    cb = null;\n                }\n                boolean toSelf = this.broadcastAddressAndPort.equals(to);\n                Node node = nodes.get(to);\n                Set<Faults> allowedFaults = allowedMessageFaults.apply(node, message);\n                if (allowedFaults.isEmpty())\n                {\n                    // enqueue so stack overflow doesn't happen with the inlining\n                    unorderedScheduled.submit(() -> node.handle(message));\n                }\n                else\n                {\n                    Runnable enqueue = () -> {\n                        if (!allowedFaults.contains(Faults.DELAY))\n                        {\n                            unorderedScheduled.submit(() -> node.handle(message));\n                        }\n                        else\n                        {\n                            if (toSelf) unorderedScheduled.submit(() -> node.handle(message));\n                            else\n                                unorderedScheduled.schedule(() -> node.handle(message), networkJitterNanos(to), TimeUnit.NANOSECONDS);\n                        }\n                    };\n\n                    if (!allowedFaults.contains(Faults.DROP)) enqueue.run();\n                    else\n                    {\n                        if (!toSelf && networkDrops(to))\n                        {\n//                            logger.warn(\"Dropped message {}\", message);\n                            // drop\n                        }\n                        else\n                        {\n                            enqueue.run();\n                        }\n                    }\n\n                    if (cb != null)\n                    {\n                        unorderedScheduled.schedule(() -> {\n                            CallbackContext ctx = callbacks.remove(new CallbackKey(message.id(), to));\n                            if (ctx != null)\n                            {\n                                assert ctx == cb;\n                                try\n                                {\n                                    ctx.onFailure(to, RequestFailureReason.TIMEOUT);\n                                }\n                                catch (Throwable t)\n                                {\n                                    failures.add(t);\n                                }\n                            }\n                        }, message.verb().expiresAfterNanos(), TimeUnit.NANOSECONDS);\n                    }\n                }\n            }\n\n            private long networkJitterNanos(InetAddressAndPort to)\n            {\n                return networkLatencies.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> {\n                    long min = TimeUnit.MICROSECONDS.toNanos(500);\n                    long maxSmall = TimeUnit.MILLISECONDS.toNanos(5);\n                    long max = TimeUnit.SECONDS.toNanos(5);\n                    LongSupplier small = () -> rs.nextLong(min, maxSmall);\n                    LongSupplier large = () -> rs.nextLong(maxSmall, max);\n                    return Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).mapToLong(b -> b ? large.getAsLong() : small.getAsLong()).asLongSupplier(rs);\n                }).getAsLong();\n            }\n\n            private boolean networkDrops(InetAddressAndPort to)\n            {\n                return networkDrops.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).asSupplier(rs)).get();\n            }\n\n            @Override\n            public <REQ, RSP> Future<Message<RSP>> sendWithResult(Message<REQ> message, InetAddressAndPort to)\n            {\n                AsyncPromise<Message<RSP>> promise = new AsyncPromise<>();\n                sendWithCallback(message, to, new RequestCallback<RSP>()\n                {\n                    @Override\n                    public void onResponse(Message<RSP> msg)\n                    {\n                        promise.trySuccess(msg);\n                    }\n\n                    @Override\n                    public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n                    {\n                        promise.tryFailure(new MessagingService.FailureResponseException(from, failureReason));\n                    }\n\n                    @Override\n                    public boolean invokeOnFailure()\n                    {\n                        return true;\n                    }\n                });\n                return promise;\n            }\n\n            @Override\n            public <V> void respond(V response, Message<?> message)\n            {\n                send(message.responseWith(response), message.respondTo());\n            }\n        }\n\n        private class Gossip implements IGossiper\n        {\n            private final Map<InetAddressAndPort, EndpointState> endpoints = new HashMap<>();\n\n            @Override\n            public void register(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Override\n            public void unregister(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Nullable\n            @Override\n            public EndpointState getEndpointStateForEndpoint(InetAddressAndPort ep)\n            {\n                return endpoints.get(ep);\n            }\n\n            @Override\n            public void notifyFailureDetector(Map<InetAddressAndPort, EndpointState> remoteEpStateMap)\n            {\n\n            }\n\n            @Override\n            public void applyStateLocally(Map<InetAddressAndPort, EndpointState> epStateMap)\n            {\n                // If we were testing paxos this would be wrong...\n                // CASSANDRA-18917 added support for simulating Gossip, but gossip issues were found so couldn't merge that patch...\n                // For the paxos repair, since we don't care about paxos messages, this is ok to no-op for now, but if paxos cleanup\n                // ever was to be tested this logic would need to be implemented\n            }\n        }\n\n        class Node implements SharedContext\n        {\n            private final ICompactionManager compactionManager = Mockito.mock(ICompactionManager.class);\n            final UUID hostId;\n            final InetAddressAndPort addressAndPort;\n            final Collection<Token> tokens;\n            final ActiveRepairService activeRepairService;\n            final IVerbHandler verbHandler;\n            final Messaging messaging;\n            final IValidationManager validationManager;\n            private FailingBiConsumer<ColumnFamilyStore, Validator> doValidation = DEFAULT_VALIDATION;\n            final PaxosRepairState paxosRepairState;\n            private final StreamExecutor defaultStreamExecutor = plan -> {\n                long delayNanos = rs.nextLong(TimeUnit.SECONDS.toNanos(5), TimeUnit.MINUTES.toNanos(10));\n                unorderedScheduled.schedule(() -> {\n                    StreamState success = new StreamState(plan.planId(), plan.streamOperation(), Collections.emptySet());\n                    for (StreamEventHandler handler : plan.handlers())\n                        handler.onSuccess(success);\n                }, delayNanos, TimeUnit.NANOSECONDS);\n                return null;\n            };\n            private StreamExecutor streamExecutor = defaultStreamExecutor;\n\n            private Node(UUID hostId, InetAddressAndPort addressAndPort, Collection<Token> tokens, Messaging messaging)\n            {\n                this.hostId = hostId;\n                this.addressAndPort = addressAndPort;\n                this.tokens = tokens;\n                this.messaging = messaging;\n                this.activeRepairService = new ActiveRepairService(this);\n                this.paxosRepairState = new PaxosRepairState(this);\n                this.validationManager = (cfs, validator) -> unorderedScheduled.submit(() -> {\n                    try\n                    {\n                        doValidation.acceptOrFail(cfs, validator);\n                    }\n                    catch (Throwable e)\n                    {\n                        validator.fail(e);\n                    }\n                });\n                this.verbHandler = new IVerbHandler<>()\n                {\n                    private final RepairMessageVerbHandler repairVerbHandler = new RepairMessageVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosStartPrepareCleanup.Request> paxosStartPrepareCleanup = PaxosStartPrepareCleanup.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupRequest> paxosCleanupRequestIVerbHandler = PaxosCleanupRequest.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupHistory> paxosFinishPrepareCleanup = PaxosFinishPrepareCleanup.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupResponse> paxosCleanupResponse = PaxosCleanupResponse.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupComplete.Request> paxosCleanupComplete = PaxosCleanupComplete.createVerbHandler(Node.this);\n                    @Override\n                    public void doVerb(Message message) throws IOException\n                    {\n                        switch (message.verb())\n                        {\n                            case PAXOS2_CLEANUP_START_PREPARE_REQ:\n                                paxosStartPrepareCleanup.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_REQ:\n                                paxosCleanupRequestIVerbHandler.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_FINISH_PREPARE_REQ:\n                                paxosFinishPrepareCleanup.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_RSP2:\n                                paxosCleanupResponse.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_COMPLETE_REQ:\n                                paxosCleanupComplete.doVerb(message);\n                                break;\n                            default:\n                                repairVerbHandler.doVerb(message);\n                        }\n                    }\n                };\n\n                activeRepairService.start();\n            }\n\n            public Closeable doValidation(FailingBiConsumer<ColumnFamilyStore, Validator> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                if (previous != DEFAULT_VALIDATION)\n                    throw new IllegalStateException(\"Attemptted to override validation, but was already overridden\");\n                this.doValidation = fn;\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doValidation(Function<FailingBiConsumer<ColumnFamilyStore, Validator>, FailingBiConsumer<ColumnFamilyStore, Validator>> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                this.doValidation = fn.apply(previous);\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doSync(StreamExecutor streamExecutor)\n            {\n                StreamExecutor previous = this.streamExecutor;\n                if (previous != defaultStreamExecutor)\n                    throw new IllegalStateException(\"Attemptted to override sync, but was already overridden\");\n                this.streamExecutor = streamExecutor;\n                return () -> this.streamExecutor = previous;\n            }\n\n            void handle(Message msg)\n            {\n                msg = serde(msg);\n                if (msg == null)\n                {\n                    logger.warn(\"Got a message that failed to serialize/deserialize\");\n                    return;\n                }\n                for (MessageListener l : listeners)\n                    l.preHandle(this, msg);\n                if (msg.verb().isResponse())\n                {\n                    // handle callbacks\n                    CallbackKey key = new CallbackKey(msg.id(), msg.from());\n                    if (messaging.callbacks.containsKey(key))\n                    {\n                        CallbackContext callback = messaging.callbacks.remove(key);\n                        if (callback == null)\n                            return;\n                        try\n                        {\n                            if (msg.isFailureResponse())\n                                callback.onFailure(msg.from(), (RequestFailureReason) msg.payload);\n                            else callback.onResponse(msg);\n                        }\n                        catch (Throwable t)\n                        {\n                            failures.add(t);\n                        }\n                    }\n                }\n                else\n                {\n                    try\n                    {\n                        verbHandler.doVerb(msg);\n                    }\n                    catch (Throwable e)\n                    {\n                        failures.add(e);\n                    }\n                }\n            }\n\n            public UUID hostId()\n            {\n                return hostId;\n            }\n\n            @Override\n            public InetAddressAndPort broadcastAddressAndPort()\n            {\n                return addressAndPort;\n            }\n\n            public Collection<Token> tokens()\n            {\n                return tokens;\n            }\n\n            public IFailureDetector failureDetector()\n            {\n                return failureDetector;\n            }\n\n            @Override\n            public IEndpointSnitch snitch()\n            {\n                return snitch;\n            }\n\n            @Override\n            public IGossiper gossiper()\n            {\n                return gossiper;\n            }\n\n            @Override\n            public ICompactionManager compactionManager()\n            {\n                return compactionManager;\n            }\n\n            public ExecutorFactory executorFactory()\n            {\n                return globalExecutor;\n            }\n\n            public ScheduledExecutorPlus optionalTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public ScheduledExecutorPlus nonPeriodicTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public ScheduledExecutorPlus scheduledTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public Supplier<Random> random()\n            {\n                return () -> rs.fork().asJdkRandom();\n            }\n\n            public Clock clock()\n            {\n                return globalExecutor;\n            }\n\n            public MessageDelivery messaging()\n            {\n                return messaging;\n            }\n\n            public MBeanWrapper mbean()\n            {\n                return mbean;\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options)\n            {\n                return repair(ks, options, true);\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options, boolean addFailureOnErrorNotification)\n            {\n                RepairCoordinator repair = new RepairCoordinator(this, (name, tables) -> StorageService.instance.getValidColumnFamilies(false, false, name, tables), name -> StorageService.instance.getReplicas(name, broadcastAddressAndPort()), 42, options, ks);\n                if (addFailureOnErrorNotification)\n                {\n                    repair.addProgressListener((tag, event) -> {\n                        if (event.getType() == ProgressEventType.ERROR)\n                            failures.add(new AssertionError(event.getMessage()));\n                    });\n                }\n                return repair;\n            }\n\n            public RangesAtEndpoint getLocalReplicas(String ks)\n            {\n                return StorageService.instance.getReplicas(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRanges(String ks)\n            {\n                return StorageService.instance.getPrimaryRangesForEndpoint(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRangesWithinDC(String ks)\n            {\n                return StorageService.instance.getPrimaryRangeForEndpointWithinDC(ks, broadcastAddressAndPort());\n            }\n\n            @Override\n            public ActiveRepairService repair()\n            {\n                return activeRepairService;\n            }\n\n            @Override\n            public IValidationManager validationManager()\n            {\n                return validationManager;\n            }\n\n            @Override\n            public TableRepairManager repairManager(ColumnFamilyStore store)\n            {\n                return new CassandraTableRepairManager(store, this)\n                {\n                    @Override\n                    public void snapshot(String name, Collection<Range<Token>> ranges, boolean force)\n                    {\n                        // no-op\n                    }\n                };\n            }\n\n            @Override\n            public StreamExecutor streamExecutor()\n            {\n                return streamExecutor;\n            }\n\n            @Override\n            public PendingRangeCalculatorService pendingRangeCalculator()\n            {\n                return PendingRangeCalculatorService.instance;\n            }\n\n            @Override\n            public PaxosRepairState paxosRepairState()\n            {\n                return paxosRepairState;\n            }\n        }\n\n        private Message serde(Message msg)\n        {\n            try (DataOutputBuffer b = DataOutputBuffer.scratchBuffer.get())\n            {\n                int messagingVersion = MessagingService.current_version;\n                Message.serializer.serialize(msg, b, messagingVersion);\n                DataInputBuffer in = new DataInputBuffer(b.unsafeGetBufferAndFlip(), false);\n                return Message.serializer.deserialize(in, msg.from(), messagingVersion);\n            }\n            catch (Throwable e)\n            {\n                failures.add(e);\n                return null;\n            }\n        }\n    }\n\n    private static <T> Gen<T> fromQT(org.quicktheories.core.Gen<T> qt)\n    {\n        return rs -> {\n            JavaRandom r = new JavaRandom(rs.asJdkRandom());\n            return qt.generate(r);\n        };\n    }\n\n    public static class HackStrat extends LocalStrategy\n    {\n        public HackStrat(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)\n        {\n            super(keyspaceName, tokenMetadata, snitch, configOptions);\n        }\n    }\n\n    /**\n     * Since the clock is accessable via {@link Global#currentTimeMillis()} and {@link Global#nanoTime()}, and only repair subsystem has the requirement not to touch that, this class\n     * acts as a safty check that validates that repair does not touch these methods but allows the other subsystems to do so.\n     */\n    public static class ClockAccess implements Clock\n    {\n        private static final Set<Thread> OWNERS = Collections.synchronizedSet(new HashSet<>());\n        private final Clock delegate = new Default();\n\n        public static void includeThreadAsOwner()\n        {\n            OWNERS.add(Thread.currentThread());\n        }\n\n        @Override\n        public long nanoTime()\n        {\n            checkAccess();\n            return delegate.nanoTime();\n        }\n\n        @Override\n        public long currentTimeMillis()\n        {\n            checkAccess();\n            return delegate.currentTimeMillis();\n        }\n\n        private enum Access\n        {MAIN_THREAD_ONLY, REJECT, IGNORE}\n\n        private void checkAccess()\n        {\n            Access access = StackWalker.getInstance().walk(frames -> {\n                Iterator<StackWalker.StackFrame> it = frames.iterator();\n                boolean topLevel = false;\n                while (it.hasNext())\n                {\n                    StackWalker.StackFrame next = it.next();\n                    if (!topLevel)\n                    {\n                        // need to find the top level!\n                        while (!Clock.Global.class.getName().equals(next.getClassName()))\n                        {\n                            assert it.hasNext();\n                            next = it.next();\n                        }\n                        topLevel = true;\n                        assert it.hasNext();\n                        next = it.next();\n                    }\n                    if (FuzzTestBase.class.getName().equals(next.getClassName())) return Access.MAIN_THREAD_ONLY;\n                    // this is non-deterministic... but since the scope of the work is testing repair and not paxos... this is unblocked for now...\n                    if ((\"org.apache.cassandra.service.paxos.Paxos\".equals(next.getClassName()) && \"newBallot\".equals(next.getMethodName()))\n                        || (\"org.apache.cassandra.service.paxos.uncommitted.PaxosBallotTracker\".equals(next.getClassName()) && \"updateLowBound\".equals(next.getMethodName())))\n                        return Access.MAIN_THREAD_ONLY;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.db.\") || next.getClassName().startsWith(\"org.apache.cassandra.gms.\") || next.getClassName().startsWith(\"org.apache.cassandra.cql3.\") || next.getClassName().startsWith(\"org.apache.cassandra.metrics.\") || next.getClassName().startsWith(\"org.apache.cassandra.utils.concurrent.\")\n                        || next.getClassName().startsWith(\"org.apache.cassandra.utils.TimeUUID\") // this would be good to solve\n                        || next.getClassName().startsWith(PendingAntiCompaction.class.getName()))\n                        return Access.IGNORE;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.repair\") || ActiveRepairService.class.getName().startsWith(next.getClassName()))\n                        return Access.REJECT;\n                }\n                return Access.IGNORE;\n            });\n            Thread current = Thread.currentThread();\n            switch (access)\n            {\n                case IGNORE:\n                    return;\n                case REJECT:\n                    throw new IllegalStateException(\"Rejecting access\");\n                case MAIN_THREAD_ONLY:\n                    if (!OWNERS.contains(current)) throw new IllegalStateException(\"Accessed in wrong thread: \" + current);\n                    break;\n            }\n        }\n    }\n\n    static class SimulatedFault extends RuntimeException\n    {\n        SimulatedFault(String message)\n        {\n            super(message);\n        }\n    }\n}\n","lineNo":865}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.repair;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.StandardCharsets;\nimport java.sql.Timestamp;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.function.BiFunction;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.LongSupplier;\nimport java.util.function.Supplier;\nimport javax.annotation.Nullable;\n\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Maps;\nimport org.apache.cassandra.config.UnitConfigOverride;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\n\nimport accord.utils.DefaultRandom;\nimport accord.utils.Gen;\nimport accord.utils.Gens;\nimport accord.utils.RandomSource;\nimport org.agrona.collections.Long2ObjectHashMap;\nimport org.agrona.collections.LongHashSet;\nimport org.apache.cassandra.concurrent.ExecutorBuilder;\nimport org.apache.cassandra.concurrent.ExecutorBuilderFactory;\nimport org.apache.cassandra.concurrent.ExecutorFactory;\nimport org.apache.cassandra.concurrent.ExecutorPlus;\nimport org.apache.cassandra.concurrent.InfiniteLoopExecutor;\nimport org.apache.cassandra.concurrent.Interruptible;\nimport org.apache.cassandra.concurrent.ScheduledExecutorPlus;\nimport org.apache.cassandra.concurrent.SequentialExecutorPlus;\nimport org.apache.cassandra.concurrent.SimulatedExecutorFactory;\nimport org.apache.cassandra.concurrent.Stage;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.CQLTester;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Digest;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.compaction.ICompactionManager;\nimport org.apache.cassandra.db.marshal.EmptyType;\nimport org.apache.cassandra.db.repair.CassandraTableRepairManager;\nimport org.apache.cassandra.db.repair.PendingAntiCompaction;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.dht.Range;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.exceptions.RequestFailureReason;\nimport org.apache.cassandra.gms.ApplicationState;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.HeartBeatState;\nimport org.apache.cassandra.gms.IEndpointStateChangeSubscriber;\nimport org.apache.cassandra.gms.IFailureDetector;\nimport org.apache.cassandra.gms.IGossiper;\nimport org.apache.cassandra.gms.VersionedValue;\nimport org.apache.cassandra.io.util.DataInputBuffer;\nimport org.apache.cassandra.io.util.DataOutputBuffer;\nimport org.apache.cassandra.locator.IEndpointSnitch;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.LocalStrategy;\nimport org.apache.cassandra.locator.RangesAtEndpoint;\nimport org.apache.cassandra.locator.TokenMetadata;\nimport org.apache.cassandra.net.ConnectionType;\nimport org.apache.cassandra.net.IVerbHandler;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.net.MessageDelivery;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.net.RequestCallback;\nimport org.apache.cassandra.repair.messages.RepairMessage;\nimport org.apache.cassandra.repair.messages.RepairOption;\nimport org.apache.cassandra.repair.messages.ValidationResponse;\nimport org.apache.cassandra.repair.state.Completable;\nimport org.apache.cassandra.repair.state.CoordinatorState;\nimport org.apache.cassandra.repair.state.JobState;\nimport org.apache.cassandra.repair.state.SessionState;\nimport org.apache.cassandra.repair.state.ValidationState;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.SystemDistributedKeyspace;\nimport org.apache.cassandra.schema.TableId;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.ActiveRepairService;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.streaming.StreamEventHandler;\nimport org.apache.cassandra.streaming.StreamReceiveException;\nimport org.apache.cassandra.streaming.StreamSession;\nimport org.apache.cassandra.streaming.StreamState;\nimport org.apache.cassandra.streaming.StreamingChannel;\nimport org.apache.cassandra.streaming.StreamingDataInputPlus;\nimport org.apache.cassandra.tools.nodetool.Repair;\nimport org.apache.cassandra.utils.AbstractTypeGenerators;\nimport org.apache.cassandra.utils.CassandraGenerators;\nimport org.apache.cassandra.utils.Clock;\nimport org.apache.cassandra.utils.Closeable;\nimport org.apache.cassandra.utils.FailingBiConsumer;\nimport org.apache.cassandra.utils.Generators;\nimport org.apache.cassandra.utils.MBeanWrapper;\nimport org.apache.cassandra.utils.MerkleTree;\nimport org.apache.cassandra.utils.MerkleTrees;\nimport org.apache.cassandra.utils.NoSpamLogger;\nimport org.apache.cassandra.utils.concurrent.AsyncPromise;\nimport org.apache.cassandra.utils.concurrent.Future;\nimport org.apache.cassandra.utils.concurrent.ImmediateFuture;\nimport org.apache.cassandra.utils.progress.ProgressEventType;\nimport org.assertj.core.api.Assertions;\nimport org.mockito.Mockito;\nimport org.quicktheories.impl.JavaRandom;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.CLOCK_GLOBAL;\nimport static org.apache.cassandra.config.CassandraRelevantProperties.ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION;\n\npublic abstract class FuzzTestBase extends CQLTester.InMemory\n{\n    private static final int MISMATCH_NUM_PARTITIONS = 1;\n    private static final Gen<String> IDENTIFIER_GEN = fromQT(Generators.IDENTIFIER_GEN);\n    private static final Gen<String> KEYSPACE_NAME_GEN = fromQT(CassandraGenerators.KEYSPACE_NAME_GEN);\n    private static final Gen<TableId> TABLE_ID_GEN = fromQT(CassandraGenerators.TABLE_ID_GEN);\n    private static final Gen<InetAddressAndPort> ADDRESS_W_PORT = fromQT(CassandraGenerators.INET_ADDRESS_AND_PORT_GEN);\n\n    private static boolean SETUP_SCHEMA = false;\n    static String KEYSPACE;\n    static List<String> TABLES;\n\n    @BeforeClass\n    public static void setUpClass()\n    {\n        ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION.setBoolean(true);\n        CLOCK_GLOBAL.setString(ClockAccess.class.getName());\n        // when running in CI an external actor will replace the test configs based off the test type (such as trie, cdc, etc.), this could then have failing tests\n        // that do not repo with the same seed!  To fix that, go to UnitConfigOverride and update the config type to match the one that failed in CI, this should then\n        // use the same config, so the seed should not reproduce.\n        UnitConfigOverride.maybeOverrideConfig();\n\n        DatabaseDescriptor.daemonInitialization();\n        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance); // TOOD (coverage): random select\n        DatabaseDescriptor.setLocalDataCenter(\"test\");\n        StreamingChannel.Factory.Global.unsafeSet(new StreamingChannel.Factory()\n        {\n            private final AtomicInteger counter = new AtomicInteger();\n\n            @Override\n            public StreamingChannel create(InetSocketAddress to, int messagingVersion, StreamingChannel.Kind kind) throws IOException\n            {\n                StreamingChannel mock = Mockito.mock(StreamingChannel.class);\n                int id = counter.incrementAndGet();\n                StreamSession session = Mockito.mock(StreamSession.class);\n                StreamReceiveException access = new StreamReceiveException(session, \"mock access rejected\");\n                StreamingDataInputPlus input = Mockito.mock(StreamingDataInputPlus.class, invocationOnMock -> {\n                    throw access;\n                });\n                Mockito.doNothing().when(input).close();\n                Mockito.when(mock.in()).thenReturn(input);\n                Mockito.when(mock.id()).thenReturn(id);\n                Mockito.when(mock.peer()).thenReturn(to);\n                Mockito.when(mock.connectedTo()).thenReturn(to);\n                Mockito.when(mock.send(Mockito.any())).thenReturn(ImmediateFuture.success(null));\n                Mockito.when(mock.close()).thenReturn(ImmediateFuture.success(null));\n                return mock;\n            }\n        });\n        ExecutorFactory delegate = ExecutorFactory.Global.executorFactory();\n        ExecutorFactory.Global.unsafeSet(new ExecutorFactory()\n        {\n            @Override\n            public LocalAwareSubFactory localAware()\n            {\n                return delegate.localAware();\n            }\n\n            @Override\n            public ScheduledExecutorPlus scheduled(boolean executeOnShutdown, String name, int priority, SimulatorSemantics simulatorSemantics)\n            {\n                return delegate.scheduled(executeOnShutdown, name, priority, simulatorSemantics);\n            }\n\n            private boolean shouldMock()\n            {\n                return StackWalker.getInstance().walk(frame -> {\n                    StackWalker.StackFrame caller = frame.skip(3).findFirst().get();\n                    return caller.getClassName().startsWith(\"org.apache.cassandra.streaming.\");\n                });\n            }\n\n            @Override\n            public Thread startThread(String name, Runnable runnable, InfiniteLoopExecutor.Daemon daemon)\n            {\n                if (shouldMock()) return new Thread();\n                return delegate.startThread(name, runnable, daemon);\n            }\n\n            @Override\n            public Interruptible infiniteLoop(String name, Interruptible.Task task, InfiniteLoopExecutor.SimulatorSafe simulatorSafe, InfiniteLoopExecutor.Daemon daemon, InfiniteLoopExecutor.Interrupts interrupts)\n            {\n                return delegate.infiniteLoop(name, task, simulatorSafe, daemon, interrupts);\n            }\n\n            @Override\n            public ThreadGroup newThreadGroup(String name)\n            {\n                return delegate.newThreadGroup(name);\n            }\n\n            @Override\n            public ExecutorBuilderFactory<ExecutorPlus, SequentialExecutorPlus> withJmx(String jmxPath)\n            {\n                return delegate.withJmx(jmxPath);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends SequentialExecutorPlus> configureSequential(String name)\n            {\n                return delegate.configureSequential(name);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends ExecutorPlus> configurePooled(String name, int threads)\n            {\n                return delegate.configurePooled(name, threads);\n            }\n        });\n\n        // will both make sure this is loaded and used\n        if (!(Clock.Global.clock() instanceof ClockAccess)) throw new IllegalStateException(\"Unable to override clock\");\n\n        // set the repair rcp timeout high so we don't hit it... this class is mostly testing repair reaching success\n        // so don't want to deal with unlucky histories...\n        DatabaseDescriptor.setRepairRpcTimeout(TimeUnit.DAYS.toMillis(1));\n\n\n        InMemory.setUpClass();\n    }\n\n    @Before\n    public void setupSchema()\n    {\n        if (SETUP_SCHEMA) return;\n        SETUP_SCHEMA = true;\n        // StorageService can not be mocked out, nor can ColumnFamilyStores, so make sure that the keyspace is a \"local\" keyspace to avoid replication as the peers don't actually exist for replication\n        schemaChange(String.format(\"CREATE KEYSPACE %s WITH REPLICATION = {'class': '%s'}\", SchemaConstants.DISTRIBUTED_KEYSPACE_NAME, HackStrat.class.getName()));\n        for (TableMetadata table : SystemDistributedKeyspace.metadata().tables)\n            schemaChange(table.toCqlString(false, false));\n\n        createSchema();\n    }\n\n    protected void cleanupRepairTables()\n    {\n        for (String table : Arrays.asList(SystemKeyspace.REPAIRS))\n            execute(String.format(\"TRUNCATE %s.%s\", SchemaConstants.SYSTEM_KEYSPACE_NAME, table));\n    }\n\n    private void createSchema()\n    {\n        // The main reason to use random here with a fixed seed is just to have a set of tables that are not hard coded.\n        // The tables will have diversity to them that most likely doesn't matter to repair (hence why the tables are shared), but\n        // is useful just in case some assumptions change.\n        RandomSource rs = new DefaultRandom(42);\n        String ks = KEYSPACE_NAME_GEN.next(rs);\n        List<String> tableNames = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(10, 100).next(rs);\n        JavaRandom qt = new JavaRandom(rs.asJdkRandom());\n        Tables.Builder tableBuilder = Tables.builder();\n        List<TableId> ids = Gens.lists(TABLE_ID_GEN).unique().ofSize(tableNames.size()).next(rs);\n        for (int i = 0; i < tableNames.size(); i++)\n        {\n            String name = tableNames.get(i);\n            TableId id = ids.get(i);\n            TableMetadata tableMetadata = new CassandraGenerators.TableMetadataBuilder().withKeyspaceName(ks).withTableName(name).withTableId(id).withTableKinds(TableMetadata.Kind.REGULAR)\n                                                                                        // shouldn't matter, just wanted to avoid UDT as that needs more setup\n                                                                                        .withDefaultTypeGen(AbstractTypeGenerators.builder().withTypeKinds(AbstractTypeGenerators.TypeKind.PRIMITIVE).withoutPrimitive(EmptyType.instance).build()).build().generate(qt);\n            tableBuilder.add(tableMetadata);\n        }\n        KeyspaceParams params = KeyspaceParams.simple(3);\n        KeyspaceMetadata metadata = KeyspaceMetadata.create(ks, params, tableBuilder.build());\n\n        // create\n        schemaChange(metadata.toCqlString(false, false));\n        KEYSPACE = ks;\n        for (TableMetadata table : metadata.tables)\n            schemaChange(table.toCqlString(false, false));\n        TABLES = tableNames;\n    }\n\n    static void enableMessageFaults(Cluster cluster)\n    {\n        cluster.allowedMessageFaults(new BiFunction<>()\n        {\n            private final LongHashSet noFaults = new LongHashSet();\n            private final LongHashSet allowDrop = new LongHashSet();\n\n            @Override\n            public Set<Faults> apply(Cluster.Node node, Message<?> message)\n            {\n                if (RepairMessage.ALLOWS_RETRY.contains(message.verb()))\n                {\n                    allowDrop.add(message.id());\n                    return Faults.DROPPED;\n                }\n                switch (message.verb())\n                {\n                    // these messages are not resilent to ephemeral issues\n                    case STATUS_REQ:\n                    case STATUS_RSP:\n                        noFaults.add(message.id());\n                        return Faults.NONE;\n                    default:\n                        if (noFaults.contains(message.id())) return Faults.NONE;\n                        if (allowDrop.contains(message.id())) return Faults.DROPPED;\n                        // was a new message added and the test not updated?\n                        IllegalStateException e = new IllegalStateException(\"Verb: \" + message.verb());\n                        cluster.failures.add(e);\n                        throw e;\n                }\n            }\n        });\n    }\n\n    static void runAndAssertSuccess(Cluster cluster, int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        cluster.processAll();\n        assertSuccess(example, shouldSync, repair);\n    }\n\n    static void assertSuccess(int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        Completable.Result result = repair.state.getResult();\n        Assertions.assertThat(result)\n                  .describedAs(\"Expected repair to have completed with success, but is still running... %s; example %d\", repair.state, example).isNotNull()\n                  .describedAs(\"Unexpected state: %s -> %s; example %d\", repair.state, result, example).isEqualTo(Completable.Result.success(repairSuccessMessage(repair)));\n        Assertions.assertThat(repair.state.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(CoordinatorState.State.class));\n        Assertions.assertThat(repair.state.getSessions()).isNotEmpty();\n        boolean shouldSnapshot = repair.state.options.getParallelism() != RepairParallelism.PARALLEL\n                                 && (!repair.state.options.isIncremental() || repair.state.options.isPreview());\n        for (SessionState session : repair.state.getSessions())\n        {\n            Assertions.assertThat(session.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(SessionState.State.class));\n            Assertions.assertThat(session.getJobs()).isNotEmpty();\n            for (JobState job : session.getJobs())\n            {\n                EnumSet<JobState.State> expected = EnumSet.allOf(JobState.State.class);\n                if (!shouldSnapshot)\n                {\n                    expected.remove(JobState.State.SNAPSHOT_START);\n                    expected.remove(JobState.State.SNAPSHOT_COMPLETE);\n                }\n                if (!shouldSync)\n                {\n                    expected.remove(JobState.State.STREAM_START);\n                }\n                Set<JobState.State> actual = job.getStateTimesMillis().keySet();\n                Assertions.assertThat(actual).isEqualTo(expected);\n            }\n        }\n    }\n\n    static String repairSuccessMessage(RepairCoordinator repair)\n    {\n        RepairOption options = repair.state.options;\n        if (options.isPreview())\n        {\n            String suffix;\n            switch (options.getPreviewKind())\n            {\n                case UNREPAIRED:\n                case ALL:\n                    suffix = \"Previewed data was in sync\";\n                    break;\n                case REPAIRED:\n                    suffix = \"Repaired data is in sync\";\n                    break;\n                default:\n                    throw new IllegalArgumentException(\"Unexpected preview repair kind: \" + options.getPreviewKind());\n            }\n            return \"Repair preview completed successfully; \" + suffix;\n        }\n        return \"Repair completed successfully\";\n    }\n\n    InetAddressAndPort pickParticipant(RandomSource rs, Cluster.Node coordinator, RepairCoordinator repair)\n    {\n        if (repair.state.isComplete())\n            throw new IllegalStateException(\"Repair is completed! \" + repair.state.getResult());\n        List<InetAddressAndPort> participaents = new ArrayList<>(repair.state.getNeighborsAndRanges().participants.size() + 1);\n        if (rs.nextBoolean()) participaents.add(coordinator.broadcastAddressAndPort());\n        participaents.addAll(repair.state.getNeighborsAndRanges().participants);\n        participaents.sort(Comparator.naturalOrder());\n\n        InetAddressAndPort selected = rs.pick(participaents);\n        return selected;\n    }\n\n    static void addMismatch(RandomSource rs, ColumnFamilyStore cfs, Validator validator)\n    {\n        ValidationState state = validator.state;\n        int maxDepth = DatabaseDescriptor.getRepairSessionMaxTreeDepth();\n        state.phase.start(MISMATCH_NUM_PARTITIONS, 1024);\n\n        MerkleTrees trees = new MerkleTrees(cfs.getPartitioner());\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            int depth = (int) Math.min(Math.ceil(Math.log(MISMATCH_NUM_PARTITIONS) / Math.log(2)), maxDepth);\n            trees.addMerkleTree((int) Math.pow(2, depth), range);\n        }\n        Set<Token> allTokens = new HashSet<>();\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            Gen<Token> gen = fromQT(CassandraGenerators.tokensInRange(range));\n            Set<Token> tokens = new LinkedHashSet<>();\n            for (int i = 0, size = rs.nextInt(1, 10); i < size; i++)\n            {\n                for (int attempt = 0; !tokens.add(gen.next(rs)) && attempt < 5; attempt++)\n                {\n                }\n            }\n            // tokens may or may not be of the expected size; this depends on how wide the range is\n            for (Token token : tokens)\n                trees.split(token);\n            allTokens.addAll(tokens);\n        }\n        for (Token token : allTokens)\n        {\n            findCorrectRange(trees, token, range -> {\n                Digest digest = Digest.forValidator();\n                digest.update(ByteBuffer.wrap(token.toString().getBytes(StandardCharsets.UTF_8)));\n                range.addHash(new MerkleTree.RowHash(token, digest.digest(), 1));\n            });\n        }\n        state.partitionsProcessed++;\n        state.bytesRead = 1024;\n        state.phase.sendingTrees();\n        Stage.ANTI_ENTROPY.execute(() -> {\n            state.phase.success();\n            validator.respond(new ValidationResponse(validator.desc, trees));\n        });\n    }\n\n    private static void findCorrectRange(MerkleTrees trees, Token token, Consumer<MerkleTree.TreeRange> fn)\n    {\n        MerkleTrees.TreeRangeIterator it = trees.rangeIterator();\n        while (it.hasNext())\n        {\n            MerkleTree.TreeRange next = it.next();\n            if (next.contains(token))\n            {\n                fn.accept(next);\n                return;\n            }\n        }\n    }\n\n    private enum RepairType\n    {FULL, IR}\n\n    private enum PreviewType\n    {NONE, REPAIRED, UNREPAIRED}\n\n    static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, List<String> tableNames)\n    {\n        return repairOption(rs, coordinator, ks, Gens.lists(Gens.pick(tableNames)).ofSizeBetween(1, tableNames.size()), Gens.enums().all(RepairType.class), Gens.enums().all(PreviewType.class), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption irOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.IR), Gens.constant(PreviewType.NONE), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption previewOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.FULL), Gens.constant(PreviewType.REPAIRED), Gens.enums().all(RepairParallelism.class));\n    }\n\n    private static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen, Gen<RepairType> repairTypeGen, Gen<PreviewType> previewTypeGen, Gen<RepairParallelism> repairParallelismGen)\n    {\n        List<String> args = new ArrayList<>();\n        args.add(ks);\n        args.addAll(tablesGen.next(rs));\n        args.add(\"-pr\");\n        RepairType type = repairTypeGen.next(rs);\n        switch (type)\n        {\n            case IR:\n                // default\n                break;\n            case FULL:\n                args.add(\"--full\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported repair type: \" + type);\n        }\n        PreviewType previewType = previewTypeGen.next(rs);\n        switch (previewType)\n        {\n            case NONE:\n                break;\n            case REPAIRED:\n                args.add(\"--validate\");\n                break;\n            case UNREPAIRED:\n                args.add(\"--preview\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported preview type: \" + previewType);\n        }\n        RepairParallelism parallelism = repairParallelismGen.next(rs);\n        switch (parallelism)\n        {\n            case SEQUENTIAL:\n                args.add(\"--sequential\");\n                break;\n            case PARALLEL:\n                // default\n                break;\n            case DATACENTER_AWARE:\n                args.add(\"--dc-parallel\");\n                break;\n            default:\n                throw new AssertionError(\"Unknown parallelism: \" + parallelism);\n        }\n        if (rs.nextBoolean()) args.add(\"--optimise-streams\");\n        RepairOption options = RepairOption.parse(Repair.parseOptionMap(() -> \"test\", args), DatabaseDescriptor.getPartitioner());\n        if (options.getRanges().isEmpty())\n        {\n            if (options.isPrimaryRange())\n            {\n                // when repairing only primary range, neither dataCenters nor hosts can be set\n                if (options.getDataCenters().isEmpty() && options.getHosts().isEmpty())\n                    options.getRanges().addAll(coordinator.getPrimaryRanges(ks));\n                    // except dataCenters only contain local DC (i.e. -local)\n                else if (options.isInLocalDCOnly())\n                    options.getRanges().addAll(coordinator.getPrimaryRangesWithinDC(ks));\n                else\n                    throw new IllegalArgumentException(\"You need to run primary range repair on all nodes in the cluster.\");\n            }\n            else\n            {\n                Iterables.addAll(options.getRanges(), coordinator.getLocalReplicas(ks).onlyFull().ranges());\n            }\n        }\n        return options;\n    }\n\n    enum Faults\n    {\n        DELAY, DROP;\n\n        public static final Set<Faults> NONE = Collections.emptySet();\n        public static final Set<Faults> DROPPED = EnumSet.of(DELAY, DROP);\n    }\n\n    private static class Connection\n    {\n        final InetAddressAndPort from, to;\n\n        private Connection(InetAddressAndPort from, InetAddressAndPort to)\n        {\n            this.from = from;\n            this.to = to;\n        }\n\n        @Override\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Connection that = (Connection) o;\n            return from.equals(that.from) && to.equals(that.to);\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(from, to);\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"Connection{\" + \"from=\" + from + \", to=\" + to + '}';\n        }\n    }\n\n    interface MessageListener\n    {\n        default void preHandle(Cluster.Node node, Message<?> msg) {}\n    }\n\n    static class Cluster\n    {\n        private static final FailingBiConsumer<ColumnFamilyStore, Validator> DEFAULT_VALIDATION = ValidationManager::doValidation;\n\n        final Map<InetAddressAndPort, Node> nodes;\n        private final IFailureDetector failureDetector = Mockito.mock(IFailureDetector.class);\n        private final IEndpointSnitch snitch = Mockito.mock(IEndpointSnitch.class);\n        private final SimulatedExecutorFactory globalExecutor;\n        final ScheduledExecutorPlus unorderedScheduled;\n        final ExecutorPlus orderedExecutor;\n        private final Gossip gossiper = new Gossip();\n        private final MBeanWrapper mbean = Mockito.mock(MBeanWrapper.class);\n        private final List<Throwable> failures = new ArrayList<>();\n        private final List<MessageListener> listeners = new ArrayList<>();\n        private final RandomSource rs;\n        private BiFunction<Node, Message<?>, Set<Faults>> allowedMessageFaults = (a, b) -> Collections.emptySet();\n\n        private final Map<Connection, LongSupplier> networkLatencies = new HashMap<>();\n        private final Map<Connection, Supplier<Boolean>> networkDrops = new HashMap<>();\n\n        Cluster(RandomSource rs)\n        {\n            ClockAccess.includeThreadAsOwner();\n            this.rs = rs;\n            globalExecutor = new SimulatedExecutorFactory(rs, fromQT(Generators.TIMESTAMP_GEN.map(Timestamp::getTime)).mapToLong(TimeUnit.MILLISECONDS::toNanos).next(rs));\n            orderedExecutor = globalExecutor.configureSequential(\"ignore\").build();\n            unorderedScheduled = globalExecutor.scheduled(\"ignored\");\n\n\n\n            // We run tests in an isolated JVM per class, so not cleaing up is safe... but if that assumption ever changes, will need to cleanup\n            Stage.ANTI_ENTROPY.unsafeSetExecutor(orderedExecutor);\n            Stage.INTERNAL_RESPONSE.unsafeSetExecutor(unorderedScheduled);\n            Mockito.when(failureDetector.isAlive(Mockito.any())).thenReturn(true);\n            Thread expectedThread = Thread.currentThread();\n            NoSpamLogger.unsafeSetClock(() -> {\n                if (Thread.currentThread() != expectedThread)\n                    throw new AssertionError(\"NoSpamLogger.Clock accessed outside of fuzzing...\");\n                return globalExecutor.nanoTime();\n            });\n\n            int numNodes = rs.nextInt(3, 10);\n            List<String> dcs = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(1, Math.min(10, numNodes)).next(rs);\n            Map<InetAddressAndPort, Node> nodes = Maps.newHashMapWithExpectedSize(numNodes);\n            Gen<Token> tokenGen = fromQT(CassandraGenerators.token(DatabaseDescriptor.getPartitioner()));\n            Gen<UUID> hostIdGen = fromQT(Generators.UUID_RANDOM_GEN);\n            Set<Token> tokens = new HashSet<>();\n            Set<UUID> hostIds = new HashSet<>();\n            for (int i = 0; i < numNodes; i++)\n            {\n                InetAddressAndPort addressAndPort = ADDRESS_W_PORT.next(rs);\n                while (nodes.containsKey(addressAndPort)) addressAndPort = ADDRESS_W_PORT.next(rs);\n                Token token;\n                while (!tokens.add(token = tokenGen.next(rs)))\n                {\n                }\n                UUID hostId;\n                while (!hostIds.add(hostId = hostIdGen.next(rs)))\n                {\n                }\n\n                String dc = rs.pick(dcs);\n                String rack = \"rack\";\n                Mockito.when(snitch.getDatacenter(Mockito.eq(addressAndPort))).thenReturn(dc);\n                Mockito.when(snitch.getRack(Mockito.eq(addressAndPort))).thenReturn(rack);\n\n                VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(DatabaseDescriptor.getPartitioner());\n                EndpointState state = new EndpointState(new HeartBeatState(42, 42));\n                state.addApplicationState(ApplicationState.STATUS, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.STATUS_WITH_PORT, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.HOST_ID, valueFactory.hostId(hostId));\n                state.addApplicationState(ApplicationState.TOKENS, valueFactory.tokens(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.DC, valueFactory.datacenter(dc));\n                state.addApplicationState(ApplicationState.RACK, valueFactory.rack(rack));\n                state.addApplicationState(ApplicationState.RELEASE_VERSION, valueFactory.releaseVersion());\n\n                gossiper.endpoints.put(addressAndPort, state);\n\n                Node node = new Node(hostId, addressAndPort, Collections.singletonList(token), new Messaging(addressAndPort));\n                nodes.put(addressAndPort, node);\n            }\n            this.nodes = nodes;\n\n            TokenMetadata tm = StorageService.instance.getTokenMetadata();\n            tm.clearUnsafe();\n            for (Node inst : nodes.values())\n            {\n                tm.updateHostId(inst.hostId(), inst.broadcastAddressAndPort());\n                for (Token token : inst.tokens())\n                    tm.updateNormalToken(token, inst.broadcastAddressAndPort());\n            }\n        }\n\n        public Closeable addListener(MessageListener listener)\n        {\n            listeners.add(listener);\n            return () -> removeListener(listener);\n        }\n\n        public void removeListener(MessageListener listener)\n        {\n            listeners.remove(listener);\n        }\n\n        public void allowedMessageFaults(BiFunction<Node, Message<?>, Set<Faults>> fn)\n        {\n            this.allowedMessageFaults = fn;\n        }\n\n        public void checkFailures()\n        {\n            if (Thread.interrupted())\n                failures.add(new InterruptedException());\n            if (failures.isEmpty()) return;\n            AssertionError error = new AssertionError(\"Unexpected exceptions found\");\n            failures.forEach(error::addSuppressed);\n            failures.clear();\n            throw error;\n        }\n\n        public boolean processOne()\n        {\n            boolean result = globalExecutor.processOne();\n            checkFailures();\n            return result;\n        }\n\n        public void processAll()\n        {\n            while (processOne())\n            {\n            }\n        }\n\n        private class CallbackContext\n        {\n            final RequestCallback callback;\n\n            private CallbackContext(RequestCallback callback)\n            {\n                this.callback = Objects.requireNonNull(callback);\n            }\n\n            public void onResponse(Message msg)\n            {\n                callback.onResponse(msg);\n            }\n\n            public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n            {\n                if (callback.invokeOnFailure()) callback.onFailure(from, failureReason);\n            }\n        }\n\n        private class Messaging implements MessageDelivery\n        {\n            final InetAddressAndPort broadcastAddressAndPort;\n            final Long2ObjectHashMap<CallbackContext> callbacks = new Long2ObjectHashMap<>();\n\n            private Messaging(InetAddressAndPort broadcastAddressAndPort)\n            {\n                this.broadcastAddressAndPort = broadcastAddressAndPort;\n            }\n\n            @Override\n            public <REQ> void send(Message<REQ> message, InetAddressAndPort to)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, null);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb, ConnectionType specifyConnection)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            private <REQ, RSP> void maybeEnqueue(Message<REQ> message, InetAddressAndPort to, @Nullable RequestCallback<RSP> callback)\n            {\n                CallbackContext cb;\n                if (callback != null)\n                {\n                    if (callbacks.containsKey(message.id()))\n                        throw new AssertionError(\"Message id \" + message.id() + \" already has a callback\");\n                    cb = new CallbackContext(callback);\n                    callbacks.put(message.id(), cb);\n                }\n                else\n                {\n                    cb = null;\n                }\n                boolean toSelf = this.broadcastAddressAndPort.equals(to);\n                Node node = nodes.get(to);\n                Set<Faults> allowedFaults = allowedMessageFaults.apply(node, message);\n                if (allowedFaults.isEmpty())\n                {\n                    // enqueue so stack overflow doesn't happen with the inlining\n                    unorderedScheduled.submit(() -> node.handle(message));\n                }\n                else\n                {\n                    Runnable enqueue = () -> {\n                        if (!allowedFaults.contains(Faults.DELAY))\n                        {\n                            unorderedScheduled.submit(() -> node.handle(message));\n                        }\n                        else\n                        {\n                            if (toSelf) unorderedScheduled.submit(() -> node.handle(message));\n                            else\n                                unorderedScheduled.schedule(() -> node.handle(message), networkJitterNanos(to), TimeUnit.NANOSECONDS);\n                        }\n                    };\n\n                    if (!allowedFaults.contains(Faults.DROP)) enqueue.run();\n                    else\n                    {\n                        if (!toSelf && networkDrops(to))\n                        {\n//                            logger.warn(\"Dropped message {}\", message);\n                            // drop\n                        }\n                        else\n                        {\n                            enqueue.run();\n                        }\n                    }\n\n                    if (cb != null)\n                    {\n                        unorderedScheduled.schedule(() -> {\n                            CallbackContext ctx = callbacks.remove(message.id());\n                            if (ctx != null)\n                            {\n                                assert ctx == cb;\n                                try\n                                {\n                                    ctx.onFailure(to, RequestFailureReason.TIMEOUT);\n                                }\n                                catch (Throwable t)\n                                {\n                                    failures.add(t);\n                                }\n                            }\n                        }, message.verb().expiresAfterNanos(), TimeUnit.NANOSECONDS);\n                    }\n                }\n            }\n\n            private long networkJitterNanos(InetAddressAndPort to)\n            {\n                return networkLatencies.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> {\n                    long min = TimeUnit.MICROSECONDS.toNanos(500);\n                    long maxSmall = TimeUnit.MILLISECONDS.toNanos(5);\n                    long max = TimeUnit.SECONDS.toNanos(5);\n                    LongSupplier small = () -> rs.nextLong(min, maxSmall);\n                    LongSupplier large = () -> rs.nextLong(maxSmall, max);\n                    return Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).mapToLong(b -> b ? large.getAsLong() : small.getAsLong()).asLongSupplier(rs);\n                }).getAsLong();\n            }\n\n            private boolean networkDrops(InetAddressAndPort to)\n            {\n                return networkDrops.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).asSupplier(rs)).get();\n            }\n\n            @Override\n            public <REQ, RSP> Future<Message<RSP>> sendWithResult(Message<REQ> message, InetAddressAndPort to)\n            {\n                AsyncPromise<Message<RSP>> promise = new AsyncPromise<>();\n                sendWithCallback(message, to, new RequestCallback<RSP>()\n                {\n                    @Override\n                    public void onResponse(Message<RSP> msg)\n                    {\n                        promise.trySuccess(msg);\n                    }\n\n                    @Override\n                    public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n                    {\n                        promise.tryFailure(new MessagingService.FailureResponseException(from, failureReason));\n                    }\n\n                    @Override\n                    public boolean invokeOnFailure()\n                    {\n                        return true;\n                    }\n                });\n                return promise;\n            }\n\n            @Override\n            public <V> void respond(V response, Message<?> message)\n            {\n                send(message.responseWith(response), message.respondTo());\n            }\n        }\n\n        private class Gossip implements IGossiper\n        {\n            private final Map<InetAddressAndPort, EndpointState> endpoints = new HashMap<>();\n\n            @Override\n            public void register(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Override\n            public void unregister(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Nullable\n            @Override\n            public EndpointState getEndpointStateForEndpoint(InetAddressAndPort ep)\n            {\n                return endpoints.get(ep);\n            }\n        }\n\n        class Node implements SharedContext\n        {\n            private final ICompactionManager compactionManager = Mockito.mock(ICompactionManager.class);\n            final UUID hostId;\n            final InetAddressAndPort addressAndPort;\n            final Collection<Token> tokens;\n            final ActiveRepairService activeRepairService;\n            final IVerbHandler verbHandler;\n            final Messaging messaging;\n            final IValidationManager validationManager;\n            private FailingBiConsumer<ColumnFamilyStore, Validator> doValidation = DEFAULT_VALIDATION;\n            private final StreamExecutor defaultStreamExecutor = plan -> {\n                long delayNanos = rs.nextLong(TimeUnit.SECONDS.toNanos(5), TimeUnit.MINUTES.toNanos(10));\n                unorderedScheduled.schedule(() -> {\n                    StreamState success = new StreamState(plan.planId(), plan.streamOperation(), Collections.emptySet());\n                    for (StreamEventHandler handler : plan.handlers())\n                        handler.onSuccess(success);\n                }, delayNanos, TimeUnit.NANOSECONDS);\n                return null;\n            };\n            private StreamExecutor streamExecutor = defaultStreamExecutor;\n\n            private Node(UUID hostId, InetAddressAndPort addressAndPort, Collection<Token> tokens, Messaging messaging)\n            {\n                this.hostId = hostId;\n                this.addressAndPort = addressAndPort;\n                this.tokens = tokens;\n                this.messaging = messaging;\n                this.activeRepairService = new ActiveRepairService(this);\n                this.validationManager = (cfs, validator) -> unorderedScheduled.submit(() -> {\n                    try\n                    {\n                        doValidation.acceptOrFail(cfs, validator);\n                    }\n                    catch (Throwable e)\n                    {\n                        validator.fail(e);\n                    }\n                });\n                this.verbHandler = new RepairMessageVerbHandler(this);\n\n                activeRepairService.start();\n            }\n\n            public Closeable doValidation(FailingBiConsumer<ColumnFamilyStore, Validator> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                if (previous != DEFAULT_VALIDATION)\n                    throw new IllegalStateException(\"Attemptted to override validation, but was already overridden\");\n                this.doValidation = fn;\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doValidation(Function<FailingBiConsumer<ColumnFamilyStore, Validator>, FailingBiConsumer<ColumnFamilyStore, Validator>> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                this.doValidation = fn.apply(previous);\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doSync(StreamExecutor streamExecutor)\n            {\n                StreamExecutor previous = this.streamExecutor;\n                if (previous != defaultStreamExecutor)\n                    throw new IllegalStateException(\"Attemptted to override sync, but was already overridden\");\n                this.streamExecutor = streamExecutor;\n                return () -> this.streamExecutor = previous;\n            }\n\n            void handle(Message msg)\n            {\n                msg = serde(msg);\n                if (msg == null)\n                {\n                    logger.warn(\"Got a message that failed to serialize/deserialize\");\n                    return;\n                }\n                for (MessageListener l : listeners)\n                    l.preHandle(this, msg);\n                if (msg.verb().isResponse())\n                {\n                    // handle callbacks\n                    if (messaging.callbacks.containsKey(msg.id()))\n                    {\n                        CallbackContext callback = messaging.callbacks.remove(msg.id());\n                        if (callback == null) return;\n                        try\n                        {\n                            if (msg.isFailureResponse())\n                                callback.onFailure(msg.from(), (RequestFailureReason) msg.payload);\n                            else callback.onResponse(msg);\n                        }\n                        catch (Throwable t)\n                        {\n                            failures.add(t);\n                        }\n                    }\n                }\n                else\n                {\n                    try\n                    {\n                        verbHandler.doVerb(msg);\n                    }\n                    catch (Throwable e)\n                    {\n                        failures.add(e);\n                    }\n                }\n            }\n\n            public UUID hostId()\n            {\n                return hostId;\n            }\n\n            @Override\n            public InetAddressAndPort broadcastAddressAndPort()\n            {\n                return addressAndPort;\n            }\n\n            public Collection<Token> tokens()\n            {\n                return tokens;\n            }\n\n            public IFailureDetector failureDetector()\n            {\n                return failureDetector;\n            }\n\n            @Override\n            public IEndpointSnitch snitch()\n            {\n                return snitch;\n            }\n\n            @Override\n            public IGossiper gossiper()\n            {\n                return gossiper;\n            }\n\n            @Override\n            public ICompactionManager compactionManager()\n            {\n                return compactionManager;\n            }\n\n            public ExecutorFactory executorFactory()\n            {\n                return globalExecutor;\n            }\n\n            public ScheduledExecutorPlus optionalTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public Supplier<Random> random()\n            {\n                return () -> rs.fork().asJdkRandom();\n            }\n\n            public Clock clock()\n            {\n                return globalExecutor;\n            }\n\n            public MessageDelivery messaging()\n            {\n                return messaging;\n            }\n\n            public MBeanWrapper mbean()\n            {\n                return mbean;\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options)\n            {\n                return repair(ks, options, true);\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options, boolean addFailureOnErrorNotification)\n            {\n                RepairCoordinator repair = new RepairCoordinator(this, (name, tables) -> StorageService.instance.getValidColumnFamilies(false, false, name, tables), name -> StorageService.instance.getReplicas(name, broadcastAddressAndPort()), 42, options, ks);\n                if (addFailureOnErrorNotification)\n                {\n                    repair.addProgressListener((tag, event) -> {\n                        if (event.getType() == ProgressEventType.ERROR)\n                            failures.add(new AssertionError(event.getMessage()));\n                    });\n                }\n                return repair;\n            }\n\n            public RangesAtEndpoint getLocalReplicas(String ks)\n            {\n                return StorageService.instance.getReplicas(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRanges(String ks)\n            {\n                return StorageService.instance.getPrimaryRangesForEndpoint(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRangesWithinDC(String ks)\n            {\n                return StorageService.instance.getPrimaryRangeForEndpointWithinDC(ks, broadcastAddressAndPort());\n            }\n\n            @Override\n            public ActiveRepairService repair()\n            {\n                return activeRepairService;\n            }\n\n            @Override\n            public IValidationManager validationManager()\n            {\n                return validationManager;\n            }\n\n            @Override\n            public TableRepairManager repairManager(ColumnFamilyStore store)\n            {\n                return new CassandraTableRepairManager(store, this)\n                {\n                    @Override\n                    public void snapshot(String name, Collection<Range<Token>> ranges, boolean force)\n                    {\n                        // no-op\n                    }\n                };\n            }\n\n            @Override\n            public StreamExecutor streamExecutor()\n            {\n                return streamExecutor;\n            }\n        }\n\n        private Message serde(Message msg)\n        {\n            try (DataOutputBuffer b = DataOutputBuffer.scratchBuffer.get())\n            {\n                int messagingVersion = MessagingService.current_version;\n                Message.serializer.serialize(msg, b, messagingVersion);\n                DataInputBuffer in = new DataInputBuffer(b.unsafeGetBufferAndFlip(), false);\n                return Message.serializer.deserialize(in, msg.from(), messagingVersion);\n            }\n            catch (Throwable e)\n            {\n                failures.add(e);\n                return null;\n            }\n        }\n    }\n\n    private static <T> Gen<T> fromQT(org.quicktheories.core.Gen<T> qt)\n    {\n        return rs -> {\n            JavaRandom r = new JavaRandom(rs.asJdkRandom());\n            return qt.generate(r);\n        };\n    }\n\n    public static class HackStrat extends LocalStrategy\n    {\n        public HackStrat(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)\n        {\n            super(keyspaceName, tokenMetadata, snitch, configOptions);\n        }\n    }\n\n    /**\n     * Since the clock is accessable via {@link Global#currentTimeMillis()} and {@link Global#nanoTime()}, and only repair subsystem has the requirement not to touch that, this class\n     * acts as a safty check that validates that repair does not touch these methods but allows the other subsystems to do so.\n     */\n    public static class ClockAccess implements Clock\n    {\n        private static final Set<Thread> OWNERS = Collections.synchronizedSet(new HashSet<>());\n        private final Clock delegate = new Default();\n\n        public static void includeThreadAsOwner()\n        {\n            OWNERS.add(Thread.currentThread());\n        }\n\n        @Override\n        public long nanoTime()\n        {\n            checkAccess();\n            return delegate.nanoTime();\n        }\n\n        @Override\n        public long currentTimeMillis()\n        {\n            checkAccess();\n            return delegate.currentTimeMillis();\n        }\n\n        private enum Access\n        {MAIN_THREAD_ONLY, REJECT, IGNORE}\n\n        private void checkAccess()\n        {\n            Access access = StackWalker.getInstance().walk(frames -> {\n                Iterator<StackWalker.StackFrame> it = frames.iterator();\n                boolean topLevel = false;\n                while (it.hasNext())\n                {\n                    StackWalker.StackFrame next = it.next();\n                    if (!topLevel)\n                    {\n                        // need to find the top level!\n                        while (!Clock.Global.class.getName().equals(next.getClassName()))\n                        {\n                            assert it.hasNext();\n                            next = it.next();\n                        }\n                        topLevel = true;\n                        assert it.hasNext();\n                        next = it.next();\n                    }\n                    if (FuzzTestBase.class.getName().equals(next.getClassName())) return Access.MAIN_THREAD_ONLY;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.db.\") || next.getClassName().startsWith(\"org.apache.cassandra.gms.\") || next.getClassName().startsWith(\"org.apache.cassandra.cql3.\") || next.getClassName().startsWith(\"org.apache.cassandra.metrics.\") || next.getClassName().startsWith(\"org.apache.cassandra.utils.concurrent.\")\n                        || next.getClassName().startsWith(\"org.apache.cassandra.utils.TimeUUID\") // this would be good to solve\n                        || next.getClassName().startsWith(PendingAntiCompaction.class.getName()))\n                        return Access.IGNORE;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.repair\") || ActiveRepairService.class.getName().startsWith(next.getClassName()))\n                        return Access.REJECT;\n                }\n                return Access.IGNORE;\n            });\n            Thread current = Thread.currentThread();\n            switch (access)\n            {\n                case IGNORE:\n                    return;\n                case REJECT:\n                    throw new IllegalStateException(\"Rejecting access\");\n                case MAIN_THREAD_ONLY:\n                    if (!OWNERS.contains(current)) throw new IllegalStateException(\"Accessed in wrong thread: \" + current);\n                    break;\n            }\n        }\n    }\n\n    static class SimulatedFault extends RuntimeException\n    {\n        SimulatedFault(String message)\n        {\n            super(message);\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.repair;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.StandardCharsets;\nimport java.sql.Timestamp;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.function.BiFunction;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.LongSupplier;\nimport java.util.function.Supplier;\nimport javax.annotation.Nullable;\n\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Maps;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\n\nimport accord.utils.DefaultRandom;\nimport accord.utils.Gen;\nimport accord.utils.Gens;\nimport accord.utils.RandomSource;\nimport org.agrona.collections.LongHashSet;\nimport org.apache.cassandra.concurrent.ExecutorBuilder;\nimport org.apache.cassandra.concurrent.ExecutorBuilderFactory;\nimport org.apache.cassandra.concurrent.ExecutorFactory;\nimport org.apache.cassandra.concurrent.ExecutorPlus;\nimport org.apache.cassandra.concurrent.InfiniteLoopExecutor;\nimport org.apache.cassandra.concurrent.Interruptible;\nimport org.apache.cassandra.concurrent.ScheduledExecutorPlus;\nimport org.apache.cassandra.concurrent.SequentialExecutorPlus;\nimport org.apache.cassandra.concurrent.SimulatedExecutorFactory;\nimport org.apache.cassandra.concurrent.Stage;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.config.UnitConfigOverride;\nimport org.apache.cassandra.cql3.CQLTester;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Digest;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.compaction.ICompactionManager;\nimport org.apache.cassandra.db.marshal.EmptyType;\nimport org.apache.cassandra.db.repair.CassandraTableRepairManager;\nimport org.apache.cassandra.db.repair.PendingAntiCompaction;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.dht.Range;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.exceptions.RequestFailureReason;\nimport org.apache.cassandra.gms.ApplicationState;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.HeartBeatState;\nimport org.apache.cassandra.gms.IEndpointStateChangeSubscriber;\nimport org.apache.cassandra.gms.IFailureDetector;\nimport org.apache.cassandra.gms.IGossiper;\nimport org.apache.cassandra.gms.VersionedValue;\nimport org.apache.cassandra.io.util.DataInputBuffer;\nimport org.apache.cassandra.io.util.DataOutputBuffer;\nimport org.apache.cassandra.locator.IEndpointSnitch;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.LocalStrategy;\nimport org.apache.cassandra.locator.RangesAtEndpoint;\nimport org.apache.cassandra.locator.TokenMetadata;\nimport org.apache.cassandra.net.ConnectionType;\nimport org.apache.cassandra.net.IVerbHandler;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.net.MessageDelivery;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.net.RequestCallback;\nimport org.apache.cassandra.repair.messages.RepairMessage;\nimport org.apache.cassandra.repair.messages.RepairOption;\nimport org.apache.cassandra.repair.messages.ValidationResponse;\nimport org.apache.cassandra.repair.state.Completable;\nimport org.apache.cassandra.repair.state.CoordinatorState;\nimport org.apache.cassandra.repair.state.JobState;\nimport org.apache.cassandra.repair.state.SessionState;\nimport org.apache.cassandra.repair.state.ValidationState;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.SystemDistributedKeyspace;\nimport org.apache.cassandra.schema.TableId;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.ActiveRepairService;\nimport org.apache.cassandra.service.PendingRangeCalculatorService;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupComplete;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupHistory;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupRequest;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosCleanupResponse;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosRepairState;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosFinishPrepareCleanup;\nimport org.apache.cassandra.service.paxos.cleanup.PaxosStartPrepareCleanup;\nimport org.apache.cassandra.streaming.StreamEventHandler;\nimport org.apache.cassandra.streaming.StreamReceiveException;\nimport org.apache.cassandra.streaming.StreamSession;\nimport org.apache.cassandra.streaming.StreamState;\nimport org.apache.cassandra.streaming.StreamingChannel;\nimport org.apache.cassandra.streaming.StreamingDataInputPlus;\nimport org.apache.cassandra.tools.nodetool.Repair;\nimport org.apache.cassandra.utils.AbstractTypeGenerators;\nimport org.apache.cassandra.utils.CassandraGenerators;\nimport org.apache.cassandra.utils.Clock;\nimport org.apache.cassandra.utils.Closeable;\nimport org.apache.cassandra.utils.FailingBiConsumer;\nimport org.apache.cassandra.utils.Generators;\nimport org.apache.cassandra.utils.MBeanWrapper;\nimport org.apache.cassandra.utils.MerkleTree;\nimport org.apache.cassandra.utils.MerkleTrees;\nimport org.apache.cassandra.utils.NoSpamLogger;\nimport org.apache.cassandra.utils.concurrent.AsyncPromise;\nimport org.apache.cassandra.utils.concurrent.Future;\nimport org.apache.cassandra.utils.concurrent.ImmediateFuture;\nimport org.apache.cassandra.utils.progress.ProgressEventType;\nimport org.assertj.core.api.Assertions;\nimport org.mockito.Mockito;\nimport org.quicktheories.impl.JavaRandom;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.CLOCK_GLOBAL;\nimport static org.apache.cassandra.config.CassandraRelevantProperties.ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION;\n\npublic abstract class FuzzTestBase extends CQLTester.InMemory\n{\n    private static final int MISMATCH_NUM_PARTITIONS = 1;\n    private static final Gen<String> IDENTIFIER_GEN = fromQT(Generators.IDENTIFIER_GEN);\n    private static final Gen<String> KEYSPACE_NAME_GEN = fromQT(CassandraGenerators.KEYSPACE_NAME_GEN);\n    private static final Gen<TableId> TABLE_ID_GEN = fromQT(CassandraGenerators.TABLE_ID_GEN);\n    private static final Gen<InetAddressAndPort> ADDRESS_W_PORT = fromQT(CassandraGenerators.INET_ADDRESS_AND_PORT_GEN);\n\n    private static boolean SETUP_SCHEMA = false;\n    static String KEYSPACE;\n    static List<String> TABLES;\n\n    @BeforeClass\n    public static void setUpClass()\n    {\n        ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION.setBoolean(true);\n        CLOCK_GLOBAL.setString(ClockAccess.class.getName());\n        // when running in CI an external actor will replace the test configs based off the test type (such as trie, cdc, etc.), this could then have failing tests\n        // that do not repo with the same seed!  To fix that, go to UnitConfigOverride and update the config type to match the one that failed in CI, this should then\n        // use the same config, so the seed should not reproduce.\n        UnitConfigOverride.maybeOverrideConfig();\n\n        DatabaseDescriptor.daemonInitialization();\n        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance); // TOOD (coverage): random select\n        DatabaseDescriptor.setLocalDataCenter(\"test\");\n        StreamingChannel.Factory.Global.unsafeSet(new StreamingChannel.Factory()\n        {\n            private final AtomicInteger counter = new AtomicInteger();\n\n            @Override\n            public StreamingChannel create(InetSocketAddress to, int messagingVersion, StreamingChannel.Kind kind) throws IOException\n            {\n                StreamingChannel mock = Mockito.mock(StreamingChannel.class);\n                int id = counter.incrementAndGet();\n                StreamSession session = Mockito.mock(StreamSession.class);\n                StreamReceiveException access = new StreamReceiveException(session, \"mock access rejected\");\n                StreamingDataInputPlus input = Mockito.mock(StreamingDataInputPlus.class, invocationOnMock -> {\n                    throw access;\n                });\n                Mockito.doNothing().when(input).close();\n                Mockito.when(mock.in()).thenReturn(input);\n                Mockito.when(mock.id()).thenReturn(id);\n                Mockito.when(mock.peer()).thenReturn(to);\n                Mockito.when(mock.connectedTo()).thenReturn(to);\n                Mockito.when(mock.send(Mockito.any())).thenReturn(ImmediateFuture.success(null));\n                Mockito.when(mock.close()).thenReturn(ImmediateFuture.success(null));\n                return mock;\n            }\n        });\n        ExecutorFactory delegate = ExecutorFactory.Global.executorFactory();\n        ExecutorFactory.Global.unsafeSet(new ExecutorFactory()\n        {\n            @Override\n            public LocalAwareSubFactory localAware()\n            {\n                return delegate.localAware();\n            }\n\n            @Override\n            public ScheduledExecutorPlus scheduled(boolean executeOnShutdown, String name, int priority, SimulatorSemantics simulatorSemantics)\n            {\n                return delegate.scheduled(executeOnShutdown, name, priority, simulatorSemantics);\n            }\n\n            private boolean shouldMock()\n            {\n                return StackWalker.getInstance().walk(frame -> {\n                    StackWalker.StackFrame caller = frame.skip(3).findFirst().get();\n                    return caller.getClassName().startsWith(\"org.apache.cassandra.streaming.\");\n                });\n            }\n\n            @Override\n            public Thread startThread(String name, Runnable runnable, InfiniteLoopExecutor.Daemon daemon)\n            {\n                if (shouldMock()) return new Thread();\n                return delegate.startThread(name, runnable, daemon);\n            }\n\n            @Override\n            public Interruptible infiniteLoop(String name, Interruptible.Task task, InfiniteLoopExecutor.SimulatorSafe simulatorSafe, InfiniteLoopExecutor.Daemon daemon, InfiniteLoopExecutor.Interrupts interrupts)\n            {\n                return delegate.infiniteLoop(name, task, simulatorSafe, daemon, interrupts);\n            }\n\n            @Override\n            public ThreadGroup newThreadGroup(String name)\n            {\n                return delegate.newThreadGroup(name);\n            }\n\n            @Override\n            public ExecutorBuilderFactory<ExecutorPlus, SequentialExecutorPlus> withJmx(String jmxPath)\n            {\n                return delegate.withJmx(jmxPath);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends SequentialExecutorPlus> configureSequential(String name)\n            {\n                return delegate.configureSequential(name);\n            }\n\n            @Override\n            public ExecutorBuilder<? extends ExecutorPlus> configurePooled(String name, int threads)\n            {\n                return delegate.configurePooled(name, threads);\n            }\n        });\n\n        // will both make sure this is loaded and used\n        if (!(Clock.Global.clock() instanceof ClockAccess)) throw new IllegalStateException(\"Unable to override clock\");\n\n        // set the repair rcp timeout high so we don't hit it... this class is mostly testing repair reaching success\n        // so don't want to deal with unlucky histories...\n        DatabaseDescriptor.setRepairRpcTimeout(TimeUnit.DAYS.toMillis(1));\n\n\n        InMemory.setUpClass();\n    }\n\n    @Before\n    public void setupSchema()\n    {\n        if (SETUP_SCHEMA) return;\n        SETUP_SCHEMA = true;\n        // StorageService can not be mocked out, nor can ColumnFamilyStores, so make sure that the keyspace is a \"local\" keyspace to avoid replication as the peers don't actually exist for replication\n        schemaChange(String.format(\"CREATE KEYSPACE %s WITH REPLICATION = {'class': '%s'}\", SchemaConstants.DISTRIBUTED_KEYSPACE_NAME, HackStrat.class.getName()));\n        for (TableMetadata table : SystemDistributedKeyspace.metadata().tables)\n            schemaChange(table.toCqlString(false, false));\n\n        createSchema();\n    }\n\n    protected void cleanupRepairTables()\n    {\n        for (String table : Arrays.asList(SystemKeyspace.REPAIRS))\n            execute(String.format(\"TRUNCATE %s.%s\", SchemaConstants.SYSTEM_KEYSPACE_NAME, table));\n    }\n\n    private void createSchema()\n    {\n        // The main reason to use random here with a fixed seed is just to have a set of tables that are not hard coded.\n        // The tables will have diversity to them that most likely doesn't matter to repair (hence why the tables are shared), but\n        // is useful just in case some assumptions change.\n        RandomSource rs = new DefaultRandom(42);\n        String ks = KEYSPACE_NAME_GEN.next(rs);\n        List<String> tableNames = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(10, 100).next(rs);\n        JavaRandom qt = new JavaRandom(rs.asJdkRandom());\n        Tables.Builder tableBuilder = Tables.builder();\n        List<TableId> ids = Gens.lists(TABLE_ID_GEN).unique().ofSize(tableNames.size()).next(rs);\n        for (int i = 0; i < tableNames.size(); i++)\n        {\n            String name = tableNames.get(i);\n            TableId id = ids.get(i);\n            TableMetadata tableMetadata = new CassandraGenerators.TableMetadataBuilder().withKeyspaceName(ks).withTableName(name).withTableId(id).withTableKinds(TableMetadata.Kind.REGULAR)\n                                                                                        // shouldn't matter, just wanted to avoid UDT as that needs more setup\n                                                                                        .withDefaultTypeGen(AbstractTypeGenerators.builder().withTypeKinds(AbstractTypeGenerators.TypeKind.PRIMITIVE).withoutPrimitive(EmptyType.instance).build()).build().generate(qt);\n            tableBuilder.add(tableMetadata);\n        }\n        KeyspaceParams params = KeyspaceParams.simple(3);\n        KeyspaceMetadata metadata = KeyspaceMetadata.create(ks, params, tableBuilder.build());\n\n        // create\n        schemaChange(metadata.toCqlString(false, false));\n        KEYSPACE = ks;\n        for (TableMetadata table : metadata.tables)\n            schemaChange(table.toCqlString(false, false));\n        TABLES = tableNames;\n    }\n\n    static void enableMessageFaults(Cluster cluster)\n    {\n        cluster.allowedMessageFaults(new BiFunction<>()\n        {\n            private final LongHashSet noFaults = new LongHashSet();\n            private final LongHashSet allowDrop = new LongHashSet();\n\n            @Override\n            public Set<Faults> apply(Cluster.Node node, Message<?> message)\n            {\n                if (RepairMessage.ALLOWS_RETRY.contains(message.verb()))\n                {\n                    allowDrop.add(message.id());\n                    return Faults.DROPPED;\n                }\n                switch (message.verb())\n                {\n                    // these messages are not resilent to ephemeral issues\n                    case STATUS_REQ:\n                    case STATUS_RSP:\n                    // paxos repair does not support faults and will cause a TIMEOUT error, failing the repair\n                    case PAXOS2_CLEANUP_COMPLETE_REQ:\n                    case PAXOS2_CLEANUP_REQ:\n                    case PAXOS2_CLEANUP_RSP2:\n                    case PAXOS2_CLEANUP_START_PREPARE_REQ:\n                    case PAXOS2_CLEANUP_FINISH_PREPARE_REQ:\n                        noFaults.add(message.id());\n                        return Faults.NONE;\n                    default:\n                        if (noFaults.contains(message.id())) return Faults.NONE;\n                        if (allowDrop.contains(message.id())) return Faults.DROPPED;\n                        // was a new message added and the test not updated?\n                        IllegalStateException e = new IllegalStateException(\"Verb: \" + message.verb());\n                        cluster.failures.add(e);\n                        throw e;\n                }\n            }\n        });\n    }\n\n    static void runAndAssertSuccess(Cluster cluster, int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        cluster.processAll();\n        assertSuccess(example, shouldSync, repair);\n    }\n\n    static void assertSuccess(int example, boolean shouldSync, RepairCoordinator repair)\n    {\n        Completable.Result result = repair.state.getResult();\n        Assertions.assertThat(result)\n                  .describedAs(\"Expected repair to have completed with success, but is still running... %s; example %d\", repair.state, example).isNotNull()\n                  .describedAs(\"Unexpected state: %s -> %s; example %d\", repair.state, result, example).isEqualTo(Completable.Result.success(repairSuccessMessage(repair)));\n        Assertions.assertThat(repair.state.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(CoordinatorState.State.class));\n        Assertions.assertThat(repair.state.getSessions()).isNotEmpty();\n        boolean shouldSnapshot = repair.state.options.getParallelism() != RepairParallelism.PARALLEL\n                                 && (!repair.state.options.isIncremental() || repair.state.options.isPreview());\n        for (SessionState session : repair.state.getSessions())\n        {\n            Assertions.assertThat(session.getStateTimesMillis().keySet()).isEqualTo(EnumSet.allOf(SessionState.State.class));\n            Assertions.assertThat(session.getJobs()).isNotEmpty();\n            for (JobState job : session.getJobs())\n            {\n                EnumSet<JobState.State> expected = EnumSet.allOf(JobState.State.class);\n                if (!shouldSnapshot)\n                {\n                    expected.remove(JobState.State.SNAPSHOT_START);\n                    expected.remove(JobState.State.SNAPSHOT_COMPLETE);\n                }\n                if (!shouldSync)\n                {\n                    expected.remove(JobState.State.STREAM_START);\n                }\n                Set<JobState.State> actual = job.getStateTimesMillis().keySet();\n                Assertions.assertThat(actual).isEqualTo(expected);\n            }\n        }\n    }\n\n    static String repairSuccessMessage(RepairCoordinator repair)\n    {\n        RepairOption options = repair.state.options;\n        if (options.isPreview())\n        {\n            String suffix;\n            switch (options.getPreviewKind())\n            {\n                case UNREPAIRED:\n                case ALL:\n                    suffix = \"Previewed data was in sync\";\n                    break;\n                case REPAIRED:\n                    suffix = \"Repaired data is in sync\";\n                    break;\n                default:\n                    throw new IllegalArgumentException(\"Unexpected preview repair kind: \" + options.getPreviewKind());\n            }\n            return \"Repair preview completed successfully; \" + suffix;\n        }\n        return \"Repair completed successfully\";\n    }\n\n    InetAddressAndPort pickParticipant(RandomSource rs, Cluster.Node coordinator, RepairCoordinator repair)\n    {\n        if (repair.state.isComplete())\n            throw new IllegalStateException(\"Repair is completed! \" + repair.state.getResult());\n        List<InetAddressAndPort> participaents = new ArrayList<>(repair.state.getNeighborsAndRanges().participants.size() + 1);\n        if (rs.nextBoolean()) participaents.add(coordinator.broadcastAddressAndPort());\n        participaents.addAll(repair.state.getNeighborsAndRanges().participants);\n        participaents.sort(Comparator.naturalOrder());\n\n        InetAddressAndPort selected = rs.pick(participaents);\n        return selected;\n    }\n\n    static void addMismatch(RandomSource rs, ColumnFamilyStore cfs, Validator validator)\n    {\n        ValidationState state = validator.state;\n        int maxDepth = DatabaseDescriptor.getRepairSessionMaxTreeDepth();\n        state.phase.start(MISMATCH_NUM_PARTITIONS, 1024);\n\n        MerkleTrees trees = new MerkleTrees(cfs.getPartitioner());\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            int depth = (int) Math.min(Math.ceil(Math.log(MISMATCH_NUM_PARTITIONS) / Math.log(2)), maxDepth);\n            trees.addMerkleTree((int) Math.pow(2, depth), range);\n        }\n        Set<Token> allTokens = new HashSet<>();\n        for (Range<Token> range : validator.desc.ranges)\n        {\n            Gen<Token> gen = fromQT(CassandraGenerators.tokensInRange(range));\n            Set<Token> tokens = new LinkedHashSet<>();\n            for (int i = 0, size = rs.nextInt(1, 10); i < size; i++)\n            {\n                for (int attempt = 0; !tokens.add(gen.next(rs)) && attempt < 5; attempt++)\n                {\n                }\n            }\n            // tokens may or may not be of the expected size; this depends on how wide the range is\n            for (Token token : tokens)\n                trees.split(token);\n            allTokens.addAll(tokens);\n        }\n        for (Token token : allTokens)\n        {\n            findCorrectRange(trees, token, range -> {\n                Digest digest = Digest.forValidator();\n                digest.update(ByteBuffer.wrap(token.toString().getBytes(StandardCharsets.UTF_8)));\n                range.addHash(new MerkleTree.RowHash(token, digest.digest(), 1));\n            });\n        }\n        state.partitionsProcessed++;\n        state.bytesRead = 1024;\n        state.phase.sendingTrees();\n        Stage.ANTI_ENTROPY.execute(() -> {\n            state.phase.success();\n            validator.respond(new ValidationResponse(validator.desc, trees));\n        });\n    }\n\n    private static void findCorrectRange(MerkleTrees trees, Token token, Consumer<MerkleTree.TreeRange> fn)\n    {\n        MerkleTrees.TreeRangeIterator it = trees.rangeIterator();\n        while (it.hasNext())\n        {\n            MerkleTree.TreeRange next = it.next();\n            if (next.contains(token))\n            {\n                fn.accept(next);\n                return;\n            }\n        }\n    }\n\n    private enum RepairType\n    {FULL, IR}\n\n    private enum PreviewType\n    {NONE, REPAIRED, UNREPAIRED}\n\n    static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, List<String> tableNames)\n    {\n        return repairOption(rs, coordinator, ks, Gens.lists(Gens.pick(tableNames)).ofSizeBetween(1, tableNames.size()), Gens.enums().all(RepairType.class), Gens.enums().all(PreviewType.class), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption irOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.IR), Gens.constant(PreviewType.NONE), Gens.enums().all(RepairParallelism.class));\n    }\n\n    static RepairOption previewOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen)\n    {\n        return repairOption(rs, coordinator, ks, tablesGen, Gens.constant(RepairType.FULL), Gens.constant(PreviewType.REPAIRED), Gens.enums().all(RepairParallelism.class));\n    }\n\n    private static RepairOption repairOption(RandomSource rs, Cluster.Node coordinator, String ks, Gen<List<String>> tablesGen, Gen<RepairType> repairTypeGen, Gen<PreviewType> previewTypeGen, Gen<RepairParallelism> repairParallelismGen)\n    {\n        List<String> args = new ArrayList<>();\n        args.add(ks);\n        args.addAll(tablesGen.next(rs));\n        args.add(\"-pr\");\n        RepairType type = repairTypeGen.next(rs);\n        switch (type)\n        {\n            case IR:\n                // default\n                break;\n            case FULL:\n                args.add(\"--full\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported repair type: \" + type);\n        }\n        PreviewType previewType = previewTypeGen.next(rs);\n        switch (previewType)\n        {\n            case NONE:\n                break;\n            case REPAIRED:\n                args.add(\"--validate\");\n                break;\n            case UNREPAIRED:\n                args.add(\"--preview\");\n                break;\n            default:\n                throw new AssertionError(\"Unsupported preview type: \" + previewType);\n        }\n        RepairParallelism parallelism = repairParallelismGen.next(rs);\n        switch (parallelism)\n        {\n            case SEQUENTIAL:\n                args.add(\"--sequential\");\n                break;\n            case PARALLEL:\n                // default\n                break;\n            case DATACENTER_AWARE:\n                args.add(\"--dc-parallel\");\n                break;\n            default:\n                throw new AssertionError(\"Unknown parallelism: \" + parallelism);\n        }\n        if (rs.nextBoolean()) args.add(\"--optimise-streams\");\n        RepairOption options = RepairOption.parse(Repair.parseOptionMap(() -> \"test\", args), DatabaseDescriptor.getPartitioner());\n        if (options.getRanges().isEmpty())\n        {\n            if (options.isPrimaryRange())\n            {\n                // when repairing only primary range, neither dataCenters nor hosts can be set\n                if (options.getDataCenters().isEmpty() && options.getHosts().isEmpty())\n                    options.getRanges().addAll(coordinator.getPrimaryRanges(ks));\n                    // except dataCenters only contain local DC (i.e. -local)\n                else if (options.isInLocalDCOnly())\n                    options.getRanges().addAll(coordinator.getPrimaryRangesWithinDC(ks));\n                else\n                    throw new IllegalArgumentException(\"You need to run primary range repair on all nodes in the cluster.\");\n            }\n            else\n            {\n                Iterables.addAll(options.getRanges(), coordinator.getLocalReplicas(ks).onlyFull().ranges());\n            }\n        }\n        return options;\n    }\n\n    enum Faults\n    {\n        DELAY, DROP;\n\n        public static final Set<Faults> NONE = Collections.emptySet();\n        public static final Set<Faults> DROPPED = EnumSet.of(DELAY, DROP);\n    }\n\n    private static class Connection\n    {\n        final InetAddressAndPort from, to;\n\n        private Connection(InetAddressAndPort from, InetAddressAndPort to)\n        {\n            this.from = from;\n            this.to = to;\n        }\n\n        @Override\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Connection that = (Connection) o;\n            return from.equals(that.from) && to.equals(that.to);\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(from, to);\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"Connection{\" + \"from=\" + from + \", to=\" + to + '}';\n        }\n    }\n\n    interface MessageListener\n    {\n        default void preHandle(Cluster.Node node, Message<?> msg) {}\n    }\n\n    static class Cluster\n    {\n        private static final FailingBiConsumer<ColumnFamilyStore, Validator> DEFAULT_VALIDATION = ValidationManager::doValidation;\n\n        final Map<InetAddressAndPort, Node> nodes;\n        private final IFailureDetector failureDetector = Mockito.mock(IFailureDetector.class);\n        private final IEndpointSnitch snitch = Mockito.mock(IEndpointSnitch.class);\n        private final SimulatedExecutorFactory globalExecutor;\n        final ScheduledExecutorPlus unorderedScheduled;\n        final ExecutorPlus orderedExecutor;\n        private final Gossip gossiper = new Gossip();\n        private final MBeanWrapper mbean = Mockito.mock(MBeanWrapper.class);\n        private final List<Throwable> failures = new ArrayList<>();\n        private final List<MessageListener> listeners = new ArrayList<>();\n        private final RandomSource rs;\n        private BiFunction<Node, Message<?>, Set<Faults>> allowedMessageFaults = (a, b) -> Collections.emptySet();\n\n        private final Map<Connection, LongSupplier> networkLatencies = new HashMap<>();\n        private final Map<Connection, Supplier<Boolean>> networkDrops = new HashMap<>();\n\n        Cluster(RandomSource rs)\n        {\n            ClockAccess.includeThreadAsOwner();\n            this.rs = rs;\n            globalExecutor = new SimulatedExecutorFactory(rs, fromQT(Generators.TIMESTAMP_GEN.map(Timestamp::getTime)).mapToLong(TimeUnit.MILLISECONDS::toNanos).next(rs));\n            orderedExecutor = globalExecutor.configureSequential(\"ignore\").build();\n            unorderedScheduled = globalExecutor.scheduled(\"ignored\");\n\n\n\n            // We run tests in an isolated JVM per class, so not cleaing up is safe... but if that assumption ever changes, will need to cleanup\n            Stage.ANTI_ENTROPY.unsafeSetExecutor(orderedExecutor);\n            Stage.MISC.unsafeSetExecutor(orderedExecutor);\n            Stage.INTERNAL_RESPONSE.unsafeSetExecutor(unorderedScheduled);\n            Mockito.when(failureDetector.isAlive(Mockito.any())).thenReturn(true);\n            Thread expectedThread = Thread.currentThread();\n            NoSpamLogger.unsafeSetClock(() -> {\n                if (Thread.currentThread() != expectedThread)\n                    throw new AssertionError(\"NoSpamLogger.Clock accessed outside of fuzzing...\");\n                return globalExecutor.nanoTime();\n            });\n\n            int numNodes = rs.nextInt(3, 10);\n            List<String> dcs = Gens.lists(IDENTIFIER_GEN).unique().ofSizeBetween(1, Math.min(10, numNodes)).next(rs);\n            Map<InetAddressAndPort, Node> nodes = Maps.newHashMapWithExpectedSize(numNodes);\n            Gen<Token> tokenGen = fromQT(CassandraGenerators.token(DatabaseDescriptor.getPartitioner()));\n            Gen<UUID> hostIdGen = fromQT(Generators.UUID_RANDOM_GEN);\n            Set<Token> tokens = new HashSet<>();\n            Set<UUID> hostIds = new HashSet<>();\n            for (int i = 0; i < numNodes; i++)\n            {\n                InetAddressAndPort addressAndPort = ADDRESS_W_PORT.next(rs);\n                while (nodes.containsKey(addressAndPort)) addressAndPort = ADDRESS_W_PORT.next(rs);\n                Token token;\n                while (!tokens.add(token = tokenGen.next(rs)))\n                {\n                }\n                UUID hostId;\n                while (!hostIds.add(hostId = hostIdGen.next(rs)))\n                {\n                }\n\n                String dc = rs.pick(dcs);\n                String rack = \"rack\";\n                Mockito.when(snitch.getDatacenter(Mockito.eq(addressAndPort))).thenReturn(dc);\n                Mockito.when(snitch.getRack(Mockito.eq(addressAndPort))).thenReturn(rack);\n\n                VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(DatabaseDescriptor.getPartitioner());\n                EndpointState state = new EndpointState(new HeartBeatState(42, 42));\n                state.addApplicationState(ApplicationState.STATUS, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.STATUS_WITH_PORT, valueFactory.normal(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.HOST_ID, valueFactory.hostId(hostId));\n                state.addApplicationState(ApplicationState.TOKENS, valueFactory.tokens(Collections.singleton(token)));\n                state.addApplicationState(ApplicationState.DC, valueFactory.datacenter(dc));\n                state.addApplicationState(ApplicationState.RACK, valueFactory.rack(rack));\n                state.addApplicationState(ApplicationState.RELEASE_VERSION, valueFactory.releaseVersion());\n\n                gossiper.endpoints.put(addressAndPort, state);\n\n                Node node = new Node(hostId, addressAndPort, Collections.singletonList(token), new Messaging(addressAndPort));\n                nodes.put(addressAndPort, node);\n            }\n            this.nodes = nodes;\n\n            TokenMetadata tm = StorageService.instance.getTokenMetadata();\n            tm.clearUnsafe();\n            for (Node inst : nodes.values())\n            {\n                tm.updateHostId(inst.hostId(), inst.broadcastAddressAndPort());\n                for (Token token : inst.tokens())\n                    tm.updateNormalToken(token, inst.broadcastAddressAndPort());\n            }\n        }\n\n        public Closeable addListener(MessageListener listener)\n        {\n            listeners.add(listener);\n            return () -> removeListener(listener);\n        }\n\n        public void removeListener(MessageListener listener)\n        {\n            listeners.remove(listener);\n        }\n\n        public void allowedMessageFaults(BiFunction<Node, Message<?>, Set<Faults>> fn)\n        {\n            this.allowedMessageFaults = fn;\n        }\n\n        public void checkFailures()\n        {\n            if (Thread.interrupted())\n                failures.add(new InterruptedException());\n            if (failures.isEmpty()) return;\n            AssertionError error = new AssertionError(\"Unexpected exceptions found\");\n            failures.forEach(error::addSuppressed);\n            failures.clear();\n            throw error;\n        }\n\n        public boolean processOne()\n        {\n            boolean result = globalExecutor.processOne();\n            checkFailures();\n            return result;\n        }\n\n        public void processAll()\n        {\n            while (processOne())\n            {\n            }\n        }\n\n        private class CallbackContext\n        {\n            final RequestCallback callback;\n\n            private CallbackContext(RequestCallback callback)\n            {\n                this.callback = Objects.requireNonNull(callback);\n            }\n\n            public void onResponse(Message msg)\n            {\n                callback.onResponse(msg);\n            }\n\n            public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n            {\n                if (callback.invokeOnFailure()) callback.onFailure(from, failureReason);\n            }\n        }\n\n        private static class CallbackKey\n        {\n            private final long id;\n            private final InetAddressAndPort peer;\n\n            private CallbackKey(long id, InetAddressAndPort peer)\n            {\n                this.id = id;\n                this.peer = peer;\n            }\n\n            @Override\n            public boolean equals(Object o)\n            {\n                if (this == o) return true;\n                if (o == null || getClass() != o.getClass()) return false;\n                CallbackKey that = (CallbackKey) o;\n                return id == that.id && peer.equals(that.peer);\n            }\n\n            @Override\n            public int hashCode()\n            {\n                return Objects.hash(id, peer);\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"CallbackKey{\" +\n                       \"id=\" + id +\n                       \", peer=\" + peer +\n                       '}';\n            }\n        }\n\n        private class Messaging implements MessageDelivery\n        {\n            final InetAddressAndPort broadcastAddressAndPort;\n            final Map<CallbackKey, CallbackContext> callbacks = new HashMap<>();\n\n            private Messaging(InetAddressAndPort broadcastAddressAndPort)\n            {\n                this.broadcastAddressAndPort = broadcastAddressAndPort;\n            }\n\n            @Override\n            public <REQ> void send(Message<REQ> message, InetAddressAndPort to)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, null);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            @Override\n            public <REQ, RSP> void sendWithCallback(Message<REQ> message, InetAddressAndPort to, RequestCallback<RSP> cb, ConnectionType specifyConnection)\n            {\n                message = message.withFrom(broadcastAddressAndPort);\n                maybeEnqueue(message, to, cb);\n            }\n\n            private <REQ, RSP> void maybeEnqueue(Message<REQ> message, InetAddressAndPort to, @Nullable RequestCallback<RSP> callback)\n            {\n                CallbackContext cb;\n                if (callback != null)\n                {\n                    CallbackKey key = new CallbackKey(message.id(), to);\n                    if (callbacks.containsKey(key))\n                        throw new AssertionError(\"Message id \" + message.id() + \" to \" + to + \" already has a callback\");\n                    cb = new CallbackContext(callback);\n                    callbacks.put(key, cb);\n                }\n                else\n                {\n                    cb = null;\n                }\n                boolean toSelf = this.broadcastAddressAndPort.equals(to);\n                Node node = nodes.get(to);\n                Set<Faults> allowedFaults = allowedMessageFaults.apply(node, message);\n                if (allowedFaults.isEmpty())\n                {\n                    // enqueue so stack overflow doesn't happen with the inlining\n                    unorderedScheduled.submit(() -> node.handle(message));\n                }\n                else\n                {\n                    Runnable enqueue = () -> {\n                        if (!allowedFaults.contains(Faults.DELAY))\n                        {\n                            unorderedScheduled.submit(() -> node.handle(message));\n                        }\n                        else\n                        {\n                            if (toSelf) unorderedScheduled.submit(() -> node.handle(message));\n                            else\n                                unorderedScheduled.schedule(() -> node.handle(message), networkJitterNanos(to), TimeUnit.NANOSECONDS);\n                        }\n                    };\n\n                    if (!allowedFaults.contains(Faults.DROP)) enqueue.run();\n                    else\n                    {\n                        if (!toSelf && networkDrops(to))\n                        {\n//                            logger.warn(\"Dropped message {}\", message);\n                            // drop\n                        }\n                        else\n                        {\n                            enqueue.run();\n                        }\n                    }\n\n                    if (cb != null)\n                    {\n                        unorderedScheduled.schedule(() -> {\n                            CallbackContext ctx = callbacks.remove(new CallbackKey(message.id(), to));\n                            if (ctx != null)\n                            {\n                                assert ctx == cb;\n                                try\n                                {\n                                    ctx.onFailure(to, RequestFailureReason.TIMEOUT);\n                                }\n                                catch (Throwable t)\n                                {\n                                    failures.add(t);\n                                }\n                            }\n                        }, message.verb().expiresAfterNanos(), TimeUnit.NANOSECONDS);\n                    }\n                }\n            }\n\n            private long networkJitterNanos(InetAddressAndPort to)\n            {\n                return networkLatencies.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> {\n                    long min = TimeUnit.MICROSECONDS.toNanos(500);\n                    long maxSmall = TimeUnit.MILLISECONDS.toNanos(5);\n                    long max = TimeUnit.SECONDS.toNanos(5);\n                    LongSupplier small = () -> rs.nextLong(min, maxSmall);\n                    LongSupplier large = () -> rs.nextLong(maxSmall, max);\n                    return Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).mapToLong(b -> b ? large.getAsLong() : small.getAsLong()).asLongSupplier(rs);\n                }).getAsLong();\n            }\n\n            private boolean networkDrops(InetAddressAndPort to)\n            {\n                return networkDrops.computeIfAbsent(new Connection(broadcastAddressAndPort, to), ignore -> Gens.bools().runs(rs.nextInt(1, 11) / 100.0D, rs.nextInt(3, 15)).asSupplier(rs)).get();\n            }\n\n            @Override\n            public <REQ, RSP> Future<Message<RSP>> sendWithResult(Message<REQ> message, InetAddressAndPort to)\n            {\n                AsyncPromise<Message<RSP>> promise = new AsyncPromise<>();\n                sendWithCallback(message, to, new RequestCallback<RSP>()\n                {\n                    @Override\n                    public void onResponse(Message<RSP> msg)\n                    {\n                        promise.trySuccess(msg);\n                    }\n\n                    @Override\n                    public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)\n                    {\n                        promise.tryFailure(new MessagingService.FailureResponseException(from, failureReason));\n                    }\n\n                    @Override\n                    public boolean invokeOnFailure()\n                    {\n                        return true;\n                    }\n                });\n                return promise;\n            }\n\n            @Override\n            public <V> void respond(V response, Message<?> message)\n            {\n                send(message.responseWith(response), message.respondTo());\n            }\n        }\n\n        private class Gossip implements IGossiper\n        {\n            private final Map<InetAddressAndPort, EndpointState> endpoints = new HashMap<>();\n\n            @Override\n            public void register(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Override\n            public void unregister(IEndpointStateChangeSubscriber subscriber)\n            {\n\n            }\n\n            @Nullable\n            @Override\n            public EndpointState getEndpointStateForEndpoint(InetAddressAndPort ep)\n            {\n                return endpoints.get(ep);\n            }\n\n            @Override\n            public void notifyFailureDetector(Map<InetAddressAndPort, EndpointState> remoteEpStateMap)\n            {\n\n            }\n\n            @Override\n            public void applyStateLocally(Map<InetAddressAndPort, EndpointState> epStateMap)\n            {\n                // If we were testing paxos this would be wrong...\n                // CASSANDRA-18917 added support for simulating Gossip, but gossip issues were found so couldn't merge that patch...\n                // For the paxos repair, since we don't care about paxos messages, this is ok to no-op for now, but if paxos cleanup\n                // ever was to be tested this logic would need to be implemented\n            }\n        }\n\n        class Node implements SharedContext\n        {\n            private final ICompactionManager compactionManager = Mockito.mock(ICompactionManager.class);\n            final UUID hostId;\n            final InetAddressAndPort addressAndPort;\n            final Collection<Token> tokens;\n            final ActiveRepairService activeRepairService;\n            final IVerbHandler verbHandler;\n            final Messaging messaging;\n            final IValidationManager validationManager;\n            private FailingBiConsumer<ColumnFamilyStore, Validator> doValidation = DEFAULT_VALIDATION;\n            final PaxosRepairState paxosRepairState;\n            private final StreamExecutor defaultStreamExecutor = plan -> {\n                long delayNanos = rs.nextLong(TimeUnit.SECONDS.toNanos(5), TimeUnit.MINUTES.toNanos(10));\n                unorderedScheduled.schedule(() -> {\n                    StreamState success = new StreamState(plan.planId(), plan.streamOperation(), Collections.emptySet());\n                    for (StreamEventHandler handler : plan.handlers())\n                        handler.onSuccess(success);\n                }, delayNanos, TimeUnit.NANOSECONDS);\n                return null;\n            };\n            private StreamExecutor streamExecutor = defaultStreamExecutor;\n\n            private Node(UUID hostId, InetAddressAndPort addressAndPort, Collection<Token> tokens, Messaging messaging)\n            {\n                this.hostId = hostId;\n                this.addressAndPort = addressAndPort;\n                this.tokens = tokens;\n                this.messaging = messaging;\n                this.activeRepairService = new ActiveRepairService(this);\n                this.paxosRepairState = new PaxosRepairState(this);\n                this.validationManager = (cfs, validator) -> unorderedScheduled.submit(() -> {\n                    try\n                    {\n                        doValidation.acceptOrFail(cfs, validator);\n                    }\n                    catch (Throwable e)\n                    {\n                        validator.fail(e);\n                    }\n                });\n                this.verbHandler = new IVerbHandler<>()\n                {\n                    private final RepairMessageVerbHandler repairVerbHandler = new RepairMessageVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosStartPrepareCleanup.Request> paxosStartPrepareCleanup = PaxosStartPrepareCleanup.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupRequest> paxosCleanupRequestIVerbHandler = PaxosCleanupRequest.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupHistory> paxosFinishPrepareCleanup = PaxosFinishPrepareCleanup.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupResponse> paxosCleanupResponse = PaxosCleanupResponse.createVerbHandler(Node.this);\n                    private final IVerbHandler<PaxosCleanupComplete.Request> paxosCleanupComplete = PaxosCleanupComplete.createVerbHandler(Node.this);\n                    @Override\n                    public void doVerb(Message message) throws IOException\n                    {\n                        switch (message.verb())\n                        {\n                            case PAXOS2_CLEANUP_START_PREPARE_REQ:\n                                paxosStartPrepareCleanup.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_REQ:\n                                paxosCleanupRequestIVerbHandler.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_FINISH_PREPARE_REQ:\n                                paxosFinishPrepareCleanup.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_RSP2:\n                                paxosCleanupResponse.doVerb(message);\n                                break;\n                            case PAXOS2_CLEANUP_COMPLETE_REQ:\n                                paxosCleanupComplete.doVerb(message);\n                                break;\n                            default:\n                                repairVerbHandler.doVerb(message);\n                        }\n                    }\n                };\n\n                activeRepairService.start();\n            }\n\n            public Closeable doValidation(FailingBiConsumer<ColumnFamilyStore, Validator> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                if (previous != DEFAULT_VALIDATION)\n                    throw new IllegalStateException(\"Attemptted to override validation, but was already overridden\");\n                this.doValidation = fn;\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doValidation(Function<FailingBiConsumer<ColumnFamilyStore, Validator>, FailingBiConsumer<ColumnFamilyStore, Validator>> fn)\n            {\n                FailingBiConsumer<ColumnFamilyStore, Validator> previous = this.doValidation;\n                this.doValidation = fn.apply(previous);\n                return () -> this.doValidation = previous;\n            }\n\n            public Closeable doSync(StreamExecutor streamExecutor)\n            {\n                StreamExecutor previous = this.streamExecutor;\n                if (previous != defaultStreamExecutor)\n                    throw new IllegalStateException(\"Attemptted to override sync, but was already overridden\");\n                this.streamExecutor = streamExecutor;\n                return () -> this.streamExecutor = previous;\n            }\n\n            void handle(Message msg)\n            {\n                msg = serde(msg);\n                if (msg == null)\n                {\n                    logger.warn(\"Got a message that failed to serialize/deserialize\");\n                    return;\n                }\n                for (MessageListener l : listeners)\n                    l.preHandle(this, msg);\n                if (msg.verb().isResponse())\n                {\n                    // handle callbacks\n                    CallbackKey key = new CallbackKey(msg.id(), msg.from());\n                    if (messaging.callbacks.containsKey(key))\n                    {\n                        CallbackContext callback = messaging.callbacks.remove(key);\n                        if (callback == null)\n                            return;\n                        try\n                        {\n                            if (msg.isFailureResponse())\n                                callback.onFailure(msg.from(), (RequestFailureReason) msg.payload);\n                            else callback.onResponse(msg);\n                        }\n                        catch (Throwable t)\n                        {\n                            failures.add(t);\n                        }\n                    }\n                }\n                else\n                {\n                    try\n                    {\n                        verbHandler.doVerb(msg);\n                    }\n                    catch (Throwable e)\n                    {\n                        failures.add(e);\n                    }\n                }\n            }\n\n            public UUID hostId()\n            {\n                return hostId;\n            }\n\n            @Override\n            public InetAddressAndPort broadcastAddressAndPort()\n            {\n                return addressAndPort;\n            }\n\n            public Collection<Token> tokens()\n            {\n                return tokens;\n            }\n\n            public IFailureDetector failureDetector()\n            {\n                return failureDetector;\n            }\n\n            @Override\n            public IEndpointSnitch snitch()\n            {\n                return snitch;\n            }\n\n            @Override\n            public IGossiper gossiper()\n            {\n                return gossiper;\n            }\n\n            @Override\n            public ICompactionManager compactionManager()\n            {\n                return compactionManager;\n            }\n\n            public ExecutorFactory executorFactory()\n            {\n                return globalExecutor;\n            }\n\n            public ScheduledExecutorPlus optionalTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public ScheduledExecutorPlus nonPeriodicTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public ScheduledExecutorPlus scheduledTasks()\n            {\n                return unorderedScheduled;\n            }\n\n            @Override\n            public Supplier<Random> random()\n            {\n                return () -> rs.fork().asJdkRandom();\n            }\n\n            public Clock clock()\n            {\n                return globalExecutor;\n            }\n\n            public MessageDelivery messaging()\n            {\n                return messaging;\n            }\n\n            public MBeanWrapper mbean()\n            {\n                return mbean;\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options)\n            {\n                return repair(ks, options, true);\n            }\n\n            public RepairCoordinator repair(String ks, RepairOption options, boolean addFailureOnErrorNotification)\n            {\n                RepairCoordinator repair = new RepairCoordinator(this, (name, tables) -> StorageService.instance.getValidColumnFamilies(false, false, name, tables), name -> StorageService.instance.getReplicas(name, broadcastAddressAndPort()), 42, options, ks);\n                if (addFailureOnErrorNotification)\n                {\n                    repair.addProgressListener((tag, event) -> {\n                        if (event.getType() == ProgressEventType.ERROR)\n                            failures.add(new AssertionError(event.getMessage()));\n                    });\n                }\n                return repair;\n            }\n\n            public RangesAtEndpoint getLocalReplicas(String ks)\n            {\n                return StorageService.instance.getReplicas(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRanges(String ks)\n            {\n                return StorageService.instance.getPrimaryRangesForEndpoint(ks, broadcastAddressAndPort());\n            }\n\n            public Collection<? extends Range<Token>> getPrimaryRangesWithinDC(String ks)\n            {\n                return StorageService.instance.getPrimaryRangeForEndpointWithinDC(ks, broadcastAddressAndPort());\n            }\n\n            @Override\n            public ActiveRepairService repair()\n            {\n                return activeRepairService;\n            }\n\n            @Override\n            public IValidationManager validationManager()\n            {\n                return validationManager;\n            }\n\n            @Override\n            public TableRepairManager repairManager(ColumnFamilyStore store)\n            {\n                return new CassandraTableRepairManager(store, this)\n                {\n                    @Override\n                    public void snapshot(String name, Collection<Range<Token>> ranges, boolean force)\n                    {\n                        // no-op\n                    }\n                };\n            }\n\n            @Override\n            public StreamExecutor streamExecutor()\n            {\n                return streamExecutor;\n            }\n\n            @Override\n            public PendingRangeCalculatorService pendingRangeCalculator()\n            {\n                return PendingRangeCalculatorService.instance;\n            }\n\n            @Override\n            public PaxosRepairState paxosRepairState()\n            {\n                return paxosRepairState;\n            }\n        }\n\n        private Message serde(Message msg)\n        {\n            try (DataOutputBuffer b = DataOutputBuffer.scratchBuffer.get())\n            {\n                int messagingVersion = MessagingService.current_version;\n                Message.serializer.serialize(msg, b, messagingVersion);\n                DataInputBuffer in = new DataInputBuffer(b.unsafeGetBufferAndFlip(), false);\n                return Message.serializer.deserialize(in, msg.from(), messagingVersion);\n            }\n            catch (Throwable e)\n            {\n                failures.add(e);\n                return null;\n            }\n        }\n    }\n\n    private static <T> Gen<T> fromQT(org.quicktheories.core.Gen<T> qt)\n    {\n        return rs -> {\n            JavaRandom r = new JavaRandom(rs.asJdkRandom());\n            return qt.generate(r);\n        };\n    }\n\n    public static class HackStrat extends LocalStrategy\n    {\n        public HackStrat(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)\n        {\n            super(keyspaceName, tokenMetadata, snitch, configOptions);\n        }\n    }\n\n    /**\n     * Since the clock is accessable via {@link Global#currentTimeMillis()} and {@link Global#nanoTime()}, and only repair subsystem has the requirement not to touch that, this class\n     * acts as a safty check that validates that repair does not touch these methods but allows the other subsystems to do so.\n     */\n    public static class ClockAccess implements Clock\n    {\n        private static final Set<Thread> OWNERS = Collections.synchronizedSet(new HashSet<>());\n        private final Clock delegate = new Default();\n\n        public static void includeThreadAsOwner()\n        {\n            OWNERS.add(Thread.currentThread());\n        }\n\n        @Override\n        public long nanoTime()\n        {\n            checkAccess();\n            return delegate.nanoTime();\n        }\n\n        @Override\n        public long currentTimeMillis()\n        {\n            checkAccess();\n            return delegate.currentTimeMillis();\n        }\n\n        private enum Access\n        {MAIN_THREAD_ONLY, REJECT, IGNORE}\n\n        private void checkAccess()\n        {\n            Access access = StackWalker.getInstance().walk(frames -> {\n                Iterator<StackWalker.StackFrame> it = frames.iterator();\n                boolean topLevel = false;\n                while (it.hasNext())\n                {\n                    StackWalker.StackFrame next = it.next();\n                    if (!topLevel)\n                    {\n                        // need to find the top level!\n                        while (!Clock.Global.class.getName().equals(next.getClassName()))\n                        {\n                            assert it.hasNext();\n                            next = it.next();\n                        }\n                        topLevel = true;\n                        assert it.hasNext();\n                        next = it.next();\n                    }\n                    if (FuzzTestBase.class.getName().equals(next.getClassName())) return Access.MAIN_THREAD_ONLY;\n                    // this is non-deterministic... but since the scope of the work is testing repair and not paxos... this is unblocked for now...\n                    if ((\"org.apache.cassandra.service.paxos.Paxos\".equals(next.getClassName()) && \"newBallot\".equals(next.getMethodName()))\n                        || (\"org.apache.cassandra.service.paxos.uncommitted.PaxosBallotTracker\".equals(next.getClassName()) && \"updateLowBound\".equals(next.getMethodName())))\n                        return Access.MAIN_THREAD_ONLY;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.db.\") || next.getClassName().startsWith(\"org.apache.cassandra.gms.\") || next.getClassName().startsWith(\"org.apache.cassandra.cql3.\") || next.getClassName().startsWith(\"org.apache.cassandra.metrics.\") || next.getClassName().startsWith(\"org.apache.cassandra.utils.concurrent.\")\n                        || next.getClassName().startsWith(\"org.apache.cassandra.utils.TimeUUID\") // this would be good to solve\n                        || next.getClassName().startsWith(PendingAntiCompaction.class.getName()))\n                        return Access.IGNORE;\n                    if (next.getClassName().startsWith(\"org.apache.cassandra.repair\") || ActiveRepairService.class.getName().startsWith(next.getClassName()))\n                        return Access.REJECT;\n                }\n                return Access.IGNORE;\n            });\n            Thread current = Thread.currentThread();\n            switch (access)\n            {\n                case IGNORE:\n                    return;\n                case REJECT:\n                    throw new IllegalStateException(\"Rejecting access\");\n                case MAIN_THREAD_ONLY:\n                    if (!OWNERS.contains(current)) throw new IllegalStateException(\"Accessed in wrong thread: \" + current);\n                    break;\n            }\n        }\n    }\n\n    static class SimulatedFault extends RuntimeException\n    {\n        SimulatedFault(String message)\n        {\n            super(message);\n        }\n    }\n}\n","lineNo":1139}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.util.concurrent.Uninterruptibles;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.BootStrapper;\nimport org.apache.cassandra.exceptions.StartupException;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.gms.NewGossiper;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.log.SystemKeyspaceStorage;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.migration.Election;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.sequences.InProgressSequences;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.sequences.ReplaceSameAddress;\nimport org.apache.cassandra.tcm.transformations.PrepareJoin;\nimport org.apache.cassandra.tcm.transformations.PrepareReplace;\nimport org.apache.cassandra.tcm.transformations.UnsafeJoin;\nimport org.apache.cassandra.tcm.transformations.cms.Initialize;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.LOCAL;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.emptyWithSchemaFromSystemTables;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.fromEndpointStates;\nimport static org.apache.cassandra.tcm.membership.NodeState.JOINED;\nimport static org.apache.cassandra.utils.FBUtilities.getBroadcastAddressAndPort;\n\n /**\n  * Initialize\n  */\n public class Startup\n{\n    private static final Logger logger = LoggerFactory.getLogger(Startup.class);\n\n    public static void initialize(Set<InetAddressAndPort> seeds) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        initialize(seeds,\n                   p -> p,\n                   () -> MessagingService.instance().waitUntilListeningUnchecked());\n    }\n\n    public static void initialize(Set<InetAddressAndPort> seeds,\n                                  Function<Processor, Processor> wrapProcessor,\n                                  Runnable initMessaging) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        switch (StartupMode.get(seeds))\n        {\n            case FIRST_CMS:\n                logger.info(\"Initializing as first CMS node in a new cluster\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeAsFirstCMSNode();\n                initMessaging.run();\n                break;\n            case NORMAL:\n                logger.info(\"Initializing as non CMS node\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initMessaging.run();\n                break;\n            case VOTE:\n                logger.info(\"Initializing for discovery\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeForDiscovery(initMessaging);\n                break;\n            case UPGRADE:\n                logger.info(\"Initializing from gossip\");\n                initializeFromGossip(wrapProcessor, initMessaging);\n                break;\n            case BOOT_WITH_CLUSTERMETADATA:\n                String fileName = CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString();\n                logger.warn(\"Initializing with cluster metadata from: {}\", fileName);\n                reinitializeWithClusterMetadata(fileName, wrapProcessor, initMessaging);\n                break;\n        }\n    }\n\n    /**\n     * Make this node a _first_ CMS node.\n     * <p>\n     * (1) Append PreInitialize transformation to local in-memory log. When distributed metadata keyspace is initialized, a no-op transformation will\n     * be added to other nodes. This is required since as of now, no node actually owns distributed metadata keyspace.\n     * (2) Commit Initialize transformation, which holds a snapshot of metadata as of now.\n     * <p>\n     * This process is applicable for gossip upgrades as well as regular vote-and-startup process.\n     */\n    public static void initializeAsFirstCMSNode()\n    {\n        ClusterMetadataService.instance().log().bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        assert ClusterMetadataService.state() == LOCAL : String.format(\"Can't initialize as node hasn't transitioned to CMS state. State: %s.\\n%s\", ClusterMetadataService.state(), ClusterMetadata.current());\n        Initialize initialize = new Initialize(ClusterMetadata.current());\n        ClusterMetadataService.instance().commit(initialize);\n    }\n\n    public static void initializeAsNonCmsNode(Function<Processor, Processor> wrapProcessor) throws StartupException\n    {\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withDefaultListeners();\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n        ClusterMetadataService.instance().log().ready();\n\n        NodeId nodeId = ClusterMetadata.current().myNodeId();\n        UUID currentHostId = SystemKeyspace.getLocalHostId();\n        if (nodeId != null && !Objects.equals(nodeId.toUUID(), currentHostId))\n        {\n            logger.info(\"NodeId is wrong, updating from {} to {}\", currentHostId, nodeId.toUUID());\n            SystemKeyspace.setLocalHostId(nodeId.toUUID());\n        }\n    }\n\n    public static void scrubDataDirectories(ClusterMetadata metadata) throws StartupException\n    {\n        // clean up debris in the rest of the keyspaces\n        for (KeyspaceMetadata keyspace : metadata.schema.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspace.name.equals(SchemaConstants.SYSTEM_KEYSPACE_NAME))\n                continue;\n\n            for (TableMetadata cfm : keyspace.tables)\n            {\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n            }\n        }\n    }\n\n    public interface AfterReplay\n    {\n        void accept(ClusterMetadata t) throws StartupException;\n    }\n    /**\n     * Initialization for Discovery.\n     *\n     * Node will attempt to discover other participants in the cluster by attempting to contact the seeds\n     * it is aware of. After discovery, the node with a smallest ip address will move to propose itself as\n     * a CMS initiator, and attempt to establish a CMS in via two-phase commit protocol.\n     */\n    public static void initializeForDiscovery(Runnable initMessaging)\n    {\n        initMessaging.run();\n        logger.debug(\"Discovering other nodes in the system\");\n        Discovery.DiscoveredNodes candidates = Discovery.instance.discover();\n        if (candidates.kind() == Discovery.DiscoveredNodes.Kind.KNOWN_PEERS)\n        {\n            logger.debug(\"Got candidates: \" + candidates);\n            Optional<InetAddressAndPort> option = candidates.nodes().stream().min(InetAddressAndPort::compareTo);\n            InetAddressAndPort min;\n            if (!option.isPresent())\n            {\n                if (DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort()))\n                    min = FBUtilities.getBroadcastAddressAndPort();\n                else\n                    throw new IllegalArgumentException(String.format(\"Found no candidates during initialization. Check if the seeds are up: %s\", DatabaseDescriptor.getSeeds()));\n            }\n            else\n            {\n                min = option.get();\n            }\n\n             // identify if you need to start the vote\n            if (min.equals(FBUtilities.getBroadcastAddressAndPort()) || FBUtilities.getBroadcastAddressAndPort().compareTo(min) < 0)\n            {\n                Election.instance.nominateSelf(candidates.nodes(),\n                                               Collections.singleton(FBUtilities.getBroadcastAddressAndPort()),\n                                               (cm) -> true,\n                                               null);\n            }\n        }\n\n        while (!ClusterMetadata.current().epoch.isAfter(Epoch.FIRST))\n        {\n            if (candidates.kind() == Discovery.DiscoveredNodes.Kind.CMS_ONLY)\n            {\n                RemoteProcessor.fetchLogAndWait(new RemoteProcessor.CandidateIterator(candidates.nodes(), false),\n                                                ClusterMetadataService.instance().log());\n            }\n            else\n            {\n                Election.Initiator initiator = Election.instance.initiator();\n                candidates = Discovery.instance.discoverOnce(initiator == null ? null : initiator.initiator);\n            }\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n        }\n\n        assert ClusterMetadata.current().epoch.isAfter(Epoch.FIRST);\n        Election.instance.migrated();\n    }\n\n    /**\n     * This should only be called during startup.\n     */\n    public static void initializeFromGossip(Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws StartupException\n    {\n        ClusterMetadata emptyFromSystemTables = emptyWithSchemaFromSystemTables(SystemKeyspace.allKnownDatacenters());\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withInitialState(emptyFromSystemTables)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners();\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        try\n        {\n            CommitLog.instance.recoverSegmentsOnDisk();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        logger.debug(\"Starting to initialize ClusterMetadata from gossip\");\n        Map<InetAddressAndPort, EndpointState> epStates = NewGossiper.instance.doShadowRound();\n        logger.debug(\"Got epStates {}\", epStates);\n        ClusterMetadata initial = fromEndpointStates(emptyFromSystemTables.schema, epStates);\n        logger.debug(\"Created initial ClusterMetadata {}\", initial);\n        SystemKeyspace.setLocalHostId(initial.myNodeId().toUUID());\n        ClusterMetadataService.instance().setFromGossip(initial);\n        Gossiper.instance.clearUnsafe();\n        Gossiper.instance.maybeInitializeLocalState(SystemKeyspace.incrementAndGetGeneration());\n        for (Map.Entry<NodeId, NodeState> entry : initial.directory.states.entrySet())\n            Gossiper.instance.mergeNodeToGossip(entry.getKey(), initial);\n\n        // double check that everything was added, can remove once we are confident\n        ClusterMetadata cmGossip = fromEndpointStates(emptyFromSystemTables.schema, Gossiper.instance.getEndpointStates());\n        assert cmGossip.equals(initial) : cmGossip + \" != \" + initial;\n    }\n\n    public static void reinitializeWithClusterMetadata(String fileName, Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws IOException, StartupException\n    {\n        ClusterMetadata prev = ClusterMetadata.currentNullable();\n        // First set a minimal ClusterMetadata as some deserialization depends\n        // on ClusterMetadata.current() to access the partitioner\n        StubClusterMetadataService initial = StubClusterMetadataService.forClientTools();\n        ClusterMetadataService.unsetInstance();\n        StubClusterMetadataService.setInstance(initial);\n\n        ClusterMetadata metadata = ClusterMetadataService.deserializeClusterMetadata(fileName);\n        // if the partitioners are mismatching, we probably won't even get this far\n        if (metadata.partitioner != DatabaseDescriptor.getPartitioner())\n            throw new IllegalStateException(String.format(\"When reinitializing with cluster metadata, the same \" +\n                                                          \"partitioner must be used. Configured: %s, Serialized: %s\",\n                                                          DatabaseDescriptor.getPartitioner().getClass().getCanonicalName(),\n                                                          metadata.partitioner.getClass().getCanonicalName()));\n\n        if (!metadata.isCMSMember(FBUtilities.getBroadcastAddressAndPort()))\n            throw new IllegalStateException(\"When reinitializing with cluster metadata, we must be in the CMS\");\n\n        metadata = metadata.forceEpoch(metadata.epoch.nextEpoch());\n        ClusterMetadataService.unsetInstance();\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (_metadata) -> StorageService.instance.registerMBeans())\n                                           .withPreviousState(prev)\n                                           .withInitialState(metadata)\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners()\n                                           .isReset(true);\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        ClusterMetadataService.instance().forceSnapshot(metadata.forceEpoch(metadata.nextEpoch()));\n        ClusterMetadataService.instance().sealPeriod();\n        CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.reset();\n        assert ClusterMetadataService.state() == LOCAL;\n        assert ClusterMetadataService.instance() != initial : \"Aborting startup as temporary metadata service is still active\";\n    }\n\n    public static void startup(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        startup(() -> getInitialTransformation(finishJoiningRing, shouldBootstrap, isReplacing), finishJoiningRing, shouldBootstrap, isReplacing);\n    }\n\n    /**\n     * Cassandra startup process:\n     *   * assume that startup could have been interrupted at any point in time, and attempt to finish any in-process\n     *     sequences\n     *   * after finishing an existing in-progress sequence, if any, we should jump to JOINED\n     *   * otherwise, we assume a fresh setup, in which case we grab a startup sequence (Join or Replace), and try to\n     *     bootstrap\n     *\n     * @param initialTransformation supplier of the Transformation which initiates the startup sequence\n     * @param finishJoiningRing if false, node is left in a survey mode, which means it will receive writes in all cases\n     *                          except if it is replacing the node with same address\n     * @param shouldBootstrap if true, bootstrap streaming will be executed\n     * @param isReplacing true, if the node is replacing some other node (with same or different address).\n     */\n    public static void startup(Supplier<Transformation> initialTransformation, boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        NodeId self = metadata.myNodeId();\n\n        // finish in-progress sequences first\n        InProgressSequences.finishInProgressSequences(self);\n        metadata = ClusterMetadata.current();\n\n        switch (metadata.directory.peerState(self))\n        {\n            case REGISTERED:\n            case LEFT:\n                if (isReplacing)\n                    ReconfigureCMS.maybeReconfigureCMS(metadata, DatabaseDescriptor.getReplaceAddress());\n\n                ClusterMetadataService.instance().commit(initialTransformation.get());\n\n                InProgressSequences.finishInProgressSequences(self);\n                metadata = ClusterMetadata.current();\n\n                if (metadata.directory.peerState(self) == JOINED)\n                    SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.COMPLETED);\n                else\n                {\n                    StorageService.instance.markBootstrapFailed();\n                    logger.info(\"Did not finish joining the ring; node state is {}, bootstrap state is {}\",\n                                metadata.directory.peerState(self),\n                                SystemKeyspace.getBootstrapState());\n                    break;\n                }\n            case JOINED:\n                if (StorageService.isReplacingSameAddress())\n                    ReplaceSameAddress.streamData(self, metadata, shouldBootstrap, finishJoiningRing);\n\n                // JOINED appears before BOOTSTRAPPING & BOOT_REPLACE so we can fall\n                // through when we start as REGISTERED/LEFT and complete a full startup\n                logger.info(\"{}\", StorageService.Mode.NORMAL);\n                break;\n            case BOOTSTRAPPING:\n            case BOOT_REPLACING:\n                if (finishJoiningRing)\n                {\n                    throw new IllegalStateException(\"Expected to complete startup sequence, but did not. \" +\n                                                    \"Can't proceed from the state \" + metadata.directory.peerState(self));\n                }\n                break;\n            default:\n                throw new IllegalStateException(\"Can't proceed from the state \" + metadata.directory.peerState(self));\n        }\n    }\n\n    /**\n     * Returns:\n     *   * {@link PrepareReplace}, the first step of the multi-step replacement process sequence, if {@param finishJoiningRing} is true\n     *   * {@link UnsafeJoin}, a single-step join transformation, if {@param shouldBootstrap} is set to false, and the node is not\n     *     in a write survey mode (in other words {@param finishJoiningRing} is true. This mode is mostly used for testing, but can\n     *     also be used to quickly set up a fresh cluster.\n     *   * and {@link PrepareJoin}, the first step of the multi-step join process, otherwise.\n     */\n    private static Transformation getInitialTransformation(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        if (isReplacing)\n        {\n            InetAddressAndPort replacingEndpoint = DatabaseDescriptor.getReplaceAddress();\n            if (FailureDetector.instance.isAlive(replacingEndpoint))\n            {\n                logger.error(\"Unable to replace live node {})\", replacingEndpoint);\n                throw new UnsupportedOperationException(\"Cannot replace a live node... \");\n            }\n\n            NodeId replaced = ClusterMetadata.current().directory.peerId(replacingEndpoint);\n\n            return new PrepareReplace(replaced,\n                                      metadata.myNodeId(),\n                                      ClusterMetadataService.instance().placementProvider(),\n                                      finishJoiningRing,\n                                      shouldBootstrap);\n        }\n        else if (finishJoiningRing && !shouldBootstrap)\n        {\n            return new UnsafeJoin(metadata.myNodeId(),\n                                  new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                  ClusterMetadataService.instance().placementProvider());\n        }\n        else\n        {\n            return new PrepareJoin(metadata.myNodeId(),\n                                   new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                   ClusterMetadataService.instance().placementProvider(),\n                                   finishJoiningRing,\n                                   shouldBootstrap);\n        }\n    }\n\n    /**\n     * Initialization process\n     */\n\n    enum StartupMode\n    {\n        /**\n         * Normal startup mode: node has already been a part of a CMS, and was restarted.\n         */\n        NORMAL,\n        /**\n         * An upgrade path from Gossip: node has been booted brefore and was a part of a Gossip cluster.\n         */\n        UPGRADE,\n        /**\n         * Node is starting for the first time, and should attempt to either discover existing CMS, or\n         * participate in the leader election to establish a new one.\n         */\n        VOTE,\n        /**\n         * Node is starting for the first time, and is a designated first CMS node and can become a first CMS\n         * node upon boot.\n         */\n        FIRST_CMS,\n        /**\n         * Node has to pick Cluster Metadata from the specified file. Used for testing and for (improbable) disaster recovery.\n         */\n        BOOT_WITH_CLUSTERMETADATA;\n\n        static StartupMode get(Set<InetAddressAndPort> seeds)\n        {\n            if (CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.isPresent())\n            {\n                logger.warn(\"Booting with ClusterMetadata from file: \" + CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString());\n                return BOOT_WITH_CLUSTERMETADATA;\n            }\n            if (seeds.isEmpty())\n                throw new IllegalArgumentException(\"Can not initialize CMS without any seeds\");\n\n            boolean hasAnyEpoch = SystemKeyspaceStorage.hasAnyEpoch();\n            // For CCM and local dev clusters\n            boolean isOnlySeed = DatabaseDescriptor.getSeeds().size() == 1\n                                 && DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort())\n                                 && DatabaseDescriptor.getSeeds().iterator().next().getAddress().isLoopbackAddress();\n            boolean hasBootedBefore = SystemKeyspace.getLocalHostId() != null;\n            logger.info(\"hasAnyEpoch = {}, hasBootedBefore = {}\", hasAnyEpoch, hasBootedBefore);\n            if (!hasAnyEpoch && hasBootedBefore)\n                return UPGRADE;\n            else if (hasAnyEpoch)\n                return NORMAL;\n            else if (isOnlySeed)\n                return FIRST_CMS;\n            else\n                return VOTE;\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.util.concurrent.Uninterruptibles;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.BootStrapper;\nimport org.apache.cassandra.exceptions.StartupException;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.gms.NewGossiper;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.log.SystemKeyspaceStorage;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.migration.Election;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.sequences.InProgressSequences;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.sequences.ReplaceSameAddress;\nimport org.apache.cassandra.tcm.transformations.PrepareJoin;\nimport org.apache.cassandra.tcm.transformations.PrepareReplace;\nimport org.apache.cassandra.tcm.transformations.UnsafeJoin;\nimport org.apache.cassandra.tcm.transformations.cms.Initialize;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.LOCAL;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.emptyWithSchemaFromSystemTables;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.fromEndpointStates;\nimport static org.apache.cassandra.tcm.membership.NodeState.JOINED;\nimport static org.apache.cassandra.utils.FBUtilities.getBroadcastAddressAndPort;\n\n /**\n  * Initialize\n  */\n public class Startup\n{\n    private static final Logger logger = LoggerFactory.getLogger(Startup.class);\n\n    public static void initialize(Set<InetAddressAndPort> seeds) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        initialize(seeds,\n                   p -> p,\n                   () -> MessagingService.instance().waitUntilListeningUnchecked());\n    }\n\n    public static void initialize(Set<InetAddressAndPort> seeds,\n                                  Function<Processor, Processor> wrapProcessor,\n                                  Runnable initMessaging) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        switch (StartupMode.get(seeds))\n        {\n            case FIRST_CMS:\n                logger.info(\"Initializing as first CMS node in a new cluster\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeAsFirstCMSNode();\n                initMessaging.run();\n                break;\n            case NORMAL:\n                logger.info(\"Initializing as non CMS node\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initMessaging.run();\n                break;\n            case VOTE:\n                logger.info(\"Initializing for discovery\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeForDiscovery(initMessaging);\n                break;\n            case UPGRADE:\n                logger.info(\"Initializing from gossip\");\n                initializeFromGossip(wrapProcessor, initMessaging);\n                break;\n            case BOOT_WITH_CLUSTERMETADATA:\n                String fileName = CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString();\n                logger.warn(\"Initializing with cluster metadata from: {}\", fileName);\n                reinitializeWithClusterMetadata(fileName, wrapProcessor, initMessaging);\n                break;\n        }\n    }\n\n    /**\n     * Make this node a _first_ CMS node.\n     * <p>\n     * (1) Append PreInitialize transformation to local in-memory log. When distributed metadata keyspace is initialized, a no-op transformation will\n     * be added to other nodes. This is required since as of now, no node actually owns distributed metadata keyspace.\n     * (2) Commit Initialize transformation, which holds a snapshot of metadata as of now.\n     * <p>\n     * This process is applicable for gossip upgrades as well as regular vote-and-startup process.\n     */\n    public static void initializeAsFirstCMSNode()\n    {\n        InetAddressAndPort addr = FBUtilities.getBroadcastAddressAndPort();\n        ClusterMetadataService.instance().log().bootstrap(addr);\n        ClusterMetadata metadata =  ClusterMetadata.current();\n        assert ClusterMetadataService.state() == LOCAL : String.format(\"Can't initialize as node hasn't transitioned to CMS state. State: %s.\\n%s\", ClusterMetadataService.state(),  metadata);\n\n        Initialize initialize = new Initialize(metadata.initializeClusterIdentifier(addr.hashCode()));\n        ClusterMetadataService.instance().commit(initialize);\n    }\n\n    public static void initializeAsNonCmsNode(Function<Processor, Processor> wrapProcessor) throws StartupException\n    {\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withDefaultListeners();\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n        ClusterMetadataService.instance().log().ready();\n\n        NodeId nodeId = ClusterMetadata.current().myNodeId();\n        UUID currentHostId = SystemKeyspace.getLocalHostId();\n        if (nodeId != null && !Objects.equals(nodeId.toUUID(), currentHostId))\n        {\n            logger.info(\"NodeId is wrong, updating from {} to {}\", currentHostId, nodeId.toUUID());\n            SystemKeyspace.setLocalHostId(nodeId.toUUID());\n        }\n    }\n\n    public static void scrubDataDirectories(ClusterMetadata metadata) throws StartupException\n    {\n        // clean up debris in the rest of the keyspaces\n        for (KeyspaceMetadata keyspace : metadata.schema.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspace.name.equals(SchemaConstants.SYSTEM_KEYSPACE_NAME))\n                continue;\n\n            for (TableMetadata cfm : keyspace.tables)\n            {\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n            }\n        }\n    }\n\n    public interface AfterReplay\n    {\n        void accept(ClusterMetadata t) throws StartupException;\n    }\n    /**\n     * Initialization for Discovery.\n     *\n     * Node will attempt to discover other participants in the cluster by attempting to contact the seeds\n     * it is aware of. After discovery, the node with a smallest ip address will move to propose itself as\n     * a CMS initiator, and attempt to establish a CMS in via two-phase commit protocol.\n     */\n    public static void initializeForDiscovery(Runnable initMessaging)\n    {\n        initMessaging.run();\n        logger.debug(\"Discovering other nodes in the system\");\n        Discovery.DiscoveredNodes candidates = Discovery.instance.discover();\n        if (candidates.kind() == Discovery.DiscoveredNodes.Kind.KNOWN_PEERS)\n        {\n            logger.debug(\"Got candidates: \" + candidates);\n            Optional<InetAddressAndPort> option = candidates.nodes().stream().min(InetAddressAndPort::compareTo);\n            InetAddressAndPort min;\n            if (!option.isPresent())\n            {\n                if (DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort()))\n                    min = FBUtilities.getBroadcastAddressAndPort();\n                else\n                    throw new IllegalArgumentException(String.format(\"Found no candidates during initialization. Check if the seeds are up: %s\", DatabaseDescriptor.getSeeds()));\n            }\n            else\n            {\n                min = option.get();\n            }\n\n             // identify if you need to start the vote\n            if (min.equals(FBUtilities.getBroadcastAddressAndPort()) || FBUtilities.getBroadcastAddressAndPort().compareTo(min) < 0)\n            {\n                Election.instance.nominateSelf(candidates.nodes(),\n                                               Collections.singleton(FBUtilities.getBroadcastAddressAndPort()),\n                                               (cm) -> true,\n                                               null);\n            }\n        }\n\n        while (!ClusterMetadata.current().epoch.isAfter(Epoch.FIRST))\n        {\n            if (candidates.kind() == Discovery.DiscoveredNodes.Kind.CMS_ONLY)\n            {\n                RemoteProcessor.fetchLogAndWait(new RemoteProcessor.CandidateIterator(candidates.nodes(), false),\n                                                ClusterMetadataService.instance().log());\n            }\n            else\n            {\n                Election.Initiator initiator = Election.instance.initiator();\n                candidates = Discovery.instance.discoverOnce(initiator == null ? null : initiator.initiator);\n            }\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n        }\n\n        assert ClusterMetadata.current().epoch.isAfter(Epoch.FIRST);\n        Election.instance.migrated();\n    }\n\n    /**\n     * This should only be called during startup.\n     */\n    public static void initializeFromGossip(Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws StartupException\n    {\n        ClusterMetadata emptyFromSystemTables = emptyWithSchemaFromSystemTables(SystemKeyspace.allKnownDatacenters());\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withInitialState(emptyFromSystemTables)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners();\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        try\n        {\n            CommitLog.instance.recoverSegmentsOnDisk();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        logger.debug(\"Starting to initialize ClusterMetadata from gossip\");\n        Map<InetAddressAndPort, EndpointState> epStates = NewGossiper.instance.doShadowRound();\n        logger.debug(\"Got epStates {}\", epStates);\n        ClusterMetadata initial = fromEndpointStates(emptyFromSystemTables.schema, epStates);\n        logger.debug(\"Created initial ClusterMetadata {}\", initial);\n        SystemKeyspace.setLocalHostId(initial.myNodeId().toUUID());\n        ClusterMetadataService.instance().setFromGossip(initial);\n        Gossiper.instance.clearUnsafe();\n        Gossiper.instance.maybeInitializeLocalState(SystemKeyspace.incrementAndGetGeneration());\n        for (Map.Entry<NodeId, NodeState> entry : initial.directory.states.entrySet())\n            Gossiper.instance.mergeNodeToGossip(entry.getKey(), initial);\n\n        // double check that everything was added, can remove once we are confident\n        ClusterMetadata cmGossip = fromEndpointStates(emptyFromSystemTables.schema, Gossiper.instance.getEndpointStates());\n        assert cmGossip.equals(initial) : cmGossip + \" != \" + initial;\n    }\n\n    public static void reinitializeWithClusterMetadata(String fileName, Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws IOException, StartupException\n    {\n        ClusterMetadata prev = ClusterMetadata.currentNullable();\n        // First set a minimal ClusterMetadata as some deserialization depends\n        // on ClusterMetadata.current() to access the partitioner\n        StubClusterMetadataService initial = StubClusterMetadataService.forClientTools();\n        ClusterMetadataService.unsetInstance();\n        StubClusterMetadataService.setInstance(initial);\n\n        ClusterMetadata metadata = ClusterMetadataService.deserializeClusterMetadata(fileName);\n        // if the partitioners are mismatching, we probably won't even get this far\n        if (metadata.partitioner != DatabaseDescriptor.getPartitioner())\n            throw new IllegalStateException(String.format(\"When reinitializing with cluster metadata, the same \" +\n                                                          \"partitioner must be used. Configured: %s, Serialized: %s\",\n                                                          DatabaseDescriptor.getPartitioner().getClass().getCanonicalName(),\n                                                          metadata.partitioner.getClass().getCanonicalName()));\n\n        if (!metadata.isCMSMember(FBUtilities.getBroadcastAddressAndPort()))\n            throw new IllegalStateException(\"When reinitializing with cluster metadata, we must be in the CMS\");\n\n        metadata = metadata.forceEpoch(metadata.epoch.nextEpoch());\n        ClusterMetadataService.unsetInstance();\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (_metadata) -> StorageService.instance.registerMBeans())\n                                           .withPreviousState(prev)\n                                           .withInitialState(metadata)\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners()\n                                           .isReset(true);\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        ClusterMetadataService.instance().forceSnapshot(metadata.forceEpoch(metadata.nextEpoch()));\n        ClusterMetadataService.instance().sealPeriod();\n        CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.reset();\n        assert ClusterMetadataService.state() == LOCAL;\n        assert ClusterMetadataService.instance() != initial : \"Aborting startup as temporary metadata service is still active\";\n    }\n\n    public static void startup(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        startup(() -> getInitialTransformation(finishJoiningRing, shouldBootstrap, isReplacing), finishJoiningRing, shouldBootstrap, isReplacing);\n    }\n\n    /**\n     * Cassandra startup process:\n     *   * assume that startup could have been interrupted at any point in time, and attempt to finish any in-process\n     *     sequences\n     *   * after finishing an existing in-progress sequence, if any, we should jump to JOINED\n     *   * otherwise, we assume a fresh setup, in which case we grab a startup sequence (Join or Replace), and try to\n     *     bootstrap\n     *\n     * @param initialTransformation supplier of the Transformation which initiates the startup sequence\n     * @param finishJoiningRing if false, node is left in a survey mode, which means it will receive writes in all cases\n     *                          except if it is replacing the node with same address\n     * @param shouldBootstrap if true, bootstrap streaming will be executed\n     * @param isReplacing true, if the node is replacing some other node (with same or different address).\n     */\n    public static void startup(Supplier<Transformation> initialTransformation, boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        NodeId self = metadata.myNodeId();\n\n        // finish in-progress sequences first\n        InProgressSequences.finishInProgressSequences(self);\n        metadata = ClusterMetadata.current();\n\n        switch (metadata.directory.peerState(self))\n        {\n            case REGISTERED:\n            case LEFT:\n                if (isReplacing)\n                    ReconfigureCMS.maybeReconfigureCMS(metadata, DatabaseDescriptor.getReplaceAddress());\n\n                ClusterMetadataService.instance().commit(initialTransformation.get());\n\n                InProgressSequences.finishInProgressSequences(self);\n                metadata = ClusterMetadata.current();\n\n                if (metadata.directory.peerState(self) == JOINED)\n                    SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.COMPLETED);\n                else\n                {\n                    StorageService.instance.markBootstrapFailed();\n                    logger.info(\"Did not finish joining the ring; node state is {}, bootstrap state is {}\",\n                                metadata.directory.peerState(self),\n                                SystemKeyspace.getBootstrapState());\n                    break;\n                }\n            case JOINED:\n                if (StorageService.isReplacingSameAddress())\n                    ReplaceSameAddress.streamData(self, metadata, shouldBootstrap, finishJoiningRing);\n\n                // JOINED appears before BOOTSTRAPPING & BOOT_REPLACE so we can fall\n                // through when we start as REGISTERED/LEFT and complete a full startup\n                logger.info(\"{}\", StorageService.Mode.NORMAL);\n                break;\n            case BOOTSTRAPPING:\n            case BOOT_REPLACING:\n                if (finishJoiningRing)\n                {\n                    throw new IllegalStateException(\"Expected to complete startup sequence, but did not. \" +\n                                                    \"Can't proceed from the state \" + metadata.directory.peerState(self));\n                }\n                break;\n            default:\n                throw new IllegalStateException(\"Can't proceed from the state \" + metadata.directory.peerState(self));\n        }\n    }\n\n    /**\n     * Returns:\n     *   * {@link PrepareReplace}, the first step of the multi-step replacement process sequence, if {@param finishJoiningRing} is true\n     *   * {@link UnsafeJoin}, a single-step join transformation, if {@param shouldBootstrap} is set to false, and the node is not\n     *     in a write survey mode (in other words {@param finishJoiningRing} is true. This mode is mostly used for testing, but can\n     *     also be used to quickly set up a fresh cluster.\n     *   * and {@link PrepareJoin}, the first step of the multi-step join process, otherwise.\n     */\n    private static Transformation getInitialTransformation(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        if (isReplacing)\n        {\n            InetAddressAndPort replacingEndpoint = DatabaseDescriptor.getReplaceAddress();\n            if (FailureDetector.instance.isAlive(replacingEndpoint))\n            {\n                logger.error(\"Unable to replace live node {})\", replacingEndpoint);\n                throw new UnsupportedOperationException(\"Cannot replace a live node... \");\n            }\n\n            NodeId replaced = ClusterMetadata.current().directory.peerId(replacingEndpoint);\n\n            return new PrepareReplace(replaced,\n                                      metadata.myNodeId(),\n                                      ClusterMetadataService.instance().placementProvider(),\n                                      finishJoiningRing,\n                                      shouldBootstrap);\n        }\n        else if (finishJoiningRing && !shouldBootstrap)\n        {\n            return new UnsafeJoin(metadata.myNodeId(),\n                                  new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                  ClusterMetadataService.instance().placementProvider());\n        }\n        else\n        {\n            return new PrepareJoin(metadata.myNodeId(),\n                                   new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                   ClusterMetadataService.instance().placementProvider(),\n                                   finishJoiningRing,\n                                   shouldBootstrap);\n        }\n    }\n\n    /**\n     * Initialization process\n     */\n\n    enum StartupMode\n    {\n        /**\n         * Normal startup mode: node has already been a part of a CMS, and was restarted.\n         */\n        NORMAL,\n        /**\n         * An upgrade path from Gossip: node has been booted brefore and was a part of a Gossip cluster.\n         */\n        UPGRADE,\n        /**\n         * Node is starting for the first time, and should attempt to either discover existing CMS, or\n         * participate in the leader election to establish a new one.\n         */\n        VOTE,\n        /**\n         * Node is starting for the first time, and is a designated first CMS node and can become a first CMS\n         * node upon boot.\n         */\n        FIRST_CMS,\n        /**\n         * Node has to pick Cluster Metadata from the specified file. Used for testing and for (improbable) disaster recovery.\n         */\n        BOOT_WITH_CLUSTERMETADATA;\n\n        static StartupMode get(Set<InetAddressAndPort> seeds)\n        {\n            if (CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.isPresent())\n            {\n                logger.warn(\"Booting with ClusterMetadata from file: \" + CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString());\n                return BOOT_WITH_CLUSTERMETADATA;\n            }\n            if (seeds.isEmpty())\n                throw new IllegalArgumentException(\"Can not initialize CMS without any seeds\");\n\n            boolean hasAnyEpoch = SystemKeyspaceStorage.hasAnyEpoch();\n            // For CCM and local dev clusters\n            boolean isOnlySeed = DatabaseDescriptor.getSeeds().size() == 1\n                                 && DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort())\n                                 && DatabaseDescriptor.getSeeds().iterator().next().getAddress().isLoopbackAddress();\n            boolean hasBootedBefore = SystemKeyspace.getLocalHostId() != null;\n            logger.info(\"hasAnyEpoch = {}, hasBootedBefore = {}\", hasAnyEpoch, hasBootedBefore);\n            if (!hasAnyEpoch && hasBootedBefore)\n                return UPGRADE;\n            else if (hasAnyEpoch)\n                return NORMAL;\n            else if (isOnlySeed)\n                return FIRST_CMS;\n            else\n                return VOTE;\n        }\n    }\n}\n","lineNo":135}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.util.concurrent.Uninterruptibles;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.BootStrapper;\nimport org.apache.cassandra.exceptions.StartupException;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.gms.NewGossiper;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.log.SystemKeyspaceStorage;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.migration.Election;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.sequences.InProgressSequences;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.sequences.ReplaceSameAddress;\nimport org.apache.cassandra.tcm.transformations.PrepareJoin;\nimport org.apache.cassandra.tcm.transformations.PrepareReplace;\nimport org.apache.cassandra.tcm.transformations.UnsafeJoin;\nimport org.apache.cassandra.tcm.transformations.cms.Initialize;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.LOCAL;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.emptyWithSchemaFromSystemTables;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.fromEndpointStates;\nimport static org.apache.cassandra.tcm.membership.NodeState.JOINED;\nimport static org.apache.cassandra.utils.FBUtilities.getBroadcastAddressAndPort;\n\n /**\n  * Initialize\n  */\n public class Startup\n{\n    private static final Logger logger = LoggerFactory.getLogger(Startup.class);\n\n    public static void initialize(Set<InetAddressAndPort> seeds) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        initialize(seeds,\n                   p -> p,\n                   () -> MessagingService.instance().waitUntilListeningUnchecked());\n    }\n\n    public static void initialize(Set<InetAddressAndPort> seeds,\n                                  Function<Processor, Processor> wrapProcessor,\n                                  Runnable initMessaging) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        switch (StartupMode.get(seeds))\n        {\n            case FIRST_CMS:\n                logger.info(\"Initializing as first CMS node in a new cluster\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeAsFirstCMSNode();\n                initMessaging.run();\n                break;\n            case NORMAL:\n                logger.info(\"Initializing as non CMS node\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initMessaging.run();\n                break;\n            case VOTE:\n                logger.info(\"Initializing for discovery\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeForDiscovery(initMessaging);\n                break;\n            case UPGRADE:\n                logger.info(\"Initializing from gossip\");\n                initializeFromGossip(wrapProcessor, initMessaging);\n                break;\n            case BOOT_WITH_CLUSTERMETADATA:\n                String fileName = CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString();\n                logger.warn(\"Initializing with cluster metadata from: {}\", fileName);\n                reinitializeWithClusterMetadata(fileName, wrapProcessor, initMessaging);\n                break;\n        }\n    }\n\n    /**\n     * Make this node a _first_ CMS node.\n     * <p>\n     * (1) Append PreInitialize transformation to local in-memory log. When distributed metadata keyspace is initialized, a no-op transformation will\n     * be added to other nodes. This is required since as of now, no node actually owns distributed metadata keyspace.\n     * (2) Commit Initialize transformation, which holds a snapshot of metadata as of now.\n     * <p>\n     * This process is applicable for gossip upgrades as well as regular vote-and-startup process.\n     */\n    public static void initializeAsFirstCMSNode()\n    {\n        ClusterMetadataService.instance().log().bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        assert ClusterMetadataService.state() == LOCAL : String.format(\"Can't initialize as node hasn't transitioned to CMS state. State: %s.\\n%s\", ClusterMetadataService.state(), ClusterMetadata.current());\n        Initialize initialize = new Initialize(ClusterMetadata.current());\n        ClusterMetadataService.instance().commit(initialize);\n    }\n\n    public static void initializeAsNonCmsNode(Function<Processor, Processor> wrapProcessor) throws StartupException\n    {\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withDefaultListeners();\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n        ClusterMetadataService.instance().log().ready();\n\n        NodeId nodeId = ClusterMetadata.current().myNodeId();\n        UUID currentHostId = SystemKeyspace.getLocalHostId();\n        if (nodeId != null && !Objects.equals(nodeId.toUUID(), currentHostId))\n        {\n            logger.info(\"NodeId is wrong, updating from {} to {}\", currentHostId, nodeId.toUUID());\n            SystemKeyspace.setLocalHostId(nodeId.toUUID());\n        }\n    }\n\n    public static void scrubDataDirectories(ClusterMetadata metadata) throws StartupException\n    {\n        // clean up debris in the rest of the keyspaces\n        for (KeyspaceMetadata keyspace : metadata.schema.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspace.name.equals(SchemaConstants.SYSTEM_KEYSPACE_NAME))\n                continue;\n\n            for (TableMetadata cfm : keyspace.tables)\n            {\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n            }\n        }\n    }\n\n    public interface AfterReplay\n    {\n        void accept(ClusterMetadata t) throws StartupException;\n    }\n    /**\n     * Initialization for Discovery.\n     *\n     * Node will attempt to discover other participants in the cluster by attempting to contact the seeds\n     * it is aware of. After discovery, the node with a smallest ip address will move to propose itself as\n     * a CMS initiator, and attempt to establish a CMS in via two-phase commit protocol.\n     */\n    public static void initializeForDiscovery(Runnable initMessaging)\n    {\n        initMessaging.run();\n        logger.debug(\"Discovering other nodes in the system\");\n        Discovery.DiscoveredNodes candidates = Discovery.instance.discover();\n        if (candidates.kind() == Discovery.DiscoveredNodes.Kind.KNOWN_PEERS)\n        {\n            logger.debug(\"Got candidates: \" + candidates);\n            Optional<InetAddressAndPort> option = candidates.nodes().stream().min(InetAddressAndPort::compareTo);\n            InetAddressAndPort min;\n            if (!option.isPresent())\n            {\n                if (DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort()))\n                    min = FBUtilities.getBroadcastAddressAndPort();\n                else\n                    throw new IllegalArgumentException(String.format(\"Found no candidates during initialization. Check if the seeds are up: %s\", DatabaseDescriptor.getSeeds()));\n            }\n            else\n            {\n                min = option.get();\n            }\n\n             // identify if you need to start the vote\n            if (min.equals(FBUtilities.getBroadcastAddressAndPort()) || FBUtilities.getBroadcastAddressAndPort().compareTo(min) < 0)\n            {\n                Election.instance.nominateSelf(candidates.nodes(),\n                                               Collections.singleton(FBUtilities.getBroadcastAddressAndPort()),\n                                               (cm) -> true,\n                                               null);\n            }\n        }\n\n        while (!ClusterMetadata.current().epoch.isAfter(Epoch.FIRST))\n        {\n            if (candidates.kind() == Discovery.DiscoveredNodes.Kind.CMS_ONLY)\n            {\n                RemoteProcessor.fetchLogAndWait(new RemoteProcessor.CandidateIterator(candidates.nodes(), false),\n                                                ClusterMetadataService.instance().log());\n            }\n            else\n            {\n                Election.Initiator initiator = Election.instance.initiator();\n                candidates = Discovery.instance.discoverOnce(initiator == null ? null : initiator.initiator);\n            }\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n        }\n\n        assert ClusterMetadata.current().epoch.isAfter(Epoch.FIRST);\n        Election.instance.migrated();\n    }\n\n    /**\n     * This should only be called during startup.\n     */\n    public static void initializeFromGossip(Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws StartupException\n    {\n        ClusterMetadata emptyFromSystemTables = emptyWithSchemaFromSystemTables(SystemKeyspace.allKnownDatacenters());\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withInitialState(emptyFromSystemTables)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners();\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        try\n        {\n            CommitLog.instance.recoverSegmentsOnDisk();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        logger.debug(\"Starting to initialize ClusterMetadata from gossip\");\n        Map<InetAddressAndPort, EndpointState> epStates = NewGossiper.instance.doShadowRound();\n        logger.debug(\"Got epStates {}\", epStates);\n        ClusterMetadata initial = fromEndpointStates(emptyFromSystemTables.schema, epStates);\n        logger.debug(\"Created initial ClusterMetadata {}\", initial);\n        SystemKeyspace.setLocalHostId(initial.myNodeId().toUUID());\n        ClusterMetadataService.instance().setFromGossip(initial);\n        Gossiper.instance.clearUnsafe();\n        Gossiper.instance.maybeInitializeLocalState(SystemKeyspace.incrementAndGetGeneration());\n        for (Map.Entry<NodeId, NodeState> entry : initial.directory.states.entrySet())\n            Gossiper.instance.mergeNodeToGossip(entry.getKey(), initial);\n\n        // double check that everything was added, can remove once we are confident\n        ClusterMetadata cmGossip = fromEndpointStates(emptyFromSystemTables.schema, Gossiper.instance.getEndpointStates());\n        assert cmGossip.equals(initial) : cmGossip + \" != \" + initial;\n    }\n\n    public static void reinitializeWithClusterMetadata(String fileName, Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws IOException, StartupException\n    {\n        ClusterMetadata prev = ClusterMetadata.currentNullable();\n        // First set a minimal ClusterMetadata as some deserialization depends\n        // on ClusterMetadata.current() to access the partitioner\n        StubClusterMetadataService initial = StubClusterMetadataService.forClientTools();\n        ClusterMetadataService.unsetInstance();\n        StubClusterMetadataService.setInstance(initial);\n\n        ClusterMetadata metadata = ClusterMetadataService.deserializeClusterMetadata(fileName);\n        // if the partitioners are mismatching, we probably won't even get this far\n        if (metadata.partitioner != DatabaseDescriptor.getPartitioner())\n            throw new IllegalStateException(String.format(\"When reinitializing with cluster metadata, the same \" +\n                                                          \"partitioner must be used. Configured: %s, Serialized: %s\",\n                                                          DatabaseDescriptor.getPartitioner().getClass().getCanonicalName(),\n                                                          metadata.partitioner.getClass().getCanonicalName()));\n\n        if (!metadata.isCMSMember(FBUtilities.getBroadcastAddressAndPort()))\n            throw new IllegalStateException(\"When reinitializing with cluster metadata, we must be in the CMS\");\n\n        metadata = metadata.forceEpoch(metadata.epoch.nextEpoch());\n        ClusterMetadataService.unsetInstance();\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (_metadata) -> StorageService.instance.registerMBeans())\n                                           .withPreviousState(prev)\n                                           .withInitialState(metadata)\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners()\n                                           .isReset(true);\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        ClusterMetadataService.instance().forceSnapshot(metadata.forceEpoch(metadata.nextEpoch()));\n        ClusterMetadataService.instance().sealPeriod();\n        CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.reset();\n        assert ClusterMetadataService.state() == LOCAL;\n        assert ClusterMetadataService.instance() != initial : \"Aborting startup as temporary metadata service is still active\";\n    }\n\n    public static void startup(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        startup(() -> getInitialTransformation(finishJoiningRing, shouldBootstrap, isReplacing), finishJoiningRing, shouldBootstrap, isReplacing);\n    }\n\n    /**\n     * Cassandra startup process:\n     *   * assume that startup could have been interrupted at any point in time, and attempt to finish any in-process\n     *     sequences\n     *   * after finishing an existing in-progress sequence, if any, we should jump to JOINED\n     *   * otherwise, we assume a fresh setup, in which case we grab a startup sequence (Join or Replace), and try to\n     *     bootstrap\n     *\n     * @param initialTransformation supplier of the Transformation which initiates the startup sequence\n     * @param finishJoiningRing if false, node is left in a survey mode, which means it will receive writes in all cases\n     *                          except if it is replacing the node with same address\n     * @param shouldBootstrap if true, bootstrap streaming will be executed\n     * @param isReplacing true, if the node is replacing some other node (with same or different address).\n     */\n    public static void startup(Supplier<Transformation> initialTransformation, boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        NodeId self = metadata.myNodeId();\n\n        // finish in-progress sequences first\n        InProgressSequences.finishInProgressSequences(self);\n        metadata = ClusterMetadata.current();\n\n        switch (metadata.directory.peerState(self))\n        {\n            case REGISTERED:\n            case LEFT:\n                if (isReplacing)\n                    ReconfigureCMS.maybeReconfigureCMS(metadata, DatabaseDescriptor.getReplaceAddress());\n\n                ClusterMetadataService.instance().commit(initialTransformation.get());\n\n                InProgressSequences.finishInProgressSequences(self);\n                metadata = ClusterMetadata.current();\n\n                if (metadata.directory.peerState(self) == JOINED)\n                    SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.COMPLETED);\n                else\n                {\n                    StorageService.instance.markBootstrapFailed();\n                    logger.info(\"Did not finish joining the ring; node state is {}, bootstrap state is {}\",\n                                metadata.directory.peerState(self),\n                                SystemKeyspace.getBootstrapState());\n                    break;\n                }\n            case JOINED:\n                if (StorageService.isReplacingSameAddress())\n                    ReplaceSameAddress.streamData(self, metadata, shouldBootstrap, finishJoiningRing);\n\n                // JOINED appears before BOOTSTRAPPING & BOOT_REPLACE so we can fall\n                // through when we start as REGISTERED/LEFT and complete a full startup\n                logger.info(\"{}\", StorageService.Mode.NORMAL);\n                break;\n            case BOOTSTRAPPING:\n            case BOOT_REPLACING:\n                if (finishJoiningRing)\n                {\n                    throw new IllegalStateException(\"Expected to complete startup sequence, but did not. \" +\n                                                    \"Can't proceed from the state \" + metadata.directory.peerState(self));\n                }\n                break;\n            default:\n                throw new IllegalStateException(\"Can't proceed from the state \" + metadata.directory.peerState(self));\n        }\n    }\n\n    /**\n     * Returns:\n     *   * {@link PrepareReplace}, the first step of the multi-step replacement process sequence, if {@param finishJoiningRing} is true\n     *   * {@link UnsafeJoin}, a single-step join transformation, if {@param shouldBootstrap} is set to false, and the node is not\n     *     in a write survey mode (in other words {@param finishJoiningRing} is true. This mode is mostly used for testing, but can\n     *     also be used to quickly set up a fresh cluster.\n     *   * and {@link PrepareJoin}, the first step of the multi-step join process, otherwise.\n     */\n    private static Transformation getInitialTransformation(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        if (isReplacing)\n        {\n            InetAddressAndPort replacingEndpoint = DatabaseDescriptor.getReplaceAddress();\n            if (FailureDetector.instance.isAlive(replacingEndpoint))\n            {\n                logger.error(\"Unable to replace live node {})\", replacingEndpoint);\n                throw new UnsupportedOperationException(\"Cannot replace a live node... \");\n            }\n\n            NodeId replaced = ClusterMetadata.current().directory.peerId(replacingEndpoint);\n\n            return new PrepareReplace(replaced,\n                                      metadata.myNodeId(),\n                                      ClusterMetadataService.instance().placementProvider(),\n                                      finishJoiningRing,\n                                      shouldBootstrap);\n        }\n        else if (finishJoiningRing && !shouldBootstrap)\n        {\n            return new UnsafeJoin(metadata.myNodeId(),\n                                  new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                  ClusterMetadataService.instance().placementProvider());\n        }\n        else\n        {\n            return new PrepareJoin(metadata.myNodeId(),\n                                   new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                   ClusterMetadataService.instance().placementProvider(),\n                                   finishJoiningRing,\n                                   shouldBootstrap);\n        }\n    }\n\n    /**\n     * Initialization process\n     */\n\n    enum StartupMode\n    {\n        /**\n         * Normal startup mode: node has already been a part of a CMS, and was restarted.\n         */\n        NORMAL,\n        /**\n         * An upgrade path from Gossip: node has been booted brefore and was a part of a Gossip cluster.\n         */\n        UPGRADE,\n        /**\n         * Node is starting for the first time, and should attempt to either discover existing CMS, or\n         * participate in the leader election to establish a new one.\n         */\n        VOTE,\n        /**\n         * Node is starting for the first time, and is a designated first CMS node and can become a first CMS\n         * node upon boot.\n         */\n        FIRST_CMS,\n        /**\n         * Node has to pick Cluster Metadata from the specified file. Used for testing and for (improbable) disaster recovery.\n         */\n        BOOT_WITH_CLUSTERMETADATA;\n\n        static StartupMode get(Set<InetAddressAndPort> seeds)\n        {\n            if (CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.isPresent())\n            {\n                logger.warn(\"Booting with ClusterMetadata from file: \" + CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString());\n                return BOOT_WITH_CLUSTERMETADATA;\n            }\n            if (seeds.isEmpty())\n                throw new IllegalArgumentException(\"Can not initialize CMS without any seeds\");\n\n            boolean hasAnyEpoch = SystemKeyspaceStorage.hasAnyEpoch();\n            // For CCM and local dev clusters\n            boolean isOnlySeed = DatabaseDescriptor.getSeeds().size() == 1\n                                 && DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort())\n                                 && DatabaseDescriptor.getSeeds().iterator().next().getAddress().isLoopbackAddress();\n            boolean hasBootedBefore = SystemKeyspace.getLocalHostId() != null;\n            logger.info(\"hasAnyEpoch = {}, hasBootedBefore = {}\", hasAnyEpoch, hasBootedBefore);\n            if (!hasAnyEpoch && hasBootedBefore)\n                return UPGRADE;\n            else if (hasAnyEpoch)\n                return NORMAL;\n            else if (isOnlySeed)\n                return FIRST_CMS;\n            else\n                return VOTE;\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.util.concurrent.Uninterruptibles;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.BootStrapper;\nimport org.apache.cassandra.exceptions.StartupException;\nimport org.apache.cassandra.gms.EndpointState;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.gms.NewGossiper;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.MessagingService;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.log.SystemKeyspaceStorage;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.migration.Election;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.sequences.InProgressSequences;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.sequences.ReplaceSameAddress;\nimport org.apache.cassandra.tcm.transformations.PrepareJoin;\nimport org.apache.cassandra.tcm.transformations.PrepareReplace;\nimport org.apache.cassandra.tcm.transformations.UnsafeJoin;\nimport org.apache.cassandra.tcm.transformations.cms.Initialize;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.LOCAL;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.emptyWithSchemaFromSystemTables;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.fromEndpointStates;\nimport static org.apache.cassandra.tcm.membership.NodeState.JOINED;\nimport static org.apache.cassandra.utils.FBUtilities.getBroadcastAddressAndPort;\n\n /**\n  * Initialize\n  */\n public class Startup\n{\n    private static final Logger logger = LoggerFactory.getLogger(Startup.class);\n\n    public static void initialize(Set<InetAddressAndPort> seeds) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        initialize(seeds,\n                   p -> p,\n                   () -> MessagingService.instance().waitUntilListeningUnchecked());\n    }\n\n    public static void initialize(Set<InetAddressAndPort> seeds,\n                                  Function<Processor, Processor> wrapProcessor,\n                                  Runnable initMessaging) throws InterruptedException, ExecutionException, IOException, StartupException\n    {\n        switch (StartupMode.get(seeds))\n        {\n            case FIRST_CMS:\n                logger.info(\"Initializing as first CMS node in a new cluster\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeAsFirstCMSNode();\n                initMessaging.run();\n                break;\n            case NORMAL:\n                logger.info(\"Initializing as non CMS node\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initMessaging.run();\n                break;\n            case VOTE:\n                logger.info(\"Initializing for discovery\");\n                initializeAsNonCmsNode(wrapProcessor);\n                initializeForDiscovery(initMessaging);\n                break;\n            case UPGRADE:\n                logger.info(\"Initializing from gossip\");\n                initializeFromGossip(wrapProcessor, initMessaging);\n                break;\n            case BOOT_WITH_CLUSTERMETADATA:\n                String fileName = CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString();\n                logger.warn(\"Initializing with cluster metadata from: {}\", fileName);\n                reinitializeWithClusterMetadata(fileName, wrapProcessor, initMessaging);\n                break;\n        }\n    }\n\n    /**\n     * Make this node a _first_ CMS node.\n     * <p>\n     * (1) Append PreInitialize transformation to local in-memory log. When distributed metadata keyspace is initialized, a no-op transformation will\n     * be added to other nodes. This is required since as of now, no node actually owns distributed metadata keyspace.\n     * (2) Commit Initialize transformation, which holds a snapshot of metadata as of now.\n     * <p>\n     * This process is applicable for gossip upgrades as well as regular vote-and-startup process.\n     */\n    public static void initializeAsFirstCMSNode()\n    {\n        InetAddressAndPort addr = FBUtilities.getBroadcastAddressAndPort();\n        ClusterMetadataService.instance().log().bootstrap(addr);\n        ClusterMetadata metadata =  ClusterMetadata.current();\n        assert ClusterMetadataService.state() == LOCAL : String.format(\"Can't initialize as node hasn't transitioned to CMS state. State: %s.\\n%s\", ClusterMetadataService.state(),  metadata);\n\n        Initialize initialize = new Initialize(metadata.initializeClusterIdentifier(addr.hashCode()));\n        ClusterMetadataService.instance().commit(initialize);\n    }\n\n    public static void initializeAsNonCmsNode(Function<Processor, Processor> wrapProcessor) throws StartupException\n    {\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withDefaultListeners();\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n        ClusterMetadataService.instance().log().ready();\n\n        NodeId nodeId = ClusterMetadata.current().myNodeId();\n        UUID currentHostId = SystemKeyspace.getLocalHostId();\n        if (nodeId != null && !Objects.equals(nodeId.toUUID(), currentHostId))\n        {\n            logger.info(\"NodeId is wrong, updating from {} to {}\", currentHostId, nodeId.toUUID());\n            SystemKeyspace.setLocalHostId(nodeId.toUUID());\n        }\n    }\n\n    public static void scrubDataDirectories(ClusterMetadata metadata) throws StartupException\n    {\n        // clean up debris in the rest of the keyspaces\n        for (KeyspaceMetadata keyspace : metadata.schema.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspace.name.equals(SchemaConstants.SYSTEM_KEYSPACE_NAME))\n                continue;\n\n            for (TableMetadata cfm : keyspace.tables)\n            {\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n            }\n        }\n    }\n\n    public interface AfterReplay\n    {\n        void accept(ClusterMetadata t) throws StartupException;\n    }\n    /**\n     * Initialization for Discovery.\n     *\n     * Node will attempt to discover other participants in the cluster by attempting to contact the seeds\n     * it is aware of. After discovery, the node with a smallest ip address will move to propose itself as\n     * a CMS initiator, and attempt to establish a CMS in via two-phase commit protocol.\n     */\n    public static void initializeForDiscovery(Runnable initMessaging)\n    {\n        initMessaging.run();\n        logger.debug(\"Discovering other nodes in the system\");\n        Discovery.DiscoveredNodes candidates = Discovery.instance.discover();\n        if (candidates.kind() == Discovery.DiscoveredNodes.Kind.KNOWN_PEERS)\n        {\n            logger.debug(\"Got candidates: \" + candidates);\n            Optional<InetAddressAndPort> option = candidates.nodes().stream().min(InetAddressAndPort::compareTo);\n            InetAddressAndPort min;\n            if (!option.isPresent())\n            {\n                if (DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort()))\n                    min = FBUtilities.getBroadcastAddressAndPort();\n                else\n                    throw new IllegalArgumentException(String.format(\"Found no candidates during initialization. Check if the seeds are up: %s\", DatabaseDescriptor.getSeeds()));\n            }\n            else\n            {\n                min = option.get();\n            }\n\n             // identify if you need to start the vote\n            if (min.equals(FBUtilities.getBroadcastAddressAndPort()) || FBUtilities.getBroadcastAddressAndPort().compareTo(min) < 0)\n            {\n                Election.instance.nominateSelf(candidates.nodes(),\n                                               Collections.singleton(FBUtilities.getBroadcastAddressAndPort()),\n                                               (cm) -> true,\n                                               null);\n            }\n        }\n\n        while (!ClusterMetadata.current().epoch.isAfter(Epoch.FIRST))\n        {\n            if (candidates.kind() == Discovery.DiscoveredNodes.Kind.CMS_ONLY)\n            {\n                RemoteProcessor.fetchLogAndWait(new RemoteProcessor.CandidateIterator(candidates.nodes(), false),\n                                                ClusterMetadataService.instance().log());\n            }\n            else\n            {\n                Election.Initiator initiator = Election.instance.initiator();\n                candidates = Discovery.instance.discoverOnce(initiator == null ? null : initiator.initiator);\n            }\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n        }\n\n        assert ClusterMetadata.current().epoch.isAfter(Epoch.FIRST);\n        Election.instance.migrated();\n    }\n\n    /**\n     * This should only be called during startup.\n     */\n    public static void initializeFromGossip(Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws StartupException\n    {\n        ClusterMetadata emptyFromSystemTables = emptyWithSchemaFromSystemTables(SystemKeyspace.allKnownDatacenters());\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withInitialState(emptyFromSystemTables)\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (metadata) -> StorageService.instance.registerMBeans())\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners();\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        try\n        {\n            CommitLog.instance.recoverSegmentsOnDisk();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        logger.debug(\"Starting to initialize ClusterMetadata from gossip\");\n        Map<InetAddressAndPort, EndpointState> epStates = NewGossiper.instance.doShadowRound();\n        logger.debug(\"Got epStates {}\", epStates);\n        ClusterMetadata initial = fromEndpointStates(emptyFromSystemTables.schema, epStates);\n        logger.debug(\"Created initial ClusterMetadata {}\", initial);\n        SystemKeyspace.setLocalHostId(initial.myNodeId().toUUID());\n        ClusterMetadataService.instance().setFromGossip(initial);\n        Gossiper.instance.clearUnsafe();\n        Gossiper.instance.maybeInitializeLocalState(SystemKeyspace.incrementAndGetGeneration());\n        for (Map.Entry<NodeId, NodeState> entry : initial.directory.states.entrySet())\n            Gossiper.instance.mergeNodeToGossip(entry.getKey(), initial);\n\n        // double check that everything was added, can remove once we are confident\n        ClusterMetadata cmGossip = fromEndpointStates(emptyFromSystemTables.schema, Gossiper.instance.getEndpointStates());\n        assert cmGossip.equals(initial) : cmGossip + \" != \" + initial;\n    }\n\n    public static void reinitializeWithClusterMetadata(String fileName, Function<Processor, Processor> wrapProcessor, Runnable initMessaging) throws IOException, StartupException\n    {\n        ClusterMetadata prev = ClusterMetadata.currentNullable();\n        // First set a minimal ClusterMetadata as some deserialization depends\n        // on ClusterMetadata.current() to access the partitioner\n        StubClusterMetadataService initial = StubClusterMetadataService.forClientTools();\n        ClusterMetadataService.unsetInstance();\n        StubClusterMetadataService.setInstance(initial);\n\n        ClusterMetadata metadata = ClusterMetadataService.deserializeClusterMetadata(fileName);\n        // if the partitioners are mismatching, we probably won't even get this far\n        if (metadata.partitioner != DatabaseDescriptor.getPartitioner())\n            throw new IllegalStateException(String.format(\"When reinitializing with cluster metadata, the same \" +\n                                                          \"partitioner must be used. Configured: %s, Serialized: %s\",\n                                                          DatabaseDescriptor.getPartitioner().getClass().getCanonicalName(),\n                                                          metadata.partitioner.getClass().getCanonicalName()));\n\n        if (!metadata.isCMSMember(FBUtilities.getBroadcastAddressAndPort()))\n            throw new IllegalStateException(\"When reinitializing with cluster metadata, we must be in the CMS\");\n\n        metadata = metadata.forceEpoch(metadata.epoch.nextEpoch());\n        ClusterMetadataService.unsetInstance();\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .afterReplay(Startup::scrubDataDirectories,\n                                                        (_metadata) -> StorageService.instance.registerMBeans())\n                                           .withPreviousState(prev)\n                                           .withInitialState(metadata)\n                                           .withStorage(LogStorage.SystemKeyspace)\n                                           .withDefaultListeners()\n                                           .isReset(true);\n\n        ClusterMetadataService.setInstance(new ClusterMetadataService(new UniformRangePlacement(),\n                                                                      wrapProcessor,\n                                                                      ClusterMetadataService::state,\n                                                                      logSpec));\n\n        ClusterMetadataService.instance().log().ready();\n        initMessaging.run();\n        ClusterMetadataService.instance().forceSnapshot(metadata.forceEpoch(metadata.nextEpoch()));\n        ClusterMetadataService.instance().sealPeriod();\n        CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.reset();\n        assert ClusterMetadataService.state() == LOCAL;\n        assert ClusterMetadataService.instance() != initial : \"Aborting startup as temporary metadata service is still active\";\n    }\n\n    public static void startup(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        startup(() -> getInitialTransformation(finishJoiningRing, shouldBootstrap, isReplacing), finishJoiningRing, shouldBootstrap, isReplacing);\n    }\n\n    /**\n     * Cassandra startup process:\n     *   * assume that startup could have been interrupted at any point in time, and attempt to finish any in-process\n     *     sequences\n     *   * after finishing an existing in-progress sequence, if any, we should jump to JOINED\n     *   * otherwise, we assume a fresh setup, in which case we grab a startup sequence (Join or Replace), and try to\n     *     bootstrap\n     *\n     * @param initialTransformation supplier of the Transformation which initiates the startup sequence\n     * @param finishJoiningRing if false, node is left in a survey mode, which means it will receive writes in all cases\n     *                          except if it is replacing the node with same address\n     * @param shouldBootstrap if true, bootstrap streaming will be executed\n     * @param isReplacing true, if the node is replacing some other node (with same or different address).\n     */\n    public static void startup(Supplier<Transformation> initialTransformation, boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        NodeId self = metadata.myNodeId();\n\n        // finish in-progress sequences first\n        InProgressSequences.finishInProgressSequences(self);\n        metadata = ClusterMetadata.current();\n\n        switch (metadata.directory.peerState(self))\n        {\n            case REGISTERED:\n            case LEFT:\n                if (isReplacing)\n                    ReconfigureCMS.maybeReconfigureCMS(metadata, DatabaseDescriptor.getReplaceAddress());\n\n                ClusterMetadataService.instance().commit(initialTransformation.get());\n\n                InProgressSequences.finishInProgressSequences(self);\n                metadata = ClusterMetadata.current();\n\n                if (metadata.directory.peerState(self) == JOINED)\n                    SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.COMPLETED);\n                else\n                {\n                    StorageService.instance.markBootstrapFailed();\n                    logger.info(\"Did not finish joining the ring; node state is {}, bootstrap state is {}\",\n                                metadata.directory.peerState(self),\n                                SystemKeyspace.getBootstrapState());\n                    break;\n                }\n            case JOINED:\n                if (StorageService.isReplacingSameAddress())\n                    ReplaceSameAddress.streamData(self, metadata, shouldBootstrap, finishJoiningRing);\n\n                // JOINED appears before BOOTSTRAPPING & BOOT_REPLACE so we can fall\n                // through when we start as REGISTERED/LEFT and complete a full startup\n                logger.info(\"{}\", StorageService.Mode.NORMAL);\n                break;\n            case BOOTSTRAPPING:\n            case BOOT_REPLACING:\n                if (finishJoiningRing)\n                {\n                    throw new IllegalStateException(\"Expected to complete startup sequence, but did not. \" +\n                                                    \"Can't proceed from the state \" + metadata.directory.peerState(self));\n                }\n                break;\n            default:\n                throw new IllegalStateException(\"Can't proceed from the state \" + metadata.directory.peerState(self));\n        }\n    }\n\n    /**\n     * Returns:\n     *   * {@link PrepareReplace}, the first step of the multi-step replacement process sequence, if {@param finishJoiningRing} is true\n     *   * {@link UnsafeJoin}, a single-step join transformation, if {@param shouldBootstrap} is set to false, and the node is not\n     *     in a write survey mode (in other words {@param finishJoiningRing} is true. This mode is mostly used for testing, but can\n     *     also be used to quickly set up a fresh cluster.\n     *   * and {@link PrepareJoin}, the first step of the multi-step join process, otherwise.\n     */\n    private static Transformation getInitialTransformation(boolean finishJoiningRing, boolean shouldBootstrap, boolean isReplacing)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        if (isReplacing)\n        {\n            InetAddressAndPort replacingEndpoint = DatabaseDescriptor.getReplaceAddress();\n            if (FailureDetector.instance.isAlive(replacingEndpoint))\n            {\n                logger.error(\"Unable to replace live node {})\", replacingEndpoint);\n                throw new UnsupportedOperationException(\"Cannot replace a live node... \");\n            }\n\n            NodeId replaced = ClusterMetadata.current().directory.peerId(replacingEndpoint);\n\n            return new PrepareReplace(replaced,\n                                      metadata.myNodeId(),\n                                      ClusterMetadataService.instance().placementProvider(),\n                                      finishJoiningRing,\n                                      shouldBootstrap);\n        }\n        else if (finishJoiningRing && !shouldBootstrap)\n        {\n            return new UnsafeJoin(metadata.myNodeId(),\n                                  new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                  ClusterMetadataService.instance().placementProvider());\n        }\n        else\n        {\n            return new PrepareJoin(metadata.myNodeId(),\n                                   new HashSet<>(BootStrapper.getBootstrapTokens(ClusterMetadata.current(), getBroadcastAddressAndPort())),\n                                   ClusterMetadataService.instance().placementProvider(),\n                                   finishJoiningRing,\n                                   shouldBootstrap);\n        }\n    }\n\n    /**\n     * Initialization process\n     */\n\n    enum StartupMode\n    {\n        /**\n         * Normal startup mode: node has already been a part of a CMS, and was restarted.\n         */\n        NORMAL,\n        /**\n         * An upgrade path from Gossip: node has been booted brefore and was a part of a Gossip cluster.\n         */\n        UPGRADE,\n        /**\n         * Node is starting for the first time, and should attempt to either discover existing CMS, or\n         * participate in the leader election to establish a new one.\n         */\n        VOTE,\n        /**\n         * Node is starting for the first time, and is a designated first CMS node and can become a first CMS\n         * node upon boot.\n         */\n        FIRST_CMS,\n        /**\n         * Node has to pick Cluster Metadata from the specified file. Used for testing and for (improbable) disaster recovery.\n         */\n        BOOT_WITH_CLUSTERMETADATA;\n\n        static StartupMode get(Set<InetAddressAndPort> seeds)\n        {\n            if (CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.isPresent())\n            {\n                logger.warn(\"Booting with ClusterMetadata from file: \" + CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.getString());\n                return BOOT_WITH_CLUSTERMETADATA;\n            }\n            if (seeds.isEmpty())\n                throw new IllegalArgumentException(\"Can not initialize CMS without any seeds\");\n\n            boolean hasAnyEpoch = SystemKeyspaceStorage.hasAnyEpoch();\n            // For CCM and local dev clusters\n            boolean isOnlySeed = DatabaseDescriptor.getSeeds().size() == 1\n                                 && DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort())\n                                 && DatabaseDescriptor.getSeeds().iterator().next().getAddress().isLoopbackAddress();\n            boolean hasBootedBefore = SystemKeyspace.getLocalHostId() != null;\n            logger.info(\"hasAnyEpoch = {}, hasBootedBefore = {}\", hasAnyEpoch, hasBootedBefore);\n            if (!hasAnyEpoch && hasBootedBefore)\n                return UPGRADE;\n            else if (hasAnyEpoch)\n                return NORMAL;\n            else if (isOnlySeed)\n                return FIRST_CMS;\n            else\n                return VOTE;\n        }\n    }\n}\n","lineNo":137}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.function.BiConsumer;\nimport java.util.function.Supplier;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.db.TypeSizes;\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.IVersionedSerializer;\nimport org.apache.cassandra.io.util.DataInputPlus;\nimport org.apache.cassandra.io.util.DataOutputPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.metrics.TCMMetrics;\nimport org.apache.cassandra.tcm.log.LogState;\nimport org.apache.cassandra.tcm.membership.NodeVersion;\nimport org.apache.cassandra.tcm.serialization.Version;\nimport org.apache.cassandra.net.*;\nimport org.apache.cassandra.tcm.membership.Directory;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.log.Entry;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.apache.cassandra.utils.vint.VIntCoding;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.*;\n\npublic class Commit\n{\n    private static final Logger logger = LoggerFactory.getLogger(Commit.class);\n\n    public static final IVersionedSerializer<Commit> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n    private static volatile Serializer serializerCache;\n    public static IVersionedSerializer<Commit> messageSerializer(Version version)\n    {\n        Serializer cached = serializerCache;\n        if (cached != null && cached.serializationVersion.equals(version))\n            return cached;\n        cached = new Serializer(version);\n        serializerCache = cached;\n        return cached;\n    }\n\n    private final Entry.Id entryId;\n    private final Transformation transform;\n    private final Epoch lastKnown;\n\n    public Commit(Entry.Id entryId, Transformation transform, Epoch lastKnown)\n    {\n        this.entryId = entryId;\n        this.transform = transform;\n        this.lastKnown = lastKnown;\n    }\n\n    public String toString()\n    {\n        return \"Commit{\" +\n               \"transformation=\" + transform +\n               \", lastKnown=\" + lastKnown +\n               '}';\n    }\n\n    static class Serializer implements IVersionedSerializer<Commit>\n    {\n        private final Version serializationVersion;\n\n        public Serializer(Version serializationVersion)\n        {\n            this.serializationVersion = serializationVersion;\n        }\n\n        public void serialize(Commit t, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeInt(serializationVersion.asInt());\n            Entry.Id.serializer.serialize(t.entryId, out, serializationVersion);\n            Transformation.transformationSerializer.serialize(t.transform, out, serializationVersion);\n            Epoch.serializer.serialize(t.lastKnown, out, serializationVersion);\n        }\n\n        public Commit deserialize(DataInputPlus in, int version) throws IOException\n        {\n            Version deserializationVersion = Version.fromInt(in.readInt());\n            Entry.Id entryId = Entry.Id.serializer.deserialize(in, deserializationVersion);\n            Transformation transform = Transformation.transformationSerializer.deserialize(in, deserializationVersion);\n            Epoch lastKnown = Epoch.serializer.deserialize(in, deserializationVersion);\n            return new Commit(entryId, transform, lastKnown);\n        }\n\n        public long serializedSize(Commit t, int version)\n        {\n            return TypeSizes.sizeof(serializationVersion.asInt()) +\n                   Transformation.transformationSerializer.serializedSize(t.transform, serializationVersion) +\n                   Entry.Id.serializer.serializedSize(t.entryId, serializationVersion) +\n                   Epoch.serializer.serializedSize(t.lastKnown, serializationVersion);\n        }\n    }\n\n    static volatile Result.Serializer resultSerializerCache;\n    public interface Result\n    {\n        boolean isSuccess();\n        boolean isFailure();\n\n        default Success success()\n        {\n            return (Success) this;\n        }\n\n        default Failure failure()\n        {\n            return (Failure) this;\n        }\n        IVersionedSerializer<Result> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n        static IVersionedSerializer<Result> messageSerializer(Version version)\n        {\n            Serializer cached = resultSerializerCache;\n            if (cached != null && cached.serializationVersion.equals(version))\n                return cached;\n            cached = new Serializer(version);\n            resultSerializerCache = cached;\n            return cached;\n        }\n\n        final class Success implements Result\n        {\n            public final Epoch epoch;\n            public final LogState logState;\n\n            public Success(Epoch epoch, LogState logState)\n            {\n                this.epoch = epoch;\n                this.logState = logState;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Success{\" +\n                       \"epoch=\" + epoch +\n                       \", logState=\" + logState +\n                       '}';\n            }\n\n            public boolean isSuccess()\n            {\n                return true;\n            }\n\n            public boolean isFailure()\n            {\n                return false;\n            }\n        }\n\n        final class Failure implements Result\n        {\n            public final ExceptionCode code;\n            public final String message;\n            // Rejection means that we were able to linearize the operation,\n            // but it was rejected by the internal logic of the transformation.\n            public final boolean rejected;\n\n            public Failure(ExceptionCode code, String message, boolean rejected)\n            {\n                if (message == null)\n                    message = \"\";\n                this.code = code;\n                // TypeSizes#sizeOf encoder only allows strings that are up to Short.MAX_VALUE bytes large\n                this.message =  message.substring(0, Math.min(message.length(), Short.MAX_VALUE));\n                this.rejected = rejected;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Failure{\" +\n                       \"code='\" + code + '\\'' +\n                       \"message='\" + message + '\\'' +\n                       \"rejected=\" + rejected +\n                       '}';\n            }\n\n            public boolean isSuccess()\n            {\n                return false;\n            }\n\n            public boolean isFailure()\n            {\n                return true;\n            }\n        }\n\n        class Serializer implements IVersionedSerializer<Result>\n        {\n            private static final byte SUCCESS = 1;\n            private static final byte REJECTED = 2;\n            private static final byte FAILED = 3;\n\n            private final Version serializationVersion;\n\n            public Serializer(Version serializationVersion)\n            {\n                this.serializationVersion = serializationVersion;\n            }\n\n            @Override\n            public void serialize(Result t, DataOutputPlus out, int version) throws IOException\n            {\n                if (t instanceof Success)\n                {\n                    out.writeByte(SUCCESS);\n                    out.writeUnsignedVInt32(serializationVersion.asInt());\n                    LogState.metadataSerializer.serialize(t.success().logState, out, serializationVersion);\n                    Epoch.serializer.serialize(t.success().epoch, out, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    Failure failure = (Failure) t;\n                    out.writeByte(failure.rejected ? REJECTED : FAILED);\n                    out.writeUnsignedVInt32(failure.code.value);\n                    out.writeUTF(failure.message);\n                }\n            }\n\n            @Override\n            public Result deserialize(DataInputPlus in, int version) throws IOException\n            {\n                int b = in.readByte();\n                if (b == SUCCESS)\n                {\n                    Version deserializationVersion = Version.fromInt(in.readUnsignedVInt32());\n                    LogState delta = LogState.metadataSerializer.deserialize(in, deserializationVersion);\n                    Epoch epoch = Epoch.serializer.deserialize(in, deserializationVersion);\n                    return new Success(epoch, delta);\n                }\n                else\n                {\n                    return new Failure(ExceptionCode.fromValue(in.readUnsignedVInt32()),\n                                       in.readUTF(),\n                                       b == REJECTED);\n                }\n            }\n\n            @Override\n            public long serializedSize(Result t, int version)\n            {\n                long size = TypeSizes.BYTE_SIZE;\n                if (t instanceof Success)\n                {\n                    size += VIntCoding.computeUnsignedVIntSize(serializationVersion.asInt());\n                    size += LogState.metadataSerializer.serializedSize(t.success().logState, serializationVersion);\n                    size += Epoch.serializer.serializedSize(t.success().epoch, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    size += VIntCoding.computeUnsignedVIntSize(((Failure) t).code.value);\n                    size += TypeSizes.sizeof(((Failure)t).message);\n                }\n                return size;\n            }\n        }\n    }\n\n    @VisibleForTesting\n    public static IVerbHandler<Commit> handlerForTests(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService)\n    {\n        return new Handler(processor, replicator, messagingService, () -> LOCAL);\n    }\n\n    static class Handler implements IVerbHandler<Commit>\n    {\n        private final Processor processor;\n        private final Replicator replicator;\n        private final BiConsumer<Message<?>, InetAddressAndPort> messagingService;\n        private final Supplier<ClusterMetadataService.State> cmsStateSupplier;\n\n        Handler(Processor processor, Replicator replicator, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this(processor, replicator, MessagingService.instance()::send, cmsStateSupplier);\n        }\n\n        Handler(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this.processor = processor;\n            this.replicator = replicator;\n            this.messagingService = messagingService;\n            this.cmsStateSupplier = cmsStateSupplier;\n        }\n\n        public void doVerb(Message<Commit> message) throws IOException\n        {\n            checkCMSState();\n            logger.info(\"Received commit request {} from {}\", message.payload, message.from());\n            Retry.Deadline retryPolicy = Retry.Deadline.at(message.expiresAtNanos(), new Retry.Jitter(TCMMetrics.instance.commitRetries));\n            Result result = processor.commit(message.payload.entryId, message.payload.transform, message.payload.lastKnown, retryPolicy);\n            if (result.isSuccess())\n            {\n                Result.Success success = result.success();\n                replicator.send(success, message.from());\n                logger.info(\"Responding with full result {} to sender {}\", result, message.from());\n                // TODO: this response message can get lost; how do we re-discover this on the other side?\n                // TODO: what if we have holes after replaying?\n                messagingService.accept(message.responseWith(result), message.from());\n            }\n            else\n            {\n                Result.Failure failure = result.failure();\n                messagingService.accept(message.responseWith(failure), message.from());\n            }\n        }\n\n        private void checkCMSState()\n        {\n            switch (cmsStateSupplier.get())\n            {\n                case RESET:\n                case LOCAL:\n                    break;\n                case REMOTE:\n                    throw new NotCMSException(\"Not currently a member of the CMS, can't commit\");\n                case GOSSIP:\n                    String msg = \"Tried to commit when in gossip mode\";\n                    logger.error(msg);\n                    throw new IllegalStateException(msg);\n                default:\n                    throw new IllegalStateException(\"Illegal state: \" + cmsStateSupplier.get());\n            }\n        }\n    }\n\n    public interface Replicator\n    {\n        Replicator NO_OP = (a,b) -> {};\n        void send(Result result, InetAddressAndPort source);\n    }\n\n    public static class DefaultReplicator implements Replicator\n    {\n        private final Supplier<Directory> directorySupplier;\n\n        public DefaultReplicator(Supplier<Directory> directorySupplier)\n        {\n            this.directorySupplier = directorySupplier;\n        }\n\n        public void send(Result result, InetAddressAndPort source)\n        {\n            if (!result.isSuccess())\n                return;\n\n            Result.Success success = result.success();\n            Directory directory = directorySupplier.get();\n\n            // Filter the log entries from the commit result for the purposes of replicating to members of the cluster\n            // other than the original submitter. We only need to include the sublist of entries starting at the one\n            // which was newly committed. We exclude entries before that one as the submitter may have been lagging and\n            // supplied a last known epoch arbitrarily in the past. We include entries after the first newly committed\n            // one as there may have been a new period automatically triggered and we'd like to push that out to all\n            // peers too. Of course, there may be other entries interspersed with these but it doesn't harm anything to\n            // include those too, it may simply be redundant.\n            LogState newlyCommitted = success.logState.retainFrom(success.epoch);\n            assert !newlyCommitted.isEmpty() : String.format(\"Nothing to replicate after retaining epochs since %s from %s\",\n                                                             success.epoch, success.logState);\n\n            for (NodeId peerId : directory.peerIds())\n            {\n                InetAddressAndPort endpoint = directory.endpoint(peerId);\n                boolean upgraded = directory.version(peerId).isUpgraded();\n                // Do not replicate to self and to the peer that has requested to commit this message\n                if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()) ||\n                    (source != null && source.equals(endpoint)) ||\n                    !upgraded)\n                {\n                    continue;\n                }\n\n                logger.info(\"Replicating newly committed transformations up to {} to {}\", newlyCommitted, endpoint);\n                MessagingService.instance().send(Message.out(Verb.TCM_REPLICATION, newlyCommitted), endpoint);\n            }\n        }\n    }\n\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.function.BiConsumer;\nimport java.util.function.Supplier;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.db.TypeSizes;\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.IVersionedSerializer;\nimport org.apache.cassandra.io.util.DataInputPlus;\nimport org.apache.cassandra.io.util.DataOutputPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.metrics.TCMMetrics;\nimport org.apache.cassandra.tcm.log.LogState;\nimport org.apache.cassandra.tcm.membership.NodeVersion;\nimport org.apache.cassandra.tcm.serialization.Version;\nimport org.apache.cassandra.net.*;\nimport org.apache.cassandra.tcm.membership.Directory;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.log.Entry;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.apache.cassandra.utils.vint.VIntCoding;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.*;\n\npublic class Commit\n{\n    private static final Logger logger = LoggerFactory.getLogger(Commit.class);\n\n    public static final IVersionedSerializer<Commit> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n    private static volatile Serializer serializerCache;\n    public static IVersionedSerializer<Commit> messageSerializer(Version version)\n    {\n        Serializer cached = serializerCache;\n        if (cached != null && cached.serializationVersion.equals(version))\n            return cached;\n        cached = new Serializer(version);\n        serializerCache = cached;\n        return cached;\n    }\n\n    private final Entry.Id entryId;\n    private final Transformation transform;\n    private final Epoch lastKnown;\n\n    public Commit(Entry.Id entryId, Transformation transform, Epoch lastKnown)\n    {\n        this.entryId = entryId;\n        this.transform = transform;\n        this.lastKnown = lastKnown;\n    }\n\n    public String toString()\n    {\n        return \"Commit{\" +\n               \"transformation=\" + transform +\n               \", lastKnown=\" + lastKnown +\n               '}';\n    }\n\n    static class Serializer implements IVersionedSerializer<Commit>\n    {\n        private final Version serializationVersion;\n\n        public Serializer(Version serializationVersion)\n        {\n            this.serializationVersion = serializationVersion;\n        }\n\n        public void serialize(Commit t, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeInt(serializationVersion.asInt());\n            Entry.Id.serializer.serialize(t.entryId, out, serializationVersion);\n            Transformation.transformationSerializer.serialize(t.transform, out, serializationVersion);\n            Epoch.serializer.serialize(t.lastKnown, out, serializationVersion);\n        }\n\n        public Commit deserialize(DataInputPlus in, int version) throws IOException\n        {\n            Version deserializationVersion = Version.fromInt(in.readInt());\n            Entry.Id entryId = Entry.Id.serializer.deserialize(in, deserializationVersion);\n            Transformation transform = Transformation.transformationSerializer.deserialize(in, deserializationVersion);\n            Epoch lastKnown = Epoch.serializer.deserialize(in, deserializationVersion);\n            return new Commit(entryId, transform, lastKnown);\n        }\n\n        public long serializedSize(Commit t, int version)\n        {\n            return TypeSizes.sizeof(serializationVersion.asInt()) +\n                   Transformation.transformationSerializer.serializedSize(t.transform, serializationVersion) +\n                   Entry.Id.serializer.serializedSize(t.entryId, serializationVersion) +\n                   Epoch.serializer.serializedSize(t.lastKnown, serializationVersion);\n        }\n    }\n\n    static volatile Result.Serializer resultSerializerCache;\n    public interface Result\n    {\n        IVersionedSerializer<Result> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n        LogState logState();\n        boolean isSuccess();\n        boolean isFailure();\n\n        default Success success()\n        {\n            return (Success) this;\n        }\n\n        default Failure failure()\n        {\n            return (Failure) this;\n        }\n\n        static IVersionedSerializer<Result> messageSerializer(Version version)\n        {\n            Serializer cached = resultSerializerCache;\n            if (cached != null && cached.serializationVersion.equals(version))\n                return cached;\n            cached = new Serializer(version);\n            resultSerializerCache = cached;\n            return cached;\n        }\n\n        final class Success implements Result\n        {\n            public final Epoch epoch;\n            public final LogState logState;\n\n            public Success(Epoch epoch, LogState logState)\n            {\n                this.epoch = epoch;\n                this.logState = logState;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Success{\" +\n                       \"epoch=\" + epoch +\n                       \", logState=\" + logState +\n                       '}';\n            }\n\n            @Override\n            public LogState logState()\n            {\n                return logState;\n            }\n\n            public boolean isSuccess()\n            {\n                return true;\n            }\n\n            public boolean isFailure()\n            {\n                return false;\n            }\n        }\n\n        static Failure rejected(ExceptionCode exceptionCode, String reason, LogState logState)\n        {\n            return new Failure(exceptionCode, reason, logState, true);\n        }\n\n        static Failure failed(ExceptionCode exceptionCode, String message)\n        {\n            return new Failure(exceptionCode, message, LogState.EMPTY, false);\n        }\n\n        final class Failure implements Result\n        {\n            public final ExceptionCode code;\n            public final String message;\n            // Rejection means that we were able to linearize the operation,\n            // but it was rejected by the internal logic of the transformation.\n            public final boolean rejected;\n            public final LogState logState;\n\n            private Failure(ExceptionCode code, String message, LogState logState, boolean rejected)\n            {\n                if (message == null)\n                    message = \"\";\n                this.code = code;\n                // TypeSizes#sizeOf encoder only allows strings that are up to Short.MAX_VALUE bytes large\n                this.message =  message.substring(0, Math.min(message.length(), Short.MAX_VALUE));\n                this.rejected = rejected;\n                this.logState = logState;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Failure{\" +\n                       \"code='\" + code + '\\'' +\n                       \"message='\" + message + '\\'' +\n                       \"rejected=\" + rejected +\n                       '}';\n            }\n\n            @Override\n            public LogState logState()\n            {\n                return logState;\n            }\n\n            public boolean isSuccess()\n            {\n                return false;\n            }\n\n            public boolean isFailure()\n            {\n                return true;\n            }\n        }\n\n        class Serializer implements IVersionedSerializer<Result>\n        {\n            private static final byte SUCCESS = 1;\n            private static final byte REJECTED = 2;\n            private static final byte FAILED = 3;\n\n            private final Version serializationVersion;\n\n            public Serializer(Version serializationVersion)\n            {\n                this.serializationVersion = serializationVersion;\n            }\n\n            @Override\n            public void serialize(Result t, DataOutputPlus out, int version) throws IOException\n            {\n                if (t instanceof Success)\n                {\n                    out.writeByte(SUCCESS);\n                    out.writeUnsignedVInt32(serializationVersion.asInt());\n                    LogState.metadataSerializer.serialize(t.logState(), out, serializationVersion);\n                    Epoch.serializer.serialize(t.success().epoch, out, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    Failure failure = (Failure) t;\n                    out.writeByte(failure.rejected ? REJECTED : FAILED);\n                    out.writeUnsignedVInt32(failure.code.value);\n                    out.writeUTF(failure.message);\n                    out.writeUnsignedVInt32(serializationVersion.asInt());\n                    LogState.metadataSerializer.serialize(t.logState(), out, serializationVersion);\n                }\n            }\n\n            @Override\n            public Result deserialize(DataInputPlus in, int version) throws IOException\n            {\n                int b = in.readByte();\n                if (b == SUCCESS)\n                {\n                    Version deserializationVersion = Version.fromInt(in.readUnsignedVInt32());\n                    LogState delta = LogState.metadataSerializer.deserialize(in, deserializationVersion);\n                    Epoch epoch = Epoch.serializer.deserialize(in, deserializationVersion);\n                    return new Success(epoch, delta);\n                }\n                else\n                {\n                    ExceptionCode exceptionCode = ExceptionCode.fromValue(in.readUnsignedVInt32());\n                    String message = in.readUTF();\n                    Version deserializationVersion = Version.fromInt(in.readUnsignedVInt32());\n                    LogState delta = LogState.metadataSerializer.deserialize(in, deserializationVersion);\n                    return new Failure(exceptionCode,\n                                       message,\n                                       delta,\n                                       b == REJECTED);\n                }\n            }\n\n            @Override\n            public long serializedSize(Result t, int version)\n            {\n                long size = TypeSizes.BYTE_SIZE;\n                if (t instanceof Success)\n                {\n                    size += VIntCoding.computeUnsignedVIntSize(serializationVersion.asInt());\n                    size += LogState.metadataSerializer.serializedSize(t.logState(), serializationVersion);\n                    size += Epoch.serializer.serializedSize(t.success().epoch, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    size += VIntCoding.computeUnsignedVIntSize(((Failure) t).code.value);\n                    size += TypeSizes.sizeof(((Failure)t).message);\n                    size += VIntCoding.computeUnsignedVIntSize(serializationVersion.asInt());\n                    size += LogState.metadataSerializer.serializedSize(t.logState(), serializationVersion);\n                }\n                return size;\n            }\n        }\n    }\n\n    @VisibleForTesting\n    public static IVerbHandler<Commit> handlerForTests(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService)\n    {\n        return new Handler(processor, replicator, messagingService, () -> LOCAL);\n    }\n\n    static class Handler implements IVerbHandler<Commit>\n    {\n        private final Processor processor;\n        private final Replicator replicator;\n        private final BiConsumer<Message<?>, InetAddressAndPort> messagingService;\n        private final Supplier<ClusterMetadataService.State> cmsStateSupplier;\n\n        Handler(Processor processor, Replicator replicator, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this(processor, replicator, MessagingService.instance()::send, cmsStateSupplier);\n        }\n\n        Handler(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this.processor = processor;\n            this.replicator = replicator;\n            this.messagingService = messagingService;\n            this.cmsStateSupplier = cmsStateSupplier;\n        }\n\n        public void doVerb(Message<Commit> message) throws IOException\n        {\n            checkCMSState();\n            logger.info(\"Received commit request {} from {}\", message.payload, message.from());\n            Retry.Deadline retryPolicy = Retry.Deadline.at(message.expiresAtNanos(), new Retry.Jitter(TCMMetrics.instance.commitRetries));\n            Result result = processor.commit(message.payload.entryId, message.payload.transform, message.payload.lastKnown, retryPolicy);\n            if (result.isSuccess())\n            {\n                Result.Success success = result.success();\n                replicator.send(success, message.from());\n                logger.info(\"Responding with full result {} to sender {}\", result, message.from());\n                // TODO: this response message can get lost; how do we re-discover this on the other side?\n                // TODO: what if we have holes after replaying?\n                messagingService.accept(message.responseWith(result), message.from());\n            }\n            else\n            {\n                Result.Failure failure = result.failure();\n                messagingService.accept(message.responseWith(failure), message.from());\n            }\n        }\n\n        private void checkCMSState()\n        {\n            switch (cmsStateSupplier.get())\n            {\n                case RESET:\n                case LOCAL:\n                    break;\n                case REMOTE:\n                    throw new NotCMSException(\"Not currently a member of the CMS, can't commit\");\n                case GOSSIP:\n                    String msg = \"Tried to commit when in gossip mode\";\n                    logger.error(msg);\n                    throw new IllegalStateException(msg);\n                default:\n                    throw new IllegalStateException(\"Illegal state: \" + cmsStateSupplier.get());\n            }\n        }\n    }\n\n    public interface Replicator\n    {\n        Replicator NO_OP = (a,b) -> {};\n        void send(Result result, InetAddressAndPort source);\n    }\n\n    public static class DefaultReplicator implements Replicator\n    {\n        private final Supplier<Directory> directorySupplier;\n\n        public DefaultReplicator(Supplier<Directory> directorySupplier)\n        {\n            this.directorySupplier = directorySupplier;\n        }\n\n        public void send(Result result, InetAddressAndPort source)\n        {\n            if (!result.isSuccess())\n                return;\n\n            Result.Success success = result.success();\n            Directory directory = directorySupplier.get();\n\n            // Filter the log entries from the commit result for the purposes of replicating to members of the cluster\n            // other than the original submitter. We only need to include the sublist of entries starting at the one\n            // which was newly committed. We exclude entries before that one as the submitter may have been lagging and\n            // supplied a last known epoch arbitrarily in the past. We include entries after the first newly committed\n            // one as there may have been a new period automatically triggered and we'd like to push that out to all\n            // peers too. Of course, there may be other entries interspersed with these but it doesn't harm anything to\n            // include those too, it may simply be redundant.\n            LogState newlyCommitted = success.logState.retainFrom(success.epoch);\n            assert !newlyCommitted.isEmpty() : String.format(\"Nothing to replicate after retaining epochs since %s from %s\",\n                                                             success.epoch, success.logState);\n\n            for (NodeId peerId : directory.peerIds())\n            {\n                InetAddressAndPort endpoint = directory.endpoint(peerId);\n                boolean upgraded = directory.version(peerId).isUpgraded();\n                // Do not replicate to self and to the peer that has requested to commit this message\n                if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()) ||\n                    (source != null && source.equals(endpoint)) ||\n                    !upgraded)\n                {\n                    continue;\n                }\n\n                logger.info(\"Replicating newly committed transformations up to {} to {}\", newlyCommitted, endpoint);\n                MessagingService.instance().send(Message.out(Verb.TCM_REPLICATION, newlyCommitted), endpoint);\n            }\n        }\n    }\n\n}\n","lineNo":290}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.function.BiConsumer;\nimport java.util.function.Supplier;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.db.TypeSizes;\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.IVersionedSerializer;\nimport org.apache.cassandra.io.util.DataInputPlus;\nimport org.apache.cassandra.io.util.DataOutputPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.metrics.TCMMetrics;\nimport org.apache.cassandra.tcm.log.LogState;\nimport org.apache.cassandra.tcm.membership.NodeVersion;\nimport org.apache.cassandra.tcm.serialization.Version;\nimport org.apache.cassandra.net.*;\nimport org.apache.cassandra.tcm.membership.Directory;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.log.Entry;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.apache.cassandra.utils.vint.VIntCoding;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.*;\n\npublic class Commit\n{\n    private static final Logger logger = LoggerFactory.getLogger(Commit.class);\n\n    public static final IVersionedSerializer<Commit> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n    private static volatile Serializer serializerCache;\n    public static IVersionedSerializer<Commit> messageSerializer(Version version)\n    {\n        Serializer cached = serializerCache;\n        if (cached != null && cached.serializationVersion.equals(version))\n            return cached;\n        cached = new Serializer(version);\n        serializerCache = cached;\n        return cached;\n    }\n\n    private final Entry.Id entryId;\n    private final Transformation transform;\n    private final Epoch lastKnown;\n\n    public Commit(Entry.Id entryId, Transformation transform, Epoch lastKnown)\n    {\n        this.entryId = entryId;\n        this.transform = transform;\n        this.lastKnown = lastKnown;\n    }\n\n    public String toString()\n    {\n        return \"Commit{\" +\n               \"transformation=\" + transform +\n               \", lastKnown=\" + lastKnown +\n               '}';\n    }\n\n    static class Serializer implements IVersionedSerializer<Commit>\n    {\n        private final Version serializationVersion;\n\n        public Serializer(Version serializationVersion)\n        {\n            this.serializationVersion = serializationVersion;\n        }\n\n        public void serialize(Commit t, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeInt(serializationVersion.asInt());\n            Entry.Id.serializer.serialize(t.entryId, out, serializationVersion);\n            Transformation.transformationSerializer.serialize(t.transform, out, serializationVersion);\n            Epoch.serializer.serialize(t.lastKnown, out, serializationVersion);\n        }\n\n        public Commit deserialize(DataInputPlus in, int version) throws IOException\n        {\n            Version deserializationVersion = Version.fromInt(in.readInt());\n            Entry.Id entryId = Entry.Id.serializer.deserialize(in, deserializationVersion);\n            Transformation transform = Transformation.transformationSerializer.deserialize(in, deserializationVersion);\n            Epoch lastKnown = Epoch.serializer.deserialize(in, deserializationVersion);\n            return new Commit(entryId, transform, lastKnown);\n        }\n\n        public long serializedSize(Commit t, int version)\n        {\n            return TypeSizes.sizeof(serializationVersion.asInt()) +\n                   Transformation.transformationSerializer.serializedSize(t.transform, serializationVersion) +\n                   Entry.Id.serializer.serializedSize(t.entryId, serializationVersion) +\n                   Epoch.serializer.serializedSize(t.lastKnown, serializationVersion);\n        }\n    }\n\n    static volatile Result.Serializer resultSerializerCache;\n    public interface Result\n    {\n        boolean isSuccess();\n        boolean isFailure();\n\n        default Success success()\n        {\n            return (Success) this;\n        }\n\n        default Failure failure()\n        {\n            return (Failure) this;\n        }\n        IVersionedSerializer<Result> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n        static IVersionedSerializer<Result> messageSerializer(Version version)\n        {\n            Serializer cached = resultSerializerCache;\n            if (cached != null && cached.serializationVersion.equals(version))\n                return cached;\n            cached = new Serializer(version);\n            resultSerializerCache = cached;\n            return cached;\n        }\n\n        final class Success implements Result\n        {\n            public final Epoch epoch;\n            public final LogState logState;\n\n            public Success(Epoch epoch, LogState logState)\n            {\n                this.epoch = epoch;\n                this.logState = logState;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Success{\" +\n                       \"epoch=\" + epoch +\n                       \", logState=\" + logState +\n                       '}';\n            }\n\n            public boolean isSuccess()\n            {\n                return true;\n            }\n\n            public boolean isFailure()\n            {\n                return false;\n            }\n        }\n\n        final class Failure implements Result\n        {\n            public final ExceptionCode code;\n            public final String message;\n            // Rejection means that we were able to linearize the operation,\n            // but it was rejected by the internal logic of the transformation.\n            public final boolean rejected;\n\n            public Failure(ExceptionCode code, String message, boolean rejected)\n            {\n                if (message == null)\n                    message = \"\";\n                this.code = code;\n                // TypeSizes#sizeOf encoder only allows strings that are up to Short.MAX_VALUE bytes large\n                this.message =  message.substring(0, Math.min(message.length(), Short.MAX_VALUE));\n                this.rejected = rejected;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Failure{\" +\n                       \"code='\" + code + '\\'' +\n                       \"message='\" + message + '\\'' +\n                       \"rejected=\" + rejected +\n                       '}';\n            }\n\n            public boolean isSuccess()\n            {\n                return false;\n            }\n\n            public boolean isFailure()\n            {\n                return true;\n            }\n        }\n\n        class Serializer implements IVersionedSerializer<Result>\n        {\n            private static final byte SUCCESS = 1;\n            private static final byte REJECTED = 2;\n            private static final byte FAILED = 3;\n\n            private final Version serializationVersion;\n\n            public Serializer(Version serializationVersion)\n            {\n                this.serializationVersion = serializationVersion;\n            }\n\n            @Override\n            public void serialize(Result t, DataOutputPlus out, int version) throws IOException\n            {\n                if (t instanceof Success)\n                {\n                    out.writeByte(SUCCESS);\n                    out.writeUnsignedVInt32(serializationVersion.asInt());\n                    LogState.metadataSerializer.serialize(t.success().logState, out, serializationVersion);\n                    Epoch.serializer.serialize(t.success().epoch, out, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    Failure failure = (Failure) t;\n                    out.writeByte(failure.rejected ? REJECTED : FAILED);\n                    out.writeUnsignedVInt32(failure.code.value);\n                    out.writeUTF(failure.message);\n                }\n            }\n\n            @Override\n            public Result deserialize(DataInputPlus in, int version) throws IOException\n            {\n                int b = in.readByte();\n                if (b == SUCCESS)\n                {\n                    Version deserializationVersion = Version.fromInt(in.readUnsignedVInt32());\n                    LogState delta = LogState.metadataSerializer.deserialize(in, deserializationVersion);\n                    Epoch epoch = Epoch.serializer.deserialize(in, deserializationVersion);\n                    return new Success(epoch, delta);\n                }\n                else\n                {\n                    return new Failure(ExceptionCode.fromValue(in.readUnsignedVInt32()),\n                                       in.readUTF(),\n                                       b == REJECTED);\n                }\n            }\n\n            @Override\n            public long serializedSize(Result t, int version)\n            {\n                long size = TypeSizes.BYTE_SIZE;\n                if (t instanceof Success)\n                {\n                    size += VIntCoding.computeUnsignedVIntSize(serializationVersion.asInt());\n                    size += LogState.metadataSerializer.serializedSize(t.success().logState, serializationVersion);\n                    size += Epoch.serializer.serializedSize(t.success().epoch, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    size += VIntCoding.computeUnsignedVIntSize(((Failure) t).code.value);\n                    size += TypeSizes.sizeof(((Failure)t).message);\n                }\n                return size;\n            }\n        }\n    }\n\n    @VisibleForTesting\n    public static IVerbHandler<Commit> handlerForTests(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService)\n    {\n        return new Handler(processor, replicator, messagingService, () -> LOCAL);\n    }\n\n    static class Handler implements IVerbHandler<Commit>\n    {\n        private final Processor processor;\n        private final Replicator replicator;\n        private final BiConsumer<Message<?>, InetAddressAndPort> messagingService;\n        private final Supplier<ClusterMetadataService.State> cmsStateSupplier;\n\n        Handler(Processor processor, Replicator replicator, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this(processor, replicator, MessagingService.instance()::send, cmsStateSupplier);\n        }\n\n        Handler(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this.processor = processor;\n            this.replicator = replicator;\n            this.messagingService = messagingService;\n            this.cmsStateSupplier = cmsStateSupplier;\n        }\n\n        public void doVerb(Message<Commit> message) throws IOException\n        {\n            checkCMSState();\n            logger.info(\"Received commit request {} from {}\", message.payload, message.from());\n            Retry.Deadline retryPolicy = Retry.Deadline.at(message.expiresAtNanos(), new Retry.Jitter(TCMMetrics.instance.commitRetries));\n            Result result = processor.commit(message.payload.entryId, message.payload.transform, message.payload.lastKnown, retryPolicy);\n            if (result.isSuccess())\n            {\n                Result.Success success = result.success();\n                replicator.send(success, message.from());\n                logger.info(\"Responding with full result {} to sender {}\", result, message.from());\n                // TODO: this response message can get lost; how do we re-discover this on the other side?\n                // TODO: what if we have holes after replaying?\n                messagingService.accept(message.responseWith(result), message.from());\n            }\n            else\n            {\n                Result.Failure failure = result.failure();\n                messagingService.accept(message.responseWith(failure), message.from());\n            }\n        }\n\n        private void checkCMSState()\n        {\n            switch (cmsStateSupplier.get())\n            {\n                case RESET:\n                case LOCAL:\n                    break;\n                case REMOTE:\n                    throw new NotCMSException(\"Not currently a member of the CMS, can't commit\");\n                case GOSSIP:\n                    String msg = \"Tried to commit when in gossip mode\";\n                    logger.error(msg);\n                    throw new IllegalStateException(msg);\n                default:\n                    throw new IllegalStateException(\"Illegal state: \" + cmsStateSupplier.get());\n            }\n        }\n    }\n\n    public interface Replicator\n    {\n        Replicator NO_OP = (a,b) -> {};\n        void send(Result result, InetAddressAndPort source);\n    }\n\n    public static class DefaultReplicator implements Replicator\n    {\n        private final Supplier<Directory> directorySupplier;\n\n        public DefaultReplicator(Supplier<Directory> directorySupplier)\n        {\n            this.directorySupplier = directorySupplier;\n        }\n\n        public void send(Result result, InetAddressAndPort source)\n        {\n            if (!result.isSuccess())\n                return;\n\n            Result.Success success = result.success();\n            Directory directory = directorySupplier.get();\n\n            // Filter the log entries from the commit result for the purposes of replicating to members of the cluster\n            // other than the original submitter. We only need to include the sublist of entries starting at the one\n            // which was newly committed. We exclude entries before that one as the submitter may have been lagging and\n            // supplied a last known epoch arbitrarily in the past. We include entries after the first newly committed\n            // one as there may have been a new period automatically triggered and we'd like to push that out to all\n            // peers too. Of course, there may be other entries interspersed with these but it doesn't harm anything to\n            // include those too, it may simply be redundant.\n            LogState newlyCommitted = success.logState.retainFrom(success.epoch);\n            assert !newlyCommitted.isEmpty() : String.format(\"Nothing to replicate after retaining epochs since %s from %s\",\n                                                             success.epoch, success.logState);\n\n            for (NodeId peerId : directory.peerIds())\n            {\n                InetAddressAndPort endpoint = directory.endpoint(peerId);\n                boolean upgraded = directory.version(peerId).isUpgraded();\n                // Do not replicate to self and to the peer that has requested to commit this message\n                if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()) ||\n                    (source != null && source.equals(endpoint)) ||\n                    !upgraded)\n                {\n                    continue;\n                }\n\n                logger.info(\"Replicating newly committed transformations up to {} to {}\", newlyCommitted, endpoint);\n                MessagingService.instance().send(Message.out(Verb.TCM_REPLICATION, newlyCommitted), endpoint);\n            }\n        }\n    }\n\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.function.BiConsumer;\nimport java.util.function.Supplier;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.db.TypeSizes;\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.IVersionedSerializer;\nimport org.apache.cassandra.io.util.DataInputPlus;\nimport org.apache.cassandra.io.util.DataOutputPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.metrics.TCMMetrics;\nimport org.apache.cassandra.tcm.log.LogState;\nimport org.apache.cassandra.tcm.membership.NodeVersion;\nimport org.apache.cassandra.tcm.serialization.Version;\nimport org.apache.cassandra.net.*;\nimport org.apache.cassandra.tcm.membership.Directory;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.log.Entry;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.apache.cassandra.utils.vint.VIntCoding;\n\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.*;\n\npublic class Commit\n{\n    private static final Logger logger = LoggerFactory.getLogger(Commit.class);\n\n    public static final IVersionedSerializer<Commit> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n    private static volatile Serializer serializerCache;\n    public static IVersionedSerializer<Commit> messageSerializer(Version version)\n    {\n        Serializer cached = serializerCache;\n        if (cached != null && cached.serializationVersion.equals(version))\n            return cached;\n        cached = new Serializer(version);\n        serializerCache = cached;\n        return cached;\n    }\n\n    private final Entry.Id entryId;\n    private final Transformation transform;\n    private final Epoch lastKnown;\n\n    public Commit(Entry.Id entryId, Transformation transform, Epoch lastKnown)\n    {\n        this.entryId = entryId;\n        this.transform = transform;\n        this.lastKnown = lastKnown;\n    }\n\n    public String toString()\n    {\n        return \"Commit{\" +\n               \"transformation=\" + transform +\n               \", lastKnown=\" + lastKnown +\n               '}';\n    }\n\n    static class Serializer implements IVersionedSerializer<Commit>\n    {\n        private final Version serializationVersion;\n\n        public Serializer(Version serializationVersion)\n        {\n            this.serializationVersion = serializationVersion;\n        }\n\n        public void serialize(Commit t, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeInt(serializationVersion.asInt());\n            Entry.Id.serializer.serialize(t.entryId, out, serializationVersion);\n            Transformation.transformationSerializer.serialize(t.transform, out, serializationVersion);\n            Epoch.serializer.serialize(t.lastKnown, out, serializationVersion);\n        }\n\n        public Commit deserialize(DataInputPlus in, int version) throws IOException\n        {\n            Version deserializationVersion = Version.fromInt(in.readInt());\n            Entry.Id entryId = Entry.Id.serializer.deserialize(in, deserializationVersion);\n            Transformation transform = Transformation.transformationSerializer.deserialize(in, deserializationVersion);\n            Epoch lastKnown = Epoch.serializer.deserialize(in, deserializationVersion);\n            return new Commit(entryId, transform, lastKnown);\n        }\n\n        public long serializedSize(Commit t, int version)\n        {\n            return TypeSizes.sizeof(serializationVersion.asInt()) +\n                   Transformation.transformationSerializer.serializedSize(t.transform, serializationVersion) +\n                   Entry.Id.serializer.serializedSize(t.entryId, serializationVersion) +\n                   Epoch.serializer.serializedSize(t.lastKnown, serializationVersion);\n        }\n    }\n\n    static volatile Result.Serializer resultSerializerCache;\n    public interface Result\n    {\n        IVersionedSerializer<Result> defaultMessageSerializer = new Serializer(NodeVersion.CURRENT.serializationVersion());\n\n        LogState logState();\n        boolean isSuccess();\n        boolean isFailure();\n\n        default Success success()\n        {\n            return (Success) this;\n        }\n\n        default Failure failure()\n        {\n            return (Failure) this;\n        }\n\n        static IVersionedSerializer<Result> messageSerializer(Version version)\n        {\n            Serializer cached = resultSerializerCache;\n            if (cached != null && cached.serializationVersion.equals(version))\n                return cached;\n            cached = new Serializer(version);\n            resultSerializerCache = cached;\n            return cached;\n        }\n\n        final class Success implements Result\n        {\n            public final Epoch epoch;\n            public final LogState logState;\n\n            public Success(Epoch epoch, LogState logState)\n            {\n                this.epoch = epoch;\n                this.logState = logState;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Success{\" +\n                       \"epoch=\" + epoch +\n                       \", logState=\" + logState +\n                       '}';\n            }\n\n            @Override\n            public LogState logState()\n            {\n                return logState;\n            }\n\n            public boolean isSuccess()\n            {\n                return true;\n            }\n\n            public boolean isFailure()\n            {\n                return false;\n            }\n        }\n\n        static Failure rejected(ExceptionCode exceptionCode, String reason, LogState logState)\n        {\n            return new Failure(exceptionCode, reason, logState, true);\n        }\n\n        static Failure failed(ExceptionCode exceptionCode, String message)\n        {\n            return new Failure(exceptionCode, message, LogState.EMPTY, false);\n        }\n\n        final class Failure implements Result\n        {\n            public final ExceptionCode code;\n            public final String message;\n            // Rejection means that we were able to linearize the operation,\n            // but it was rejected by the internal logic of the transformation.\n            public final boolean rejected;\n            public final LogState logState;\n\n            private Failure(ExceptionCode code, String message, LogState logState, boolean rejected)\n            {\n                if (message == null)\n                    message = \"\";\n                this.code = code;\n                // TypeSizes#sizeOf encoder only allows strings that are up to Short.MAX_VALUE bytes large\n                this.message =  message.substring(0, Math.min(message.length(), Short.MAX_VALUE));\n                this.rejected = rejected;\n                this.logState = logState;\n            }\n\n            @Override\n            public String toString()\n            {\n                return \"Failure{\" +\n                       \"code='\" + code + '\\'' +\n                       \"message='\" + message + '\\'' +\n                       \"rejected=\" + rejected +\n                       '}';\n            }\n\n            @Override\n            public LogState logState()\n            {\n                return logState;\n            }\n\n            public boolean isSuccess()\n            {\n                return false;\n            }\n\n            public boolean isFailure()\n            {\n                return true;\n            }\n        }\n\n        class Serializer implements IVersionedSerializer<Result>\n        {\n            private static final byte SUCCESS = 1;\n            private static final byte REJECTED = 2;\n            private static final byte FAILED = 3;\n\n            private final Version serializationVersion;\n\n            public Serializer(Version serializationVersion)\n            {\n                this.serializationVersion = serializationVersion;\n            }\n\n            @Override\n            public void serialize(Result t, DataOutputPlus out, int version) throws IOException\n            {\n                if (t instanceof Success)\n                {\n                    out.writeByte(SUCCESS);\n                    out.writeUnsignedVInt32(serializationVersion.asInt());\n                    LogState.metadataSerializer.serialize(t.logState(), out, serializationVersion);\n                    Epoch.serializer.serialize(t.success().epoch, out, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    Failure failure = (Failure) t;\n                    out.writeByte(failure.rejected ? REJECTED : FAILED);\n                    out.writeUnsignedVInt32(failure.code.value);\n                    out.writeUTF(failure.message);\n                    out.writeUnsignedVInt32(serializationVersion.asInt());\n                    LogState.metadataSerializer.serialize(t.logState(), out, serializationVersion);\n                }\n            }\n\n            @Override\n            public Result deserialize(DataInputPlus in, int version) throws IOException\n            {\n                int b = in.readByte();\n                if (b == SUCCESS)\n                {\n                    Version deserializationVersion = Version.fromInt(in.readUnsignedVInt32());\n                    LogState delta = LogState.metadataSerializer.deserialize(in, deserializationVersion);\n                    Epoch epoch = Epoch.serializer.deserialize(in, deserializationVersion);\n                    return new Success(epoch, delta);\n                }\n                else\n                {\n                    ExceptionCode exceptionCode = ExceptionCode.fromValue(in.readUnsignedVInt32());\n                    String message = in.readUTF();\n                    Version deserializationVersion = Version.fromInt(in.readUnsignedVInt32());\n                    LogState delta = LogState.metadataSerializer.deserialize(in, deserializationVersion);\n                    return new Failure(exceptionCode,\n                                       message,\n                                       delta,\n                                       b == REJECTED);\n                }\n            }\n\n            @Override\n            public long serializedSize(Result t, int version)\n            {\n                long size = TypeSizes.BYTE_SIZE;\n                if (t instanceof Success)\n                {\n                    size += VIntCoding.computeUnsignedVIntSize(serializationVersion.asInt());\n                    size += LogState.metadataSerializer.serializedSize(t.logState(), serializationVersion);\n                    size += Epoch.serializer.serializedSize(t.success().epoch, serializationVersion);\n                }\n                else\n                {\n                    assert t instanceof Failure;\n                    size += VIntCoding.computeUnsignedVIntSize(((Failure) t).code.value);\n                    size += TypeSizes.sizeof(((Failure)t).message);\n                    size += VIntCoding.computeUnsignedVIntSize(serializationVersion.asInt());\n                    size += LogState.metadataSerializer.serializedSize(t.logState(), serializationVersion);\n                }\n                return size;\n            }\n        }\n    }\n\n    @VisibleForTesting\n    public static IVerbHandler<Commit> handlerForTests(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService)\n    {\n        return new Handler(processor, replicator, messagingService, () -> LOCAL);\n    }\n\n    static class Handler implements IVerbHandler<Commit>\n    {\n        private final Processor processor;\n        private final Replicator replicator;\n        private final BiConsumer<Message<?>, InetAddressAndPort> messagingService;\n        private final Supplier<ClusterMetadataService.State> cmsStateSupplier;\n\n        Handler(Processor processor, Replicator replicator, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this(processor, replicator, MessagingService.instance()::send, cmsStateSupplier);\n        }\n\n        Handler(Processor processor, Replicator replicator, BiConsumer<Message<?>, InetAddressAndPort> messagingService, Supplier<ClusterMetadataService.State> cmsStateSupplier)\n        {\n            this.processor = processor;\n            this.replicator = replicator;\n            this.messagingService = messagingService;\n            this.cmsStateSupplier = cmsStateSupplier;\n        }\n\n        public void doVerb(Message<Commit> message) throws IOException\n        {\n            checkCMSState();\n            logger.info(\"Received commit request {} from {}\", message.payload, message.from());\n            Retry.Deadline retryPolicy = Retry.Deadline.at(message.expiresAtNanos(), new Retry.Jitter(TCMMetrics.instance.commitRetries));\n            Result result = processor.commit(message.payload.entryId, message.payload.transform, message.payload.lastKnown, retryPolicy);\n            if (result.isSuccess())\n            {\n                Result.Success success = result.success();\n                replicator.send(success, message.from());\n                logger.info(\"Responding with full result {} to sender {}\", result, message.from());\n                // TODO: this response message can get lost; how do we re-discover this on the other side?\n                // TODO: what if we have holes after replaying?\n                messagingService.accept(message.responseWith(result), message.from());\n            }\n            else\n            {\n                Result.Failure failure = result.failure();\n                messagingService.accept(message.responseWith(failure), message.from());\n            }\n        }\n\n        private void checkCMSState()\n        {\n            switch (cmsStateSupplier.get())\n            {\n                case RESET:\n                case LOCAL:\n                    break;\n                case REMOTE:\n                    throw new NotCMSException(\"Not currently a member of the CMS, can't commit\");\n                case GOSSIP:\n                    String msg = \"Tried to commit when in gossip mode\";\n                    logger.error(msg);\n                    throw new IllegalStateException(msg);\n                default:\n                    throw new IllegalStateException(\"Illegal state: \" + cmsStateSupplier.get());\n            }\n        }\n    }\n\n    public interface Replicator\n    {\n        Replicator NO_OP = (a,b) -> {};\n        void send(Result result, InetAddressAndPort source);\n    }\n\n    public static class DefaultReplicator implements Replicator\n    {\n        private final Supplier<Directory> directorySupplier;\n\n        public DefaultReplicator(Supplier<Directory> directorySupplier)\n        {\n            this.directorySupplier = directorySupplier;\n        }\n\n        public void send(Result result, InetAddressAndPort source)\n        {\n            if (!result.isSuccess())\n                return;\n\n            Result.Success success = result.success();\n            Directory directory = directorySupplier.get();\n\n            // Filter the log entries from the commit result for the purposes of replicating to members of the cluster\n            // other than the original submitter. We only need to include the sublist of entries starting at the one\n            // which was newly committed. We exclude entries before that one as the submitter may have been lagging and\n            // supplied a last known epoch arbitrarily in the past. We include entries after the first newly committed\n            // one as there may have been a new period automatically triggered and we'd like to push that out to all\n            // peers too. Of course, there may be other entries interspersed with these but it doesn't harm anything to\n            // include those too, it may simply be redundant.\n            LogState newlyCommitted = success.logState.retainFrom(success.epoch);\n            assert !newlyCommitted.isEmpty() : String.format(\"Nothing to replicate after retaining epochs since %s from %s\",\n                                                             success.epoch, success.logState);\n\n            for (NodeId peerId : directory.peerIds())\n            {\n                InetAddressAndPort endpoint = directory.endpoint(peerId);\n                boolean upgraded = directory.version(peerId).isUpgraded();\n                // Do not replicate to self and to the peer that has requested to commit this message\n                if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()) ||\n                    (source != null && source.equals(endpoint)) ||\n                    !upgraded)\n                {\n                    continue;\n                }\n\n                logger.info(\"Replicating newly committed transformations up to {} to {}\", newlyCommitted, endpoint);\n                MessagingService.instance().send(Message.out(Verb.TCM_REPLICATION, newlyCommitted), endpoint);\n            }\n        }\n    }\n\n}\n","lineNo":291}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.util.concurrent.Uninterruptibles;\nimport org.junit.After;\nimport org.junit.AfterClass;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.ConsistencyLevel;\nimport org.apache.cassandra.distributed.api.ICluster;\nimport org.apache.cassandra.distributed.shared.InstanceClassLoader;\nimport org.apache.cassandra.exceptions.CasWriteTimeoutException;\nimport org.apache.cassandra.exceptions.CasWriteUnknownResultException;\nimport org.apache.cassandra.net.Verb;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.hamcrest.BaseMatcher;\nimport org.hamcrest.Description;\n\nimport static org.hamcrest.CoreMatchers.containsString;\nimport static org.apache.cassandra.distributed.shared.AssertUtils.*;\n\n// TODO: this test should be removed after running in-jvm dtests is set up via the shared API repository\npublic class CasWriteTest extends TestBaseImpl\n{\n    // Sharing the same cluster to boost test speed. Using a pkGen to make sure queries has distinct pk value for paxos instances.\n    private static ICluster cluster;\n    private static final AtomicInteger pkGen = new AtomicInteger(1_000); // preserve any pk values less than 1000 for manual queries.\n    private static final Logger logger = LoggerFactory.getLogger(CasWriteTest.class);\n\n    @Rule\n    public ExpectedException thrown = ExpectedException.none();\n\n    @BeforeClass\n    public static void setupCluster() throws Throwable\n    {\n        cluster = init(Cluster.build().withNodes(3).start());\n        cluster.schemaChange(\"CREATE TABLE \" + KEYSPACE + \".tbl (pk int, ck int, v int, PRIMARY KEY (pk, ck))\");\n    }\n\n    @AfterClass\n    public static void close() throws Exception\n    {\n        cluster.close();\n        cluster = null;\n    }\n\n    @Before @After\n    public void resetFilters()\n    {\n        cluster.filters().reset();\n    }\n\n    @Test\n    public void testCasWriteSuccessWithNoContention()\n    {\n        cluster.coordinator(1).execute(\"INSERT INTO \" + KEYSPACE + \".tbl (pk, ck, v) VALUES (1, 1, 1) IF NOT EXISTS\",\n                                       ConsistencyLevel.QUORUM);\n        assertRows(cluster.coordinator(1).execute(\"SELECT * FROM \" + KEYSPACE + \".tbl WHERE pk = 1\",\n                                                  ConsistencyLevel.QUORUM),\n                   row(1, 1, 1));\n\n        cluster.coordinator(1).execute(\"UPDATE \" + KEYSPACE + \".tbl SET v = 2 WHERE pk = 1 AND ck = 1 IF v = 1\",\n                                       ConsistencyLevel.QUORUM);\n        assertRows(cluster.coordinator(1).execute(\"SELECT * FROM \" + KEYSPACE + \".tbl WHERE pk = 1\",\n                                                  ConsistencyLevel.QUORUM),\n                   row(1, 1, 2));\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtPreparePhase_ReqLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PREPARE_REQ.id).from(1).to(2, 3).drop().on(); // drop the internode messages to acceptors\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtPreparePhase_RspLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PREPARE_RSP.id).from(2, 3).to(1).drop().on(); // drop the internode messages to acceptors\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtProposePhase_ReqLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2, 3).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtProposePhase_RspLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PROPOSE_RSP.id).from(2, 3).to(1).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtCommitPhase_ReqLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_COMMIT_REQ.id).from(1).to(2, 3).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtCommitPhase_RspLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_COMMIT_RSP.id).from(2, 3).to(1).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n\n\n    @Test\n    public void casWriteContentionTimeoutTest() throws InterruptedException\n    {\n        testWithContention(101,\n                           Arrays.asList(1, 3),\n                           c -> {\n                               c.filters().reset();\n                               c.filters().verbs(Verb.PAXOS_PREPARE_REQ.id).from(1).to(3).drop();\n                               c.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2).drop();\n                           },\n                           failure ->\n                               failure.get() != null &&\n                               failure.get()\n                                      .getClass().getCanonicalName()\n                                      .equals(CasWriteTimeoutException.class.getCanonicalName()),\n                           \"Expecting cause to be CasWriteTimeoutException\");\n    }\n\n    private void testWithContention(int testUid,\n                                    List<Integer> contendingNodes,\n                                    Consumer<ICluster> setupForEachRound,\n                                    Function<AtomicReference<Throwable>, Boolean> expectedException,\n                                    String assertHintMessage) throws InterruptedException\n    {\n        assert contendingNodes.size() == 2;\n        AtomicInteger curPk = new AtomicInteger(1);\n        ExecutorService es = Executors.newFixedThreadPool(3);\n        AtomicReference<Throwable> failure = new AtomicReference<>();\n        Supplier<Boolean> hasExpectedException = () -> expectedException.apply(failure);\n        while (!hasExpectedException.get())\n        {\n            failure.set(null);\n            setupForEachRound.accept(cluster);\n\n            List<Future<?>> futures = new ArrayList<>();\n            CountDownLatch latch = new CountDownLatch(3);\n            contendingNodes.forEach(nodeId -> {\n                String query = mkCasInsertQuery((a) -> curPk.get(), testUid, nodeId);\n                futures.add(es.submit(() -> {\n                    try\n                    {\n                        latch.countDown();\n                        latch.await(1, TimeUnit.SECONDS); // help threads start at approximately same time\n                        cluster.coordinator(nodeId).execute(query, ConsistencyLevel.QUORUM);\n                    }\n                    catch (Throwable t)\n                    {\n                        failure.set(t);\n                    }\n                }));\n            });\n\n            FBUtilities.waitOnFutures(futures);\n            curPk.incrementAndGet();\n        }\n\n        es.shutdownNow();\n        es.awaitTermination(1, TimeUnit.MINUTES);\n        Assert.assertTrue(assertHintMessage, hasExpectedException.get());\n    }\n\n    private void expectCasWriteTimeout()\n    {\n        thrown.expect(new BaseMatcher<Throwable>()\n        {\n            public boolean matches(Object item)\n            {\n                return InstanceClassLoader.wasLoadedByAnInstanceClassLoader(item.getClass());\n            }\n\n            public void describeTo(Description description)\n            {\n                description.appendText(\"Cause should be loaded by InstanceClassLoader\");\n            }\n        });\n        // unable to assert on class becuase the exception thrown was loaded by a differnet classloader, InstanceClassLoader\n        // therefor asserts the FQCN name present in the message as a workaround\n        thrown.expect(new BaseMatcher<Throwable>()\n        {\n            public boolean matches(Object item)\n            {\n                return item.getClass().getCanonicalName().equals(CasWriteTimeoutException.class.getCanonicalName());\n            }\n\n            public void describeTo(Description description)\n            {\n                description.appendText(\"Class was expected to be \" + CasWriteTimeoutException.class.getCanonicalName() + \" but was not\");\n            }\n        });\n        thrown.expectMessage(containsString(\"CAS operation timed out\"));\n    }\n\n    @Test\n    public void testWriteUnknownResult()\n    {\n        cluster.filters().reset();\n        int pk = pkGen.getAndIncrement();\n        CountDownLatch ready = new CountDownLatch(1);\n        cluster.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2, 3).messagesMatching((from, to, msg) -> {\n            if (to == 2)\n            {\n                // Inject a single CAS request in-between prepare and propose phases\n                cluster.coordinator(2).execute(mkCasInsertQuery((a) -> pk, 1, 2),\n                                               ConsistencyLevel.QUORUM);\n                ready.countDown();\n            } else {\n                Uninterruptibles.awaitUninterruptibly(ready);\n            }\n            return false;\n        }).drop();\n\n        try\n        {\n            cluster.coordinator(1).execute(mkCasInsertQuery((a) -> pk, 1, 1), ConsistencyLevel.QUORUM);\n        }\n        catch (Throwable t)\n        {\n            Assert.assertEquals(\"Expecting cause to be CasWriteUnknownResultException\",\n                                CasWriteUnknownResultException.class.getCanonicalName(), t.getClass().getCanonicalName());\n            return;\n        }\n        Assert.fail(\"Expecting test to throw a CasWriteUnknownResultException\");\n    }\n\n    // every invokation returns a query with an unique pk\n    private String mkUniqueCasInsertQuery(int v)\n    {\n        return mkCasInsertQuery(AtomicInteger::getAndIncrement, 1, v);\n    }\n\n    private String mkCasInsertQuery(Function<AtomicInteger, Integer> pkFunc, int ck, int v)\n    {\n        String query = String.format(\"INSERT INTO %s.tbl (pk, ck, v) VALUES (%d, %d, %d) IF NOT EXISTS\", KEYSPACE, pkFunc.apply(pkGen), ck, v);\n        logger.info(\"Generated query: \" + query);\n        return query;\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.util.concurrent.Uninterruptibles;\nimport org.junit.After;\nimport org.junit.AfterClass;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.config.Config;\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.ConsistencyLevel;\nimport org.apache.cassandra.distributed.api.ICluster;\nimport org.apache.cassandra.distributed.api.IMessageFilters;\nimport org.apache.cassandra.distributed.shared.InstanceClassLoader;\nimport org.apache.cassandra.exceptions.CasWriteTimeoutException;\nimport org.apache.cassandra.exceptions.CasWriteUnknownResultException;\nimport org.apache.cassandra.net.Verb;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.hamcrest.BaseMatcher;\nimport org.hamcrest.Description;\n\nimport static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;\nimport static org.apache.cassandra.distributed.shared.AssertUtils.row;\nimport static org.hamcrest.CoreMatchers.containsString;\n\n// TODO: this test should be removed after running in-jvm dtests is set up via the shared API repository\npublic class CasWriteTest extends TestBaseImpl\n{\n    // Sharing the same cluster to boost test speed. Using a pkGen to make sure queries has distinct pk value for paxos instances.\n    private static ICluster cluster;\n    private static final AtomicInteger pkGen = new AtomicInteger(1_000); // preserve any pk values less than 1000 for manual queries.\n    private static final Logger logger = LoggerFactory.getLogger(CasWriteTest.class);\n\n    @Rule\n    public ExpectedException thrown = ExpectedException.none();\n\n    @BeforeClass\n    public static void setupCluster() throws Throwable\n    {\n        cluster = init(Cluster.build().withNodes(3).start());\n        cluster.schemaChange(\"CREATE TABLE \" + KEYSPACE + \".tbl (pk int, ck int, v int, PRIMARY KEY (pk, ck))\");\n    }\n\n    @AfterClass\n    public static void close() throws Exception\n    {\n        cluster.close();\n        cluster = null;\n    }\n\n    @Before @After\n    public void resetFilters()\n    {\n        cluster.filters().reset();\n    }\n\n    @Test\n    public void testCasWriteSuccessWithNoContention()\n    {\n        cluster.coordinator(1).execute(\"INSERT INTO \" + KEYSPACE + \".tbl (pk, ck, v) VALUES (1, 1, 1) IF NOT EXISTS\",\n                                       ConsistencyLevel.QUORUM);\n        assertRows(cluster.coordinator(1).execute(\"SELECT * FROM \" + KEYSPACE + \".tbl WHERE pk = 1\",\n                                                  ConsistencyLevel.QUORUM),\n                   row(1, 1, 1));\n\n        cluster.coordinator(1).execute(\"UPDATE \" + KEYSPACE + \".tbl SET v = 2 WHERE pk = 1 AND ck = 1 IF v = 1\",\n                                       ConsistencyLevel.QUORUM);\n        assertRows(cluster.coordinator(1).execute(\"SELECT * FROM \" + KEYSPACE + \".tbl WHERE pk = 1\",\n                                                  ConsistencyLevel.QUORUM),\n                   row(1, 1, 2));\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtPreparePhase_ReqLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PREPARE_REQ.id).from(1).to(2, 3).drop().on(); // drop the internode messages to acceptors\n        cluster.filters().verbs(Verb.PAXOS2_PREPARE_REQ.id).from(1).to(2, 3).drop().on(); // drop the internode messages to acceptors\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtPreparePhase_RspLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PREPARE_RSP.id).from(2, 3).to(1).drop().on(); // drop the internode messages to acceptors\n        cluster.filters().verbs(Verb.PAXOS2_PREPARE_RSP.id).from(2, 3).to(1).drop().on(); // drop the internode messages to acceptors\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtProposePhase_ReqLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2, 3).drop().on();\n        cluster.filters().verbs(Verb.PAXOS2_PROPOSE_REQ.id).from(1).to(2, 3).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtProposePhase_RspLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_PROPOSE_RSP.id).from(2, 3).to(1).drop().on();\n        cluster.filters().verbs(Verb.PAXOS2_PROPOSE_RSP.id).from(2, 3).to(1).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtCommitPhase_ReqLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_COMMIT_REQ.id).from(1).to(2, 3).drop().on();\n        cluster.filters().verbs(Verb.PAXOS2_COMMIT_AND_PREPARE_REQ.id).from(1).to(2, 3).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n    @Test\n    public void testCasWriteTimeoutAtCommitPhase_RspLost()\n    {\n        expectCasWriteTimeout();\n        cluster.filters().verbs(Verb.PAXOS_COMMIT_RSP.id).from(2, 3).to(1).drop().on();\n        cluster.filters().verbs(Verb.PAXOS2_COMMIT_REMOTE_RSP.id).from(2, 3).to(1).drop().on();\n        cluster.coordinator(1).execute(mkUniqueCasInsertQuery(1), ConsistencyLevel.QUORUM);\n    }\n\n\n\n    @Test\n    public void casWriteContentionTimeoutTest() throws InterruptedException\n    {\n        testWithContention(101,\n                           Arrays.asList(1, 3),\n                           c -> {\n                               c.filters().reset();\n                               c.filters().verbs(Verb.PAXOS_PREPARE_REQ.id).from(1).to(3).drop();\n                               c.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2).drop();\n                               c.filters().verbs(Verb.PAXOS2_PREPARE_REQ.id).from(1).to(3).drop();\n                               c.filters().verbs(Verb.PAXOS2_PROPOSE_REQ.id).from(1).to(2).drop();\n                           },\n                           failure ->\n                               failure.get() != null &&\n                               failure.get()\n                                      .getClass().getCanonicalName()\n                                      .equals(CasWriteTimeoutException.class.getCanonicalName()),\n                           \"Expecting cause to be CasWriteTimeoutException\");\n    }\n\n    private void testWithContention(int testUid,\n                                    List<Integer> contendingNodes,\n                                    Consumer<ICluster> setupForEachRound,\n                                    Function<AtomicReference<Throwable>, Boolean> expectedException,\n                                    String assertHintMessage) throws InterruptedException\n    {\n        assert contendingNodes.size() == 2;\n        AtomicInteger curPk = new AtomicInteger(1);\n        ExecutorService es = Executors.newFixedThreadPool(3);\n        AtomicReference<Throwable> failure = new AtomicReference<>();\n        Supplier<Boolean> hasExpectedException = () -> expectedException.apply(failure);\n        while (!hasExpectedException.get())\n        {\n            failure.set(null);\n            setupForEachRound.accept(cluster);\n\n            List<Future<?>> futures = new ArrayList<>();\n            CountDownLatch latch = new CountDownLatch(3);\n            contendingNodes.forEach(nodeId -> {\n                String query = mkCasInsertQuery((a) -> curPk.get(), testUid, nodeId);\n                futures.add(es.submit(() -> {\n                    try\n                    {\n                        latch.countDown();\n                        latch.await(1, TimeUnit.SECONDS); // help threads start at approximately same time\n                        cluster.coordinator(nodeId).execute(query, ConsistencyLevel.QUORUM);\n                    }\n                    catch (Throwable t)\n                    {\n                        failure.set(t);\n                    }\n                }));\n            });\n\n            FBUtilities.waitOnFutures(futures);\n            curPk.incrementAndGet();\n        }\n\n        es.shutdownNow();\n        es.awaitTermination(1, TimeUnit.MINUTES);\n        Assert.assertTrue(assertHintMessage, hasExpectedException.get());\n    }\n\n    private void expectCasWriteTimeout()\n    {\n        thrown.expect(new BaseMatcher<Throwable>()\n        {\n            public boolean matches(Object item)\n            {\n                return InstanceClassLoader.wasLoadedByAnInstanceClassLoader(item.getClass());\n            }\n\n            public void describeTo(Description description)\n            {\n                description.appendText(\"Cause should be loaded by InstanceClassLoader\");\n            }\n        });\n        // unable to assert on class becuase the exception thrown was loaded by a differnet classloader, InstanceClassLoader\n        // therefor asserts the FQCN name present in the message as a workaround\n        thrown.expect(new BaseMatcher<Throwable>()\n        {\n            public boolean matches(Object item)\n            {\n                return item.getClass().getCanonicalName().equals(CasWriteTimeoutException.class.getCanonicalName());\n            }\n\n            public void describeTo(Description description)\n            {\n                description.appendText(\"Class was expected to be \" + CasWriteTimeoutException.class.getCanonicalName() + \" but was not\");\n            }\n        });\n        thrown.expectMessage(containsString(\"CAS operation timed out\"));\n    }\n\n    @Test\n    public void testWriteUnknownResult()\n    {\n        cluster.filters().reset();\n        int pk = pkGen.getAndIncrement();\n        CountDownLatch ready = new CountDownLatch(1);\n        final IMessageFilters.Matcher matcher = (from, to, msg) -> {\n            if (to == 2)\n            {\n                // Inject a single CAS request in-between prepare and propose phases\n                cluster.coordinator(2).execute(mkCasInsertQuery((a) -> pk, 1, 2),\n                                               ConsistencyLevel.QUORUM);\n                ready.countDown();\n            }\n            else\n            {\n                Uninterruptibles.awaitUninterruptibly(ready);\n            }\n            return false;\n        };\n        cluster.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2, 3).messagesMatching(matcher).drop();\n        cluster.filters().verbs(Verb.PAXOS2_PROPOSE_REQ.id).from(1).to(2, 3).messagesMatching(matcher).drop();\n\n        try\n        {\n            cluster.coordinator(1).execute(mkCasInsertQuery((a) -> pk, 1, 1), ConsistencyLevel.QUORUM);\n        }\n        catch (Throwable t)\n        {\n            final Class<?> exceptionClass = isPaxosVariant2() ? CasWriteTimeoutException.class : CasWriteUnknownResultException.class;\n            Assert.assertEquals(\"Expecting cause to be \" + exceptionClass.getSimpleName(),\n                                exceptionClass.getCanonicalName(), t.getClass().getCanonicalName());\n            return;\n        }\n        Assert.fail(\"Expecting test to throw a CasWriteUnknownResultException\");\n    }\n\n    private static boolean isPaxosVariant2()\n    {\n        return Config.PaxosVariant.v2.name().equals(cluster.coordinator(1).instance().config().getString(\"paxos_variant\"));\n    }\n\n    // every invokation returns a query with an unique pk\n    private String mkUniqueCasInsertQuery(int v)\n    {\n        return mkCasInsertQuery(AtomicInteger::getAndIncrement, 1, v);\n    }\n\n    private String mkCasInsertQuery(Function<AtomicInteger, Integer> pkFunc, int ck, int v)\n    {\n        String query = String.format(\"INSERT INTO %s.tbl (pk, ck, v) VALUES (%d, %d, %d) IF NOT EXISTS\", KEYSPACE, pkFunc.apply(pkGen), ck, v);\n        logger.info(\"Generated query: \" + query);\n        return query;\n    }\n}\n","lineNo":267}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.io.IOException;\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport com.google.common.collect.Sets;\nimport org.junit.Test;\n\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.dht.Bounds;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.ConsistencyLevel;\nimport org.apache.cassandra.distributed.api.IIsolatedExecutor;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.utils.concurrent.Refs;\n\nimport static java.util.concurrent.TimeUnit.MILLISECONDS;\nimport static java.util.concurrent.TimeUnit.MINUTES;\nimport static org.apache.cassandra.distributed.api.Feature.GOSSIP;\nimport static org.apache.cassandra.distributed.api.Feature.NETWORK;\nimport static org.awaitility.Awaitility.await;\nimport static org.hamcrest.Matchers.emptyString;\nimport static org.hamcrest.Matchers.not;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertFalse;\n\npublic class PreviewRepairSnapshotTest extends TestBaseImpl\n{\n    /**\n     * Makes sure we only snapshot sstables containing the mismatching token\n     * <p>\n     * 1. create 100 sstables per instance, compaction disabled, one token per sstable\n     * 2. make 3 tokens mismatching on node2, one token per sstable\n     * 3. run preview repair\n     * 4. make sure that only the sstables containing the token are in the snapshot\n     */\n    @Test\n    public void testSnapshotOfSStablesContainingMismatchingTokens() throws IOException\n    {\n        try (Cluster cluster = init(Cluster.build(2).withConfig(config ->\n                                                                config.set(\"snapshot_on_repaired_data_mismatch\", true)\n                                                                      .with(GOSSIP)\n                                                                      .with(NETWORK)).start()))\n        {\n            Set<Integer> tokensToMismatch = Sets.newHashSet(1, 50, 99);\n            cluster.schemaChange(withKeyspace(\"create table %s.tbl (id int primary key) with compaction = {'class' : 'SizeTieredCompactionStrategy', 'enabled':false }\"));\n            // 1 token per sstable;\n            for (int i = 0; i < 100; i++)\n            {\n                cluster.coordinator(1).execute(withKeyspace(\"insert into %s.tbl (id) values (?)\"), ConsistencyLevel.ALL, i);\n                cluster.stream().forEach(instance -> instance.flush(KEYSPACE));\n            }\n            cluster.stream().forEach(instance -> instance.flush(KEYSPACE));\n            for (int i = 1; i <= 2; i++)\n                markRepaired(cluster, i);\n\n            cluster.get(1)\n                   .nodetoolResult(\"repair\", \"-vd\", \"-pr\", KEYSPACE, \"tbl\")\n                   .asserts()\n                   .success()\n                   .stdoutContains(\"Repaired data is in sync\");\n\n            Set<Token> mismatchingTokens = new HashSet<>();\n            for (Integer token : tokensToMismatch)\n            {\n                cluster.get(2).executeInternal(withKeyspace(\"insert into %s.tbl (id) values (?)\"), token);\n                cluster.get(2).flush(KEYSPACE);\n                Object[][] res = cluster.get(2).executeInternal(withKeyspace(\"select token(id) from %s.tbl where id = ?\"), token);\n                mismatchingTokens.add(new Murmur3Partitioner.LongToken((long) res[0][0]));\n            }\n\n            markRepaired(cluster, 2);\n\n            cluster.get(1)\n                   .nodetoolResult(\"repair\", \"-vd\", KEYSPACE, \"tbl\")\n                   .asserts()\n                   .success()\n                   .stdoutContains(\"Repaired data is inconsistent\");\n\n            cluster.get(1).runOnInstance(checkSnapshot(mismatchingTokens, 3));\n            // node2 got the duplicate mismatch-tokens above, so it should exist in exactly 6 sstables\n            cluster.get(2).runOnInstance(checkSnapshot(mismatchingTokens, 6));\n        }\n    }\n\n    private IIsolatedExecutor.SerializableRunnable checkSnapshot(Set<Token> mismatchingTokens, int expectedSnapshotSize)\n    {\n        return () -> {\n            ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(\"tbl\");\n\n            String snapshotTag = await().atMost(1, MINUTES)\n                                        .pollInterval(100, MILLISECONDS)\n                                        .until(() -> {\n                                            for (String tag : cfs.listSnapshots().keySet())\n                                            {\n                                                // we create the snapshot schema file last, so when this exists we know the snapshot is complete;\n                                                if (cfs.getDirectories().getSnapshotSchemaFile(tag).exists())\n                                                    return tag;\n                                            }\n\n                                            return \"\";\n                                        }, not(emptyString()));\n\n            Set<SSTableReader> inSnapshot = new HashSet<>();\n\n            try (Refs<SSTableReader> sstables = cfs.getSnapshotSSTableReaders(snapshotTag))\n            {\n                inSnapshot.addAll(sstables);\n            }\n            catch (IOException e)\n            {\n                throw new RuntimeException(e);\n            }\n            assertEquals(expectedSnapshotSize, inSnapshot.size());\n\n            for (SSTableReader sstable : cfs.getLiveSSTables())\n            {\n                Bounds<Token> sstableBounds = new Bounds<>(sstable.getFirst().getToken(), sstable.getLast().getToken());\n                boolean shouldBeInSnapshot = false;\n                for (Token mismatchingToken : mismatchingTokens)\n                {\n                    if (sstableBounds.contains(mismatchingToken))\n                    {\n                        assertFalse(shouldBeInSnapshot);\n                        shouldBeInSnapshot = true;\n                    }\n                }\n                assertEquals(shouldBeInSnapshot, inSnapshot.contains(sstable));\n            }\n        };\n    }\n\n    private void markRepaired(Cluster cluster, int instance)\n    {\n        cluster.get(instance).runOnInstance(() -> {\n            ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(\"tbl\");\n            for (SSTableReader sstable : cfs.getLiveSSTables())\n            {\n                try\n                {\n                    sstable.descriptor.getMetadataSerializer().mutateRepairMetadata(sstable.descriptor,\n                                                                                    System.currentTimeMillis(),\n                                                                                    null,\n                                                                                    false);\n                    sstable.reloadSSTableMetadata();\n                }\n                catch (IOException e)\n                {\n                    throw new RuntimeException(e);\n                }\n            }\n        });\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport com.google.common.collect.Sets;\nimport org.junit.Test;\n\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.dht.Bounds;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.ConsistencyLevel;\nimport org.apache.cassandra.distributed.api.IIsolatedExecutor;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.utils.concurrent.Refs;\n\nimport static java.util.concurrent.TimeUnit.MILLISECONDS;\nimport static java.util.concurrent.TimeUnit.MINUTES;\nimport static org.apache.cassandra.distributed.api.Feature.GOSSIP;\nimport static org.apache.cassandra.distributed.api.Feature.NETWORK;\nimport static org.awaitility.Awaitility.await;\nimport static org.hamcrest.Matchers.emptyString;\nimport static org.hamcrest.Matchers.not;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertFalse;\n\npublic class PreviewRepairSnapshotTest extends TestBaseImpl\n{\n    /**\n     * Makes sure we only snapshot sstables containing the mismatching token\n     * <p>\n     * 1. create 100 sstables per instance, compaction disabled, one token per sstable\n     * 2. make 3 tokens mismatching on node2, one token per sstable\n     * 3. run preview repair\n     * 4. make sure that only the sstables containing the token are in the snapshot\n     */\n    @Test\n    public void testSnapshotOfSStablesContainingMismatchingTokens() throws IOException\n    {\n        try (Cluster cluster = init(Cluster.build(2).withConfig(config ->\n                                                                config.set(\"snapshot_on_repaired_data_mismatch\", true)\n                                                                      .with(GOSSIP)\n                                                                      .with(NETWORK)).start()))\n        {\n            Set<Integer> tokensToMismatch = Sets.newHashSet(1, 50, 99);\n            cluster.schemaChange(withKeyspace(\"create table %s.tbl (id blob primary key) with compaction = {'class' : 'SizeTieredCompactionStrategy', 'enabled':false }\"));\n            // 1 token per sstable;\n            for (int i = 0; i < 100; i++)\n            {\n                // BigFormat severely overestimates the number of partitions per range when the sstable size is small.\n                // Do multiple writes per sstable, with the same token, to compensate.\n                for (int j = 0; j < 10; ++j)\n                    cluster.coordinator(1).execute(withKeyspace(\"insert into %s.tbl (id) values (?)\"), ConsistencyLevel.ALL, matchingHashBlob(i, j));\n                cluster.stream().forEach(instance -> instance.flush(KEYSPACE));\n            }\n            cluster.stream().forEach(instance -> instance.flush(KEYSPACE));\n            for (int i = 1; i <= 2; i++)\n                markRepaired(cluster, i);\n\n            cluster.get(1)\n                   .nodetoolResult(\"repair\", \"-vd\", \"-pr\", KEYSPACE, \"tbl\")\n                   .asserts()\n                   .success()\n                   .stdoutContains(\"Repaired data is in sync\");\n\n            Set<Token> mismatchingTokens = new HashSet<>();\n            for (Integer token : tokensToMismatch)\n            {\n                final ByteBuffer b = matchingHashBlob(token, 0);\n                cluster.get(2).executeInternal(withKeyspace(\"insert into %s.tbl (id) values (?)\"), b);\n                cluster.get(2).flush(KEYSPACE);\n                Object[][] res = cluster.get(2).executeInternal(withKeyspace(\"select token(id) from %s.tbl where id = ?\"), b);\n                mismatchingTokens.add(new Murmur3Partitioner.LongToken((long) res[0][0]));\n            }\n\n            markRepaired(cluster, 2);\n\n            cluster.get(1)\n                   .nodetoolResult(\"repair\", \"-vd\", KEYSPACE, \"tbl\")\n                   .asserts()\n                   .success()\n                   .stdoutContains(\"Repaired data is inconsistent\");\n\n            cluster.get(1).runOnInstance(checkSnapshot(mismatchingTokens, 3));\n            // node2 got the duplicate mismatch-tokens above, so it should exist in exactly 6 sstables\n            cluster.get(2).runOnInstance(checkSnapshot(mismatchingTokens, 6));\n        }\n    }\n\n    private ByteBuffer matchingHashBlob(int hashAffectingComponent, int hashUnaffectingComponent)\n    {\n        // Generate blobs with mathing hash for the same i, but different for the different j\n        ByteBuffer base = ByteBuffer.wrap(Integer.toHexString(hashAffectingComponent).getBytes());\n        return Util.generateMurmurCollision(base, Integer.toHexString(hashUnaffectingComponent).getBytes());\n    }\n\n    private IIsolatedExecutor.SerializableRunnable checkSnapshot(Set<Token> mismatchingTokens, int expectedSnapshotSize)\n    {\n        return () -> {\n            ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(\"tbl\");\n\n            String snapshotTag = await().atMost(1, MINUTES)\n                                        .pollInterval(100, MILLISECONDS)\n                                        .until(() -> {\n                                            for (String tag : cfs.listSnapshots().keySet())\n                                            {\n                                                // we create the snapshot schema file last, so when this exists we know the snapshot is complete;\n                                                if (cfs.getDirectories().getSnapshotSchemaFile(tag).exists())\n                                                    return tag;\n                                            }\n\n                                            return \"\";\n                                        }, not(emptyString()));\n\n            Set<SSTableReader> inSnapshot = new HashSet<>();\n\n            try (Refs<SSTableReader> sstables = cfs.getSnapshotSSTableReaders(snapshotTag))\n            {\n                inSnapshot.addAll(sstables);\n            }\n            catch (IOException e)\n            {\n                throw new RuntimeException(e);\n            }\n            assertEquals(expectedSnapshotSize, inSnapshot.size());\n\n            for (SSTableReader sstable : cfs.getLiveSSTables())\n            {\n                Bounds<Token> sstableBounds = new Bounds<>(sstable.getFirst().getToken(), sstable.getLast().getToken());\n                boolean shouldBeInSnapshot = false;\n                for (Token mismatchingToken : mismatchingTokens)\n                {\n                    if (sstableBounds.contains(mismatchingToken))\n                    {\n                        assertFalse(shouldBeInSnapshot);\n                        shouldBeInSnapshot = true;\n                    }\n                }\n                assertEquals(shouldBeInSnapshot, inSnapshot.contains(sstable));\n            }\n        };\n    }\n\n    private void markRepaired(Cluster cluster, int instance)\n    {\n        cluster.get(instance).runOnInstance(() -> {\n            ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(\"tbl\");\n            for (SSTableReader sstable : cfs.getLiveSSTables())\n            {\n                try\n                {\n                    sstable.descriptor.getMetadataSerializer().mutateRepairMetadata(sstable.descriptor,\n                                                                                    System.currentTimeMillis(),\n                                                                                    null,\n                                                                                    false);\n                    sstable.reloadSSTableMetadata();\n                }\n                catch (IOException e)\n                {\n                    throw new RuntimeException(e);\n                }\n            }\n        });\n    }\n}\n","lineNo":93}
{"Smelly Sample":"/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n  * regarding copyright ownership.  The ASF licenses this file\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n  *\n  *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n\npackage org.apache.cassandra.harry.model;\n\nimport java.util.*;\n\nimport org.apache.cassandra.harry.data.ResultSetRow;\nimport org.apache.cassandra.harry.ddl.ColumnSpec;\nimport org.apache.cassandra.harry.ddl.SchemaSpec;\nimport org.apache.cassandra.harry.gen.DataGenerators;\nimport org.apache.cassandra.harry.sut.SystemUnderTest;\nimport org.apache.cassandra.harry.operations.CompiledStatement;\nimport org.apache.cassandra.harry.operations.Relation;\nimport org.apache.cassandra.harry.operations.Query;\n\nimport static org.apache.cassandra.harry.gen.DataGenerators.UNSET_DESCR;\n\npublic class SelectHelper\n{\n    private static final long[] EMPTY_ARR = new long[]{};\n    public static CompiledStatement selectWildcard(SchemaSpec schema, long pd)\n    {\n        return select(schema, pd, null, Collections.emptyList(), false, true);\n    }\n\n    public static CompiledStatement select(SchemaSpec schema, long pd)\n    {\n        return select(schema, pd, schema.allColumnsSet, Collections.emptyList(), false, true);\n    }\n\n    /**\n     * Here, {@code reverse} should be understood not in ASC/DESC sense, but rather in terms\n     * of how we're going to iterate through this partition (in other words, if first clustering component order\n     * is DESC, we'll iterate in ASC order)\n     */\n    public static CompiledStatement select(SchemaSpec schema, long pd, List<Relation> relations, boolean reverse, boolean includeWriteTime)\n    {\n        return select(schema, pd, schema.allColumnsSet, relations, reverse, includeWriteTime);\n    }\n\n    public static CompiledStatement selectWildcard(SchemaSpec schema, long pd, List<Relation> relations, boolean reverse, boolean includeWriteTime)\n    {\n        return select(schema, pd, null, relations, reverse, includeWriteTime);\n    }\n\n    public static CompiledStatement select(SchemaSpec schema, long pd, Set<ColumnSpec<?>> columns, List<Relation> relations, boolean reverse, boolean includeWriteTime)\n    {\n        boolean isWildcardQuery = columns == null;\n        if (isWildcardQuery)\n        {\n            columns = schema.allColumnsSet;\n            includeWriteTime = false;\n        }\n\n        StringBuilder b = new StringBuilder();\n        b.append(\"SELECT \");\n\n        boolean isFirst = true;\n        if (isWildcardQuery)\n        {\n            b.append(\"*\");\n        }\n        else\n        {\n            for (int i = 0; i < schema.allColumns.size(); i++)\n            {\n                ColumnSpec<?> spec = schema.allColumns.get(i);\n                if (columns != null && !columns.contains(spec))\n                    continue;\n\n                if (isFirst)\n                    isFirst = false;\n                else\n                    b.append(\", \");\n                b.append(spec.name);\n            }\n        }\n\n        if (includeWriteTime)\n        {\n            for (ColumnSpec<?> spec : schema.staticColumns)\n            {\n                if (columns != null && !columns.contains(spec))\n                    continue;\n                b.append(\", \")\n                 .append(\"writetime(\")\n                 .append(spec.name)\n                 .append(\")\");\n            }\n\n            for (ColumnSpec<?> spec : schema.regularColumns)\n            {\n                if (columns != null && !columns.contains(spec))\n                    continue;\n                b.append(\", \")\n                 .append(\"writetime(\")\n                 .append(spec.name)\n                 .append(\")\");\n            }\n        }\n\n        if (schema.trackLts)\n            b.append(\", visited_lts\");\n\n        b.append(\" FROM \")\n         .append(schema.keyspace)\n         .append(\".\")\n         .append(schema.table)\n         .append(\" WHERE \");\n\n        List<Object> bindings = new ArrayList<>();\n\n        schema.inflateRelations(pd,\n                                relations,\n                                new SchemaSpec.AddRelationCallback()\n                                {\n                                    boolean isFirst = true;\n                                    public void accept(ColumnSpec<?> spec, Relation.RelationKind kind, Object value)\n                                    {\n                                        if (isFirst)\n                                            isFirst = false;\n                                        else\n                                            b.append(\" AND \");\n                                        b.append(kind.getClause(spec));\n                                        bindings.add(value);\n                                    }\n                                });\n        addOrderBy(schema, b, reverse);\n        b.append(\";\");\n        Object[] bindingsArr = bindings.toArray(new Object[bindings.size()]);\n        return new CompiledStatement(b.toString(), bindingsArr);\n    }\n\n    public static CompiledStatement count(SchemaSpec schema, long pd)\n    {\n        StringBuilder b = new StringBuilder();\n        b.append(\"SELECT count(*) \");\n\n        b.append(\" FROM \")\n         .append(schema.keyspace)\n         .append(\".\")\n         .append(schema.table)\n         .append(\" WHERE \");\n\n        List<Object> bindings = new ArrayList<>(schema.partitionKeys.size());\n\n        schema.inflateRelations(pd,\n                                Collections.emptyList(),\n                                new SchemaSpec.AddRelationCallback()\n                                {\n                                    boolean isFirst = true;\n                                    public void accept(ColumnSpec<?> spec, Relation.RelationKind kind, Object value)\n                                    {\n                                        if (isFirst)\n                                            isFirst = false;\n                                        else\n                                            b.append(\" AND \");\n                                        b.append(kind.getClause(spec));\n                                        bindings.add(value);\n                                    }\n                                });\n\n        Object[] bindingsArr = bindings.toArray(new Object[bindings.size()]);\n        return new CompiledStatement(b.toString(), bindingsArr);\n    }\n\n    private static void addOrderBy(SchemaSpec schema, StringBuilder b, boolean reverse)\n    {\n        if (reverse && schema.clusteringKeys.size() > 0)\n        {\n            b.append(\" ORDER BY \");\n            for (int i = 0; i < schema.clusteringKeys.size(); i++)\n            {\n                ColumnSpec<?> c = schema.clusteringKeys.get(i);\n                if (i > 0)\n                    b.append(\", \");\n                b.append(c.isReversed() ? asc(c.name) : desc(c.name));\n            }\n        }\n    }\n\n    public static String asc(String name)\n    {\n        return name + \" ASC\";\n    }\n\n    public static String desc(String name)\n    {\n        return name + \" DESC\";\n    }\n\n\n    public static Object[] broadenResult(SchemaSpec schemaSpec, Set<ColumnSpec<?>> columns, Object[] result)\n    {\n        boolean isWildcardQuery = columns == null;\n\n        if (isWildcardQuery)\n            columns = schemaSpec.allColumnsSet;\n        else if (schemaSpec.allColumns.size() == columns.size())\n            return result;\n\n        Object[] newRes = new Object[schemaSpec.allColumns.size() + schemaSpec.staticColumns.size() + schemaSpec.regularColumns.size()];\n\n        int origPointer = 0;\n        int newPointer = 0;\n        for (int i = 0; i < schemaSpec.allColumns.size(); i++)\n        {\n            ColumnSpec<?> column = schemaSpec.allColumns.get(i);\n            if (columns.contains(column))\n                newRes[newPointer] = result[origPointer++];\n            else\n                newRes[newPointer] = DataGenerators.UNSET_VALUE;\n            newPointer++;\n        }\n\n        // Make sure to include writetime, but only in case query actually includes writetime (for example, it's not a wildcard query)\n        for (int i = 0; i < schemaSpec.staticColumns.size() && origPointer < result.length; i++)\n        {\n            ColumnSpec<?> column = schemaSpec.staticColumns.get(i);\n            if (columns.contains(column))\n                newRes[newPointer] = result[origPointer++];\n            else\n                newRes[newPointer] = null;\n            newPointer++;\n        }\n\n        for (int i = 0; i < schemaSpec.regularColumns.size() && origPointer < result.length; i++)\n        {\n            ColumnSpec<?> column = schemaSpec.regularColumns.get(i);\n            if (columns.contains(column))\n                newRes[newPointer] = result[origPointer++];\n            else\n                newRes[newPointer] = null;\n            newPointer++;\n        }\n\n        return newRes;\n    }\n\n    static boolean isDeflatable(Object[] columns)\n    {\n        for (Object column : columns)\n        {\n            if (column == DataGenerators.UNSET_VALUE)\n                return false;\n        }\n        return true;\n    }\n\n    public static ResultSetRow resultSetToRow(SchemaSpec schema, OpSelectors.Clock clock, Object[] result)\n    {\n        Object[] partitionKey = new Object[schema.partitionKeys.size()];\n        Object[] clusteringKey = new Object[schema.clusteringKeys.size()];\n        Object[] staticColumns = new Object[schema.staticColumns.size()];\n        Object[] regularColumns = new Object[schema.regularColumns.size()];\n\n        System.arraycopy(result, 0, partitionKey, 0, partitionKey.length);\n        System.arraycopy(result, partitionKey.length, clusteringKey, 0, clusteringKey.length);\n        System.arraycopy(result, partitionKey.length + clusteringKey.length, staticColumns, 0, staticColumns.length);\n        System.arraycopy(result, partitionKey.length + clusteringKey.length + staticColumns.length, regularColumns, 0, regularColumns.length);\n\n\n        List<Long> visited_lts_list;\n        if (schema.trackLts)\n        {\n            visited_lts_list = (List<Long>) result[result.length - 1];\n            visited_lts_list.sort(Long::compare);\n        }\n        else\n        {\n            visited_lts_list = Collections.emptyList();\n        }\n\n        long[] slts = new long[schema.staticColumns.size()];\n        for (int i = 0; i < slts.length; i++)\n        {\n            Object v = result[schema.allColumns.size() + i];\n            slts[i] = v == null ? Model.NO_TIMESTAMP : clock.lts((long) v);\n        }\n\n        long[] lts = new long[schema.regularColumns.size()];\n        for (int i = 0; i < lts.length; i++)\n        {\n            Object v = result[schema.allColumns.size() + slts.length + i];\n            lts[i] = v == null ? Model.NO_TIMESTAMP : clock.lts((long) v);\n        }\n\n        return new ResultSetRow(isDeflatable(partitionKey) ? schema.deflatePartitionKey(partitionKey) : UNSET_DESCR,\n                                isDeflatable(clusteringKey) ? schema.deflateClusteringKey(clusteringKey) : UNSET_DESCR,\n                                schema.staticColumns.isEmpty() ? EMPTY_ARR : schema.deflateStaticColumns(staticColumns),\n                                schema.staticColumns.isEmpty() ? EMPTY_ARR : slts,\n                                schema.deflateRegularColumns(regularColumns),\n                                lts,\n                                visited_lts_list);\n    }\n\n    public static List<ResultSetRow> execute(SystemUnderTest sut, OpSelectors.Clock clock, Query query)\n    {\n        return execute(sut, clock, query, query.schemaSpec.allColumnsSet);\n    }\n\n    public static List<ResultSetRow> execute(SystemUnderTest sut, OpSelectors.Clock clock, Query query, Set<ColumnSpec<?>> columns)\n    {\n        CompiledStatement compiled = query.toSelectStatement(columns, true);\n        Object[][] objects = sut.executeIdempotent(compiled.cql(), SystemUnderTest.ConsistencyLevel.QUORUM, compiled.bindings());\n        List<ResultSetRow> result = new ArrayList<>();\n        for (Object[] obj : objects)\n            result.add(resultSetToRow(query.schemaSpec, clock, broadenResult(query.schemaSpec, columns, obj)));\n        return result;\n    }\n\n    public static List<ResultSetRow> execute(SystemUnderTest sut, OpSelectors.Clock clock, CompiledStatement compiled, SchemaSpec schemaSpec)\n    {\n        Set<ColumnSpec<?>> columns = schemaSpec.allColumnsSet;\n        Object[][] objects = sut.executeIdempotent(compiled.cql(), SystemUnderTest.ConsistencyLevel.QUORUM, compiled.bindings());\n        List<ResultSetRow> result = new ArrayList<>();\n        for (Object[] obj : objects)\n            result.add(resultSetToRow(schemaSpec, clock, broadenResult(schemaSpec, columns, obj)));\n        return result;\n    }\n}\n","Method after Refactoring":"/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n  * regarding copyright ownership.  The ASF licenses this file\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n  *\n  *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n\npackage org.apache.cassandra.harry.model;\n\nimport java.util.*;\n\nimport org.apache.cassandra.harry.data.ResultSetRow;\nimport org.apache.cassandra.harry.ddl.ColumnSpec;\nimport org.apache.cassandra.harry.ddl.SchemaSpec;\nimport org.apache.cassandra.harry.gen.DataGenerators;\nimport org.apache.cassandra.harry.sut.SystemUnderTest;\nimport org.apache.cassandra.harry.operations.CompiledStatement;\nimport org.apache.cassandra.harry.operations.Relation;\nimport org.apache.cassandra.harry.operations.Query;\n\nimport static org.apache.cassandra.harry.gen.DataGenerators.UNSET_DESCR;\n\npublic class SelectHelper\n{\n    private static final long[] EMPTY_ARR = new long[]{};\n    public static CompiledStatement selectWildcard(SchemaSpec schema, long pd)\n    {\n        return select(schema, pd, null, Collections.emptyList(), false, true);\n    }\n\n    public static CompiledStatement select(SchemaSpec schema, long pd)\n    {\n        return select(schema, pd, schema.allColumnsSet, Collections.emptyList(), false, true);\n    }\n\n    /**\n     * Here, {@code reverse} should be understood not in ASC/DESC sense, but rather in terms\n     * of how we're going to iterate through this partition (in other words, if first clustering component order\n     * is DESC, we'll iterate in ASC order)\n     */\n    public static CompiledStatement select(SchemaSpec schema, long pd, List<Relation> relations, boolean reverse, boolean includeWriteTime)\n    {\n        return select(schema, pd, schema.allColumnsSet, relations, reverse, includeWriteTime);\n    }\n\n    public static CompiledStatement selectWildcard(SchemaSpec schema, long pd, List<Relation> relations, boolean reverse, boolean includeWriteTime)\n    {\n        return select(schema, pd, null, relations, reverse, includeWriteTime);\n    }\n\n    public static CompiledStatement select(SchemaSpec schema, Long pd, Set<ColumnSpec<?>> columns, List<Relation> relations, boolean reverse, boolean includeWriteTime)\n    {\n        boolean isWildcardQuery = columns == null;\n        if (isWildcardQuery)\n        {\n            columns = schema.allColumnsSet;\n            includeWriteTime = false;\n        }\n\n        StringBuilder b = new StringBuilder();\n        b.append(\"SELECT \");\n\n        boolean isFirst = true;\n        if (isWildcardQuery)\n        {\n            b.append(\"*\");\n        }\n        else\n        {\n            for (int i = 0; i < schema.allColumns.size(); i++)\n            {\n                ColumnSpec<?> spec = schema.allColumns.get(i);\n                if (columns != null && !columns.contains(spec))\n                    continue;\n\n                if (isFirst)\n                    isFirst = false;\n                else\n                    b.append(\", \");\n                b.append(spec.name);\n            }\n        }\n\n        if (includeWriteTime)\n        {\n            for (ColumnSpec<?> spec : schema.staticColumns)\n            {\n                if (columns != null && !columns.contains(spec))\n                    continue;\n                b.append(\", \")\n                 .append(\"writetime(\")\n                 .append(spec.name)\n                 .append(\")\");\n            }\n\n            for (ColumnSpec<?> spec : schema.regularColumns)\n            {\n                if (columns != null && !columns.contains(spec))\n                    continue;\n                b.append(\", \")\n                 .append(\"writetime(\")\n                 .append(spec.name)\n                 .append(\")\");\n            }\n        }\n\n        if (schema.trackLts)\n            b.append(\", visited_lts\");\n\n        b.append(\" FROM \")\n         .append(schema.keyspace)\n         .append(\".\")\n         .append(schema.table)\n         .append(\" WHERE \");\n\n        List<Object> bindings = new ArrayList<>();\n\n        SchemaSpec.AddRelationCallback consumer =  new SchemaSpec.AddRelationCallback()\n        {\n            boolean isFirst = true;\n            public void accept(ColumnSpec<?> spec, Relation.RelationKind kind, Object value)\n            {\n                if (isFirst)\n                    isFirst = false;\n                else\n                    b.append(\" AND \");\n                b.append(kind.getClause(spec));\n                bindings.add(value);\n            }\n        };\n        if (pd != null)\n        {\n            Object[] pk = schema.inflatePartitionKey(pd);\n            for (int i = 0; i < pk.length; i++)\n                consumer.accept(schema.partitionKeys.get(i), Relation.RelationKind.EQ, pk[i]);\n\n        }\n        schema.inflateRelations(relations, consumer);\n\n        addOrderBy(schema, b, reverse);\n        b.append(\";\");\n        Object[] bindingsArr = bindings.toArray(new Object[bindings.size()]);\n        return new CompiledStatement(b.toString(), bindingsArr);\n    }\n\n    public static CompiledStatement count(SchemaSpec schema, long pd)\n    {\n        StringBuilder b = new StringBuilder();\n        b.append(\"SELECT count(*) \");\n\n        b.append(\" FROM \")\n         .append(schema.keyspace)\n         .append(\".\")\n         .append(schema.table)\n         .append(\" WHERE \");\n\n        List<Object> bindings = new ArrayList<>(schema.partitionKeys.size());\n\n        schema.inflateRelations(pd,\n                                Collections.emptyList(),\n                                new SchemaSpec.AddRelationCallback()\n                                {\n                                    boolean isFirst = true;\n                                    public void accept(ColumnSpec<?> spec, Relation.RelationKind kind, Object value)\n                                    {\n                                        if (isFirst)\n                                            isFirst = false;\n                                        else\n                                            b.append(\" AND \");\n                                        b.append(kind.getClause(spec));\n                                        bindings.add(value);\n                                    }\n                                });\n\n        Object[] bindingsArr = bindings.toArray(new Object[bindings.size()]);\n        return new CompiledStatement(b.toString(), bindingsArr);\n    }\n\n    private static void addOrderBy(SchemaSpec schema, StringBuilder b, boolean reverse)\n    {\n        if (reverse && schema.clusteringKeys.size() > 0)\n        {\n            b.append(\" ORDER BY \");\n            for (int i = 0; i < schema.clusteringKeys.size(); i++)\n            {\n                ColumnSpec<?> c = schema.clusteringKeys.get(i);\n                if (i > 0)\n                    b.append(\", \");\n                b.append(c.isReversed() ? asc(c.name) : desc(c.name));\n            }\n        }\n    }\n\n    public static String asc(String name)\n    {\n        return name + \" ASC\";\n    }\n\n    public static String desc(String name)\n    {\n        return name + \" DESC\";\n    }\n\n\n    public static Object[] broadenResult(SchemaSpec schemaSpec, Set<ColumnSpec<?>> columns, Object[] result)\n    {\n        boolean isWildcardQuery = columns == null;\n\n        if (isWildcardQuery)\n            columns = schemaSpec.allColumnsSet;\n        else if (schemaSpec.allColumns.size() == columns.size())\n            return result;\n\n        Object[] newRes = new Object[schemaSpec.allColumns.size() + schemaSpec.staticColumns.size() + schemaSpec.regularColumns.size()];\n\n        int origPointer = 0;\n        int newPointer = 0;\n        for (int i = 0; i < schemaSpec.allColumns.size(); i++)\n        {\n            ColumnSpec<?> column = schemaSpec.allColumns.get(i);\n            if (columns.contains(column))\n                newRes[newPointer] = result[origPointer++];\n            else\n                newRes[newPointer] = DataGenerators.UNSET_VALUE;\n            newPointer++;\n        }\n\n        // Make sure to include writetime, but only in case query actually includes writetime (for example, it's not a wildcard query)\n        for (int i = 0; i < schemaSpec.staticColumns.size() && origPointer < result.length; i++)\n        {\n            ColumnSpec<?> column = schemaSpec.staticColumns.get(i);\n            if (columns.contains(column))\n                newRes[newPointer] = result[origPointer++];\n            else\n                newRes[newPointer] = null;\n            newPointer++;\n        }\n\n        for (int i = 0; i < schemaSpec.regularColumns.size() && origPointer < result.length; i++)\n        {\n            ColumnSpec<?> column = schemaSpec.regularColumns.get(i);\n            if (columns.contains(column))\n                newRes[newPointer] = result[origPointer++];\n            else\n                newRes[newPointer] = null;\n            newPointer++;\n        }\n\n        return newRes;\n    }\n\n    static boolean isDeflatable(Object[] columns)\n    {\n        for (Object column : columns)\n        {\n            if (column == DataGenerators.UNSET_VALUE)\n                return false;\n        }\n        return true;\n    }\n\n    public static ResultSetRow resultSetToRow(SchemaSpec schema, OpSelectors.Clock clock, Object[] result)\n    {\n        Object[] partitionKey = new Object[schema.partitionKeys.size()];\n        Object[] clusteringKey = new Object[schema.clusteringKeys.size()];\n        Object[] staticColumns = new Object[schema.staticColumns.size()];\n        Object[] regularColumns = new Object[schema.regularColumns.size()];\n\n        System.arraycopy(result, 0, partitionKey, 0, partitionKey.length);\n        System.arraycopy(result, partitionKey.length, clusteringKey, 0, clusteringKey.length);\n        System.arraycopy(result, partitionKey.length + clusteringKey.length, staticColumns, 0, staticColumns.length);\n        System.arraycopy(result, partitionKey.length + clusteringKey.length + staticColumns.length, regularColumns, 0, regularColumns.length);\n\n\n        List<Long> visited_lts_list;\n        if (schema.trackLts)\n        {\n            visited_lts_list = (List<Long>) result[result.length - 1];\n            visited_lts_list.sort(Long::compare);\n        }\n        else\n        {\n            visited_lts_list = Collections.emptyList();\n        }\n\n        long[] slts = new long[schema.staticColumns.size()];\n        for (int i = 0; i < slts.length; i++)\n        {\n            Object v = result[schema.allColumns.size() + i];\n            slts[i] = v == null ? Model.NO_TIMESTAMP : clock.lts((long) v);\n        }\n\n        long[] lts = new long[schema.regularColumns.size()];\n        for (int i = 0; i < lts.length; i++)\n        {\n            Object v = result[schema.allColumns.size() + slts.length + i];\n            lts[i] = v == null ? Model.NO_TIMESTAMP : clock.lts((long) v);\n        }\n\n        return new ResultSetRow(isDeflatable(partitionKey) ? schema.deflatePartitionKey(partitionKey) : UNSET_DESCR,\n                                isDeflatable(clusteringKey) ? schema.deflateClusteringKey(clusteringKey) : UNSET_DESCR,\n                                schema.staticColumns.isEmpty() ? EMPTY_ARR : schema.deflateStaticColumns(staticColumns),\n                                schema.staticColumns.isEmpty() ? EMPTY_ARR : slts,\n                                schema.deflateRegularColumns(regularColumns),\n                                lts,\n                                visited_lts_list);\n    }\n\n    public static List<ResultSetRow> execute(SystemUnderTest sut, OpSelectors.Clock clock, Query query)\n    {\n        return execute(sut, clock, query, query.schemaSpec.allColumnsSet);\n    }\n\n    public static List<ResultSetRow> execute(SystemUnderTest sut, OpSelectors.Clock clock, Query query, Set<ColumnSpec<?>> columns)\n    {\n        CompiledStatement compiled = query.toSelectStatement(columns, true);\n        Object[][] objects = sut.executeIdempotent(compiled.cql(), SystemUnderTest.ConsistencyLevel.QUORUM, compiled.bindings());\n        List<ResultSetRow> result = new ArrayList<>();\n        for (Object[] obj : objects)\n            result.add(resultSetToRow(query.schemaSpec, clock, broadenResult(query.schemaSpec, columns, obj)));\n        return result;\n    }\n\n    public static List<ResultSetRow> execute(SystemUnderTest sut, OpSelectors.Clock clock, CompiledStatement compiled, SchemaSpec schemaSpec)\n    {\n        Set<ColumnSpec<?>> columns = schemaSpec.allColumnsSet;\n        Object[][] objects = sut.executeIdempotent(compiled.cql(), SystemUnderTest.ConsistencyLevel.QUORUM, compiled.bindings());\n        List<ResultSetRow> result = new ArrayList<>();\n        for (Object[] obj : objects)\n            result.add(resultSetToRow(schemaSpec, clock, broadenResult(schemaSpec, columns, obj)));\n        return result;\n    }\n}\n","lineNo":129}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.index.sai.cql.intersection;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Predicate;\nimport java.util.stream.Collectors;\n\nimport org.junit.Before;\nimport org.junit.runners.Parameterized;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.cql3.UntypedResultSet;\nimport org.apache.cassandra.index.sai.utils.SAIRandomizedTester;\n\npublic abstract class RandomIntersectionTester extends SAIRandomizedTester\n{\n    private static final Object[][] EMPTY_ROWS = new Object[][]{};\n\n    protected enum Mode { REGULAR, STATIC, REGULAR_STATIC, TWO_REGULAR_ONE_STATIC }\n\n    @Parameterized.Parameter\n    public String testName;\n\n    @Parameterized.Parameter(1)\n    public boolean partitionRestricted;\n\n    @Parameterized.Parameter(2)\n    public boolean largePartition;\n\n    @Parameterized.Parameter(3)\n    public boolean v1Cardinality;\n\n    @Parameterized.Parameter(4)\n    public boolean v2Cardinality;\n\n    @Parameterized.Parameter(5)\n    public Mode mode;\n\n    private int numPartitions;\n\n    @Before\n    public void createTableAndIndexes()\n    {\n        CassandraRelevantProperties.SAI_INTERSECTION_CLAUSE_LIMIT.setInt(3);\n\n        createTable(\"CREATE TABLE %s (pk int, ck int, v1 int, v2 int, s1 int static, s2 int static, PRIMARY KEY(pk, ck))\");\n        createIndex(\"CREATE INDEX ON %s(v1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(v2) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s2) USING 'sai'\");\n\n        numPartitions = nextInt(15000, 100000);\n    }\n\n    protected void runRestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            for (int queryCount = 0; queryCount < nextInt(10, 100); queryCount++)\n            {\n                int pk = testRowMap.keySet().stream().skip(nextInt(0, testRowMap.size())).findFirst().orElseThrow();\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n\n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 > v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2 && row.s2 > v2;\n\n                assert predicate != null : \"Predicate should be assigned!\";\n\n                List<Object[]> expected = testRowMap.get(pk)\n                                                    .stream()\n                                                    .sorted(Comparator.comparingInt(o -> o.ck))\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n\n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ?\", pk, v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND s1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ? AND s2 > ?\", pk, v1, v2, v2);\n\n                assertRows(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    protected void runUnrestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            for (int queryCount = 0; queryCount < nextInt(10, 100); queryCount++)\n            {\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n                \n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 == v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2 && row.s2 > v2;\n                \n                assert predicate != null : \"Predicate should be assigned!\";\n                \n                List<Object[]> expected = testRowMap.values()\n                                                    .stream()\n                                                    .flatMap(Collection::stream)\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n                \n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ?\", v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE s1 > ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ? AND s2 > ?\", v1, v2, v2);\n\n                assertRowsIgnoringOrder(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    private Map<Integer, List<TestRow>> buildAndLoadTestRows()\n    {\n        Map<Integer, List<TestRow>> testRowMap = new HashMap<>();\n\n        int clusterSize = nextPartitionSize();\n        int partition = nextInt(0, numPartitions);\n        int s1 = nextV1();\n        int s2 = nextV2();\n        List<TestRow> rowList = new ArrayList<>(clusterSize);\n        testRowMap.put(partition, rowList);\n        int clusterCount = 0;\n\n        for (int index = 0; index < numPartitions; index++)\n        {\n            TestRow row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n            while (rowList.contains(row))\n                row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n\n            rowList.add(row);\n            clusterCount++;\n\n            if (clusterCount == clusterSize)\n            {\n                clusterCount = 0;\n                clusterSize = nextPartitionSize();\n                partition = nextInt(0, numPartitions);\n                while (testRowMap.containsKey(partition))\n                    partition = nextInt(0, numPartitions);\n                rowList = new ArrayList<>(clusterSize);\n                testRowMap.put(partition, rowList);\n            }\n        }\n       \n        testRowMap.values().stream().flatMap(Collection::stream).forEach(row -> {\n            execute(\"INSERT INTO %s (pk, ck, v1, v2) VALUES (?, ?, ?, ?)\", row.pk, row.ck, row.v1, row.v2);\n            execute(\"INSERT INTO %s (pk, s1, s2) VALUES (?, ?, ?)\", row.pk, row.s1, row.s2);\n        });\n\n        return testRowMap;\n    }\n\n    private int nextPartitionSize()\n    {\n        return largePartition ? nextInt(1024, 4096) : nextInt(1, 64);\n    }\n\n    private int nextV1()\n    {\n        return v1Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private int nextV2()\n    {\n        return v2Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private static class TestRow implements Comparable<TestRow>\n    {\n        final int pk;\n        final int ck;\n        final int v1;\n        final int v2;\n        final int s1;\n        final int s2;\n\n        TestRow(int pk, int ck, int v1, int v2, int s1, int s2)\n        {\n            this.pk = pk;\n            this.ck = ck;\n            this.v1 = v1;\n            this.v2 = v2;\n            this.s1 = s1;\n            this.s2 = s2;\n        }\n\n        @Override\n        public int compareTo(TestRow other)\n        {\n            int cmp = Integer.compare(pk, other.pk);\n            if (cmp != 0)\n                return cmp;\n            return Integer.compare(ck, other.ck);\n        }\n\n        @Override\n        public boolean equals(Object obj)\n        {\n            if (obj instanceof TestRow)\n                return compareTo((TestRow) obj) == 0;\n\n            return false;\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(pk, ck);\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.index.sai.cql.intersection;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Predicate;\nimport java.util.stream.Collectors;\n\nimport org.junit.Before;\nimport org.junit.runners.Parameterized;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.cql3.UntypedResultSet;\nimport org.apache.cassandra.index.sai.utils.SAIRandomizedTester;\n\npublic abstract class RandomIntersectionTester extends SAIRandomizedTester\n{\n    private static final Object[][] EMPTY_ROWS = new Object[][]{};\n\n    protected enum Mode { REGULAR, STATIC, REGULAR_STATIC, TWO_REGULAR_ONE_STATIC }\n\n    @Parameterized.Parameter\n    public String testName;\n\n    @Parameterized.Parameter(1)\n    public boolean partitionRestricted;\n\n    @Parameterized.Parameter(2)\n    public boolean largePartition;\n\n    @Parameterized.Parameter(3)\n    public boolean v1Cardinality;\n\n    @Parameterized.Parameter(4)\n    public boolean v2Cardinality;\n\n    @Parameterized.Parameter(5)\n    public Mode mode;\n\n    private int numPartitions;\n\n    @Before\n    public void createTableAndIndexes()\n    {\n        CassandraRelevantProperties.SAI_INTERSECTION_CLAUSE_LIMIT.setInt(3);\n\n        createTable(\"CREATE TABLE %s (pk int, ck int, v1 int, v2 int, s1 int static, s2 int static, PRIMARY KEY(pk, ck))\");\n        createIndex(\"CREATE INDEX ON %s(v1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(v2) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s2) USING 'sai'\");\n\n        numPartitions = nextInt(15000, 100000);\n    }\n\n    protected void runRestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            int queryCount = nextInt(10, 80);\n            for (int i = 0; i < queryCount; i++)\n            {\n                int pk = testRowMap.keySet().stream().skip(nextInt(0, testRowMap.size())).findFirst().orElseThrow();\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n\n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 > v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2 && row.s2 > v2;\n\n                assert predicate != null : \"Predicate should be assigned!\";\n\n                List<Object[]> expected = testRowMap.get(pk)\n                                                    .stream()\n                                                    .sorted(Comparator.comparingInt(o -> o.ck))\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n\n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ?\", pk, v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND s1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ? AND s2 > ?\", pk, v1, v2, v2);\n\n                assertRows(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    protected void runUnrestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            int queryCount = nextInt(10, 80);\n            for (int i = 0; i < queryCount; i++)\n            {\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n                \n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 == v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2 && row.s2 > v2;\n                \n                assert predicate != null : \"Predicate should be assigned!\";\n                \n                List<Object[]> expected = testRowMap.values()\n                                                    .stream()\n                                                    .flatMap(Collection::stream)\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n                \n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ?\", v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE s1 > ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ? AND s2 > ?\", v1, v2, v2);\n\n                assertRowsIgnoringOrder(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    private Map<Integer, List<TestRow>> buildAndLoadTestRows()\n    {\n        Map<Integer, List<TestRow>> testRowMap = new HashMap<>();\n\n        int clusterSize = nextPartitionSize();\n        int partition = nextInt(0, numPartitions);\n        int s1 = nextV1();\n        int s2 = nextV2();\n        List<TestRow> rowList = new ArrayList<>(clusterSize);\n        testRowMap.put(partition, rowList);\n        int clusterCount = 0;\n\n        for (int index = 0; index < numPartitions; index++)\n        {\n            TestRow row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n            while (rowList.contains(row))\n                row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n\n            rowList.add(row);\n            clusterCount++;\n\n            if (clusterCount == clusterSize)\n            {\n                clusterCount = 0;\n                clusterSize = nextPartitionSize();\n                partition = nextInt(0, numPartitions);\n                while (testRowMap.containsKey(partition))\n                    partition = nextInt(0, numPartitions);\n                rowList = new ArrayList<>(clusterSize);\n                testRowMap.put(partition, rowList);\n            }\n        }\n       \n        testRowMap.values().stream().flatMap(Collection::stream).forEach(row -> {\n            execute(\"INSERT INTO %s (pk, ck, v1, v2) VALUES (?, ?, ?, ?)\", row.pk, row.ck, row.v1, row.v2);\n            execute(\"INSERT INTO %s (pk, s1, s2) VALUES (?, ?, ?)\", row.pk, row.s1, row.s2);\n        });\n\n        return testRowMap;\n    }\n\n    private int nextPartitionSize()\n    {\n        return largePartition ? nextInt(1024, 4096) : nextInt(1, 64);\n    }\n\n    private int nextV1()\n    {\n        return v1Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private int nextV2()\n    {\n        return v2Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private static class TestRow implements Comparable<TestRow>\n    {\n        final int pk;\n        final int ck;\n        final int v1;\n        final int v2;\n        final int s1;\n        final int s2;\n\n        TestRow(int pk, int ck, int v1, int v2, int s1, int s2)\n        {\n            this.pk = pk;\n            this.ck = ck;\n            this.v1 = v1;\n            this.v2 = v2;\n            this.s1 = s1;\n            this.s2 = s2;\n        }\n\n        @Override\n        public int compareTo(TestRow other)\n        {\n            int cmp = Integer.compare(pk, other.pk);\n            if (cmp != 0)\n                return cmp;\n            return Integer.compare(ck, other.ck);\n        }\n\n        @Override\n        public boolean equals(Object obj)\n        {\n            if (obj instanceof TestRow)\n                return compareTo((TestRow) obj) == 0;\n\n            return false;\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(pk, ck);\n        }\n    }\n}\n","lineNo":83}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.index.sai.cql.intersection;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Predicate;\nimport java.util.stream.Collectors;\n\nimport org.junit.Before;\nimport org.junit.runners.Parameterized;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.cql3.UntypedResultSet;\nimport org.apache.cassandra.index.sai.utils.SAIRandomizedTester;\n\npublic abstract class RandomIntersectionTester extends SAIRandomizedTester\n{\n    private static final Object[][] EMPTY_ROWS = new Object[][]{};\n\n    protected enum Mode { REGULAR, STATIC, REGULAR_STATIC, TWO_REGULAR_ONE_STATIC }\n\n    @Parameterized.Parameter\n    public String testName;\n\n    @Parameterized.Parameter(1)\n    public boolean partitionRestricted;\n\n    @Parameterized.Parameter(2)\n    public boolean largePartition;\n\n    @Parameterized.Parameter(3)\n    public boolean v1Cardinality;\n\n    @Parameterized.Parameter(4)\n    public boolean v2Cardinality;\n\n    @Parameterized.Parameter(5)\n    public Mode mode;\n\n    private int numPartitions;\n\n    @Before\n    public void createTableAndIndexes()\n    {\n        CassandraRelevantProperties.SAI_INTERSECTION_CLAUSE_LIMIT.setInt(3);\n\n        createTable(\"CREATE TABLE %s (pk int, ck int, v1 int, v2 int, s1 int static, s2 int static, PRIMARY KEY(pk, ck))\");\n        createIndex(\"CREATE INDEX ON %s(v1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(v2) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s2) USING 'sai'\");\n\n        numPartitions = nextInt(15000, 100000);\n    }\n\n    protected void runRestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            for (int queryCount = 0; queryCount < nextInt(10, 100); queryCount++)\n            {\n                int pk = testRowMap.keySet().stream().skip(nextInt(0, testRowMap.size())).findFirst().orElseThrow();\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n\n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 > v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2 && row.s2 > v2;\n\n                assert predicate != null : \"Predicate should be assigned!\";\n\n                List<Object[]> expected = testRowMap.get(pk)\n                                                    .stream()\n                                                    .sorted(Comparator.comparingInt(o -> o.ck))\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n\n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ?\", pk, v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND s1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ? AND s2 > ?\", pk, v1, v2, v2);\n\n                assertRows(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    protected void runUnrestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            for (int queryCount = 0; queryCount < nextInt(10, 100); queryCount++)\n            {\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n                \n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 == v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2 && row.s2 > v2;\n                \n                assert predicate != null : \"Predicate should be assigned!\";\n                \n                List<Object[]> expected = testRowMap.values()\n                                                    .stream()\n                                                    .flatMap(Collection::stream)\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n                \n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ?\", v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE s1 > ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ? AND s2 > ?\", v1, v2, v2);\n\n                assertRowsIgnoringOrder(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    private Map<Integer, List<TestRow>> buildAndLoadTestRows()\n    {\n        Map<Integer, List<TestRow>> testRowMap = new HashMap<>();\n\n        int clusterSize = nextPartitionSize();\n        int partition = nextInt(0, numPartitions);\n        int s1 = nextV1();\n        int s2 = nextV2();\n        List<TestRow> rowList = new ArrayList<>(clusterSize);\n        testRowMap.put(partition, rowList);\n        int clusterCount = 0;\n\n        for (int index = 0; index < numPartitions; index++)\n        {\n            TestRow row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n            while (rowList.contains(row))\n                row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n\n            rowList.add(row);\n            clusterCount++;\n\n            if (clusterCount == clusterSize)\n            {\n                clusterCount = 0;\n                clusterSize = nextPartitionSize();\n                partition = nextInt(0, numPartitions);\n                while (testRowMap.containsKey(partition))\n                    partition = nextInt(0, numPartitions);\n                rowList = new ArrayList<>(clusterSize);\n                testRowMap.put(partition, rowList);\n            }\n        }\n       \n        testRowMap.values().stream().flatMap(Collection::stream).forEach(row -> {\n            execute(\"INSERT INTO %s (pk, ck, v1, v2) VALUES (?, ?, ?, ?)\", row.pk, row.ck, row.v1, row.v2);\n            execute(\"INSERT INTO %s (pk, s1, s2) VALUES (?, ?, ?)\", row.pk, row.s1, row.s2);\n        });\n\n        return testRowMap;\n    }\n\n    private int nextPartitionSize()\n    {\n        return largePartition ? nextInt(1024, 4096) : nextInt(1, 64);\n    }\n\n    private int nextV1()\n    {\n        return v1Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private int nextV2()\n    {\n        return v2Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private static class TestRow implements Comparable<TestRow>\n    {\n        final int pk;\n        final int ck;\n        final int v1;\n        final int v2;\n        final int s1;\n        final int s2;\n\n        TestRow(int pk, int ck, int v1, int v2, int s1, int s2)\n        {\n            this.pk = pk;\n            this.ck = ck;\n            this.v1 = v1;\n            this.v2 = v2;\n            this.s1 = s1;\n            this.s2 = s2;\n        }\n\n        @Override\n        public int compareTo(TestRow other)\n        {\n            int cmp = Integer.compare(pk, other.pk);\n            if (cmp != 0)\n                return cmp;\n            return Integer.compare(ck, other.ck);\n        }\n\n        @Override\n        public boolean equals(Object obj)\n        {\n            if (obj instanceof TestRow)\n                return compareTo((TestRow) obj) == 0;\n\n            return false;\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(pk, ck);\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.index.sai.cql.intersection;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Predicate;\nimport java.util.stream.Collectors;\n\nimport org.junit.Before;\nimport org.junit.runners.Parameterized;\n\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.cql3.UntypedResultSet;\nimport org.apache.cassandra.index.sai.utils.SAIRandomizedTester;\n\npublic abstract class RandomIntersectionTester extends SAIRandomizedTester\n{\n    private static final Object[][] EMPTY_ROWS = new Object[][]{};\n\n    protected enum Mode { REGULAR, STATIC, REGULAR_STATIC, TWO_REGULAR_ONE_STATIC }\n\n    @Parameterized.Parameter\n    public String testName;\n\n    @Parameterized.Parameter(1)\n    public boolean partitionRestricted;\n\n    @Parameterized.Parameter(2)\n    public boolean largePartition;\n\n    @Parameterized.Parameter(3)\n    public boolean v1Cardinality;\n\n    @Parameterized.Parameter(4)\n    public boolean v2Cardinality;\n\n    @Parameterized.Parameter(5)\n    public Mode mode;\n\n    private int numPartitions;\n\n    @Before\n    public void createTableAndIndexes()\n    {\n        CassandraRelevantProperties.SAI_INTERSECTION_CLAUSE_LIMIT.setInt(3);\n\n        createTable(\"CREATE TABLE %s (pk int, ck int, v1 int, v2 int, s1 int static, s2 int static, PRIMARY KEY(pk, ck))\");\n        createIndex(\"CREATE INDEX ON %s(v1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(v2) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s1) USING 'sai'\");\n        createIndex(\"CREATE INDEX ON %s(s2) USING 'sai'\");\n\n        numPartitions = nextInt(15000, 100000);\n    }\n\n    protected void runRestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            int queryCount = nextInt(10, 80);\n            for (int i = 0; i < queryCount; i++)\n            {\n                int pk = testRowMap.keySet().stream().skip(nextInt(0, testRowMap.size())).findFirst().orElseThrow();\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n\n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 > v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 > v1 && row.v2 > v2 && row.s2 > v2;\n\n                assert predicate != null : \"Predicate should be assigned!\";\n\n                List<Object[]> expected = testRowMap.get(pk)\n                                                    .stream()\n                                                    .sorted(Comparator.comparingInt(o -> o.ck))\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n\n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ?\", pk, v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND s1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND s2 > ?\", pk, v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE pk = ? AND v1 > ? AND v2 > ? AND s2 > ?\", pk, v1, v2, v2);\n\n                assertRows(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    protected void runUnrestrictedQueries() throws Throwable\n    {\n        Map<Integer, List<TestRow>> testRowMap = buildAndLoadTestRows();\n\n        beforeAndAfterFlush(() -> {\n            int queryCount = nextInt(10, 80);\n            for (int i = 0; i < queryCount; i++)\n            {\n                int v1 = nextV1();\n                int v2 = nextV2();\n\n                Predicate<TestRow> predicate = null;\n                \n                if (mode == Mode.REGULAR)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2;\n                else if (mode == Mode.STATIC)\n                    predicate = row -> row.s1 > v1 && row.s2 > v2;\n                else if (mode == Mode.REGULAR_STATIC)\n                    predicate = row -> row.v1 == v1 && row.s2 > v2;\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    predicate = row -> row.v1 == v1 && row.v2 > v2 && row.s2 > v2;\n                \n                assert predicate != null : \"Predicate should be assigned!\";\n                \n                List<Object[]> expected = testRowMap.values()\n                                                    .stream()\n                                                    .flatMap(Collection::stream)\n                                                    .filter(predicate)\n                                                    .map(row -> row(row.pk, row.ck))\n                                                    .collect(Collectors.toList());\n\n                UntypedResultSet result = null;\n                \n                if (mode == Mode.REGULAR)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ?\", v1, v2);\n                else if (mode == Mode.STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE s1 > ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.REGULAR_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND s2 > ?\", v1, v2);\n                else if (mode == Mode.TWO_REGULAR_ONE_STATIC)\n                    result = execute(\"SELECT pk, ck FROM %s WHERE v1 = ? AND v2 > ? AND s2 > ?\", v1, v2, v2);\n\n                assertRowsIgnoringOrder(result, expected.toArray(EMPTY_ROWS));\n            }\n        });\n    }\n\n    private Map<Integer, List<TestRow>> buildAndLoadTestRows()\n    {\n        Map<Integer, List<TestRow>> testRowMap = new HashMap<>();\n\n        int clusterSize = nextPartitionSize();\n        int partition = nextInt(0, numPartitions);\n        int s1 = nextV1();\n        int s2 = nextV2();\n        List<TestRow> rowList = new ArrayList<>(clusterSize);\n        testRowMap.put(partition, rowList);\n        int clusterCount = 0;\n\n        for (int index = 0; index < numPartitions; index++)\n        {\n            TestRow row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n            while (rowList.contains(row))\n                row = new TestRow(partition, nextInt(10, numPartitions), nextV1(), nextV2(), s1, s2);\n\n            rowList.add(row);\n            clusterCount++;\n\n            if (clusterCount == clusterSize)\n            {\n                clusterCount = 0;\n                clusterSize = nextPartitionSize();\n                partition = nextInt(0, numPartitions);\n                while (testRowMap.containsKey(partition))\n                    partition = nextInt(0, numPartitions);\n                rowList = new ArrayList<>(clusterSize);\n                testRowMap.put(partition, rowList);\n            }\n        }\n       \n        testRowMap.values().stream().flatMap(Collection::stream).forEach(row -> {\n            execute(\"INSERT INTO %s (pk, ck, v1, v2) VALUES (?, ?, ?, ?)\", row.pk, row.ck, row.v1, row.v2);\n            execute(\"INSERT INTO %s (pk, s1, s2) VALUES (?, ?, ?)\", row.pk, row.s1, row.s2);\n        });\n\n        return testRowMap;\n    }\n\n    private int nextPartitionSize()\n    {\n        return largePartition ? nextInt(1024, 4096) : nextInt(1, 64);\n    }\n\n    private int nextV1()\n    {\n        return v1Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private int nextV2()\n    {\n        return v2Cardinality ? nextInt(10, numPartitions / 10) : nextInt(10, numPartitions / 1000);\n    }\n\n    private static class TestRow implements Comparable<TestRow>\n    {\n        final int pk;\n        final int ck;\n        final int v1;\n        final int v2;\n        final int s1;\n        final int s2;\n\n        TestRow(int pk, int ck, int v1, int v2, int s1, int s2)\n        {\n            this.pk = pk;\n            this.ck = ck;\n            this.v1 = v1;\n            this.v2 = v2;\n            this.s1 = s1;\n            this.s2 = s2;\n        }\n\n        @Override\n        public int compareTo(TestRow other)\n        {\n            int cmp = Integer.compare(pk, other.pk);\n            if (cmp != 0)\n                return cmp;\n            return Integer.compare(ck, other.ck);\n        }\n\n        @Override\n        public boolean equals(Object obj)\n        {\n            if (obj instanceof TestRow)\n                return compareTo((TestRow) obj) == 0;\n\n            return false;\n        }\n\n        @Override\n        public int hashCode()\n        {\n            return Objects.hash(pk, ck);\n        }\n    }\n}\n","lineNo":131}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.index.sai.plan;\n\nimport java.nio.ByteBuffer;\nimport java.util.Arrays;\nimport java.util.EnumMap;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Preconditions;\nimport com.google.common.collect.Multimap;\nimport com.google.common.collect.Sets;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.cql3.statements.schema.IndexTarget;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.PartitionRangeReadCommand;\nimport org.apache.cassandra.db.ReadCommand;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.CollectionType;\nimport org.apache.cassandra.db.marshal.CompositeType;\nimport org.apache.cassandra.db.marshal.DoubleType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.LongType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.UTF8Type;\nimport org.apache.cassandra.db.rows.BTreeRow;\nimport org.apache.cassandra.db.rows.BufferCell;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.db.rows.Unfiltered;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.index.sai.QueryContext;\nimport org.apache.cassandra.index.sai.StorageAttachedIndex;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.IndexMetadata;\nimport org.apache.cassandra.schema.Indexes;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.CASSANDRA_CONFIG;\nimport static org.apache.cassandra.db.marshal.Int32Type.instance;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\n\npublic class OperationTest\n{\n    private static final String KS_NAME = \"sai\";\n    private static final String CF_NAME = \"test_cf\";\n    private static final String CLUSTERING_CF_NAME = \"clustering_test_cf\";\n    private static final String STATIC_CF_NAME = \"static_sai_test_cf\";\n\n    private static ColumnFamilyStore BACKEND;\n    private static ColumnFamilyStore CLUSTERING_BACKEND;\n    private static ColumnFamilyStore STATIC_BACKEND;\n\n    private QueryController controller;\n    private QueryController controllerClustering;\n    private QueryController controllerStatic;\n\n    @BeforeClass\n    public static void loadSchema() throws ConfigurationException\n    {\n        CASSANDRA_CONFIG.setString(\"cassandra-murmur.yaml\");\n\n        SchemaLoader.loadSchema();\n\n        SchemaLoader.createKeyspace(KS_NAME,\n                                    KeyspaceParams.simpleTransient(1),\n                                    skinnySAITableMetadata(KS_NAME, CF_NAME),\n                                    clusteringSAITableMetadata(KS_NAME, CLUSTERING_CF_NAME),\n                                    staticSAITableMetadata(KS_NAME, STATIC_CF_NAME));\n\n        BACKEND = Keyspace.open(KS_NAME).getColumnFamilyStore(CF_NAME);\n        CLUSTERING_BACKEND = Keyspace.open(KS_NAME).getColumnFamilyStore(CLUSTERING_CF_NAME);\n        STATIC_BACKEND = Keyspace.open(KS_NAME).getColumnFamilyStore(STATIC_CF_NAME);\n    }\n\n    @Before\n    public void beforeTest()\n    {\n        ReadCommand command = PartitionRangeReadCommand.allDataRead(BACKEND.metadata(), FBUtilities.nowInSeconds());\n        controller = new QueryController(BACKEND,\n                                         command,\n                                         null,\n                                         new QueryContext(command, DatabaseDescriptor.getRangeRpcTimeout(TimeUnit.MILLISECONDS)));\n\n        command = PartitionRangeReadCommand.allDataRead(CLUSTERING_BACKEND.metadata(), FBUtilities.nowInSeconds());\n        controllerClustering = new QueryController(CLUSTERING_BACKEND,\n                                                   command,\n                                                   null,\n                                                   new QueryContext(command, DatabaseDescriptor.getRangeRpcTimeout(TimeUnit.MILLISECONDS)));\n\n        command = PartitionRangeReadCommand.allDataRead(STATIC_BACKEND.metadata(), FBUtilities.nowInSeconds());\n        controllerStatic = new QueryController(STATIC_BACKEND,\n                                               command,\n                                               null,\n                                               new QueryContext(command, DatabaseDescriptor.getRangeRpcTimeout(TimeUnit.MILLISECONDS)));\n    }\n\n    @Test\n    public void testAnalyze()\n    {\n        final ColumnMetadata age = getColumn(UTF8Type.instance.decompose(\"age\"));\n\n        // age > 1 AND age < 7\n        Map<Expression.IndexOperator, Expression> expressions = convert(Operation.buildIndexExpressions(controller,\n                                                                                                        Arrays.asList(new SimpleExpression(age, Operator.GT, Int32Type.instance.decompose(1)),\n                                                                                                                      new SimpleExpression(age, Operator.LT, Int32Type.instance.decompose(7)))));\n\n        assertEquals(1, expressions.size());\n\n        Expression rangeExpression = expressions.get(Expression.IndexOperator.RANGE);\n\n        assertExpression(rangeExpression, Expression.IndexOperator.RANGE, Int32Type.instance.decompose(1), false, Int32Type.instance.decompose(7), false);\n    }\n\n    @Test\n    public void testSatisfiedBy()\n    {\n        final ColumnMetadata timestamp = getColumn(UTF8Type.instance.decompose(\"timestamp\"));\n        final ColumnMetadata age = getColumn(UTF8Type.instance.decompose(\"age\"));\n\n        Operation.Node node = new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(5)));\n        FilterTree filterTree = node.buildFilter(controller);\n\n        DecoratedKey key = buildKey(\"0\");\n        Unfiltered row = buildRow(buildCell(age, instance.decompose(6), System.currentTimeMillis()));\n        Row staticRow = buildRow(Clustering.STATIC_CLUSTERING);\n\n        assertFalse(filterTree.isSatisfiedBy(key, row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(5), System.currentTimeMillis()));\n\n        assertTrue(filterTree.isSatisfiedBy(key, row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(6), System.currentTimeMillis()));\n\n        assertFalse(filterTree.isSatisfiedBy(key, row, staticRow));\n\n        // range with exclusions - age > 1 AND age <= 10\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GT, Int32Type.instance.decompose(1))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.LTE, Int32Type.instance.decompose(10))));\n        filterTree = node.buildFilter(controller);\n\n        Set<Integer> exclusions = Sets.newHashSet(0, 1, 11);\n        for (int i = 0; i <= 11; i++)\n        {\n            row = buildRow(buildCell(age, instance.decompose(i), System.currentTimeMillis()));\n\n            boolean result = filterTree.isSatisfiedBy(key, row, staticRow);\n            assertTrue(exclusions.contains(i) != result);\n        }\n\n        // now let's test aggregated AND commands\n        node = new Operation.AndNode();\n\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GTE, Int32Type.instance.decompose(0))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.LT, Int32Type.instance.decompose(10))));\n\n        filterTree = node.buildFilter(controller);\n\n        for (int i = 0; i < 10; i++)\n        {\n            row = buildRow(buildCell(age, instance.decompose(i), System.currentTimeMillis()));\n\n            boolean result = filterTree.isSatisfiedBy(key, row, staticRow);\n            assertTrue(result);\n        }\n\n        // multiple analyzed expressions in the Operation timestamp >= 10 AND age = 5\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(timestamp, Operator.GTE, LongType.instance.decompose(10L))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(5))));\n\n        filterTree = node.buildFilter(controller);\n\n        row = buildRow(buildCell(age, instance.decompose(6), System.currentTimeMillis()),\n                                  buildCell(timestamp, LongType.instance.decompose(11L), System.currentTimeMillis()));\n\n        assertFalse(filterTree.isSatisfiedBy(key, row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(5), System.currentTimeMillis()),\n                                  buildCell(timestamp, LongType.instance.decompose(22L), System.currentTimeMillis()));\n\n        assertTrue(filterTree.isSatisfiedBy(key, row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(5), System.currentTimeMillis()),\n                                  buildCell(timestamp, LongType.instance.decompose(9L), System.currentTimeMillis()));\n\n        assertFalse(filterTree.isSatisfiedBy(key, row, staticRow));\n\n    }\n\n    @Test\n    public void testAnalyzeNotIndexedButDefinedColumn()\n    {\n        final ColumnMetadata firstName = getColumn(UTF8Type.instance.decompose(\"first_name\"));\n        final ColumnMetadata height = getColumn(UTF8Type.instance.decompose(\"height\"));\n\n        // first_name = 'a' AND height > 5\n        Map<Expression.IndexOperator, Expression> expressions;\n        expressions = convert(Operation.buildIndexExpressions(controller,\n                                                              Arrays.asList(new SimpleExpression(firstName, Operator.EQ, UTF8Type.instance.decompose(\"a\")),\n                                                                   new SimpleExpression(height, Operator.GT, Int32Type.instance.decompose(5)))));\n\n        assertEquals(2, expressions.size());\n\n        expressions = convert(Operation.buildIndexExpressions(controller,\n                                                              Arrays.asList(new SimpleExpression(firstName, Operator.EQ, UTF8Type.instance.decompose(\"a\")),\n                                                                   new SimpleExpression(height, Operator.GT, Int32Type.instance.decompose(0)),\n                                                                   new SimpleExpression(height, Operator.EQ, Int32Type.instance.decompose(5)))));\n\n        assertEquals(2, expressions.size());\n\n        Expression rangeExpression = expressions.get(Expression.IndexOperator.RANGE);\n\n        assertExpression(rangeExpression, Expression.IndexOperator.RANGE, Int32Type.instance.decompose(0), false, Int32Type.instance.decompose(5), true);\n\n        expressions = convert(Operation.buildIndexExpressions(controller,\n                                                              Arrays.asList(new SimpleExpression(firstName, Operator.EQ, UTF8Type.instance.decompose(\"a\")),\n                                                                            new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(0)),\n                                                                            new SimpleExpression(height, Operator.LT, Int32Type.instance.decompose(10)))));\n\n        assertEquals(2, expressions.size());\n\n        rangeExpression = expressions.get(Expression.IndexOperator.RANGE);\n\n        assertExpression(rangeExpression, Expression.IndexOperator.RANGE, Int32Type.instance.decompose(0), true, Int32Type.instance.decompose(10), false);\n    }\n\n    @Test\n    public void testSatisfiedByWithClustering()\n    {\n        ColumnMetadata location = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"location\"));\n        ColumnMetadata age = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"age\"));\n        ColumnMetadata height = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"height\"));\n        ColumnMetadata score = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"score\"));\n\n        DecoratedKey key = buildKey(CLUSTERING_BACKEND, \"0\");\n        Unfiltered row = buildRow(Clustering.make(UTF8Type.instance.fromString(\"US\"), Int32Type.instance.decompose(27)),\n                                  buildCell(height, instance.decompose(182), System.currentTimeMillis()),\n                                  buildCell(score, DoubleType.instance.decompose(1.0d), System.currentTimeMillis()));\n        Row staticRow = buildRow(Clustering.STATIC_CLUSTERING);\n\n        Operation.Node node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(27))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.EQ, Int32Type.instance.decompose(182))));\n\n        assertTrue(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(28))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.EQ, Int32Type.instance.decompose(182))));\n\n        assertFalse(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"US\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GTE, Int32Type.instance.decompose(27))));\n\n        assertTrue(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"BY\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GTE, Int32Type.instance.decompose(28))));\n\n        assertFalse(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"US\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.LTE, Int32Type.instance.decompose(27))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(182))));\n\n        assertTrue(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"US\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(182))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(score, Operator.EQ, DoubleType.instance.decompose(1.0d))));\n\n        assertTrue(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(182))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(score, Operator.EQ, DoubleType.instance.decompose(1.0d))));\n\n        assertTrue(node.buildFilter(controllerClustering).isSatisfiedBy(key, row, staticRow));\n    }\n\n    private Map<Expression.IndexOperator, Expression> convert(Multimap<ColumnMetadata, Expression> expressions)\n    {\n        Map<Expression.IndexOperator, Expression> converted = new EnumMap<>(Expression.IndexOperator.class);\n        for (Expression expression : expressions.values())\n        {\n            Expression column = converted.get(expression.getIndexOperator());\n            assert column == null; // sanity check\n            converted.put(expression.getIndexOperator(), expression);\n        }\n\n        return converted;\n    }\n\n    @Test\n    public void testSatisfiedByWithStatic()\n    {\n        final ColumnMetadata sensorType = getColumn(STATIC_BACKEND, UTF8Type.instance.decompose(\"sensor_type\"));\n        final ColumnMetadata value = getColumn(STATIC_BACKEND, UTF8Type.instance.decompose(\"value\"));\n\n        DecoratedKey key = buildKey(STATIC_BACKEND, 0);\n        Unfiltered row = buildRow(Clustering.make(UTF8Type.instance.fromString(\"date\"), LongType.instance.decompose(20160401L)),\n                                  buildCell(value, DoubleType.instance.decompose(24.56), System.currentTimeMillis()));\n        Row staticRow = buildRow(Clustering.STATIC_CLUSTERING,\n                                 buildCell(sensorType, UTF8Type.instance.decompose(\"TEMPERATURE\"), System.currentTimeMillis()));\n\n        // sensor_type ='TEMPERATURE' AND value = 24.56\n        Operation.Node node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(sensorType, Operator.EQ, UTF8Type.instance.decompose(\"TEMPERATURE\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(value, Operator.EQ, DoubleType.instance.decompose(24.56))));\n\n        assertTrue(node.buildFilter(controllerStatic).isSatisfiedBy(key, row, staticRow));\n\n        // sensor_type ='TEMPERATURE' AND value = 30\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(sensorType, Operator.EQ, UTF8Type.instance.decompose(\"TEMPERATURE\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(value, Operator.EQ, DoubleType.instance.decompose(30.00))));\n\n        assertFalse(node.buildFilter(controllerStatic).isSatisfiedBy(key, row, staticRow));\n    }\n\n    public static TableMetadata.Builder skinnySAITableMetadata(String keyspace, String table)\n    {\n        TableMetadata.Builder builder =\n        TableMetadata.builder(keyspace, table)\n                     .addPartitionKeyColumn(\"id\", UTF8Type.instance)\n                     .addRegularColumn(\"first_name\", UTF8Type.instance)\n                     .addRegularColumn(\"last_name\", UTF8Type.instance)\n                     .addRegularColumn(\"age\", Int32Type.instance)\n                     .addRegularColumn(\"height\", Int32Type.instance)\n                     .addRegularColumn(\"timestamp\", LongType.instance)\n                     .addRegularColumn(\"address\", UTF8Type.instance)\n                     .addRegularColumn(\"score\", DoubleType.instance);\n\n        Indexes.Builder indexes = Indexes.builder();\n        addIndex(indexes, table, \"first_name\");\n        addIndex(indexes, table, \"last_name\");\n        addIndex(indexes, table, \"age\");\n        addIndex(indexes, table, \"timestamp\");\n        addIndex(indexes, table, \"address\");\n        addIndex(indexes, table, \"score\");\n\n        return builder.indexes(indexes.build());\n    }\n\n    public static TableMetadata.Builder clusteringSAITableMetadata(String keyspace, String table)\n    {\n        return clusteringSAITableMetadata(keyspace, table, \"location\", \"age\", \"height\", \"score\");\n    }\n\n    public static TableMetadata.Builder clusteringSAITableMetadata(String keyspace, String table, String...indexedColumns)\n    {\n        Indexes.Builder indexes = Indexes.builder();\n        for (String indexedColumn : indexedColumns)\n        {\n            addIndex(indexes, table, indexedColumn);\n        }\n\n        return TableMetadata.builder(keyspace, table)\n                            .addPartitionKeyColumn(\"name\", UTF8Type.instance)\n                            .addClusteringColumn(\"location\", UTF8Type.instance)\n                            .addClusteringColumn(\"age\", Int32Type.instance)\n                            .addRegularColumn(\"height\", Int32Type.instance)\n                            .addRegularColumn(\"score\", DoubleType.instance)\n                            .indexes(indexes.build());\n    }\n\n    public static TableMetadata.Builder staticSAITableMetadata(String keyspace, String table)\n    {\n        TableMetadata.Builder builder =\n        TableMetadata.builder(keyspace, table)\n                     .addPartitionKeyColumn(\"sensor_id\", Int32Type.instance)\n                     .addStaticColumn(\"sensor_type\", UTF8Type.instance)\n                     .addClusteringColumn(\"date\", LongType.instance)\n                     .addRegularColumn(\"value\", DoubleType.instance)\n                     .addRegularColumn(\"variance\", Int32Type.instance);\n\n        Indexes.Builder indexes = Indexes.builder();\n\n        addIndex(indexes, table, \"sensor_type\");\n        addIndex(indexes, table, \"value\");\n        addIndex(indexes, table, \"variance\");\n\n        return builder.indexes(indexes.build());\n    }\n\n    private void assertExpression(Expression expression, Expression.IndexOperator indexOperator, ByteBuffer lower,\n                                  boolean lowerInclusive, ByteBuffer upper, boolean upperInclusive)\n    {\n        assertEquals(indexOperator, expression.getIndexOperator());\n        assertEquals(lower, expression.lower().value.raw);\n        assertEquals(lowerInclusive, expression.lower().inclusive);\n        assertEquals(upper, expression.upper().value.raw);\n        assertEquals(upperInclusive, expression.upper().inclusive);\n    }\n\n    private static void addIndex(Indexes.Builder indexes, String table, String column)\n    {\n        String indexName = table + '_' + column;\n        indexes.add(IndexMetadata.fromSchemaMetadata(indexName, IndexMetadata.Kind.CUSTOM, new HashMap<String, String>()\n        {{\n            put(IndexTarget.CUSTOM_INDEX_OPTION_NAME, StorageAttachedIndex.class.getName());\n            put(IndexTarget.TARGET_OPTION_NAME, column);\n        }}));\n    }\n\n    private static DecoratedKey buildKey(Object... key)\n    {\n        return buildKey(BACKEND, key);\n    }\n\n    private static DecoratedKey buildKey(ColumnFamilyStore cfs, Object... key)\n    {\n        AbstractType<?> type = cfs.metadata().partitionKeyType;\n        ByteBuffer decomposed;\n        if(type instanceof CompositeType)\n        {\n            Preconditions.checkArgument(key.length == type.subTypes().size());\n            decomposed = ((CompositeType) type).decompose(key);\n        }\n        else\n        {\n            Preconditions.checkArgument(key.length == 1);\n            decomposed = ((AbstractType) type).decompose(key[0]);\n        }\n        return Murmur3Partitioner.instance.decorateKey(decomposed);\n    }\n\n    private static Unfiltered buildRow(Cell<?>... cells)\n    {\n        return buildRow(Clustering.EMPTY, cells);\n    }\n\n    private static Row buildRow(Clustering<?> clustering, Cell<?>... cells)\n    {\n        Row.Builder rowBuilder = BTreeRow.sortedBuilder();\n        rowBuilder.newRow(clustering);\n        for (Cell<?> c : cells)\n            rowBuilder.addCell(c);\n\n        return rowBuilder.build();\n    }\n\n    private static Cell<?> buildCell(ColumnMetadata column, ByteBuffer value, long timestamp)\n    {\n        return BufferCell.live(column, timestamp, value);\n    }\n\n    private static ColumnMetadata getColumn(ByteBuffer name)\n    {\n        return getColumn(BACKEND, name);\n    }\n\n    private static ColumnMetadata getColumn(ColumnFamilyStore cfs, ByteBuffer name)\n    {\n        return cfs.metadata().getColumn(name);\n    }\n\n    private static class SimpleExpression extends RowFilter.Expression\n    {\n        SimpleExpression(ColumnMetadata column, Operator operator, ByteBuffer value)\n        {\n            super(column, operator, value);\n        }\n\n        @Override\n        public Kind kind()\n        {\n            return Kind.SIMPLE;\n        }\n\n        @Override\n        public boolean isSatisfiedBy(TableMetadata metadata, DecoratedKey partitionKey, Row row)\n        {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        protected String toString(boolean cql)\n        {\n            AbstractType<?> type = column.type;\n            switch (operator)\n            {\n                case CONTAINS:\n                    assert type instanceof CollectionType;\n                    CollectionType<?> ct = (CollectionType<?>)type;\n                    type = ct.kind == CollectionType.Kind.SET ? ct.nameComparator() : ct.valueComparator();\n                    break;\n                case CONTAINS_KEY:\n                    assert type instanceof MapType;\n                    type = ((MapType<?, ?>)type).nameComparator();\n                    break;\n                case IN:\n                    type = ListType.getInstance(type, false);\n                    break;\n                default:\n                    break;\n            }\n            return cql\n                   ? String.format(\"%s %s %s\", column.name.toCQLString(), operator, type.toCQLString(value) )\n                   : String.format(\"%s %s %s\", column.name.toString(), operator, type.getString(value));\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.index.sai.plan;\n\nimport java.nio.ByteBuffer;\nimport java.util.Arrays;\nimport java.util.EnumMap;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Preconditions;\nimport com.google.common.collect.Multimap;\nimport com.google.common.collect.Sets;\nimport org.junit.Before;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.cql3.statements.schema.IndexTarget;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.PartitionRangeReadCommand;\nimport org.apache.cassandra.db.ReadCommand;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.CollectionType;\nimport org.apache.cassandra.db.marshal.CompositeType;\nimport org.apache.cassandra.db.marshal.DoubleType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.LongType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.UTF8Type;\nimport org.apache.cassandra.db.rows.BTreeRow;\nimport org.apache.cassandra.db.rows.BufferCell;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.db.rows.Unfiltered;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.index.sai.QueryContext;\nimport org.apache.cassandra.index.sai.StorageAttachedIndex;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.IndexMetadata;\nimport org.apache.cassandra.schema.Indexes;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.CASSANDRA_CONFIG;\nimport static org.apache.cassandra.db.marshal.Int32Type.instance;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\n\npublic class OperationTest\n{\n    private static final String KS_NAME = \"sai\";\n    private static final String CF_NAME = \"test_cf\";\n    private static final String CLUSTERING_CF_NAME = \"clustering_test_cf\";\n    private static final String STATIC_CF_NAME = \"static_sai_test_cf\";\n\n    private static ColumnFamilyStore BACKEND;\n    private static ColumnFamilyStore CLUSTERING_BACKEND;\n    private static ColumnFamilyStore STATIC_BACKEND;\n\n    private QueryController controller;\n    private QueryController controllerClustering;\n    private QueryController controllerStatic;\n\n    @BeforeClass\n    public static void loadSchema() throws ConfigurationException\n    {\n        CASSANDRA_CONFIG.setString(\"cassandra-murmur.yaml\");\n\n        SchemaLoader.loadSchema();\n\n        SchemaLoader.createKeyspace(KS_NAME,\n                                    KeyspaceParams.simpleTransient(1),\n                                    skinnySAITableMetadata(KS_NAME, CF_NAME),\n                                    clusteringSAITableMetadata(KS_NAME, CLUSTERING_CF_NAME),\n                                    staticSAITableMetadata(KS_NAME, STATIC_CF_NAME));\n\n        BACKEND = Keyspace.open(KS_NAME).getColumnFamilyStore(CF_NAME);\n        CLUSTERING_BACKEND = Keyspace.open(KS_NAME).getColumnFamilyStore(CLUSTERING_CF_NAME);\n        STATIC_BACKEND = Keyspace.open(KS_NAME).getColumnFamilyStore(STATIC_CF_NAME);\n    }\n\n    @Before\n    public void beforeTest()\n    {\n        ReadCommand command = PartitionRangeReadCommand.allDataRead(BACKEND.metadata(), FBUtilities.nowInSeconds());\n        controller = new QueryController(BACKEND, command, null, contextWithUnrepairedMatches(command));\n\n        command = PartitionRangeReadCommand.allDataRead(CLUSTERING_BACKEND.metadata(), FBUtilities.nowInSeconds());\n        controllerClustering = new QueryController(CLUSTERING_BACKEND, command, null, contextWithUnrepairedMatches(command));\n\n        command = PartitionRangeReadCommand.allDataRead(STATIC_BACKEND.metadata(), FBUtilities.nowInSeconds());\n        controllerStatic = new QueryController(STATIC_BACKEND, command, null, contextWithUnrepairedMatches(command));\n    }\n\n    private static QueryContext contextWithUnrepairedMatches(ReadCommand command)\n    {\n        QueryContext context = new QueryContext(command, DatabaseDescriptor.getRangeRpcTimeout(TimeUnit.MILLISECONDS));\n        context.hasUnrepairedMatches = true;\n        return context;\n    }\n\n    @Test\n    public void testAnalyze()\n    {\n        final ColumnMetadata age = getColumn(UTF8Type.instance.decompose(\"age\"));\n\n        // age > 1 AND age < 7\n        Map<Expression.IndexOperator, Expression> expressions = convert(Operation.buildIndexExpressions(controller,\n                                                                                                        Arrays.asList(new SimpleExpression(age, Operator.GT, Int32Type.instance.decompose(1)),\n                                                                                                                      new SimpleExpression(age, Operator.LT, Int32Type.instance.decompose(7)))));\n\n        assertEquals(1, expressions.size());\n\n        Expression rangeExpression = expressions.get(Expression.IndexOperator.RANGE);\n\n        assertExpression(rangeExpression, Expression.IndexOperator.RANGE, Int32Type.instance.decompose(1), false, Int32Type.instance.decompose(7), false);\n    }\n\n    @Test\n    public void testSatisfiedBy()\n    {\n        final ColumnMetadata timestamp = getColumn(UTF8Type.instance.decompose(\"timestamp\"));\n        final ColumnMetadata age = getColumn(UTF8Type.instance.decompose(\"age\"));\n\n        Operation.Node node = new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(5)));\n        FilterTree filterTree = node.buildFilter(controller, true);\n\n        DecoratedKey key = buildKey(\"0\");\n        Unfiltered row = buildRow(buildCell(age, instance.decompose(6), System.currentTimeMillis()));\n        Row staticRow = buildRow(Clustering.STATIC_CLUSTERING);\n\n        assertFalse(filterTree.isSatisfiedBy(key, (Row) row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(5), System.currentTimeMillis()));\n\n        assertTrue(filterTree.isSatisfiedBy(key, (Row) row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(6), System.currentTimeMillis()));\n\n        assertFalse(filterTree.isSatisfiedBy(key, (Row) row, staticRow));\n\n        // range with exclusions - age > 1 AND age <= 10\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GT, Int32Type.instance.decompose(1))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.LTE, Int32Type.instance.decompose(10))));\n        filterTree = node.buildFilter(controller, true);\n\n        Set<Integer> exclusions = Sets.newHashSet(0, 1, 11);\n        for (int i = 0; i <= 11; i++)\n        {\n            row = buildRow(buildCell(age, instance.decompose(i), System.currentTimeMillis()));\n\n            boolean result = filterTree.isSatisfiedBy(key, (Row) row, staticRow);\n            assertTrue(exclusions.contains(i) != result);\n        }\n\n        // now let's test aggregated AND commands\n        node = new Operation.AndNode();\n\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GTE, Int32Type.instance.decompose(0))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.LT, Int32Type.instance.decompose(10))));\n\n        filterTree = node.buildFilter(controller, true);\n\n        for (int i = 0; i < 10; i++)\n        {\n            row = buildRow(buildCell(age, instance.decompose(i), System.currentTimeMillis()));\n\n            boolean result = filterTree.isSatisfiedBy(key, (Row) row, staticRow);\n            assertTrue(result);\n        }\n\n        // multiple analyzed expressions in the Operation timestamp >= 10 AND age = 5\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(timestamp, Operator.GTE, LongType.instance.decompose(10L))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(5))));\n\n        FilterTree filterTreeStrict = node.buildFilter(controller, true);\n        FilterTree filterTreeNonStrict = node.buildFilter(controller, false);\n\n        long startTime = System.currentTimeMillis();\n        row = buildRow(buildCell(age, instance.decompose(6), startTime),\n                       buildCell(timestamp, LongType.instance.decompose(11L), startTime + 1));\n\n        assertFalse(filterTreeStrict.isSatisfiedBy(key, (Row) row, staticRow));\n        assertTrue(filterTreeNonStrict.isSatisfiedBy(key, (Row) row, staticRow)); // matches on timestamp >= 10\n\n        row = buildRow(buildCell(age, instance.decompose(5), startTime + 2),\n                       buildCell(timestamp, LongType.instance.decompose(22L), startTime + 3));\n\n        assertTrue(filterTreeStrict.isSatisfiedBy(key, (Row) row, staticRow));\n        assertTrue(filterTreeNonStrict.isSatisfiedBy(key, (Row) row, staticRow));\n\n        row = buildRow(buildCell(age, instance.decompose(5), startTime + 4),\n                       buildCell(timestamp, LongType.instance.decompose(9L), startTime + 5));\n\n        assertFalse(filterTreeStrict.isSatisfiedBy(key, (Row) row, staticRow));\n        assertTrue(filterTreeNonStrict.isSatisfiedBy(key, (Row) row, staticRow)); // matches on age = 5\n    }\n\n    @Test\n    public void testAnalyzeNotIndexedButDefinedColumn()\n    {\n        final ColumnMetadata firstName = getColumn(UTF8Type.instance.decompose(\"first_name\"));\n        final ColumnMetadata height = getColumn(UTF8Type.instance.decompose(\"height\"));\n\n        // first_name = 'a' AND height > 5\n        Map<Expression.IndexOperator, Expression> expressions;\n        expressions = convert(Operation.buildIndexExpressions(controller,\n                                                              Arrays.asList(new SimpleExpression(firstName, Operator.EQ, UTF8Type.instance.decompose(\"a\")),\n                                                                   new SimpleExpression(height, Operator.GT, Int32Type.instance.decompose(5)))));\n\n        assertEquals(2, expressions.size());\n\n        expressions = convert(Operation.buildIndexExpressions(controller,\n                                                              Arrays.asList(new SimpleExpression(firstName, Operator.EQ, UTF8Type.instance.decompose(\"a\")),\n                                                                   new SimpleExpression(height, Operator.GT, Int32Type.instance.decompose(0)),\n                                                                   new SimpleExpression(height, Operator.EQ, Int32Type.instance.decompose(5)))));\n\n        assertEquals(2, expressions.size());\n\n        Expression rangeExpression = expressions.get(Expression.IndexOperator.RANGE);\n\n        assertExpression(rangeExpression, Expression.IndexOperator.RANGE, Int32Type.instance.decompose(0), false, Int32Type.instance.decompose(5), true);\n\n        expressions = convert(Operation.buildIndexExpressions(controller,\n                                                              Arrays.asList(new SimpleExpression(firstName, Operator.EQ, UTF8Type.instance.decompose(\"a\")),\n                                                                            new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(0)),\n                                                                            new SimpleExpression(height, Operator.LT, Int32Type.instance.decompose(10)))));\n\n        assertEquals(2, expressions.size());\n\n        rangeExpression = expressions.get(Expression.IndexOperator.RANGE);\n\n        assertExpression(rangeExpression, Expression.IndexOperator.RANGE, Int32Type.instance.decompose(0), true, Int32Type.instance.decompose(10), false);\n    }\n\n    @Test\n    public void testSatisfiedByWithClustering()\n    {\n        ColumnMetadata location = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"location\"));\n        ColumnMetadata age = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"age\"));\n        ColumnMetadata height = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"height\"));\n        ColumnMetadata score = getColumn(CLUSTERING_BACKEND, UTF8Type.instance.decompose(\"score\"));\n\n        DecoratedKey key = buildKey(CLUSTERING_BACKEND, \"0\");\n        Row row = buildRow(Clustering.make(UTF8Type.instance.fromString(\"US\"), Int32Type.instance.decompose(27)),\n                                  buildCell(height, instance.decompose(182), System.currentTimeMillis()),\n                                  buildCell(score, DoubleType.instance.decompose(1.0d), System.currentTimeMillis()));\n        Row staticRow = buildRow(Clustering.STATIC_CLUSTERING);\n\n        Operation.Node node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(27))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.EQ, Int32Type.instance.decompose(182))));\n\n        assertTrue(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.EQ, Int32Type.instance.decompose(28))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.EQ, Int32Type.instance.decompose(182))));\n\n        assertFalse(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"US\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GTE, Int32Type.instance.decompose(27))));\n\n        assertTrue(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"BY\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.GTE, Int32Type.instance.decompose(28))));\n\n        assertFalse(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"US\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(age, Operator.LTE, Int32Type.instance.decompose(27))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(182))));\n\n        assertTrue(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(location, Operator.EQ, UTF8Type.instance.decompose(\"US\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(182))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(score, Operator.EQ, DoubleType.instance.decompose(1.0d))));\n\n        assertTrue(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(height, Operator.GTE, Int32Type.instance.decompose(182))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(score, Operator.EQ, DoubleType.instance.decompose(1.0d))));\n\n        assertTrue(node.buildFilter(controllerClustering, true).isSatisfiedBy(key, row, staticRow));\n    }\n\n    private Map<Expression.IndexOperator, Expression> convert(Multimap<ColumnMetadata, Expression> expressions)\n    {\n        Map<Expression.IndexOperator, Expression> converted = new EnumMap<>(Expression.IndexOperator.class);\n        for (Expression expression : expressions.values())\n        {\n            Expression column = converted.get(expression.getIndexOperator());\n            assert column == null; // sanity check\n            converted.put(expression.getIndexOperator(), expression);\n        }\n\n        return converted;\n    }\n\n    @Test\n    public void testSatisfiedByWithStatic()\n    {\n        final ColumnMetadata sensorType = getColumn(STATIC_BACKEND, UTF8Type.instance.decompose(\"sensor_type\"));\n        final ColumnMetadata value = getColumn(STATIC_BACKEND, UTF8Type.instance.decompose(\"value\"));\n\n        DecoratedKey key = buildKey(STATIC_BACKEND, 0);\n        Row row = buildRow(Clustering.make(UTF8Type.instance.fromString(\"date\"), LongType.instance.decompose(20160401L)),\n                                  buildCell(value, DoubleType.instance.decompose(24.56), System.currentTimeMillis()));\n        Row staticRow = buildRow(Clustering.STATIC_CLUSTERING,\n                                 buildCell(sensorType, UTF8Type.instance.decompose(\"TEMPERATURE\"), System.currentTimeMillis()));\n\n        // sensor_type ='TEMPERATURE' AND value = 24.56\n        Operation.Node node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(sensorType, Operator.EQ, UTF8Type.instance.decompose(\"TEMPERATURE\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(value, Operator.EQ, DoubleType.instance.decompose(24.56))));\n\n        assertTrue(node.buildFilter(controllerStatic, true).isSatisfiedBy(key, row, staticRow));\n\n        // sensor_type ='TEMPERATURE' AND value = 30\n        node = new Operation.AndNode();\n        node.add(new Operation.ExpressionNode(new SimpleExpression(sensorType, Operator.EQ, UTF8Type.instance.decompose(\"TEMPERATURE\"))));\n        node.add(new Operation.ExpressionNode(new SimpleExpression(value, Operator.EQ, DoubleType.instance.decompose(30.00))));\n\n        assertFalse(node.buildFilter(controllerStatic, true).isSatisfiedBy(key, row, staticRow));\n    }\n\n    public static TableMetadata.Builder skinnySAITableMetadata(String keyspace, String table)\n    {\n        TableMetadata.Builder builder =\n        TableMetadata.builder(keyspace, table)\n                     .addPartitionKeyColumn(\"id\", UTF8Type.instance)\n                     .addRegularColumn(\"first_name\", UTF8Type.instance)\n                     .addRegularColumn(\"last_name\", UTF8Type.instance)\n                     .addRegularColumn(\"age\", Int32Type.instance)\n                     .addRegularColumn(\"height\", Int32Type.instance)\n                     .addRegularColumn(\"timestamp\", LongType.instance)\n                     .addRegularColumn(\"address\", UTF8Type.instance)\n                     .addRegularColumn(\"score\", DoubleType.instance);\n\n        Indexes.Builder indexes = Indexes.builder();\n        addIndex(indexes, table, \"first_name\");\n        addIndex(indexes, table, \"last_name\");\n        addIndex(indexes, table, \"age\");\n        addIndex(indexes, table, \"timestamp\");\n        addIndex(indexes, table, \"address\");\n        addIndex(indexes, table, \"score\");\n\n        return builder.indexes(indexes.build());\n    }\n\n    public static TableMetadata.Builder clusteringSAITableMetadata(String keyspace, String table)\n    {\n        return clusteringSAITableMetadata(keyspace, table, \"location\", \"age\", \"height\", \"score\");\n    }\n\n    public static TableMetadata.Builder clusteringSAITableMetadata(String keyspace, String table, String...indexedColumns)\n    {\n        Indexes.Builder indexes = Indexes.builder();\n        for (String indexedColumn : indexedColumns)\n        {\n            addIndex(indexes, table, indexedColumn);\n        }\n\n        return TableMetadata.builder(keyspace, table)\n                            .addPartitionKeyColumn(\"name\", UTF8Type.instance)\n                            .addClusteringColumn(\"location\", UTF8Type.instance)\n                            .addClusteringColumn(\"age\", Int32Type.instance)\n                            .addRegularColumn(\"height\", Int32Type.instance)\n                            .addRegularColumn(\"score\", DoubleType.instance)\n                            .indexes(indexes.build());\n    }\n\n    public static TableMetadata.Builder staticSAITableMetadata(String keyspace, String table)\n    {\n        TableMetadata.Builder builder =\n        TableMetadata.builder(keyspace, table)\n                     .addPartitionKeyColumn(\"sensor_id\", Int32Type.instance)\n                     .addStaticColumn(\"sensor_type\", UTF8Type.instance)\n                     .addClusteringColumn(\"date\", LongType.instance)\n                     .addRegularColumn(\"value\", DoubleType.instance)\n                     .addRegularColumn(\"variance\", Int32Type.instance);\n\n        Indexes.Builder indexes = Indexes.builder();\n\n        addIndex(indexes, table, \"sensor_type\");\n        addIndex(indexes, table, \"value\");\n        addIndex(indexes, table, \"variance\");\n\n        return builder.indexes(indexes.build());\n    }\n\n    private void assertExpression(Expression expression, Expression.IndexOperator indexOperator, ByteBuffer lower,\n                                  boolean lowerInclusive, ByteBuffer upper, boolean upperInclusive)\n    {\n        assertEquals(indexOperator, expression.getIndexOperator());\n        assertEquals(lower, expression.lower().value.raw);\n        assertEquals(lowerInclusive, expression.lower().inclusive);\n        assertEquals(upper, expression.upper().value.raw);\n        assertEquals(upperInclusive, expression.upper().inclusive);\n    }\n\n    private static void addIndex(Indexes.Builder indexes, String table, String column)\n    {\n        String indexName = table + '_' + column;\n        indexes.add(IndexMetadata.fromSchemaMetadata(indexName, IndexMetadata.Kind.CUSTOM, new HashMap<String, String>()\n        {{\n            put(IndexTarget.CUSTOM_INDEX_OPTION_NAME, StorageAttachedIndex.class.getName());\n            put(IndexTarget.TARGET_OPTION_NAME, column);\n        }}));\n    }\n\n    private static DecoratedKey buildKey(Object... key)\n    {\n        return buildKey(BACKEND, key);\n    }\n\n    private static DecoratedKey buildKey(ColumnFamilyStore cfs, Object... key)\n    {\n        AbstractType<?> type = cfs.metadata().partitionKeyType;\n        ByteBuffer decomposed;\n        if(type instanceof CompositeType)\n        {\n            Preconditions.checkArgument(key.length == type.subTypes().size());\n            decomposed = ((CompositeType) type).decompose(key);\n        }\n        else\n        {\n            Preconditions.checkArgument(key.length == 1);\n            decomposed = ((AbstractType) type).decompose(key[0]);\n        }\n        return Murmur3Partitioner.instance.decorateKey(decomposed);\n    }\n\n    private static Unfiltered buildRow(Cell<?>... cells)\n    {\n        return buildRow(Clustering.EMPTY, cells);\n    }\n\n    private static Row buildRow(Clustering<?> clustering, Cell<?>... cells)\n    {\n        Row.Builder rowBuilder = BTreeRow.sortedBuilder();\n        rowBuilder.newRow(clustering);\n        for (Cell<?> c : cells)\n            rowBuilder.addCell(c);\n\n        return rowBuilder.build();\n    }\n\n    private static Cell<?> buildCell(ColumnMetadata column, ByteBuffer value, long timestamp)\n    {\n        return BufferCell.live(column, timestamp, value);\n    }\n\n    private static ColumnMetadata getColumn(ByteBuffer name)\n    {\n        return getColumn(BACKEND, name);\n    }\n\n    private static ColumnMetadata getColumn(ColumnFamilyStore cfs, ByteBuffer name)\n    {\n        return cfs.metadata().getColumn(name);\n    }\n\n    private static class SimpleExpression extends RowFilter.Expression\n    {\n        SimpleExpression(ColumnMetadata column, Operator operator, ByteBuffer value)\n        {\n            super(column, operator, value);\n        }\n\n        @Override\n        public Kind kind()\n        {\n            return Kind.SIMPLE;\n        }\n\n        @Override\n        public boolean isSatisfiedBy(TableMetadata metadata, DecoratedKey partitionKey, Row row)\n        {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        protected String toString(boolean cql)\n        {\n            AbstractType<?> type = column.type;\n            switch (operator)\n            {\n                case CONTAINS:\n                    assert type instanceof CollectionType;\n                    CollectionType<?> ct = (CollectionType<?>)type;\n                    type = ct.kind == CollectionType.Kind.SET ? ct.nameComparator() : ct.valueComparator();\n                    break;\n                case CONTAINS_KEY:\n                    assert type instanceof MapType;\n                    type = ((MapType<?, ?>)type).nameComparator();\n                    break;\n                case IN:\n                    type = ListType.getInstance(type, false);\n                    break;\n                default:\n                    break;\n            }\n            return cql\n                   ? String.format(\"%s %s %s\", column.name.toCQLString(), operator, type.toCQLString(value) )\n                   : String.format(\"%s %s %s\", column.name.toString(), operator, type.getString(value));\n        }\n    }\n}\n","lineNo":210}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.locator;\n\nimport com.carrotsearch.hppc.ObjectIntHashMap;\nimport com.carrotsearch.hppc.cursors.ObjectIntCursor;\nimport com.carrotsearch.hppc.cursors.ObjectObjectCursor;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.ArrayListMultimap;\nimport com.google.common.collect.HashMultimap;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.ListMultimap;\nimport com.google.common.collect.Lists;\nimport com.google.common.collect.Multimap;\n\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ConsistencyLevel;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.PartitionPosition;\nimport org.apache.cassandra.dht.AbstractBounds;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.exceptions.UnavailableException;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.service.reads.AlwaysSpeculativeRetryPolicy;\nimport org.apache.cassandra.service.reads.SpeculativeRetryPolicy;\n\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.Predicate;\n\nimport static com.google.common.collect.Iterables.any;\nimport static com.google.common.collect.Iterables.filter;\nimport static org.apache.cassandra.db.ConsistencyLevel.EACH_QUORUM;\nimport static org.apache.cassandra.db.ConsistencyLevel.eachQuorumForRead;\nimport static org.apache.cassandra.db.ConsistencyLevel.eachQuorumForWrite;\nimport static org.apache.cassandra.db.ConsistencyLevel.localQuorumFor;\nimport static org.apache.cassandra.db.ConsistencyLevel.localQuorumForOurDc;\nimport static org.apache.cassandra.locator.Replicas.addToCountPerDc;\nimport static org.apache.cassandra.locator.Replicas.countInOurDc;\nimport static org.apache.cassandra.locator.Replicas.countPerDc;\n\npublic class ReplicaPlans\n{\n    private static final Logger logger = LoggerFactory.getLogger(ReplicaPlans.class);\n\n    public static boolean isSufficientLiveReplicasForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> liveReplicas)\n    {\n        switch (consistencyLevel)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                return true;\n            case LOCAL_ONE:\n                return countInOurDc(liveReplicas).hasAtleast(1, 1);\n            case LOCAL_QUORUM:\n                return countInOurDc(liveReplicas).hasAtleast(localQuorumForOurDc(replicationStrategy), 1);\n            case EACH_QUORUM:\n                if (replicationStrategy instanceof NetworkTopologyStrategy)\n                {\n                    int fullCount = 0;\n                    Collection<String> dcs = ((NetworkTopologyStrategy) replicationStrategy).getDatacenters();\n                    for (ObjectObjectCursor<String, Replicas.ReplicaCount> entry : countPerDc(dcs, liveReplicas))\n                    {\n                        Replicas.ReplicaCount count = entry.value;\n                        if (!count.hasAtleast(localQuorumFor(replicationStrategy, entry.key), 0))\n                            return false;\n                        fullCount += count.fullReplicas();\n                    }\n                    return fullCount > 0;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                return liveReplicas.size() >= consistencyLevel.blockFor(replicationStrategy)\n                        && Replicas.countFull(liveReplicas) > 0;\n        }\n    }\n\n    static void assureSufficientLiveReplicasForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> liveReplicas) throws UnavailableException\n    {\n        assureSufficientLiveReplicas(replicationStrategy, consistencyLevel, liveReplicas, consistencyLevel.blockFor(replicationStrategy), 1);\n    }\n    static void assureSufficientLiveReplicasForWrite(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> allLive, Endpoints<?> pendingWithDown) throws UnavailableException\n    {\n        assureSufficientLiveReplicas(replicationStrategy, consistencyLevel, allLive, consistencyLevel.blockForWrite(replicationStrategy, pendingWithDown), 0);\n    }\n    static void assureSufficientLiveReplicas(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> allLive, int blockFor, int blockForFullReplicas) throws UnavailableException\n    {\n        switch (consistencyLevel)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                break;\n            case LOCAL_ONE:\n            {\n                Replicas.ReplicaCount localLive = countInOurDc(allLive);\n                if (!localLive.hasAtleast(blockFor, blockForFullReplicas))\n                    throw UnavailableException.create(consistencyLevel, 1, blockForFullReplicas, localLive.allReplicas(), localLive.fullReplicas());\n                break;\n            }\n            case LOCAL_QUORUM:\n            {\n                Replicas.ReplicaCount localLive = countInOurDc(allLive);\n                if (!localLive.hasAtleast(blockFor, blockForFullReplicas))\n                {\n                    if (logger.isTraceEnabled())\n                    {\n                        logger.trace(String.format(\"Local replicas %s are insufficient to satisfy LOCAL_QUORUM requirement of %d live replicas and %d full replicas in '%s'\",\n                                allLive.filter(InOurDcTester.replicas()), blockFor, blockForFullReplicas, DatabaseDescriptor.getLocalDataCenter()));\n                    }\n                    throw UnavailableException.create(consistencyLevel, blockFor, blockForFullReplicas, localLive.allReplicas(), localLive.fullReplicas());\n                }\n                break;\n            }\n            case EACH_QUORUM:\n                if (replicationStrategy instanceof NetworkTopologyStrategy)\n                {\n                    int total = 0;\n                    int totalFull = 0;\n                    Collection<String> dcs = ((NetworkTopologyStrategy) replicationStrategy).getDatacenters();\n                    for (ObjectObjectCursor<String, Replicas.ReplicaCount> entry : countPerDc(dcs, allLive))\n                    {\n                        int dcBlockFor = localQuorumFor(replicationStrategy, entry.key);\n                        Replicas.ReplicaCount dcCount = entry.value;\n                        if (!dcCount.hasAtleast(dcBlockFor, 0))\n                            throw UnavailableException.create(consistencyLevel, entry.key, dcBlockFor, dcCount.allReplicas(), 0, dcCount.fullReplicas());\n                        totalFull += dcCount.fullReplicas();\n                        total += dcCount.allReplicas();\n                    }\n                    if (totalFull < blockForFullReplicas)\n                        throw UnavailableException.create(consistencyLevel, blockFor, total, blockForFullReplicas, totalFull);\n                    break;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                int live = allLive.size();\n                int full = Replicas.countFull(allLive);\n                if (live < blockFor || full < blockForFullReplicas)\n                {\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Live nodes {} do not satisfy ConsistencyLevel ({} required)\", Iterables.toString(allLive), blockFor);\n                    throw UnavailableException.create(consistencyLevel, blockFor, blockForFullReplicas, live, full);\n                }\n                break;\n        }\n    }\n\n    /**\n     * Construct a ReplicaPlan for writing to exactly one node, with CL.ONE. This node is *assumed* to be alive.\n     */\n    public static ReplicaPlan.ForTokenWrite forSingleReplicaWrite(Keyspace keyspace, Token token, Replica replica)\n    {\n        EndpointsForToken one = EndpointsForToken.of(token, replica);\n        EndpointsForToken empty = EndpointsForToken.empty(token);\n        return new ReplicaPlan.ForTokenWrite(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, empty, one, one, one);\n    }\n\n    /**\n     * A forwarding counter write is always sent to a single owning coordinator for the range, by the original coordinator\n     * (if it is not itself an owner)\n     */\n    public static ReplicaPlan.ForTokenWrite forForwardingCounterWrite(Keyspace keyspace, Token token, Replica replica)\n    {\n        return forSingleReplicaWrite(keyspace, token, replica);\n    }\n\n    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()\n    {\n        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();\n        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);\n        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());\n\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(\n                systemKeypsace.getReplicationStrategy(),\n                EndpointsForToken.of(token, localSystemReplica),\n                EndpointsForToken.empty(token)\n        );\n        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);\n    }\n\n    /**\n     * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.\n     * Note that the liveAndDown collection and live are equal to the provided endpoints.\n     *\n     * @param isAny if batch consistency level is ANY, in which case a local node will be picked\n     */\n    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(boolean isAny) throws UnavailableException\n    {\n        // A single case we write not for range or token, but multiple mutations to many tokens\n        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();\n\n        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();\n        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks()\n                                                                                          .get(snitch.getLocalDatacenter()));\n        // Replicas are picked manually:\n        //  - replicas should be alive according to the failure detector\n        //  - replicas should be in the local datacenter\n        //  - choose min(2, number of qualifying candiates above)\n        //  - allow the local node to be the only replica only if it's a single-node DC\n        Collection<InetAddressAndPort> chosenEndpoints = filterBatchlogEndpoints(snitch.getLocalRack(), localEndpoints);\n\n        if (chosenEndpoints.isEmpty() && isAny)\n            chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());\n\n        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(\n                systemKeypsace.getReplicationStrategy(),\n                SystemReplicas.getSystemReplicas(chosenEndpoints).forToken(token),\n                EndpointsForToken.empty(token)\n        );\n        // Batchlog is hosted by either one node or two nodes from different racks.\n        ConsistencyLevel consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;\n        // assume that we have already been given live endpoints, and skip applying the failure detector\n        return forWrite(systemKeypsace, consistencyLevel, liveAndDown, liveAndDown, writeAll);\n    }\n\n    private static Collection<InetAddressAndPort> filterBatchlogEndpoints(String localRack,\n                                                                          Multimap<String, InetAddressAndPort> endpoints)\n    {\n        return filterBatchlogEndpoints(localRack,\n                                       endpoints,\n                                       Collections::shuffle,\n                                       FailureDetector.isEndpointAlive,\n                                       ThreadLocalRandom.current()::nextInt);\n    }\n\n    // Collect a list of candidates for batchlog hosting. If possible these will be two nodes from different racks.\n    @VisibleForTesting\n    public static Collection<InetAddressAndPort> filterBatchlogEndpoints(String localRack,\n                                                                         Multimap<String, InetAddressAndPort> endpoints,\n                                                                         Consumer<List<?>> shuffle,\n                                                                         Predicate<InetAddressAndPort> isAlive,\n                                                                         Function<Integer, Integer> indexPicker)\n    {\n        // special case for single-node data centers\n        if (endpoints.values().size() == 1)\n            return endpoints.values();\n\n        // strip out dead endpoints and localhost\n        ListMultimap<String, InetAddressAndPort> validated = ArrayListMultimap.create();\n        for (Map.Entry<String, InetAddressAndPort> entry : endpoints.entries())\n        {\n            InetAddressAndPort addr = entry.getValue();\n            if (!addr.equals(FBUtilities.getBroadcastAddressAndPort()) && isAlive.test(addr))\n                validated.put(entry.getKey(), entry.getValue());\n        }\n\n        if (validated.size() <= 2)\n            return validated.values();\n\n        if (validated.size() - validated.get(localRack).size() >= 2)\n        {\n            // we have enough endpoints in other racks\n            validated.removeAll(localRack);\n        }\n\n        if (validated.keySet().size() == 1)\n        {\n            /*\n             * we have only 1 `other` rack to select replicas from (whether it be the local rack or a single non-local rack)\n             * pick two random nodes from there; we are guaranteed to have at least two nodes in the single remaining rack\n             * because of the preceding if block.\n             */\n            List<InetAddressAndPort> otherRack = Lists.newArrayList(validated.values());\n            shuffle.accept(otherRack);\n            return otherRack.subList(0, 2);\n        }\n\n        // randomize which racks we pick from if more than 2 remaining\n        Collection<String> racks;\n        if (validated.keySet().size() == 2)\n        {\n            racks = validated.keySet();\n        }\n        else\n        {\n            racks = Lists.newArrayList(validated.keySet());\n            shuffle.accept((List<?>) racks);\n        }\n\n        // grab a random member of up to two racks\n        List<InetAddressAndPort> result = new ArrayList<>(2);\n        for (String rack : Iterables.limit(racks, 2))\n        {\n            List<InetAddressAndPort> rackMembers = validated.get(rack);\n            result.add(rackMembers.get(indexPicker.apply(rackMembers.size())));\n        }\n\n        return result;\n    }\n\n    public static ReplicaPlan.ForTokenWrite forReadRepair(Token token, ReplicaPlan.ForRead<?> readPlan) throws UnavailableException\n    {\n        return forWrite(readPlan.keyspace, readPlan.consistencyLevel, token, writeReadRepair(readPlan));\n    }\n\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, Token token, Selector selector) throws UnavailableException\n    {\n        return forWrite(keyspace, consistencyLevel, ReplicaLayout.forTokenWriteLiveAndDown(keyspace, token), selector);\n    }\n\n    @VisibleForTesting\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForToken natural, EndpointsForToken pending, Predicate<Replica> isAlive, Selector selector) throws UnavailableException\n    {\n        return forWrite(keyspace, consistencyLevel, ReplicaLayout.forTokenWrite(keyspace.getReplicationStrategy(), natural, pending), isAlive, selector);\n    }\n\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, Selector selector) throws UnavailableException\n    {\n        return forWrite(keyspace, consistencyLevel, liveAndDown, FailureDetector.isReplicaAlive, selector);\n    }\n\n    private static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, Predicate<Replica> isAlive, Selector selector) throws UnavailableException\n    {\n        ReplicaLayout.ForTokenWrite live = liveAndDown.filter(isAlive);\n        return forWrite(keyspace, consistencyLevel, liveAndDown, live, selector);\n    }\n\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, ReplicaLayout.ForTokenWrite live, Selector selector) throws UnavailableException\n    {\n        assert liveAndDown.replicationStrategy() == live.replicationStrategy()\n               : \"ReplicaLayout liveAndDown and live should be derived from the same replication strategy.\";\n        AbstractReplicationStrategy replicationStrategy = liveAndDown.replicationStrategy();\n        EndpointsForToken contacts = selector.select(consistencyLevel, liveAndDown, live);\n        assureSufficientLiveReplicasForWrite(replicationStrategy, consistencyLevel, live.all(), liveAndDown.pending());\n        return new ReplicaPlan.ForTokenWrite(keyspace, replicationStrategy, consistencyLevel, liveAndDown.pending(), liveAndDown.all(), live.all(), contacts);\n    }\n\n    public interface Selector\n    {\n        /**\n         * Select the {@code Endpoints} from {@param liveAndDown} and {@param live} to contact according to the consistency level.\n         */\n        <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live);\n    }\n\n    /**\n     * Select all nodes, transient or otherwise, as targets for the operation.\n     *\n     * This is may no longer be useful once we finish implementing transient replication support, however\n     * it can be of value to stipulate that a location writes to all nodes without regard to transient status.\n     */\n    public static final Selector writeAll = new Selector()\n    {\n        @Override\n        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)\n        {\n            return liveAndDown.all();\n        }\n    };\n\n    /**\n     * Select all full nodes, live or down, as write targets.  If there are insufficient nodes to complete the write,\n     * but there are live transient nodes, select a sufficient number of these to reach our consistency level.\n     *\n     * Pending nodes are always contacted, whether or not they are full.  When a transient replica is undergoing\n     * a pending move to a new node, if we write (transiently) to it, this write would not be replicated to the\n     * pending transient node, and so when completing the move, the write could effectively have not reached the\n     * promised consistency level.\n     */\n    public static final Selector writeNormal = new Selector()\n    {\n        @Override\n        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)\n        {\n            if (!any(liveAndDown.all(), Replica::isTransient))\n                return liveAndDown.all();\n\n            ReplicaCollection.Builder<E> contacts = liveAndDown.all().newBuilder(liveAndDown.all().size());\n            contacts.addAll(filter(liveAndDown.natural(), Replica::isFull));\n            contacts.addAll(liveAndDown.pending());\n\n            /**\n             * Per CASSANDRA-14768, we ensure we write to at least a QUORUM of nodes in every DC,\n             * regardless of how many responses we need to wait for and our requested consistencyLevel.\n             * This is to minimally surprise users with transient replication; with normal writes, we\n             * soft-ensure that we reach QUORUM in all DCs we are able to, by writing to every node;\n             * even if we don't wait for ACK, we have in both cases sent sufficient messages.\n              */\n            ObjectIntHashMap<String> requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());\n            addToCountPerDc(requiredPerDc, live.natural().filter(Replica::isFull), -1);\n            addToCountPerDc(requiredPerDc, live.pending(), -1);\n\n            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n            for (Replica replica : filter(live.natural(), Replica::isTransient))\n            {\n                String dc = snitch.getDatacenter(replica);\n                if (requiredPerDc.addTo(dc, -1) >= 0)\n                    contacts.add(replica);\n            }\n            return contacts.build();\n        }\n    };\n\n    /**\n     * TODO: Transient Replication C-14404/C-14665\n     * TODO: We employ this even when there is no monotonicity to guarantee,\n     *          e.g. in case of CL.TWO, CL.ONE with speculation, etc.\n     *\n     * Construct a read-repair write plan to provide monotonicity guarantees on any data we return as part of a read.\n     *\n     * Since this is not a regular write, this is just to guarantee future reads will read this data, we select only\n     * the minimal number of nodes to meet the consistency level, and prefer nodes we contacted on read to minimise\n     * data transfer.\n     */\n    public static Selector writeReadRepair(ReplicaPlan.ForRead<?> readPlan)\n    {\n        return new Selector()\n        {\n            @Override\n            public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n            E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)\n            {\n                assert !any(liveAndDown.all(), Replica::isTransient);\n\n                ReplicaCollection.Builder<E> contacts = live.all().newBuilder(live.all().size());\n                // add all live nodes we might write to that we have already contacted on read\n                contacts.addAll(filter(live.all(), r -> readPlan.contacts().endpoints().contains(r.endpoint())));\n\n                // finally, add sufficient nodes to achieve our consistency level\n                if (consistencyLevel != EACH_QUORUM)\n                {\n                    int add = consistencyLevel.blockForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending()) - contacts.size();\n                    if (add > 0)\n                    {\n                        for (Replica replica : filter(live.all(), r -> !contacts.contains(r)))\n                        {\n                            contacts.add(replica);\n                            if (--add == 0)\n                                break;\n                        }\n                    }\n                }\n                else\n                {\n                    ObjectIntHashMap<String> requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());\n                    addToCountPerDc(requiredPerDc, contacts.snapshot(), -1);\n                    IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n                    for (Replica replica : filter(live.all(), r -> !contacts.contains(r)))\n                    {\n                        String dc = snitch.getDatacenter(replica);\n                        if (requiredPerDc.addTo(dc, -1) >= 0)\n                            contacts.add(replica);\n                    }\n                }\n                return contacts.build();\n            }\n        };\n    }\n\n    /**\n     * Construct the plan for a paxos round - NOT the write or read consistency level for either the write or comparison,\n     * but for the paxos linearisation agreement.\n     *\n     * This will select all live nodes as the candidates for the operation.  Only the required number of participants\n     */\n    public static ReplicaPlan.ForPaxosWrite forPaxos(Keyspace keyspace, DecoratedKey key, ConsistencyLevel consistencyForPaxos) throws UnavailableException\n    {\n        Token tk = key.getToken();\n\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWriteLiveAndDown(keyspace, tk);\n\n        Replicas.temporaryAssertFull(liveAndDown.all()); // TODO CASSANDRA-14547\n\n        if (consistencyForPaxos == ConsistencyLevel.LOCAL_SERIAL)\n        {\n            // TODO: we should cleanup our semantics here, as we're filtering ALL nodes to localDC which is unexpected for ReplicaPlan\n            // Restrict natural and pending to node in the local DC only\n            liveAndDown = liveAndDown.filter(InOurDcTester.replicas());\n        }\n\n        ReplicaLayout.ForTokenWrite live = liveAndDown.filter(FailureDetector.isReplicaAlive);\n\n        // TODO: this should use assureSufficientReplicas\n        int participants = liveAndDown.all().size();\n        int requiredParticipants = participants / 2 + 1; // See CASSANDRA-8346, CASSANDRA-833\n\n        EndpointsForToken contacts = live.all();\n        if (contacts.size() < requiredParticipants)\n            throw UnavailableException.create(consistencyForPaxos, requiredParticipants, contacts.size());\n\n        // We cannot allow CAS operations with 2 or more pending endpoints, see #8346.\n        // Note that we fake an impossible number of required nodes in the unavailable exception\n        // to nail home the point that it's an impossible operation no matter how many nodes are live.\n        if (liveAndDown.pending().size() > 1)\n            throw new UnavailableException(String.format(\"Cannot perform LWT operation as there is more than one (%d) pending range movement\", liveAndDown.all().size()),\n                    consistencyForPaxos,\n                    participants + 1,\n                    contacts.size());\n\n        return new ReplicaPlan.ForPaxosWrite(keyspace, consistencyForPaxos, liveAndDown.pending(), liveAndDown.all(), live.all(), contacts, requiredParticipants);\n    }\n\n\n    private static <E extends Endpoints<E>> E candidatesForRead(ConsistencyLevel consistencyLevel, E liveNaturalReplicas)\n    {\n        return consistencyLevel.isDatacenterLocal()\n                ? liveNaturalReplicas.filter(InOurDcTester.replicas())\n                : liveNaturalReplicas;\n    }\n\n    private static <E extends Endpoints<E>> E contactForEachQuorumRead(NetworkTopologyStrategy replicationStrategy, E candidates)\n    {\n        ObjectIntHashMap<String> perDc = eachQuorumForRead(replicationStrategy);\n\n        final IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n        return candidates.filter(replica -> {\n            String dc = snitch.getDatacenter(replica);\n            return perDc.addTo(dc, -1) >= 0;\n        });\n    }\n\n    private static <E extends Endpoints<E>> E contactForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, boolean alwaysSpeculate, E candidates)\n    {\n        /*\n         * If we are doing an each quorum query, we have to make sure that the endpoints we select\n         * provide a quorum for each data center. If we are not using a NetworkTopologyStrategy,\n         * we should fall through and grab a quorum in the replication strategy.\n         *\n         * We do not speculate for EACH_QUORUM.\n         *\n         * TODO: this is still very inconistently managed between {LOCAL,EACH}_QUORUM and other consistency levels - should address this in a follow-up\n         */\n        if (consistencyLevel == EACH_QUORUM && replicationStrategy instanceof NetworkTopologyStrategy)\n            return contactForEachQuorumRead((NetworkTopologyStrategy) replicationStrategy, candidates);\n\n        int count = consistencyLevel.blockFor(replicationStrategy) + (alwaysSpeculate ? 1 : 0);\n        return candidates.subList(0, Math.min(count, candidates.size()));\n    }\n\n\n    /**\n     * Construct a plan for reading from a single node - this permits no speculation or read-repair\n     */\n    public static ReplicaPlan.ForTokenRead forSingleReplicaRead(Keyspace keyspace, Token token, Replica replica)\n    {\n        EndpointsForToken one = EndpointsForToken.of(token, replica);\n        return new ReplicaPlan.ForTokenRead(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, one, one);\n    }\n\n    /**\n     * Construct a plan for reading from a single node - this permits no speculation or read-repair\n     */\n    public static ReplicaPlan.ForRangeRead forSingleReplicaRead(Keyspace keyspace, AbstractBounds<PartitionPosition> range, Replica replica, int vnodeCount)\n    {\n        // TODO: this is unsafe, as one.range() may be inconsistent with our supplied range; should refactor Range/AbstractBounds to single class\n        EndpointsForRange one = EndpointsForRange.of(replica);\n        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, range, one, one, vnodeCount);\n    }\n\n    /**\n     * Construct a plan for reading the provided token at the provided consistency level.  This translates to a collection of\n     *   - candidates who are: alive, replicate the token, and are sorted by their snitch scores\n     *   - contacts who are: the first blockFor + (retry == ALWAYS ? 1 : 0) candidates\n     *\n     * The candidate collection can be used for speculation, although at present\n     * it would break EACH_QUORUM to do so without further filtering\n     */\n    public static ReplicaPlan.ForTokenRead forRead(Keyspace keyspace, Token token, ConsistencyLevel consistencyLevel, SpeculativeRetryPolicy retry)\n    {\n        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();\n        EndpointsForToken candidates = candidatesForRead(consistencyLevel, ReplicaLayout.forTokenReadLiveSorted(replicationStrategy, token).natural());\n        EndpointsForToken contacts = contactForRead(replicationStrategy, consistencyLevel, retry.equals(AlwaysSpeculativeRetryPolicy.INSTANCE), candidates);\n\n        assureSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, contacts);\n        return new ReplicaPlan.ForTokenRead(keyspace, replicationStrategy, consistencyLevel, candidates, contacts);\n    }\n\n    /**\n     * Construct a plan for reading the provided range at the provided consistency level.  This translates to a collection of\n     *   - candidates who are: alive, replicate the range, and are sorted by their snitch scores\n     *   - contacts who are: the first blockFor candidates\n     *\n     * There is no speculation for range read queries at present, so we never 'always speculate' here, and a failed response fails the query.\n     */\n    public static ReplicaPlan.ForRangeRead forRangeRead(Keyspace keyspace, ConsistencyLevel consistencyLevel, AbstractBounds<PartitionPosition> range, int vnodeCount)\n    {\n        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();\n        EndpointsForRange candidates = candidatesForRead(consistencyLevel, ReplicaLayout.forRangeReadLiveSorted(replicationStrategy, range).natural());\n        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, candidates);\n\n        assureSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, contacts);\n        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, range, candidates, contacts, vnodeCount);\n    }\n\n    /**\n     * Take two range read plans for adjacent ranges, and check if it is OK (and worthwhile) to combine them into a single plan\n     */\n    public static ReplicaPlan.ForRangeRead maybeMerge(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaPlan.ForRangeRead left, ReplicaPlan.ForRangeRead right)\n    {\n        // TODO: should we be asserting that the ranges are adjacent?\n        AbstractBounds<PartitionPosition> newRange = left.range().withNewRight(right.range().right);\n        EndpointsForRange mergedCandidates = left.candidates().keep(right.candidates().endpoints());\n        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();\n\n        // Check if there are enough shared endpoints for the merge to be possible.\n        if (!isSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, mergedCandidates))\n            return null;\n\n        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, mergedCandidates);\n\n        // Estimate whether merging will be a win or not\n        if (!DatabaseDescriptor.getEndpointSnitch().isWorthMergingForRangeQuery(contacts, left.contacts(), right.contacts()))\n            return null;\n\n        // If we get there, merge this range and the next one\n        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, newRange, mergedCandidates, contacts, left.vnodeCount() + right.vnodeCount());\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.locator;\n\nimport com.carrotsearch.hppc.ObjectIntHashMap;\nimport com.carrotsearch.hppc.cursors.ObjectObjectCursor;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.ArrayListMultimap;\nimport com.google.common.collect.HashMultimap;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.ListMultimap;\nimport com.google.common.collect.Lists;\nimport com.google.common.collect.Multimap;\n\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ConsistencyLevel;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.PartitionPosition;\nimport org.apache.cassandra.dht.AbstractBounds;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.exceptions.UnavailableException;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.service.reads.AlwaysSpeculativeRetryPolicy;\nimport org.apache.cassandra.service.reads.SpeculativeRetryPolicy;\n\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport java.util.function.Predicate;\n\nimport static com.google.common.collect.Iterables.any;\nimport static com.google.common.collect.Iterables.filter;\nimport static org.apache.cassandra.db.ConsistencyLevel.EACH_QUORUM;\nimport static org.apache.cassandra.db.ConsistencyLevel.eachQuorumForRead;\nimport static org.apache.cassandra.db.ConsistencyLevel.eachQuorumForWrite;\nimport static org.apache.cassandra.db.ConsistencyLevel.localQuorumFor;\nimport static org.apache.cassandra.db.ConsistencyLevel.localQuorumForOurDc;\nimport static org.apache.cassandra.locator.Replicas.addToCountPerDc;\nimport static org.apache.cassandra.locator.Replicas.countInOurDc;\nimport static org.apache.cassandra.locator.Replicas.countPerDc;\n\npublic class ReplicaPlans\n{\n    private static final Logger logger = LoggerFactory.getLogger(ReplicaPlans.class);\n\n    public static boolean isSufficientLiveReplicasForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> liveReplicas)\n    {\n        switch (consistencyLevel)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                return true;\n            case LOCAL_ONE:\n                return countInOurDc(liveReplicas).hasAtleast(1, 1);\n            case LOCAL_QUORUM:\n                return countInOurDc(liveReplicas).hasAtleast(localQuorumForOurDc(replicationStrategy), 1);\n            case EACH_QUORUM:\n                if (replicationStrategy instanceof NetworkTopologyStrategy)\n                {\n                    int fullCount = 0;\n                    Collection<String> dcs = ((NetworkTopologyStrategy) replicationStrategy).getDatacenters();\n                    for (ObjectObjectCursor<String, Replicas.ReplicaCount> entry : countPerDc(dcs, liveReplicas))\n                    {\n                        Replicas.ReplicaCount count = entry.value;\n                        if (!count.hasAtleast(localQuorumFor(replicationStrategy, entry.key), 0))\n                            return false;\n                        fullCount += count.fullReplicas();\n                    }\n                    return fullCount > 0;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                return liveReplicas.size() >= consistencyLevel.blockFor(replicationStrategy)\n                        && Replicas.countFull(liveReplicas) > 0;\n        }\n    }\n\n    static void assureSufficientLiveReplicasForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> liveReplicas) throws UnavailableException\n    {\n        assureSufficientLiveReplicas(replicationStrategy, consistencyLevel, liveReplicas, consistencyLevel.blockFor(replicationStrategy), 1);\n    }\n    static void assureSufficientLiveReplicasForWrite(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> allLive, Endpoints<?> pendingWithDown) throws UnavailableException\n    {\n        assureSufficientLiveReplicas(replicationStrategy, consistencyLevel, allLive, consistencyLevel.blockForWrite(replicationStrategy, pendingWithDown), 0);\n    }\n    static void assureSufficientLiveReplicas(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints<?> allLive, int blockFor, int blockForFullReplicas) throws UnavailableException\n    {\n        switch (consistencyLevel)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                break;\n            case LOCAL_ONE:\n            {\n                Replicas.ReplicaCount localLive = countInOurDc(allLive);\n                if (!localLive.hasAtleast(blockFor, blockForFullReplicas))\n                    throw UnavailableException.create(consistencyLevel, 1, blockForFullReplicas, localLive.allReplicas(), localLive.fullReplicas());\n                break;\n            }\n            case LOCAL_QUORUM:\n            {\n                Replicas.ReplicaCount localLive = countInOurDc(allLive);\n                if (!localLive.hasAtleast(blockFor, blockForFullReplicas))\n                {\n                    if (logger.isTraceEnabled())\n                    {\n                        logger.trace(String.format(\"Local replicas %s are insufficient to satisfy LOCAL_QUORUM requirement of %d live replicas and %d full replicas in '%s'\",\n                                allLive.filter(InOurDcTester.replicas()), blockFor, blockForFullReplicas, DatabaseDescriptor.getLocalDataCenter()));\n                    }\n                    throw UnavailableException.create(consistencyLevel, blockFor, blockForFullReplicas, localLive.allReplicas(), localLive.fullReplicas());\n                }\n                break;\n            }\n            case EACH_QUORUM:\n                if (replicationStrategy instanceof NetworkTopologyStrategy)\n                {\n                    int total = 0;\n                    int totalFull = 0;\n                    Collection<String> dcs = ((NetworkTopologyStrategy) replicationStrategy).getDatacenters();\n                    for (ObjectObjectCursor<String, Replicas.ReplicaCount> entry : countPerDc(dcs, allLive))\n                    {\n                        int dcBlockFor = localQuorumFor(replicationStrategy, entry.key);\n                        Replicas.ReplicaCount dcCount = entry.value;\n                        if (!dcCount.hasAtleast(dcBlockFor, 0))\n                            throw UnavailableException.create(consistencyLevel, entry.key, dcBlockFor, dcCount.allReplicas(), 0, dcCount.fullReplicas());\n                        totalFull += dcCount.fullReplicas();\n                        total += dcCount.allReplicas();\n                    }\n                    if (totalFull < blockForFullReplicas)\n                        throw UnavailableException.create(consistencyLevel, blockFor, total, blockForFullReplicas, totalFull);\n                    break;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                int live = allLive.size();\n                int full = Replicas.countFull(allLive);\n                if (live < blockFor || full < blockForFullReplicas)\n                {\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Live nodes {} do not satisfy ConsistencyLevel ({} required)\", Iterables.toString(allLive), blockFor);\n                    throw UnavailableException.create(consistencyLevel, blockFor, blockForFullReplicas, live, full);\n                }\n                break;\n        }\n    }\n\n    /**\n     * Construct a ReplicaPlan for writing to exactly one node, with CL.ONE. This node is *assumed* to be alive.\n     */\n    public static ReplicaPlan.ForTokenWrite forSingleReplicaWrite(Keyspace keyspace, Token token, Replica replica)\n    {\n        EndpointsForToken one = EndpointsForToken.of(token, replica);\n        EndpointsForToken empty = EndpointsForToken.empty(token);\n        return new ReplicaPlan.ForTokenWrite(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, empty, one, one, one);\n    }\n\n    /**\n     * A forwarding counter write is always sent to a single owning coordinator for the range, by the original coordinator\n     * (if it is not itself an owner)\n     */\n    public static ReplicaPlan.ForTokenWrite forForwardingCounterWrite(Keyspace keyspace, Token token, Replica replica)\n    {\n        return forSingleReplicaWrite(keyspace, token, replica);\n    }\n\n    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()\n    {\n        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();\n        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);\n        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());\n\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(\n                systemKeypsace.getReplicationStrategy(),\n                EndpointsForToken.of(token, localSystemReplica),\n                EndpointsForToken.empty(token)\n        );\n        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);\n    }\n\n    /**\n     * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.\n     * Note that the liveAndDown collection and live are equal to the provided endpoints.\n     *\n     * @param isAny if batch consistency level is ANY, in which case a local node will be picked\n     */\n    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(boolean isAny) throws UnavailableException\n    {\n        // A single case we write not for range or token, but multiple mutations to many tokens\n        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();\n\n        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();\n        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks()\n                                                                                          .get(snitch.getLocalDatacenter()));\n        // Replicas are picked manually:\n        //  - replicas should be alive according to the failure detector\n        //  - replicas should be in the local datacenter\n        //  - choose min(2, number of qualifying candiates above)\n        //  - allow the local node to be the only replica only if it's a single-node DC\n        Collection<InetAddressAndPort> chosenEndpoints = filterBatchlogEndpoints(snitch.getLocalRack(), localEndpoints);\n\n        if (chosenEndpoints.isEmpty() && isAny)\n            chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());\n\n        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(\n                systemKeypsace.getReplicationStrategy(),\n                SystemReplicas.getSystemReplicas(chosenEndpoints).forToken(token),\n                EndpointsForToken.empty(token)\n        );\n        // Batchlog is hosted by either one node or two nodes from different racks.\n        ConsistencyLevel consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;\n        // assume that we have already been given live endpoints, and skip applying the failure detector\n        return forWrite(systemKeypsace, consistencyLevel, liveAndDown, liveAndDown, writeAll);\n    }\n\n    private static Collection<InetAddressAndPort> filterBatchlogEndpoints(String localRack,\n                                                                          Multimap<String, InetAddressAndPort> endpoints)\n    {\n        return filterBatchlogEndpoints(localRack,\n                                       endpoints,\n                                       Collections::shuffle,\n                                       FailureDetector.isEndpointAlive,\n                                       ThreadLocalRandom.current()::nextInt);\n    }\n\n    // Collect a list of candidates for batchlog hosting. If possible these will be two nodes from different racks.\n    @VisibleForTesting\n    public static Collection<InetAddressAndPort> filterBatchlogEndpoints(String localRack,\n                                                                         Multimap<String, InetAddressAndPort> endpoints,\n                                                                         Consumer<List<?>> shuffle,\n                                                                         Predicate<InetAddressAndPort> isAlive,\n                                                                         Function<Integer, Integer> indexPicker)\n    {\n        // special case for single-node data centers\n        if (endpoints.values().size() == 1)\n            return endpoints.values();\n\n        // strip out dead endpoints and localhost\n        ListMultimap<String, InetAddressAndPort> validated = ArrayListMultimap.create();\n        for (Map.Entry<String, InetAddressAndPort> entry : endpoints.entries())\n        {\n            InetAddressAndPort addr = entry.getValue();\n            if (!addr.equals(FBUtilities.getBroadcastAddressAndPort()) && isAlive.test(addr))\n                validated.put(entry.getKey(), entry.getValue());\n        }\n\n        if (validated.size() <= 2)\n            return validated.values();\n\n        if (validated.size() - validated.get(localRack).size() >= 2)\n        {\n            // we have enough endpoints in other racks\n            validated.removeAll(localRack);\n        }\n\n        if (validated.keySet().size() == 1)\n        {\n            /*\n             * we have only 1 `other` rack to select replicas from (whether it be the local rack or a single non-local rack)\n             * pick two random nodes from there; we are guaranteed to have at least two nodes in the single remaining rack\n             * because of the preceding if block.\n             */\n            List<InetAddressAndPort> otherRack = Lists.newArrayList(validated.values());\n            shuffle.accept(otherRack);\n            return otherRack.subList(0, 2);\n        }\n\n        // randomize which racks we pick from if more than 2 remaining\n        Collection<String> racks;\n        if (validated.keySet().size() == 2)\n        {\n            racks = validated.keySet();\n        }\n        else\n        {\n            racks = Lists.newArrayList(validated.keySet());\n            shuffle.accept((List<?>) racks);\n        }\n\n        // grab a random member of up to two racks\n        List<InetAddressAndPort> result = new ArrayList<>(2);\n        for (String rack : Iterables.limit(racks, 2))\n        {\n            List<InetAddressAndPort> rackMembers = validated.get(rack);\n            result.add(rackMembers.get(indexPicker.apply(rackMembers.size())));\n        }\n\n        return result;\n    }\n\n    public static ReplicaPlan.ForTokenWrite forReadRepair(Token token, ReplicaPlan.ForRead<?> readPlan) throws UnavailableException\n    {\n        return forWrite(readPlan.keyspace, readPlan.consistencyLevel, token, writeReadRepair(readPlan));\n    }\n\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, Token token, Selector selector) throws UnavailableException\n    {\n        return forWrite(keyspace, consistencyLevel, ReplicaLayout.forTokenWriteLiveAndDown(keyspace, token), selector);\n    }\n\n    @VisibleForTesting\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForToken natural, EndpointsForToken pending, Predicate<Replica> isAlive, Selector selector) throws UnavailableException\n    {\n        return forWrite(keyspace, consistencyLevel, ReplicaLayout.forTokenWrite(keyspace.getReplicationStrategy(), natural, pending), isAlive, selector);\n    }\n\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, Selector selector) throws UnavailableException\n    {\n        return forWrite(keyspace, consistencyLevel, liveAndDown, FailureDetector.isReplicaAlive, selector);\n    }\n\n    private static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, Predicate<Replica> isAlive, Selector selector) throws UnavailableException\n    {\n        ReplicaLayout.ForTokenWrite live = liveAndDown.filter(isAlive);\n        return forWrite(keyspace, consistencyLevel, liveAndDown, live, selector);\n    }\n\n    public static ReplicaPlan.ForTokenWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, ReplicaLayout.ForTokenWrite live, Selector selector) throws UnavailableException\n    {\n        assert liveAndDown.replicationStrategy() == live.replicationStrategy()\n               : \"ReplicaLayout liveAndDown and live should be derived from the same replication strategy.\";\n        AbstractReplicationStrategy replicationStrategy = liveAndDown.replicationStrategy();\n        EndpointsForToken contacts = selector.select(consistencyLevel, liveAndDown, live);\n        assureSufficientLiveReplicasForWrite(replicationStrategy, consistencyLevel, live.all(), liveAndDown.pending());\n        return new ReplicaPlan.ForTokenWrite(keyspace, replicationStrategy, consistencyLevel, liveAndDown.pending(), liveAndDown.all(), live.all(), contacts);\n    }\n\n    public interface Selector\n    {\n        /**\n         * Select the {@code Endpoints} from {@param liveAndDown} and {@param live} to contact according to the consistency level.\n         */\n        <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live);\n    }\n\n    /**\n     * Select all nodes, transient or otherwise, as targets for the operation.\n     *\n     * This is may no longer be useful once we finish implementing transient replication support, however\n     * it can be of value to stipulate that a location writes to all nodes without regard to transient status.\n     */\n    public static final Selector writeAll = new Selector()\n    {\n        @Override\n        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)\n        {\n            return liveAndDown.all();\n        }\n    };\n\n    /**\n     * Select all full nodes, live or down, as write targets.  If there are insufficient nodes to complete the write,\n     * but there are live transient nodes, select a sufficient number of these to reach our consistency level.\n     *\n     * Pending nodes are always contacted, whether or not they are full.  When a transient replica is undergoing\n     * a pending move to a new node, if we write (transiently) to it, this write would not be replicated to the\n     * pending transient node, and so when completing the move, the write could effectively have not reached the\n     * promised consistency level.\n     */\n    public static final Selector writeNormal = new Selector()\n    {\n        @Override\n        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)\n        {\n            if (!any(liveAndDown.all(), Replica::isTransient))\n                return liveAndDown.all();\n\n            ReplicaCollection.Builder<E> contacts = liveAndDown.all().newBuilder(liveAndDown.all().size());\n            contacts.addAll(filter(liveAndDown.natural(), Replica::isFull));\n            contacts.addAll(liveAndDown.pending());\n\n            /**\n             * Per CASSANDRA-14768, we ensure we write to at least a QUORUM of nodes in every DC,\n             * regardless of how many responses we need to wait for and our requested consistencyLevel.\n             * This is to minimally surprise users with transient replication; with normal writes, we\n             * soft-ensure that we reach QUORUM in all DCs we are able to, by writing to every node;\n             * even if we don't wait for ACK, we have in both cases sent sufficient messages.\n              */\n            ObjectIntHashMap<String> requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());\n            addToCountPerDc(requiredPerDc, live.natural().filter(Replica::isFull), -1);\n            addToCountPerDc(requiredPerDc, live.pending(), -1);\n\n            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n            for (Replica replica : filter(live.natural(), Replica::isTransient))\n            {\n                String dc = snitch.getDatacenter(replica);\n                if (requiredPerDc.addTo(dc, -1) >= 0)\n                    contacts.add(replica);\n            }\n            return contacts.build();\n        }\n    };\n\n    /**\n     * TODO: Transient Replication C-14404/C-14665\n     * TODO: We employ this even when there is no monotonicity to guarantee,\n     *          e.g. in case of CL.TWO, CL.ONE with speculation, etc.\n     *\n     * Construct a read-repair write plan to provide monotonicity guarantees on any data we return as part of a read.\n     *\n     * Since this is not a regular write, this is just to guarantee future reads will read this data, we select only\n     * the minimal number of nodes to meet the consistency level, and prefer nodes we contacted on read to minimise\n     * data transfer.\n     */\n    public static Selector writeReadRepair(ReplicaPlan.ForRead<?> readPlan)\n    {\n        return new Selector()\n        {\n\n            @Override\n            public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>\n            E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)\n            {\n                assert !any(liveAndDown.all(), Replica::isTransient);\n\n                ReplicaCollection.Builder<E> contacts = live.all().newBuilder(live.all().size());\n                // add all live nodes we might write to that we have already contacted on read\n                contacts.addAll(filter(live.all(), r -> readPlan.contacts().endpoints().contains(r.endpoint())));\n\n                // finally, add sufficient nodes to achieve our consistency level\n                if (consistencyLevel != EACH_QUORUM)\n                {\n                    int add = consistencyLevel.blockForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending()) - contacts.size();\n                    if (add > 0)\n                    {\n                        E all = consistencyLevel.isDatacenterLocal() ? live.all().filter(InOurDcTester.replicas()) : live.all();\n                        for (Replica replica : filter(all, r -> !contacts.contains(r)))\n                        {\n                            contacts.add(replica);\n                            if (--add == 0)\n                                break;\n                        }\n                    }\n                }\n                else\n                {\n                    ObjectIntHashMap<String> requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());\n                    addToCountPerDc(requiredPerDc, contacts.snapshot(), -1);\n                    IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n                    for (Replica replica : filter(live.all(), r -> !contacts.contains(r)))\n                    {\n                        String dc = snitch.getDatacenter(replica);\n                        if (requiredPerDc.addTo(dc, -1) >= 0)\n                            contacts.add(replica);\n                    }\n                }\n                return contacts.build();\n            }\n        };\n    }\n\n    /**\n     * Construct the plan for a paxos round - NOT the write or read consistency level for either the write or comparison,\n     * but for the paxos linearisation agreement.\n     *\n     * This will select all live nodes as the candidates for the operation.  Only the required number of participants\n     */\n    public static ReplicaPlan.ForPaxosWrite forPaxos(Keyspace keyspace, DecoratedKey key, ConsistencyLevel consistencyForPaxos) throws UnavailableException\n    {\n        Token tk = key.getToken();\n\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWriteLiveAndDown(keyspace, tk);\n\n        Replicas.temporaryAssertFull(liveAndDown.all()); // TODO CASSANDRA-14547\n\n        if (consistencyForPaxos == ConsistencyLevel.LOCAL_SERIAL)\n        {\n            // TODO: we should cleanup our semantics here, as we're filtering ALL nodes to localDC which is unexpected for ReplicaPlan\n            // Restrict natural and pending to node in the local DC only\n            liveAndDown = liveAndDown.filter(InOurDcTester.replicas());\n        }\n\n        ReplicaLayout.ForTokenWrite live = liveAndDown.filter(FailureDetector.isReplicaAlive);\n\n        // TODO: this should use assureSufficientReplicas\n        int participants = liveAndDown.all().size();\n        int requiredParticipants = participants / 2 + 1; // See CASSANDRA-8346, CASSANDRA-833\n\n        EndpointsForToken contacts = live.all();\n        if (contacts.size() < requiredParticipants)\n            throw UnavailableException.create(consistencyForPaxos, requiredParticipants, contacts.size());\n\n        // We cannot allow CAS operations with 2 or more pending endpoints, see #8346.\n        // Note that we fake an impossible number of required nodes in the unavailable exception\n        // to nail home the point that it's an impossible operation no matter how many nodes are live.\n        if (liveAndDown.pending().size() > 1)\n            throw new UnavailableException(String.format(\"Cannot perform LWT operation as there is more than one (%d) pending range movement\", liveAndDown.all().size()),\n                    consistencyForPaxos,\n                    participants + 1,\n                    contacts.size());\n\n        return new ReplicaPlan.ForPaxosWrite(keyspace, consistencyForPaxos, liveAndDown.pending(), liveAndDown.all(), live.all(), contacts, requiredParticipants);\n    }\n\n\n    private static <E extends Endpoints<E>> E candidatesForRead(ConsistencyLevel consistencyLevel, E liveNaturalReplicas)\n    {\n        return consistencyLevel.isDatacenterLocal()\n                ? liveNaturalReplicas.filter(InOurDcTester.replicas())\n                : liveNaturalReplicas;\n    }\n\n    private static <E extends Endpoints<E>> E contactForEachQuorumRead(NetworkTopologyStrategy replicationStrategy, E candidates)\n    {\n        ObjectIntHashMap<String> perDc = eachQuorumForRead(replicationStrategy);\n\n        final IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n        return candidates.filter(replica -> {\n            String dc = snitch.getDatacenter(replica);\n            return perDc.addTo(dc, -1) >= 0;\n        });\n    }\n\n    private static <E extends Endpoints<E>> E contactForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, boolean alwaysSpeculate, E candidates)\n    {\n        /*\n         * If we are doing an each quorum query, we have to make sure that the endpoints we select\n         * provide a quorum for each data center. If we are not using a NetworkTopologyStrategy,\n         * we should fall through and grab a quorum in the replication strategy.\n         *\n         * We do not speculate for EACH_QUORUM.\n         *\n         * TODO: this is still very inconistently managed between {LOCAL,EACH}_QUORUM and other consistency levels - should address this in a follow-up\n         */\n        if (consistencyLevel == EACH_QUORUM && replicationStrategy instanceof NetworkTopologyStrategy)\n            return contactForEachQuorumRead((NetworkTopologyStrategy) replicationStrategy, candidates);\n\n        int count = consistencyLevel.blockFor(replicationStrategy) + (alwaysSpeculate ? 1 : 0);\n        return candidates.subList(0, Math.min(count, candidates.size()));\n    }\n\n\n    /**\n     * Construct a plan for reading from a single node - this permits no speculation or read-repair\n     */\n    public static ReplicaPlan.ForTokenRead forSingleReplicaRead(Keyspace keyspace, Token token, Replica replica)\n    {\n        EndpointsForToken one = EndpointsForToken.of(token, replica);\n        return new ReplicaPlan.ForTokenRead(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, one, one);\n    }\n\n    /**\n     * Construct a plan for reading from a single node - this permits no speculation or read-repair\n     */\n    public static ReplicaPlan.ForRangeRead forSingleReplicaRead(Keyspace keyspace, AbstractBounds<PartitionPosition> range, Replica replica, int vnodeCount)\n    {\n        // TODO: this is unsafe, as one.range() may be inconsistent with our supplied range; should refactor Range/AbstractBounds to single class\n        EndpointsForRange one = EndpointsForRange.of(replica);\n        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, range, one, one, vnodeCount);\n    }\n\n    /**\n     * Construct a plan for reading the provided token at the provided consistency level.  This translates to a collection of\n     *   - candidates who are: alive, replicate the token, and are sorted by their snitch scores\n     *   - contacts who are: the first blockFor + (retry == ALWAYS ? 1 : 0) candidates\n     *\n     * The candidate collection can be used for speculation, although at present\n     * it would break EACH_QUORUM to do so without further filtering\n     */\n    public static ReplicaPlan.ForTokenRead forRead(Keyspace keyspace, Token token, ConsistencyLevel consistencyLevel, SpeculativeRetryPolicy retry)\n    {\n        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();\n        EndpointsForToken candidates = candidatesForRead(consistencyLevel, ReplicaLayout.forTokenReadLiveSorted(replicationStrategy, token).natural());\n        EndpointsForToken contacts = contactForRead(replicationStrategy, consistencyLevel, retry.equals(AlwaysSpeculativeRetryPolicy.INSTANCE), candidates);\n\n        assureSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, contacts);\n        return new ReplicaPlan.ForTokenRead(keyspace, replicationStrategy, consistencyLevel, candidates, contacts);\n    }\n\n    /**\n     * Construct a plan for reading the provided range at the provided consistency level.  This translates to a collection of\n     *   - candidates who are: alive, replicate the range, and are sorted by their snitch scores\n     *   - contacts who are: the first blockFor candidates\n     *\n     * There is no speculation for range read queries at present, so we never 'always speculate' here, and a failed response fails the query.\n     */\n    public static ReplicaPlan.ForRangeRead forRangeRead(Keyspace keyspace, ConsistencyLevel consistencyLevel, AbstractBounds<PartitionPosition> range, int vnodeCount)\n    {\n        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();\n        EndpointsForRange candidates = candidatesForRead(consistencyLevel, ReplicaLayout.forRangeReadLiveSorted(replicationStrategy, range).natural());\n        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, candidates);\n\n        assureSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, contacts);\n        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, range, candidates, contacts, vnodeCount);\n    }\n\n    /**\n     * Take two range read plans for adjacent ranges, and check if it is OK (and worthwhile) to combine them into a single plan\n     */\n    public static ReplicaPlan.ForRangeRead maybeMerge(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaPlan.ForRangeRead left, ReplicaPlan.ForRangeRead right)\n    {\n        // TODO: should we be asserting that the ranges are adjacent?\n        AbstractBounds<PartitionPosition> newRange = left.range().withNewRight(right.range().right);\n        EndpointsForRange mergedCandidates = left.candidates().keep(right.candidates().endpoints());\n        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();\n\n        // Check if there are enough shared endpoints for the merge to be possible.\n        if (!isSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, mergedCandidates))\n            return null;\n\n        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, mergedCandidates);\n\n        // Estimate whether merging will be a win or not\n        if (!DatabaseDescriptor.getEndpointSnitch().isWorthMergingForRangeQuery(contacts, left.contacts(), right.contacts()))\n            return null;\n\n        // If we get there, merge this range and the next one\n        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, newRange, mergedCandidates, contacts, left.vnodeCount() + right.vnodeCount());\n    }\n}\n","lineNo":458}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.service.reads.repair;\n\nimport java.nio.ByteBuffer;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Consumer;\n\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport com.google.common.primitives.Ints;\n\nimport org.apache.cassandra.dht.ByteOrderedPartitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.locator.EndpointsForToken;\nimport org.apache.cassandra.locator.ReplicaPlan;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.Ignore;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.ColumnIdentifier;\nimport org.apache.cassandra.cql3.statements.schema.CreateTableStatement;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.ConsistencyLevel;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.Mutation;\nimport org.apache.cassandra.db.ReadCommand;\nimport org.apache.cassandra.db.ReadResponse;\nimport org.apache.cassandra.db.partitions.PartitionIterator;\nimport org.apache.cassandra.db.partitions.PartitionUpdate;\nimport org.apache.cassandra.db.partitions.SingletonUnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterators;\nimport org.apache.cassandra.db.rows.BTreeRow;\nimport org.apache.cassandra.db.rows.BufferCell;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.db.rows.RowIterator;\nimport org.apache.cassandra.locator.EndpointsForRange;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.locator.ReplicaPlans;\nimport org.apache.cassandra.locator.ReplicaUtils;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.MigrationManager;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static org.apache.cassandra.locator.Replica.fullReplica;\nimport static org.apache.cassandra.locator.ReplicaUtils.FULL_RANGE;\nimport static org.apache.cassandra.net.Verb.INTERNAL_RSP;\n\n@Ignore\npublic abstract  class AbstractReadRepairTest\n{\n    static Keyspace ks;\n    static ColumnFamilyStore cfs;\n    static TableMetadata cfm;\n    static InetAddressAndPort target1;\n    static InetAddressAndPort target2;\n    static InetAddressAndPort target3;\n    static List<InetAddressAndPort> targets;\n\n    static Replica replica1;\n    static Replica replica2;\n    static Replica replica3;\n    static EndpointsForRange replicas;\n    static ReplicaPlan.ForRead<?> replicaPlan;\n\n    static long now = TimeUnit.NANOSECONDS.toMicros(System.nanoTime());\n    static DecoratedKey key;\n    static Cell<?> cell1;\n    static Cell<?> cell2;\n    static Cell<?> cell3;\n    static Mutation resolved;\n\n    static ReadCommand command;\n\n    static void assertRowsEqual(Row expected, Row actual)\n    {\n        try\n        {\n            Assert.assertEquals(expected == null, actual == null);\n            if (expected == null)\n                return;\n            Assert.assertEquals(expected.clustering(), actual.clustering());\n            Assert.assertEquals(expected.deletion(), actual.deletion());\n            Assert.assertArrayEquals(Iterables.toArray(expected.cells(), Cell.class), Iterables.toArray(expected.cells(), Cell.class));\n        } catch (Throwable t)\n        {\n            throw new AssertionError(String.format(\"Row comparison failed, expected %s got %s\", expected, actual), t);\n        }\n    }\n\n    static void assertRowsEqual(RowIterator expected, RowIterator actual)\n    {\n        assertRowsEqual(expected.staticRow(), actual.staticRow());\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n        assert !actual.hasNext();\n    }\n\n    static void assertPartitionsEqual(PartitionIterator expected, PartitionIterator actual)\n    {\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n\n        assert !actual.hasNext();\n    }\n\n    static void assertMutationEqual(Mutation expected, Mutation actual)\n    {\n        Assert.assertEquals(expected.getKeyspaceName(), actual.getKeyspaceName());\n        Assert.assertEquals(expected.key(), actual.key());\n        Assert.assertEquals(expected.key(), actual.key());\n        PartitionUpdate expectedUpdate = Iterables.getOnlyElement(expected.getPartitionUpdates());\n        PartitionUpdate actualUpdate = Iterables.getOnlyElement(actual.getPartitionUpdates());\n        assertRowsEqual(Iterables.getOnlyElement(expectedUpdate), Iterables.getOnlyElement(actualUpdate));\n    }\n\n    static DecoratedKey dk(int v)\n    {\n        return DatabaseDescriptor.getPartitioner().decorateKey(ByteBufferUtil.bytes(v));\n    }\n\n    static Cell<?> cell(String name, String value, long timestamp)\n    {\n        return BufferCell.live(cfm.getColumn(ColumnIdentifier.getInterned(name, false)), timestamp, ByteBufferUtil.bytes(value));\n    }\n\n    static PartitionUpdate update(Cell<?>... cells)\n    {\n        Row.Builder builder = BTreeRow.unsortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        for (Cell<?> cell: cells)\n        {\n            builder.addCell(cell);\n        }\n        return PartitionUpdate.singleRowUpdate(cfm, key, builder.build());\n    }\n\n    static PartitionIterator partition(Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return UnfilteredPartitionIterators.filter(iter, Ints.checkedCast(TimeUnit.MICROSECONDS.toSeconds(now)));\n    }\n\n    static Mutation mutation(Cell<?>... cells)\n    {\n        return new Mutation(update(cells));\n    }\n\n    @SuppressWarnings(\"resource\")\n    static Message<ReadResponse> msg(InetAddressAndPort from, Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return Message.builder(INTERNAL_RSP, ReadResponse.createDataResponse(iter, command, command.executionController().getRepairedDataInfo()))\n                      .from(from)\n                      .build();\n    }\n\n    static class ResultConsumer implements Consumer<PartitionIterator>\n    {\n\n        PartitionIterator result = null;\n\n        @Override\n        public void accept(PartitionIterator partitionIterator)\n        {\n            Assert.assertNotNull(partitionIterator);\n            result = partitionIterator;\n        }\n    }\n\n    private static boolean configured = false;\n\n    static void configureClass(ReadRepairStrategy repairStrategy) throws Throwable\n    {\n        SchemaLoader.loadSchema();\n        String ksName = \"ks\";\n\n        String ddl = String.format(\"CREATE TABLE tbl (k int primary key, v text) WITH read_repair='%s'\",\n                                   repairStrategy.toString().toLowerCase());\n\n        cfm = CreateTableStatement.parse(ddl, ksName).build();\n        assert cfm.params.readRepair == repairStrategy;\n        KeyspaceMetadata ksm = KeyspaceMetadata.create(ksName, KeyspaceParams.simple(3), Tables.of(cfm));\n        MigrationManager.announceNewKeyspace(ksm, false);\n\n        ks = Keyspace.open(ksName);\n        cfs = ks.getColumnFamilyStore(\"tbl\");\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n\n        target1 = InetAddressAndPort.getByName(\"127.0.0.255\");\n        target2 = InetAddressAndPort.getByName(\"127.0.0.254\");\n        target3 = InetAddressAndPort.getByName(\"127.0.0.253\");\n\n        targets = ImmutableList.of(target1, target2, target3);\n\n        replica1 = fullReplica(target1, FULL_RANGE);\n        replica2 = fullReplica(target2, FULL_RANGE);\n        replica3 = fullReplica(target3, FULL_RANGE);\n        replicas = EndpointsForRange.of(replica1, replica2, replica3);\n\n        replicaPlan = replicaPlan(ConsistencyLevel.QUORUM, replicas);\n\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 0 })), replica1.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 1 })), replica2.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 2 })), replica3.endpoint());\n        Gossiper.instance.initializeNodeUnsafe(replica1.endpoint(), UUID.randomUUID(), 1);\n        Gossiper.instance.initializeNodeUnsafe(replica2.endpoint(), UUID.randomUUID(), 1);\n        Gossiper.instance.initializeNodeUnsafe(replica3.endpoint(), UUID.randomUUID(), 1);\n\n        // default test values\n        key  = dk(5);\n        cell1 = cell(\"v\", \"val1\", now);\n        cell2 = cell(\"v\", \"val2\", now);\n        cell3 = cell(\"v\", \"val3\", now);\n        resolved = mutation(cell1, cell2);\n\n        command = Util.cmd(cfs, 1).build();\n\n        configured = true;\n    }\n\n    static Set<InetAddressAndPort> epSet(InetAddressAndPort... eps)\n    {\n        return Sets.newHashSet(eps);\n    }\n\n    @Before\n    public void setUp()\n    {\n        assert configured : \"configureClass must be called in a @BeforeClass method\";\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n    }\n\n    static ReplicaPlan.ForRangeRead replicaPlan(ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(ks, consistencyLevel, replicas, replicas);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan)\n    {\n        return repairPlan(readPlan, readPlan.candidates());\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(EndpointsForRange liveAndDown, EndpointsForRange targets)\n    {\n        return repairPlan(replicaPlan(liveAndDown, targets), liveAndDown);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan, EndpointsForRange liveAndDown)\n    {\n        Token token = readPlan.range().left.getToken();\n        EndpointsForToken pending = EndpointsForToken.empty(token);\n        return ReplicaPlans.forWrite(readPlan.keyspace(),\n                                     ConsistencyLevel.TWO,\n                                     liveAndDown.forToken(token),\n                                     pending,\n                                     replica -> true,\n                                     ReplicaPlans.writeReadRepair(readPlan));\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return replicaPlan(ks, ConsistencyLevel.QUORUM, replicas, targets);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(keyspace, consistencyLevel, replicas, replicas);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), consistencyLevel, ReplicaUtils.FULL_BOUNDS, replicas, targets, 1);\n    }\n\n    public abstract InstrumentedReadRepair createInstrumentedReadRepair(ReadCommand command, ReplicaPlan.Shared<?, ?> replicaPlan, long queryStartNanoTime);\n\n    public InstrumentedReadRepair createInstrumentedReadRepair(ReplicaPlan.Shared<?, ?> replicaPlan)\n    {\n        return createInstrumentedReadRepair(command, replicaPlan, System.nanoTime());\n\n    }\n\n    /**\n     * If we haven't received enough full data responses by the time the speculation\n     * timeout occurs, we should send read requests to additional replicas\n     */\n    @Test\n    public void readSpeculationCycle()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2, target3), repair.getReadRecipients());\n        Assert.assertNull(consumer.result);\n    }\n\n    /**\n     * If we receive enough data responses by the before the speculation timeout\n     * passes, we shouldn't send additional read requests\n     */\n    @Test\n    public void noSpeculationRequired()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.getReadCallback().onResponse(msg(target1, cell1));\n        repair.getReadCallback().onResponse(msg(target2, cell1));\n\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n\n        repair.awaitReads();\n\n        assertPartitionsEqual(partition(cell1), consumer.result);\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.service.reads.repair;\n\nimport java.nio.ByteBuffer;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Consumer;\n\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport com.google.common.primitives.Ints;\n\nimport org.apache.cassandra.dht.ByteOrderedPartitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.locator.AbstractNetworkTopologySnitch;\nimport org.apache.cassandra.locator.EndpointsForToken;\nimport org.apache.cassandra.locator.ReplicaPlan;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.Ignore;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.ColumnIdentifier;\nimport org.apache.cassandra.cql3.statements.schema.CreateTableStatement;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.ConsistencyLevel;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.Mutation;\nimport org.apache.cassandra.db.ReadCommand;\nimport org.apache.cassandra.db.ReadResponse;\nimport org.apache.cassandra.db.partitions.PartitionIterator;\nimport org.apache.cassandra.db.partitions.PartitionUpdate;\nimport org.apache.cassandra.db.partitions.SingletonUnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterators;\nimport org.apache.cassandra.db.rows.BTreeRow;\nimport org.apache.cassandra.db.rows.BufferCell;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.db.rows.RowIterator;\nimport org.apache.cassandra.locator.EndpointsForRange;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.locator.ReplicaPlans;\nimport org.apache.cassandra.locator.ReplicaUtils;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.MigrationManager;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static org.apache.cassandra.locator.Replica.fullReplica;\nimport static org.apache.cassandra.locator.ReplicaUtils.FULL_RANGE;\nimport static org.apache.cassandra.net.Verb.INTERNAL_RSP;\n\n@Ignore\npublic abstract  class AbstractReadRepairTest\n{\n    static Keyspace ks;\n    static ColumnFamilyStore cfs;\n    static TableMetadata cfm;\n    static InetAddressAndPort target1;\n    static InetAddressAndPort target2;\n    static InetAddressAndPort target3;\n    static InetAddressAndPort remote1;\n    static InetAddressAndPort remote2;\n    static InetAddressAndPort remote3;\n    static List<InetAddressAndPort> targets;\n    static List<InetAddressAndPort> remotes;\n\n    static Replica replica1;\n    static Replica replica2;\n    static Replica replica3;\n    static EndpointsForRange replicas;\n    static Replica remoteReplica1;\n    static Replica remoteReplica2;\n    static Replica remoteReplica3;\n    static EndpointsForRange remoteReplicas;\n\n    static long now = TimeUnit.NANOSECONDS.toMicros(System.nanoTime());\n    static DecoratedKey key;\n    static Cell<?> cell1;\n    static Cell<?> cell2;\n    static Cell<?> cell3;\n    static Mutation resolved;\n\n    static ReadCommand command;\n\n    static void assertRowsEqual(Row expected, Row actual)\n    {\n        try\n        {\n            Assert.assertEquals(expected == null, actual == null);\n            if (expected == null)\n                return;\n            Assert.assertEquals(expected.clustering(), actual.clustering());\n            Assert.assertEquals(expected.deletion(), actual.deletion());\n            Assert.assertArrayEquals(Iterables.toArray(expected.cells(), Cell.class), Iterables.toArray(expected.cells(), Cell.class));\n        } catch (Throwable t)\n        {\n            throw new AssertionError(String.format(\"Row comparison failed, expected %s got %s\", expected, actual), t);\n        }\n    }\n\n    static void assertRowsEqual(RowIterator expected, RowIterator actual)\n    {\n        assertRowsEqual(expected.staticRow(), actual.staticRow());\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n        assert !actual.hasNext();\n    }\n\n    static void assertPartitionsEqual(PartitionIterator expected, PartitionIterator actual)\n    {\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n\n        assert !actual.hasNext();\n    }\n\n    static void assertMutationEqual(Mutation expected, Mutation actual)\n    {\n        Assert.assertEquals(expected.getKeyspaceName(), actual.getKeyspaceName());\n        Assert.assertEquals(expected.key(), actual.key());\n        Assert.assertEquals(expected.key(), actual.key());\n        PartitionUpdate expectedUpdate = Iterables.getOnlyElement(expected.getPartitionUpdates());\n        PartitionUpdate actualUpdate = Iterables.getOnlyElement(actual.getPartitionUpdates());\n        assertRowsEqual(Iterables.getOnlyElement(expectedUpdate), Iterables.getOnlyElement(actualUpdate));\n    }\n\n    static DecoratedKey dk(int v)\n    {\n        return DatabaseDescriptor.getPartitioner().decorateKey(ByteBufferUtil.bytes(v));\n    }\n\n    static Cell<?> cell(String name, String value, long timestamp)\n    {\n        return BufferCell.live(cfm.getColumn(ColumnIdentifier.getInterned(name, false)), timestamp, ByteBufferUtil.bytes(value));\n    }\n\n    static PartitionUpdate update(Cell<?>... cells)\n    {\n        Row.Builder builder = BTreeRow.unsortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        for (Cell<?> cell: cells)\n        {\n            builder.addCell(cell);\n        }\n        return PartitionUpdate.singleRowUpdate(cfm, key, builder.build());\n    }\n\n    static PartitionIterator partition(Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return UnfilteredPartitionIterators.filter(iter, Ints.checkedCast(TimeUnit.MICROSECONDS.toSeconds(now)));\n    }\n\n    static Mutation mutation(Cell<?>... cells)\n    {\n        return new Mutation(update(cells));\n    }\n\n    @SuppressWarnings(\"resource\")\n    static Message<ReadResponse> msg(InetAddressAndPort from, Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return Message.builder(INTERNAL_RSP, ReadResponse.createDataResponse(iter, command, command.executionController().getRepairedDataInfo()))\n                      .from(from)\n                      .build();\n    }\n\n    static class ResultConsumer implements Consumer<PartitionIterator>\n    {\n\n        PartitionIterator result = null;\n\n        @Override\n        public void accept(PartitionIterator partitionIterator)\n        {\n            Assert.assertNotNull(partitionIterator);\n            result = partitionIterator;\n        }\n    }\n\n    private static boolean configured = false;\n\n    static void configureClass(ReadRepairStrategy repairStrategy) throws Throwable\n    {\n        SchemaLoader.loadSchema();\n\n        DatabaseDescriptor.setEndpointSnitch(new AbstractNetworkTopologySnitch()\n        {\n            public String getRack(InetAddressAndPort endpoint)\n            {\n                return \"rack1\";\n            }\n\n            public String getDatacenter(InetAddressAndPort endpoint)\n            {\n                byte[] address = endpoint.addressBytes;\n                if (address[1] == 2) {\n                    return \"datacenter2\";\n                }\n                return \"datacenter1\";\n            }\n        });\n\n        target1 = InetAddressAndPort.getByName(\"127.1.0.255\");\n        target2 = InetAddressAndPort.getByName(\"127.1.0.254\");\n        target3 = InetAddressAndPort.getByName(\"127.1.0.253\");\n\n        remote1 = InetAddressAndPort.getByName(\"127.2.0.255\");\n        remote2 = InetAddressAndPort.getByName(\"127.2.0.254\");\n        remote3 = InetAddressAndPort.getByName(\"127.2.0.253\");\n\n        targets = ImmutableList.of(target1, target2, target3);\n        remotes = ImmutableList.of(remote1, remote2, remote3);\n\n        replica1 = fullReplica(target1, FULL_RANGE);\n        replica2 = fullReplica(target2, FULL_RANGE);\n        replica3 = fullReplica(target3, FULL_RANGE);\n        replicas = EndpointsForRange.of(replica1, replica2, replica3);\n\n        remoteReplica1 = fullReplica(remote1, FULL_RANGE);\n        remoteReplica2 = fullReplica(remote2, FULL_RANGE);\n        remoteReplica3 = fullReplica(remote3, FULL_RANGE);\n        remoteReplicas = EndpointsForRange.of(remoteReplica1, remoteReplica2, remoteReplica3);\n\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 0 })), replica1.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 1 })), replica2.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 2 })), replica3.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 3 })), remoteReplica1.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 4 })), remoteReplica2.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 5 })), remoteReplica3.endpoint());\n\n        for (Replica replica : replicas)\n        {\n            UUID hostId = UUID.randomUUID();\n            Gossiper.instance.initializeNodeUnsafe(replica.endpoint(), hostId, 1);\n            StorageService.instance.getTokenMetadata().updateHostId(hostId, replica.endpoint());\n        }\n        for (Replica replica : remoteReplicas)\n        {\n            UUID hostId = UUID.randomUUID();\n            Gossiper.instance.initializeNodeUnsafe(replica.endpoint(), hostId, 1);\n            StorageService.instance.getTokenMetadata().updateHostId(hostId, replica.endpoint());\n        }\n\n        String ksName = \"ks\";\n\n        String ddl = String.format(\"CREATE TABLE tbl (k int primary key, v text) WITH read_repair='%s'\",\n                                   repairStrategy.toString().toLowerCase());\n\n        cfm = CreateTableStatement.parse(ddl, ksName).build();\n        assert cfm.params.readRepair == repairStrategy;\n        KeyspaceMetadata ksm = KeyspaceMetadata.create(ksName, KeyspaceParams.nts(\"datacenter1\", 3, \"datacenter2\", 3), Tables.of(cfm));\n        MigrationManager.announceNewKeyspace(ksm, true);\n\n        ks = Keyspace.open(ksName);\n        cfs = ks.getColumnFamilyStore(\"tbl\");\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n\n        // default test values\n        key  = dk(5);\n        cell1 = cell(\"v\", \"val1\", now);\n        cell2 = cell(\"v\", \"val2\", now);\n        cell3 = cell(\"v\", \"val3\", now);\n        resolved = mutation(cell1, cell2);\n\n        command = Util.cmd(cfs, 1).build();\n\n        configured = true;\n    }\n\n    static Set<InetAddressAndPort> epSet(InetAddressAndPort... eps)\n    {\n        return Sets.newHashSet(eps);\n    }\n\n    @Before\n    public void setUp()\n    {\n        assert configured : \"configureClass must be called in a @BeforeClass method\";\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n    }\n\n    static ReplicaPlan.ForRangeRead replicaPlan(ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(ks, consistencyLevel, replicas, replicas);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan)\n    {\n        return repairPlan(readPlan, readPlan.candidates());\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(EndpointsForRange liveAndDown, EndpointsForRange targets)\n    {\n        return repairPlan(replicaPlan(liveAndDown, targets), liveAndDown);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan, EndpointsForRange liveAndDown, EndpointsForToken pending)\n    {\n        Token token = readPlan.range().left.getToken();\n        return ReplicaPlans.forWrite(readPlan.keyspace(),\n                                     readPlan.consistencyLevel(),\n                                     liveAndDown.forToken(token),\n                                     pending,\n                                     replica -> true,\n                                     ReplicaPlans.writeReadRepair(readPlan));\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan, EndpointsForRange liveAndDown)\n    {\n        Token token = readPlan.range().left.getToken();\n        EndpointsForToken pending = EndpointsForToken.empty(token);\n        return ReplicaPlans.forWrite(readPlan.keyspace(),\n                                     readPlan.consistencyLevel(),\n                                     liveAndDown.forToken(token),\n                                     pending,\n                                     replica -> true,\n                                     ReplicaPlans.writeReadRepair(readPlan));\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return replicaPlan(ks, ConsistencyLevel.LOCAL_QUORUM, replicas, targets);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(keyspace, consistencyLevel, replicas, replicas);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), consistencyLevel, ReplicaUtils.FULL_BOUNDS, replicas, targets, 1);\n    }\n\n    public abstract InstrumentedReadRepair createInstrumentedReadRepair(ReadCommand command, ReplicaPlan.Shared<?, ?> replicaPlan, long queryStartNanoTime);\n\n    public InstrumentedReadRepair createInstrumentedReadRepair(ReplicaPlan.Shared<?, ?> replicaPlan)\n    {\n        return createInstrumentedReadRepair(command, replicaPlan, System.nanoTime());\n\n    }\n\n    /**\n     * If we haven't received enough full data responses by the time the speculation\n     * timeout occurs, we should send read requests to additional replicas\n     */\n    @Test\n    public void readSpeculationCycle()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2, target3), repair.getReadRecipients());\n        Assert.assertNull(consumer.result);\n    }\n\n    /**\n     * If we receive enough data responses by the before the speculation timeout\n     * passes, we shouldn't send additional read requests\n     */\n    @Test\n    public void noSpeculationRequired()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.getReadCallback().onResponse(msg(target1, cell1));\n        repair.getReadCallback().onResponse(msg(target2, cell1));\n\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n\n        repair.awaitReads();\n\n        assertPartitionsEqual(partition(cell1), consumer.result);\n    }\n}\n","lineNo":274}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.service.reads.repair;\n\nimport java.nio.ByteBuffer;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Consumer;\n\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport com.google.common.primitives.Ints;\n\nimport org.apache.cassandra.dht.ByteOrderedPartitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.locator.EndpointsForToken;\nimport org.apache.cassandra.locator.ReplicaPlan;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.Ignore;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.ColumnIdentifier;\nimport org.apache.cassandra.cql3.statements.schema.CreateTableStatement;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.ConsistencyLevel;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.Mutation;\nimport org.apache.cassandra.db.ReadCommand;\nimport org.apache.cassandra.db.ReadResponse;\nimport org.apache.cassandra.db.partitions.PartitionIterator;\nimport org.apache.cassandra.db.partitions.PartitionUpdate;\nimport org.apache.cassandra.db.partitions.SingletonUnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterators;\nimport org.apache.cassandra.db.rows.BTreeRow;\nimport org.apache.cassandra.db.rows.BufferCell;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.db.rows.RowIterator;\nimport org.apache.cassandra.locator.EndpointsForRange;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.locator.ReplicaPlans;\nimport org.apache.cassandra.locator.ReplicaUtils;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.MigrationManager;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static org.apache.cassandra.locator.Replica.fullReplica;\nimport static org.apache.cassandra.locator.ReplicaUtils.FULL_RANGE;\nimport static org.apache.cassandra.net.Verb.INTERNAL_RSP;\n\n@Ignore\npublic abstract  class AbstractReadRepairTest\n{\n    static Keyspace ks;\n    static ColumnFamilyStore cfs;\n    static TableMetadata cfm;\n    static InetAddressAndPort target1;\n    static InetAddressAndPort target2;\n    static InetAddressAndPort target3;\n    static List<InetAddressAndPort> targets;\n\n    static Replica replica1;\n    static Replica replica2;\n    static Replica replica3;\n    static EndpointsForRange replicas;\n    static ReplicaPlan.ForRead<?> replicaPlan;\n\n    static long now = TimeUnit.NANOSECONDS.toMicros(System.nanoTime());\n    static DecoratedKey key;\n    static Cell<?> cell1;\n    static Cell<?> cell2;\n    static Cell<?> cell3;\n    static Mutation resolved;\n\n    static ReadCommand command;\n\n    static void assertRowsEqual(Row expected, Row actual)\n    {\n        try\n        {\n            Assert.assertEquals(expected == null, actual == null);\n            if (expected == null)\n                return;\n            Assert.assertEquals(expected.clustering(), actual.clustering());\n            Assert.assertEquals(expected.deletion(), actual.deletion());\n            Assert.assertArrayEquals(Iterables.toArray(expected.cells(), Cell.class), Iterables.toArray(expected.cells(), Cell.class));\n        } catch (Throwable t)\n        {\n            throw new AssertionError(String.format(\"Row comparison failed, expected %s got %s\", expected, actual), t);\n        }\n    }\n\n    static void assertRowsEqual(RowIterator expected, RowIterator actual)\n    {\n        assertRowsEqual(expected.staticRow(), actual.staticRow());\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n        assert !actual.hasNext();\n    }\n\n    static void assertPartitionsEqual(PartitionIterator expected, PartitionIterator actual)\n    {\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n\n        assert !actual.hasNext();\n    }\n\n    static void assertMutationEqual(Mutation expected, Mutation actual)\n    {\n        Assert.assertEquals(expected.getKeyspaceName(), actual.getKeyspaceName());\n        Assert.assertEquals(expected.key(), actual.key());\n        Assert.assertEquals(expected.key(), actual.key());\n        PartitionUpdate expectedUpdate = Iterables.getOnlyElement(expected.getPartitionUpdates());\n        PartitionUpdate actualUpdate = Iterables.getOnlyElement(actual.getPartitionUpdates());\n        assertRowsEqual(Iterables.getOnlyElement(expectedUpdate), Iterables.getOnlyElement(actualUpdate));\n    }\n\n    static DecoratedKey dk(int v)\n    {\n        return DatabaseDescriptor.getPartitioner().decorateKey(ByteBufferUtil.bytes(v));\n    }\n\n    static Cell<?> cell(String name, String value, long timestamp)\n    {\n        return BufferCell.live(cfm.getColumn(ColumnIdentifier.getInterned(name, false)), timestamp, ByteBufferUtil.bytes(value));\n    }\n\n    static PartitionUpdate update(Cell<?>... cells)\n    {\n        Row.Builder builder = BTreeRow.unsortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        for (Cell<?> cell: cells)\n        {\n            builder.addCell(cell);\n        }\n        return PartitionUpdate.singleRowUpdate(cfm, key, builder.build());\n    }\n\n    static PartitionIterator partition(Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return UnfilteredPartitionIterators.filter(iter, Ints.checkedCast(TimeUnit.MICROSECONDS.toSeconds(now)));\n    }\n\n    static Mutation mutation(Cell<?>... cells)\n    {\n        return new Mutation(update(cells));\n    }\n\n    @SuppressWarnings(\"resource\")\n    static Message<ReadResponse> msg(InetAddressAndPort from, Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return Message.builder(INTERNAL_RSP, ReadResponse.createDataResponse(iter, command, command.executionController().getRepairedDataInfo()))\n                      .from(from)\n                      .build();\n    }\n\n    static class ResultConsumer implements Consumer<PartitionIterator>\n    {\n\n        PartitionIterator result = null;\n\n        @Override\n        public void accept(PartitionIterator partitionIterator)\n        {\n            Assert.assertNotNull(partitionIterator);\n            result = partitionIterator;\n        }\n    }\n\n    private static boolean configured = false;\n\n    static void configureClass(ReadRepairStrategy repairStrategy) throws Throwable\n    {\n        SchemaLoader.loadSchema();\n        String ksName = \"ks\";\n\n        String ddl = String.format(\"CREATE TABLE tbl (k int primary key, v text) WITH read_repair='%s'\",\n                                   repairStrategy.toString().toLowerCase());\n\n        cfm = CreateTableStatement.parse(ddl, ksName).build();\n        assert cfm.params.readRepair == repairStrategy;\n        KeyspaceMetadata ksm = KeyspaceMetadata.create(ksName, KeyspaceParams.simple(3), Tables.of(cfm));\n        MigrationManager.announceNewKeyspace(ksm, false);\n\n        ks = Keyspace.open(ksName);\n        cfs = ks.getColumnFamilyStore(\"tbl\");\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n\n        target1 = InetAddressAndPort.getByName(\"127.0.0.255\");\n        target2 = InetAddressAndPort.getByName(\"127.0.0.254\");\n        target3 = InetAddressAndPort.getByName(\"127.0.0.253\");\n\n        targets = ImmutableList.of(target1, target2, target3);\n\n        replica1 = fullReplica(target1, FULL_RANGE);\n        replica2 = fullReplica(target2, FULL_RANGE);\n        replica3 = fullReplica(target3, FULL_RANGE);\n        replicas = EndpointsForRange.of(replica1, replica2, replica3);\n\n        replicaPlan = replicaPlan(ConsistencyLevel.QUORUM, replicas);\n\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 0 })), replica1.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 1 })), replica2.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 2 })), replica3.endpoint());\n        Gossiper.instance.initializeNodeUnsafe(replica1.endpoint(), UUID.randomUUID(), 1);\n        Gossiper.instance.initializeNodeUnsafe(replica2.endpoint(), UUID.randomUUID(), 1);\n        Gossiper.instance.initializeNodeUnsafe(replica3.endpoint(), UUID.randomUUID(), 1);\n\n        // default test values\n        key  = dk(5);\n        cell1 = cell(\"v\", \"val1\", now);\n        cell2 = cell(\"v\", \"val2\", now);\n        cell3 = cell(\"v\", \"val3\", now);\n        resolved = mutation(cell1, cell2);\n\n        command = Util.cmd(cfs, 1).build();\n\n        configured = true;\n    }\n\n    static Set<InetAddressAndPort> epSet(InetAddressAndPort... eps)\n    {\n        return Sets.newHashSet(eps);\n    }\n\n    @Before\n    public void setUp()\n    {\n        assert configured : \"configureClass must be called in a @BeforeClass method\";\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n    }\n\n    static ReplicaPlan.ForRangeRead replicaPlan(ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(ks, consistencyLevel, replicas, replicas);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan)\n    {\n        return repairPlan(readPlan, readPlan.candidates());\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(EndpointsForRange liveAndDown, EndpointsForRange targets)\n    {\n        return repairPlan(replicaPlan(liveAndDown, targets), liveAndDown);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan, EndpointsForRange liveAndDown)\n    {\n        Token token = readPlan.range().left.getToken();\n        EndpointsForToken pending = EndpointsForToken.empty(token);\n        return ReplicaPlans.forWrite(readPlan.keyspace(),\n                                     ConsistencyLevel.TWO,\n                                     liveAndDown.forToken(token),\n                                     pending,\n                                     replica -> true,\n                                     ReplicaPlans.writeReadRepair(readPlan));\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return replicaPlan(ks, ConsistencyLevel.QUORUM, replicas, targets);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(keyspace, consistencyLevel, replicas, replicas);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), consistencyLevel, ReplicaUtils.FULL_BOUNDS, replicas, targets, 1);\n    }\n\n    public abstract InstrumentedReadRepair createInstrumentedReadRepair(ReadCommand command, ReplicaPlan.Shared<?, ?> replicaPlan, long queryStartNanoTime);\n\n    public InstrumentedReadRepair createInstrumentedReadRepair(ReplicaPlan.Shared<?, ?> replicaPlan)\n    {\n        return createInstrumentedReadRepair(command, replicaPlan, System.nanoTime());\n\n    }\n\n    /**\n     * If we haven't received enough full data responses by the time the speculation\n     * timeout occurs, we should send read requests to additional replicas\n     */\n    @Test\n    public void readSpeculationCycle()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2, target3), repair.getReadRecipients());\n        Assert.assertNull(consumer.result);\n    }\n\n    /**\n     * If we receive enough data responses by the before the speculation timeout\n     * passes, we shouldn't send additional read requests\n     */\n    @Test\n    public void noSpeculationRequired()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.getReadCallback().onResponse(msg(target1, cell1));\n        repair.getReadCallback().onResponse(msg(target2, cell1));\n\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n\n        repair.awaitReads();\n\n        assertPartitionsEqual(partition(cell1), consumer.result);\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.service.reads.repair;\n\nimport java.nio.ByteBuffer;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Consumer;\n\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport com.google.common.primitives.Ints;\n\nimport org.apache.cassandra.dht.ByteOrderedPartitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.gms.Gossiper;\nimport org.apache.cassandra.locator.AbstractNetworkTopologySnitch;\nimport org.apache.cassandra.locator.EndpointsForToken;\nimport org.apache.cassandra.locator.ReplicaPlan;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.Ignore;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.ColumnIdentifier;\nimport org.apache.cassandra.cql3.statements.schema.CreateTableStatement;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.ConsistencyLevel;\nimport org.apache.cassandra.db.DecoratedKey;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.Mutation;\nimport org.apache.cassandra.db.ReadCommand;\nimport org.apache.cassandra.db.ReadResponse;\nimport org.apache.cassandra.db.partitions.PartitionIterator;\nimport org.apache.cassandra.db.partitions.PartitionUpdate;\nimport org.apache.cassandra.db.partitions.SingletonUnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;\nimport org.apache.cassandra.db.partitions.UnfilteredPartitionIterators;\nimport org.apache.cassandra.db.rows.BTreeRow;\nimport org.apache.cassandra.db.rows.BufferCell;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.db.rows.RowIterator;\nimport org.apache.cassandra.locator.EndpointsForRange;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.locator.ReplicaPlans;\nimport org.apache.cassandra.locator.ReplicaUtils;\nimport org.apache.cassandra.net.Message;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.MigrationManager;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.schema.Tables;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static org.apache.cassandra.locator.Replica.fullReplica;\nimport static org.apache.cassandra.locator.ReplicaUtils.FULL_RANGE;\nimport static org.apache.cassandra.net.Verb.INTERNAL_RSP;\n\n@Ignore\npublic abstract  class AbstractReadRepairTest\n{\n    static Keyspace ks;\n    static ColumnFamilyStore cfs;\n    static TableMetadata cfm;\n    static InetAddressAndPort target1;\n    static InetAddressAndPort target2;\n    static InetAddressAndPort target3;\n    static InetAddressAndPort remote1;\n    static InetAddressAndPort remote2;\n    static InetAddressAndPort remote3;\n    static List<InetAddressAndPort> targets;\n    static List<InetAddressAndPort> remotes;\n\n    static Replica replica1;\n    static Replica replica2;\n    static Replica replica3;\n    static EndpointsForRange replicas;\n    static Replica remoteReplica1;\n    static Replica remoteReplica2;\n    static Replica remoteReplica3;\n    static EndpointsForRange remoteReplicas;\n\n    static long now = TimeUnit.NANOSECONDS.toMicros(System.nanoTime());\n    static DecoratedKey key;\n    static Cell<?> cell1;\n    static Cell<?> cell2;\n    static Cell<?> cell3;\n    static Mutation resolved;\n\n    static ReadCommand command;\n\n    static void assertRowsEqual(Row expected, Row actual)\n    {\n        try\n        {\n            Assert.assertEquals(expected == null, actual == null);\n            if (expected == null)\n                return;\n            Assert.assertEquals(expected.clustering(), actual.clustering());\n            Assert.assertEquals(expected.deletion(), actual.deletion());\n            Assert.assertArrayEquals(Iterables.toArray(expected.cells(), Cell.class), Iterables.toArray(expected.cells(), Cell.class));\n        } catch (Throwable t)\n        {\n            throw new AssertionError(String.format(\"Row comparison failed, expected %s got %s\", expected, actual), t);\n        }\n    }\n\n    static void assertRowsEqual(RowIterator expected, RowIterator actual)\n    {\n        assertRowsEqual(expected.staticRow(), actual.staticRow());\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n        assert !actual.hasNext();\n    }\n\n    static void assertPartitionsEqual(PartitionIterator expected, PartitionIterator actual)\n    {\n        while (expected.hasNext())\n        {\n            assert actual.hasNext();\n            assertRowsEqual(expected.next(), actual.next());\n        }\n\n        assert !actual.hasNext();\n    }\n\n    static void assertMutationEqual(Mutation expected, Mutation actual)\n    {\n        Assert.assertEquals(expected.getKeyspaceName(), actual.getKeyspaceName());\n        Assert.assertEquals(expected.key(), actual.key());\n        Assert.assertEquals(expected.key(), actual.key());\n        PartitionUpdate expectedUpdate = Iterables.getOnlyElement(expected.getPartitionUpdates());\n        PartitionUpdate actualUpdate = Iterables.getOnlyElement(actual.getPartitionUpdates());\n        assertRowsEqual(Iterables.getOnlyElement(expectedUpdate), Iterables.getOnlyElement(actualUpdate));\n    }\n\n    static DecoratedKey dk(int v)\n    {\n        return DatabaseDescriptor.getPartitioner().decorateKey(ByteBufferUtil.bytes(v));\n    }\n\n    static Cell<?> cell(String name, String value, long timestamp)\n    {\n        return BufferCell.live(cfm.getColumn(ColumnIdentifier.getInterned(name, false)), timestamp, ByteBufferUtil.bytes(value));\n    }\n\n    static PartitionUpdate update(Cell<?>... cells)\n    {\n        Row.Builder builder = BTreeRow.unsortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        for (Cell<?> cell: cells)\n        {\n            builder.addCell(cell);\n        }\n        return PartitionUpdate.singleRowUpdate(cfm, key, builder.build());\n    }\n\n    static PartitionIterator partition(Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return UnfilteredPartitionIterators.filter(iter, Ints.checkedCast(TimeUnit.MICROSECONDS.toSeconds(now)));\n    }\n\n    static Mutation mutation(Cell<?>... cells)\n    {\n        return new Mutation(update(cells));\n    }\n\n    @SuppressWarnings(\"resource\")\n    static Message<ReadResponse> msg(InetAddressAndPort from, Cell<?>... cells)\n    {\n        UnfilteredPartitionIterator iter = new SingletonUnfilteredPartitionIterator(update(cells).unfilteredIterator());\n        return Message.builder(INTERNAL_RSP, ReadResponse.createDataResponse(iter, command, command.executionController().getRepairedDataInfo()))\n                      .from(from)\n                      .build();\n    }\n\n    static class ResultConsumer implements Consumer<PartitionIterator>\n    {\n\n        PartitionIterator result = null;\n\n        @Override\n        public void accept(PartitionIterator partitionIterator)\n        {\n            Assert.assertNotNull(partitionIterator);\n            result = partitionIterator;\n        }\n    }\n\n    private static boolean configured = false;\n\n    static void configureClass(ReadRepairStrategy repairStrategy) throws Throwable\n    {\n        SchemaLoader.loadSchema();\n\n        DatabaseDescriptor.setEndpointSnitch(new AbstractNetworkTopologySnitch()\n        {\n            public String getRack(InetAddressAndPort endpoint)\n            {\n                return \"rack1\";\n            }\n\n            public String getDatacenter(InetAddressAndPort endpoint)\n            {\n                byte[] address = endpoint.addressBytes;\n                if (address[1] == 2) {\n                    return \"datacenter2\";\n                }\n                return \"datacenter1\";\n            }\n        });\n\n        target1 = InetAddressAndPort.getByName(\"127.1.0.255\");\n        target2 = InetAddressAndPort.getByName(\"127.1.0.254\");\n        target3 = InetAddressAndPort.getByName(\"127.1.0.253\");\n\n        remote1 = InetAddressAndPort.getByName(\"127.2.0.255\");\n        remote2 = InetAddressAndPort.getByName(\"127.2.0.254\");\n        remote3 = InetAddressAndPort.getByName(\"127.2.0.253\");\n\n        targets = ImmutableList.of(target1, target2, target3);\n        remotes = ImmutableList.of(remote1, remote2, remote3);\n\n        replica1 = fullReplica(target1, FULL_RANGE);\n        replica2 = fullReplica(target2, FULL_RANGE);\n        replica3 = fullReplica(target3, FULL_RANGE);\n        replicas = EndpointsForRange.of(replica1, replica2, replica3);\n\n        remoteReplica1 = fullReplica(remote1, FULL_RANGE);\n        remoteReplica2 = fullReplica(remote2, FULL_RANGE);\n        remoteReplica3 = fullReplica(remote3, FULL_RANGE);\n        remoteReplicas = EndpointsForRange.of(remoteReplica1, remoteReplica2, remoteReplica3);\n\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 0 })), replica1.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 1 })), replica2.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 2 })), replica3.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 3 })), remoteReplica1.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 4 })), remoteReplica2.endpoint());\n        StorageService.instance.getTokenMetadata().updateNormalToken(ByteOrderedPartitioner.instance.getToken(ByteBuffer.wrap(new byte[] { 5 })), remoteReplica3.endpoint());\n\n        for (Replica replica : replicas)\n        {\n            UUID hostId = UUID.randomUUID();\n            Gossiper.instance.initializeNodeUnsafe(replica.endpoint(), hostId, 1);\n            StorageService.instance.getTokenMetadata().updateHostId(hostId, replica.endpoint());\n        }\n        for (Replica replica : remoteReplicas)\n        {\n            UUID hostId = UUID.randomUUID();\n            Gossiper.instance.initializeNodeUnsafe(replica.endpoint(), hostId, 1);\n            StorageService.instance.getTokenMetadata().updateHostId(hostId, replica.endpoint());\n        }\n\n        String ksName = \"ks\";\n\n        String ddl = String.format(\"CREATE TABLE tbl (k int primary key, v text) WITH read_repair='%s'\",\n                                   repairStrategy.toString().toLowerCase());\n\n        cfm = CreateTableStatement.parse(ddl, ksName).build();\n        assert cfm.params.readRepair == repairStrategy;\n        KeyspaceMetadata ksm = KeyspaceMetadata.create(ksName, KeyspaceParams.nts(\"datacenter1\", 3, \"datacenter2\", 3), Tables.of(cfm));\n        MigrationManager.announceNewKeyspace(ksm, true);\n\n        ks = Keyspace.open(ksName);\n        cfs = ks.getColumnFamilyStore(\"tbl\");\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n\n        // default test values\n        key  = dk(5);\n        cell1 = cell(\"v\", \"val1\", now);\n        cell2 = cell(\"v\", \"val2\", now);\n        cell3 = cell(\"v\", \"val3\", now);\n        resolved = mutation(cell1, cell2);\n\n        command = Util.cmd(cfs, 1).build();\n\n        configured = true;\n    }\n\n    static Set<InetAddressAndPort> epSet(InetAddressAndPort... eps)\n    {\n        return Sets.newHashSet(eps);\n    }\n\n    @Before\n    public void setUp()\n    {\n        assert configured : \"configureClass must be called in a @BeforeClass method\";\n\n        cfs.sampleReadLatencyNanos = 0;\n        cfs.additionalWriteLatencyNanos = 0;\n    }\n\n    static ReplicaPlan.ForRangeRead replicaPlan(ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(ks, consistencyLevel, replicas, replicas);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan)\n    {\n        return repairPlan(readPlan, readPlan.candidates());\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(EndpointsForRange liveAndDown, EndpointsForRange targets)\n    {\n        return repairPlan(replicaPlan(liveAndDown, targets), liveAndDown);\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan, EndpointsForRange liveAndDown, EndpointsForToken pending)\n    {\n        Token token = readPlan.range().left.getToken();\n        return ReplicaPlans.forWrite(readPlan.keyspace(),\n                                     readPlan.consistencyLevel(),\n                                     liveAndDown.forToken(token),\n                                     pending,\n                                     replica -> true,\n                                     ReplicaPlans.writeReadRepair(readPlan));\n    }\n\n    static ReplicaPlan.ForTokenWrite repairPlan(ReplicaPlan.ForRangeRead readPlan, EndpointsForRange liveAndDown)\n    {\n        Token token = readPlan.range().left.getToken();\n        EndpointsForToken pending = EndpointsForToken.empty(token);\n        return ReplicaPlans.forWrite(readPlan.keyspace(),\n                                     readPlan.consistencyLevel(),\n                                     liveAndDown.forToken(token),\n                                     pending,\n                                     replica -> true,\n                                     ReplicaPlans.writeReadRepair(readPlan));\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return replicaPlan(ks, ConsistencyLevel.LOCAL_QUORUM, replicas, targets);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas)\n    {\n        return replicaPlan(keyspace, consistencyLevel, replicas, replicas);\n    }\n    static ReplicaPlan.ForRangeRead replicaPlan(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForRange replicas, EndpointsForRange targets)\n    {\n        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), consistencyLevel, ReplicaUtils.FULL_BOUNDS, replicas, targets, 1);\n    }\n\n    public abstract InstrumentedReadRepair createInstrumentedReadRepair(ReadCommand command, ReplicaPlan.Shared<?, ?> replicaPlan, long queryStartNanoTime);\n\n    public InstrumentedReadRepair createInstrumentedReadRepair(ReplicaPlan.Shared<?, ?> replicaPlan)\n    {\n        return createInstrumentedReadRepair(command, replicaPlan, System.nanoTime());\n\n    }\n\n    /**\n     * If we haven't received enough full data responses by the time the speculation\n     * timeout occurs, we should send read requests to additional replicas\n     */\n    @Test\n    public void readSpeculationCycle()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2, target3), repair.getReadRecipients());\n        Assert.assertNull(consumer.result);\n    }\n\n    /**\n     * If we receive enough data responses by the before the speculation timeout\n     * passes, we shouldn't send additional read requests\n     */\n    @Test\n    public void noSpeculationRequired()\n    {\n        InstrumentedReadRepair repair = createInstrumentedReadRepair(ReplicaPlan.shared(replicaPlan(replicas, EndpointsForRange.of(replica1, replica2))));\n        ResultConsumer consumer = new ResultConsumer();\n\n        Assert.assertEquals(epSet(), repair.getReadRecipients());\n        repair.startRepair(null, consumer);\n\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n        repair.getReadCallback().onResponse(msg(target1, cell1));\n        repair.getReadCallback().onResponse(msg(target2, cell1));\n\n        repair.maybeSendAdditionalReads();\n        Assert.assertEquals(epSet(target1, target2), repair.getReadRecipients());\n\n        repair.awaitReads();\n\n        assertPartitionsEqual(partition(cell1), consumer.result);\n    }\n}\n","lineNo":280}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm.sequences;\n\nimport java.io.IOException;\n\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.util.DataInputPlus;\nimport org.apache.cassandra.io.util.DataOutputPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.schema.ReplicationParams;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.Transformation;\nimport org.apache.cassandra.tcm.ownership.DataPlacement;\nimport org.apache.cassandra.tcm.ownership.EntireRange;\nimport org.apache.cassandra.tcm.serialization.AsymmetricMetadataSerializer;\nimport org.apache.cassandra.tcm.serialization.Version;\n\nimport static org.apache.cassandra.tcm.ownership.EntireRange.entireRange;\n\npublic class CancelCMSReconfiguration implements Transformation\n{\n    public static final Serializer serializer = new Serializer();\n\n    public static final CancelCMSReconfiguration instance = new CancelCMSReconfiguration();\n    private CancelCMSReconfiguration()\n    {\n    }\n\n    @Override\n    public Kind kind()\n    {\n        return Kind.CANCEL_CMS_RECONFIGURATION;\n    }\n\n    @Override\n    public Result execute(ClusterMetadata prev)\n    {\n        ReconfigureCMS reconfigureCMS = (ReconfigureCMS) prev.inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n        if (reconfigureCMS == null)\n            return new Rejected(ExceptionCode.INVALID, \"Can not cancel reconfiguration since there does not seem to be any in-flight\");\n\n        ReplicationParams metaParams = ReplicationParams.meta(prev);\n        ClusterMetadata.Transformer transformer = prev.transformer();\n        if (reconfigureCMS.next.activeTransition != null)\n        {\n            InetAddressAndPort pendingEndpoint = prev.directory.endpoint(reconfigureCMS.next.activeTransition.nodeId);\n            Replica pendingReplica = new Replica(pendingEndpoint, entireRange, true);\n            DataPlacement.Builder builder = prev.placements.get(metaParams).unbuild()\n                                                           .withoutWriteReplica(prev.nextEpoch(), pendingReplica);\n\n            DataPlacement placement = builder.build();\n            if (!placement.reads.equals(placement.writes))\n                return new Rejected(ExceptionCode.INVALID, String.format(\"Placements will be inconsistent if this transformation is applied:\\nReads %s\\nWrites: %s\",\n                                                                         placement.reads,\n                                                                         placement.writes));\n\n            transformer = transformer.with(prev.placements.unbuild().with(metaParams, placement).build());\n        }\n\n        return Transformation.success(transformer.with(prev.inProgressSequences.without(ReconfigureCMS.SequenceKey.instance))\n                                                 .with(prev.lockedRanges.unlock(reconfigureCMS.next.lockKey)),\n                                      EntireRange.affectedRanges(prev));\n    }\n\n    @Override\n    public String toString()\n    {\n        return \"CancelCMSReconfiguration{}\";\n    }\n\n    public static class Serializer implements AsymmetricMetadataSerializer<Transformation, CancelCMSReconfiguration>\n    {\n        public void serialize(Transformation t, DataOutputPlus out, Version version) throws IOException\n        {\n        }\n\n        public CancelCMSReconfiguration deserialize(DataInputPlus in, Version version) throws IOException\n        {\n            return instance;\n        }\n\n        public long serializedSize(Transformation t, Version version)\n        {\n            return 0;\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm.sequences;\n\nimport java.io.IOException;\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.util.DataInputPlus;\nimport org.apache.cassandra.io.util.DataOutputPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.schema.DistributedSchema;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.schema.ReplicationParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.Transformation;\nimport org.apache.cassandra.tcm.membership.Directory;\nimport org.apache.cassandra.tcm.ownership.DataPlacement;\nimport org.apache.cassandra.tcm.ownership.DataPlacements;\nimport org.apache.cassandra.tcm.ownership.EntireRange;\nimport org.apache.cassandra.tcm.serialization.AsymmetricMetadataSerializer;\nimport org.apache.cassandra.tcm.serialization.Version;\n\nimport static org.apache.cassandra.tcm.ownership.EntireRange.entireRange;\n\npublic class CancelCMSReconfiguration implements Transformation\n{\n    public static final Serializer serializer = new Serializer();\n\n    public static final CancelCMSReconfiguration instance = new CancelCMSReconfiguration();\n    private CancelCMSReconfiguration()\n    {\n    }\n\n    @Override\n    public Kind kind()\n    {\n        return Kind.CANCEL_CMS_RECONFIGURATION;\n    }\n\n    @Override\n    public Result execute(ClusterMetadata prev)\n    {\n        ReconfigureCMS reconfigureCMS = (ReconfigureCMS) prev.inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n        if (reconfigureCMS == null)\n            return new Rejected(ExceptionCode.INVALID, \"Can not cancel reconfiguration since there does not seem to be any in-flight\");\n\n        ReplicationParams metaParams = ReplicationParams.meta(prev);\n        ClusterMetadata.Transformer transformer = prev.transformer();\n        DataPlacement placement = prev.placements.get(metaParams);\n        // Reset any partially completed transition by removing the pending replica from the write group\n        if (reconfigureCMS.next.activeTransition != null)\n        {\n            InetAddressAndPort pendingEndpoint = prev.directory.endpoint(reconfigureCMS.next.activeTransition.nodeId);\n            Replica pendingReplica = new Replica(pendingEndpoint, entireRange, true);\n            placement = placement.unbuild()\n                                 .withoutWriteReplica(prev.nextEpoch(), pendingReplica)\n                                 .build();\n        }\n        if (!placement.reads.equals(placement.writes))\n            return new Rejected(ExceptionCode.INVALID, String.format(\"Placements will be inconsistent if this transformation is applied:\\nReads %s\\nWrites: %s\",\n                                                                     placement.reads,\n                                                                     placement.writes));\n\n        // Reset the replication params for the meta keyspace based on the actual placement in case they no longer match\n        ReplicationParams fromPlacement = getAccurateReplication(prev.directory, placement);\n\n        // If they no longer match, i.e. the transitions completed so far did not bring the placements into line with\n        // the configuration, remove the entry keyed by the existing configured params.\n        DataPlacements.Builder builder = prev.placements.unbuild();\n        if (!metaParams.equals(fromPlacement))\n        {\n            builder = builder.without(metaParams);\n\n            // Also update schema with the corrected params\n            KeyspaceMetadata keyspace = prev.schema.getKeyspaceMetadata(SchemaConstants.METADATA_KEYSPACE_NAME);\n            KeyspaceMetadata newKeyspace = keyspace.withSwapped(new KeyspaceParams(keyspace.params.durableWrites, fromPlacement));\n            transformer = transformer.with(new DistributedSchema(prev.schema.getKeyspaces().withAddedOrUpdated(newKeyspace)));\n        }\n\n        // finally, add the possibly corrected placement keyed by the possibly corrected params\n        builder = builder.with(fromPlacement, placement);\n        transformer = transformer.with(builder.build());\n\n        return Transformation.success(transformer.with(prev.inProgressSequences.without(ReconfigureCMS.SequenceKey.instance))\n                                                 .with(prev.lockedRanges.unlock(reconfigureCMS.next.lockKey)),\n                                      EntireRange.affectedRanges(prev));\n    }\n\n    private ReplicationParams getAccurateReplication(Directory directory, DataPlacement placement)\n    {\n        Map<String, Integer> replicasPerDc = new HashMap<>();\n        placement.writes.byEndpoint().keySet().forEach(i -> {\n            String dc = directory.location(directory.peerId(i)).datacenter;\n            int count = replicasPerDc.getOrDefault(dc, 0);\n            replicasPerDc.put(dc, ++count);\n        });\n        return ReplicationParams.ntsMeta(replicasPerDc);\n    }\n\n    @Override\n    public String toString()\n    {\n        return \"CancelCMSReconfiguration{}\";\n    }\n\n    public static class Serializer implements AsymmetricMetadataSerializer<Transformation, CancelCMSReconfiguration>\n    {\n        public void serialize(Transformation t, DataOutputPlus out, Version version) throws IOException\n        {\n        }\n\n        public CancelCMSReconfiguration deserialize(DataInputPlus in, Version version) throws IOException\n        {\n            return instance;\n        }\n\n        public long serializedSize(Transformation t, Version version)\n        {\n            return 0;\n        }\n    }\n}\n","lineNo":90}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test.log;\n\nimport java.util.Random;\nimport java.util.function.Supplier;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.ConsistencyLevel;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.schema.DistributedMetadataLogKeyspace;\nimport org.apache.cassandra.schema.ReplicationParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.ClusterMetadataService;\nimport org.apache.cassandra.tcm.ownership.DataPlacement;\nimport org.apache.cassandra.tcm.ownership.EntireRange;\nimport org.apache.cassandra.tcm.sequences.CancelCMSReconfiguration;\nimport org.apache.cassandra.tcm.sequences.ProgressBarrier;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.transformations.cms.PrepareCMSReconfiguration;\nimport org.apache.cassandra.utils.FBUtilities;\n\npublic class ReconfigureCMSTest extends FuzzTestBase\n{\n    @Test\n    public void expandAndShrinkCMSTest() throws Throwable\n    {\n        try (Cluster cluster = Cluster.build(6)\n                                      .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(5, \"dc0\", \"rack0\"))\n                                      .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                              .with(Feature.NETWORK, Feature.GOSSIP))\n                                      .start())\n        {\n            cluster.setUncaughtExceptionsFilter(t -> t.getMessage() != null && t.getMessage().contains(\"There are not enough nodes in dc0 datacenter to satisfy replication factor\"));\n            Random rnd = new Random(2);\n            Supplier<Integer> nodeSelector = () -> rnd.nextInt(cluster.size() - 1) + 1;\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"--sync\", \"0\").asserts().failure();\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"--sync\", \"500\").asserts().failure();\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"--sync\", \"5\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertEquals(5, metadata.fullCMSMembers().size());\n                Assert.assertEquals(ReplicationParams.simpleMeta(5, metadata.directory.knownDatacenters()),\n                                    metadata.placements.keys().stream().filter(ReplicationParams::isMeta).findFirst().get());\n            });\n            cluster.stream().forEach(i -> {\n                Assert.assertTrue(i.executeInternal(String.format(\"SELECT * FROM %s.%s\", SchemaConstants.METADATA_KEYSPACE_NAME, DistributedMetadataLogKeyspace.TABLE_NAME)).length > 0);\n            });\n\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"--sync\", \"1\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertEquals(1, metadata.fullCMSMembers().size());\n                Assert.assertEquals(ReplicationParams.simpleMeta(1, metadata.directory.knownDatacenters()),\n                                    metadata.placements.keys().stream().filter(ReplicationParams::isMeta).findFirst().get());\n            });\n        }\n    }\n\n    @Test\n    public void cancelCMSReconfigurationTest() throws Throwable\n    {\n        try (Cluster cluster = Cluster.build(4)\n                                      .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(5, \"dc0\", \"rack0\"))\n                                      .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                              .set(\"progress_barrier_default_consistency_level\", ConsistencyLevel.ALL)\n                                                              .with(Feature.NETWORK, Feature.GOSSIP))\n                                      .start())\n        {\n            cluster.get(1).nodetoolResult(\"reconfigurecms\", \"--sync\", \"2\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadataService.instance().commit(new PrepareCMSReconfiguration.Complex(ReplicationParams.simple(3).asMeta()));\n                ReconfigureCMS reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                try\n                {\n                    ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                    Assert.fail(\"Should not be possible to commit same `advance` twice\");\n                }\n                catch (Throwable t)\n                {\n                    Assert.assertTrue(t.getMessage().contains(\"This transformation (0) has already been applied\"));\n                }\n                reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                Assert.assertNotNull(reconfigureCMS.next.activeTransition);\n\n                ClusterMetadataService.instance().commit(CancelCMSReconfiguration.instance);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertNull(metadata.inProgressSequences.get(ReconfigureCMS.SequenceKey.instance));\n                Assert.assertEquals(2, metadata.fullCMSMembers().size());\n                DataPlacement placements = metadata.placements.get(ReplicationParams.meta(metadata));\n                Assert.assertEquals(placements.reads, placements.writes);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadataService.instance().commit(new PrepareCMSReconfiguration.Complex(ReplicationParams.simple(4).asMeta()));\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n\n                ReconfigureCMS reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                Assert.assertNull(reconfigureCMS.next.activeTransition);\n\n                ClusterMetadataService.instance().commit(CancelCMSReconfiguration.instance);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertNull(metadata.inProgressSequences.get(ReconfigureCMS.SequenceKey.instance));\n                Assert.assertTrue(metadata.fullCMSMembers().contains(FBUtilities.getBroadcastAddressAndPort()));\n                Assert.assertEquals(3, metadata.fullCMSMembers().size());\n                DataPlacement placements = metadata.placements.get(ReplicationParams.meta(metadata));\n                Assert.assertEquals(placements.reads, placements.writes);\n            });\n        }\n    }}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test.log;\n\nimport java.util.Random;\nimport java.util.function.Supplier;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.ConsistencyLevel;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.schema.DistributedMetadataLogKeyspace;\nimport org.apache.cassandra.schema.ReplicationParams;\nimport org.apache.cassandra.schema.SchemaConstants;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.ClusterMetadataService;\nimport org.apache.cassandra.tcm.ownership.DataPlacement;\nimport org.apache.cassandra.tcm.ownership.EntireRange;\nimport org.apache.cassandra.tcm.sequences.ProgressBarrier;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.transformations.cms.PrepareCMSReconfiguration;\nimport org.apache.cassandra.utils.FBUtilities;\n\npublic class ReconfigureCMSTest extends FuzzTestBase\n{\n    @Test\n    public void expandAndShrinkCMSTest() throws Throwable\n    {\n        try (Cluster cluster = Cluster.build(6)\n                                      .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(5, \"dc0\", \"rack0\"))\n                                      .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                              .with(Feature.NETWORK, Feature.GOSSIP))\n                                      .start())\n        {\n            cluster.setUncaughtExceptionsFilter(t -> t.getMessage() != null && t.getMessage().contains(\"There are not enough nodes in dc0 datacenter to satisfy replication factor\"));\n            Random rnd = new Random(2);\n            Supplier<Integer> nodeSelector = () -> rnd.nextInt(cluster.size() - 1) + 1;\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"0\").asserts().failure();\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"500\").asserts().failure();\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"5\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertEquals(5, metadata.fullCMSMembers().size());\n                Assert.assertEquals(ReplicationParams.simpleMeta(5, metadata.directory.knownDatacenters()),\n                                    metadata.placements.keys().stream().filter(ReplicationParams::isMeta).findFirst().get());\n            });\n            cluster.stream().forEach(i -> {\n                Assert.assertTrue(i.executeInternal(String.format(\"SELECT * FROM %s.%s\", SchemaConstants.METADATA_KEYSPACE_NAME, DistributedMetadataLogKeyspace.TABLE_NAME)).length > 0);\n            });\n\n            cluster.get(nodeSelector.get()).nodetoolResult(\"reconfigurecms\", \"1\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertEquals(1, metadata.fullCMSMembers().size());\n                Assert.assertEquals(ReplicationParams.simpleMeta(1, metadata.directory.knownDatacenters()),\n                                    metadata.placements.keys().stream().filter(ReplicationParams::isMeta).findFirst().get());\n            });\n        }\n    }\n\n    @Test\n    public void cancelCMSReconfigurationTest() throws Throwable\n    {\n        try (Cluster cluster = Cluster.build(4)\n                                      .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(5, \"dc0\", \"rack0\"))\n                                      .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                              .set(\"progress_barrier_default_consistency_level\", ConsistencyLevel.ALL)\n                                                              .with(Feature.NETWORK, Feature.GOSSIP))\n                                      .start())\n        {\n            cluster.get(1).nodetoolResult(\"reconfigurecms\", \"2\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadataService.instance().commit(new PrepareCMSReconfiguration.Complex(ReplicationParams.simple(3).asMeta()));\n                ReconfigureCMS reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                try\n                {\n                    ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                    Assert.fail(\"Should not be possible to commit same `advance` twice\");\n                }\n                catch (Throwable t)\n                {\n                    Assert.assertTrue(t.getMessage().contains(\"This transformation (0) has already been applied\"));\n                }\n                reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                Assert.assertNotNull(reconfigureCMS.next.activeTransition);\n            });\n            cluster.get(1).nodetoolResult(\"reconfigurecms\", \"--cancel\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertNull(metadata.inProgressSequences.get(ReconfigureCMS.SequenceKey.instance));\n                Assert.assertEquals(2, metadata.fullCMSMembers().size());\n                ReplicationParams params = ReplicationParams.meta(metadata);\n                DataPlacement placements = metadata.placements.get(params);\n                Assert.assertEquals(placements.reads, placements.writes);\n                Assert.assertEquals(metadata.fullCMSMembers().size(), Integer.parseInt(params.asMap().get(\"dc0\")));\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadataService.instance().commit(new PrepareCMSReconfiguration.Complex(ReplicationParams.simple(4).asMeta()));\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n\n                ReconfigureCMS reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                ClusterMetadataService.instance().commit(reconfigureCMS.next);\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                reconfigureCMS = (ReconfigureCMS) ClusterMetadata.current().inProgressSequences.get(ReconfigureCMS.SequenceKey.instance);\n                Assert.assertNull(reconfigureCMS.next.activeTransition);\n            });\n            cluster.get(1).nodetoolResult(\"reconfigurecms\", \"--cancel\").asserts().success();\n            cluster.get(1).runOnInstance(() -> {\n                ProgressBarrier.propagateLast(EntireRange.affectedRanges(ClusterMetadata.current()));\n                ClusterMetadata metadata = ClusterMetadata.current();\n                Assert.assertNull(metadata.inProgressSequences.get(ReconfigureCMS.SequenceKey.instance));\n                Assert.assertTrue(metadata.fullCMSMembers().contains(FBUtilities.getBroadcastAddressAndPort()));\n                Assert.assertEquals(3, metadata.fullCMSMembers().size());\n                DataPlacement placements = metadata.placements.get(ReplicationParams.meta(metadata));\n                Assert.assertEquals(placements.reads, placements.writes);\n            });\n        }\n    }}\n","lineNo":114}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tools.nodetool;\n\nimport java.io.PrintStream;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\n\nimport io.airlift.airline.Command;\nimport io.airlift.airline.Option;\nimport org.apache.cassandra.tools.NodeProbe;\nimport org.apache.cassandra.tools.NodeTool.NodeToolCmd;\nimport org.apache.cassandra.tools.nodetool.formatter.TableBuilder;\nimport org.apache.cassandra.transport.ClientStat;\nimport org.apache.cassandra.transport.ConnectedClient;\n\n@Command(name = \"clientstats\", description = \"Print information about connected clients\")\npublic class ClientStats extends NodeToolCmd\n{\n    @Option(title = \"list_connections\", name = \"--all\", description = \"Lists all connections\")\n    private boolean listConnections = false;\n\n    @Option(title = \"by_protocol\", name = \"--by-protocol\", description = \"Lists most recent client connections by protocol version\")\n    private boolean connectionsByProtocolVersion = false;\n\n    @Option(title = \"clear_history\", name = \"--clear-history\", description = \"Clear the history of connected clients\")\n    private boolean clearConnectionHistory = false;\n\n    @Option(title = \"list_connections_with_client_options\", name = \"--client-options\", description = \"Lists all connections and the client options\")\n    private boolean clientOptions = false;\n\n    @Override\n    public void execute(NodeProbe probe)\n    {\n        PrintStream out = probe.output().out;\n        if (clearConnectionHistory)\n        {\n            out.println(\"Clearing connection history\");\n            probe.clearConnectionHistory();\n            return;\n        }\n\n        if (connectionsByProtocolVersion)\n        {\n            SimpleDateFormat sdf = new SimpleDateFormat(\"MMM dd, yyyy HH:mm:ss\");\n\n            out.println(\"Clients by protocol version\");\n            out.println();\n\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"clientsByProtocolVersion\");\n\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Protocol-Version\", \"IP-Address\", \"Last-Seen\");\n\n                for (Map<String, String> client : clients)\n                {\n                    table.add(client.get(ClientStat.PROTOCOL_VERSION),\n                              client.get(ClientStat.INET_ADDRESS),\n                              sdf.format(new Date(Long.valueOf(client.get(ClientStat.LAST_SEEN_TIME)))));\n                }\n\n                table.printTo(out);\n                out.println();\n            }\n\n            return;\n        }\n\n        if (listConnections)\n        {\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\", \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\", \"Driver-Version\");\n                for (Map<String, String> conn : clients)\n                {\n                    table.add(conn.get(ConnectedClient.ADDRESS),\n                              conn.get(ConnectedClient.SSL),\n                              conn.get(ConnectedClient.CIPHER),\n                              conn.get(ConnectedClient.PROTOCOL),\n                              conn.get(ConnectedClient.VERSION),\n                              conn.get(ConnectedClient.USER),\n                              conn.get(ConnectedClient.KEYSPACE),\n                              conn.get(ConnectedClient.REQUESTS),\n                              conn.get(ConnectedClient.DRIVER_NAME),\n                              conn.get(ConnectedClient.DRIVER_VERSION));\n                }\n                table.printTo(out);\n                out.println();\n            }\n        }\n\n        if (clientOptions)\n        {\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\", \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\", \"Driver-Version\", \"Client-Options\");\n                for (Map<String, String> conn : clients)\n                {\n                    table.add(conn.get(ConnectedClient.ADDRESS),\n                              conn.get(ConnectedClient.SSL),\n                              conn.get(ConnectedClient.CIPHER),\n                              conn.get(ConnectedClient.PROTOCOL),\n                              conn.get(ConnectedClient.VERSION),\n                              conn.get(ConnectedClient.USER),\n                              conn.get(ConnectedClient.KEYSPACE),\n                              conn.get(ConnectedClient.REQUESTS),\n                              conn.get(ConnectedClient.DRIVER_NAME),\n                              conn.get(ConnectedClient.DRIVER_VERSION),\n                              conn.get(ConnectedClient.CLIENT_OPTIONS));\n                }\n                table.printTo(out);\n                out.println();\n            }\n        }\n\n        Map<String, Integer> connectionsByUser = (Map<String, Integer>) probe.getClientMetric(\"connectedNativeClientsByUser\");\n        int total = connectionsByUser.values().stream().reduce(0, Integer::sum);\n        out.println(\"Total connected clients: \" + total);\n        out.println();\n        TableBuilder table = new TableBuilder();\n        table.add(\"User\", \"Connections\");\n        for (Entry<String, Integer> entry : connectionsByUser.entrySet())\n        {\n            table.add(entry.getKey(), entry.getValue().toString());\n        }\n        table.printTo(out);\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tools.nodetool;\n\nimport java.io.PrintStream;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.stream.Collectors;\n\nimport com.google.common.collect.ImmutableList;\nimport io.airlift.airline.Command;\nimport io.airlift.airline.Option;\nimport org.apache.cassandra.tools.NodeProbe;\nimport org.apache.cassandra.tools.NodeTool.NodeToolCmd;\nimport org.apache.cassandra.tools.nodetool.formatter.TableBuilder;\nimport org.apache.cassandra.transport.ClientStat;\nimport org.apache.cassandra.transport.ConnectedClient;\n\n@Command(name = \"clientstats\", description = \"Print information about connected clients\")\npublic class ClientStats extends NodeToolCmd\n{\n    @Option(title = \"list_connections\", name = \"--all\", description = \"Lists all connections\")\n    private boolean listConnections = false;\n\n    @Option(title = \"by_protocol\", name = \"--by-protocol\", description = \"Lists most recent client connections by protocol version\")\n    private boolean connectionsByProtocolVersion = false;\n\n    @Option(title = \"clear_history\", name = \"--clear-history\", description = \"Clear the history of connected clients\")\n    private boolean clearConnectionHistory = false;\n\n    @Option(title = \"list_connections_with_client_options\", name = \"--client-options\", description = \"Lists all connections and the client options\")\n    private boolean clientOptions = false;\n\n    @Option(title = \"verbose\", name = \"--verbose\", description = \"Lists all connections with additional details (client options, authenticator-specific metadata and more)\")\n    private boolean verbose = false;\n\n    @Override\n    public void execute(NodeProbe probe)\n    {\n        PrintStream out = probe.output().out;\n        if (clearConnectionHistory)\n        {\n            out.println(\"Clearing connection history\");\n            probe.clearConnectionHistory();\n            return;\n        }\n\n        if (connectionsByProtocolVersion)\n        {\n            SimpleDateFormat sdf = new SimpleDateFormat(\"MMM dd, yyyy HH:mm:ss\");\n\n            out.println(\"Clients by protocol version\");\n            out.println();\n\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"clientsByProtocolVersion\");\n\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Protocol-Version\", \"IP-Address\", \"Last-Seen\");\n\n                for (Map<String, String> client : clients)\n                {\n                    table.add(client.get(ClientStat.PROTOCOL_VERSION),\n                              client.get(ClientStat.INET_ADDRESS),\n                              sdf.format(new Date(Long.valueOf(client.get(ClientStat.LAST_SEEN_TIME)))));\n                }\n\n                table.printTo(out);\n                out.println();\n            }\n\n            return;\n        }\n\n        // Note: for compatbility with existing implementation if someone passes --all (listConnections),\n        // --client-options, and --metadata all three will be printed.\n        List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");\n        if (!clients.isEmpty() && (listConnections || clientOptions || verbose))\n        {\n            ImmutableList.Builder<String> tableHeaderBuilder = ImmutableList.<String>builder()\n                                                                            .add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\",\n                                                                                 \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\",\n                                                                                 \"Driver-Version\");\n            ImmutableList.Builder<String> tableFieldsBuilder = ImmutableList.<String>builder()\n                                                                            .add(ConnectedClient.ADDRESS, ConnectedClient.SSL,\n                                                                                 ConnectedClient.CIPHER, ConnectedClient.PROTOCOL,\n                                                                                 ConnectedClient.VERSION, ConnectedClient.USER,\n                                                                                 ConnectedClient.KEYSPACE, ConnectedClient.REQUESTS,\n                                                                                 ConnectedClient.DRIVER_NAME, ConnectedClient.DRIVER_VERSION);\n            if (clientOptions || verbose)\n            {\n                tableHeaderBuilder.add(\"Client-Options\");\n                tableFieldsBuilder.add(ConnectedClient.CLIENT_OPTIONS);\n            }\n\n            if (verbose)\n            {\n                tableHeaderBuilder.add(\"Auth-Mode\", \"Auth-Metadata\");\n                tableFieldsBuilder.add(ConnectedClient.AUTHENTICATION_MODE, ConnectedClient.AUTHENTICATION_METADATA);\n            }\n\n            List<String> tableHeader = tableHeaderBuilder.build();\n            List<String> tableFields = tableFieldsBuilder.build();\n            printTable(out, tableHeader, tableFields, clients);\n        }\n\n        Map<String, Integer> connectionsByUser = (Map<String, Integer>) probe.getClientMetric(\"connectedNativeClientsByUser\");\n        int total = connectionsByUser.values().stream().reduce(0, Integer::sum);\n        out.println(\"Total connected clients: \" + total);\n        out.println();\n        TableBuilder table = new TableBuilder();\n        table.add(\"User\", \"Connections\");\n        for (Entry<String, Integer> entry : connectionsByUser.entrySet())\n        {\n            table.add(entry.getKey(), entry.getValue().toString());\n        }\n        table.printTo(out);\n    }\n\n    /**\n     * Convenience function to print a table with the given header and the resolved fields for each connection.\n     *\n     * @param out         print stream to print to.\n     * @param headers     headers for the table\n     * @param tableFields the fields from {@link ConnectedClient} to retrieve from each client connection.\n     * @param clients     the clients to print, each client being a row inthe table.\n     */\n    private void printTable(PrintStream out, List<String> headers, List<String> tableFields, List<Map<String, String>> clients)\n    {\n        TableBuilder table = new TableBuilder();\n        table.add(headers);\n        for (Map<String, String> conn : clients)\n        {\n            List<String> connectionFieldValues = tableFields.stream()\n                    .map(conn::get)\n                    .collect(Collectors.toList());\n            table.add(connectionFieldValues);\n        }\n        table.printTo(out);\n        out.println();\n    }\n}\n","lineNo":99}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tools.nodetool;\n\nimport java.io.PrintStream;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\n\nimport io.airlift.airline.Command;\nimport io.airlift.airline.Option;\nimport org.apache.cassandra.tools.NodeProbe;\nimport org.apache.cassandra.tools.NodeTool.NodeToolCmd;\nimport org.apache.cassandra.tools.nodetool.formatter.TableBuilder;\nimport org.apache.cassandra.transport.ClientStat;\nimport org.apache.cassandra.transport.ConnectedClient;\n\n@Command(name = \"clientstats\", description = \"Print information about connected clients\")\npublic class ClientStats extends NodeToolCmd\n{\n    @Option(title = \"list_connections\", name = \"--all\", description = \"Lists all connections\")\n    private boolean listConnections = false;\n\n    @Option(title = \"by_protocol\", name = \"--by-protocol\", description = \"Lists most recent client connections by protocol version\")\n    private boolean connectionsByProtocolVersion = false;\n\n    @Option(title = \"clear_history\", name = \"--clear-history\", description = \"Clear the history of connected clients\")\n    private boolean clearConnectionHistory = false;\n\n    @Option(title = \"list_connections_with_client_options\", name = \"--client-options\", description = \"Lists all connections and the client options\")\n    private boolean clientOptions = false;\n\n    @Override\n    public void execute(NodeProbe probe)\n    {\n        PrintStream out = probe.output().out;\n        if (clearConnectionHistory)\n        {\n            out.println(\"Clearing connection history\");\n            probe.clearConnectionHistory();\n            return;\n        }\n\n        if (connectionsByProtocolVersion)\n        {\n            SimpleDateFormat sdf = new SimpleDateFormat(\"MMM dd, yyyy HH:mm:ss\");\n\n            out.println(\"Clients by protocol version\");\n            out.println();\n\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"clientsByProtocolVersion\");\n\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Protocol-Version\", \"IP-Address\", \"Last-Seen\");\n\n                for (Map<String, String> client : clients)\n                {\n                    table.add(client.get(ClientStat.PROTOCOL_VERSION),\n                              client.get(ClientStat.INET_ADDRESS),\n                              sdf.format(new Date(Long.valueOf(client.get(ClientStat.LAST_SEEN_TIME)))));\n                }\n\n                table.printTo(out);\n                out.println();\n            }\n\n            return;\n        }\n\n        if (listConnections)\n        {\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\", \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\", \"Driver-Version\");\n                for (Map<String, String> conn : clients)\n                {\n                    table.add(conn.get(ConnectedClient.ADDRESS),\n                              conn.get(ConnectedClient.SSL),\n                              conn.get(ConnectedClient.CIPHER),\n                              conn.get(ConnectedClient.PROTOCOL),\n                              conn.get(ConnectedClient.VERSION),\n                              conn.get(ConnectedClient.USER),\n                              conn.get(ConnectedClient.KEYSPACE),\n                              conn.get(ConnectedClient.REQUESTS),\n                              conn.get(ConnectedClient.DRIVER_NAME),\n                              conn.get(ConnectedClient.DRIVER_VERSION));\n                }\n                table.printTo(out);\n                out.println();\n            }\n        }\n\n        if (clientOptions)\n        {\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\", \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\", \"Driver-Version\", \"Client-Options\");\n                for (Map<String, String> conn : clients)\n                {\n                    table.add(conn.get(ConnectedClient.ADDRESS),\n                              conn.get(ConnectedClient.SSL),\n                              conn.get(ConnectedClient.CIPHER),\n                              conn.get(ConnectedClient.PROTOCOL),\n                              conn.get(ConnectedClient.VERSION),\n                              conn.get(ConnectedClient.USER),\n                              conn.get(ConnectedClient.KEYSPACE),\n                              conn.get(ConnectedClient.REQUESTS),\n                              conn.get(ConnectedClient.DRIVER_NAME),\n                              conn.get(ConnectedClient.DRIVER_VERSION),\n                              conn.get(ConnectedClient.CLIENT_OPTIONS));\n                }\n                table.printTo(out);\n                out.println();\n            }\n        }\n\n        Map<String, Integer> connectionsByUser = (Map<String, Integer>) probe.getClientMetric(\"connectedNativeClientsByUser\");\n        int total = connectionsByUser.values().stream().reduce(0, Integer::sum);\n        out.println(\"Total connected clients: \" + total);\n        out.println();\n        TableBuilder table = new TableBuilder();\n        table.add(\"User\", \"Connections\");\n        for (Entry<String, Integer> entry : connectionsByUser.entrySet())\n        {\n            table.add(entry.getKey(), entry.getValue().toString());\n        }\n        table.printTo(out);\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.tools.nodetool;\n\nimport java.io.PrintStream;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.stream.Collectors;\n\nimport com.google.common.collect.ImmutableList;\nimport io.airlift.airline.Command;\nimport io.airlift.airline.Option;\nimport org.apache.cassandra.tools.NodeProbe;\nimport org.apache.cassandra.tools.NodeTool.NodeToolCmd;\nimport org.apache.cassandra.tools.nodetool.formatter.TableBuilder;\nimport org.apache.cassandra.transport.ClientStat;\nimport org.apache.cassandra.transport.ConnectedClient;\n\n@Command(name = \"clientstats\", description = \"Print information about connected clients\")\npublic class ClientStats extends NodeToolCmd\n{\n    @Option(title = \"list_connections\", name = \"--all\", description = \"Lists all connections\")\n    private boolean listConnections = false;\n\n    @Option(title = \"by_protocol\", name = \"--by-protocol\", description = \"Lists most recent client connections by protocol version\")\n    private boolean connectionsByProtocolVersion = false;\n\n    @Option(title = \"clear_history\", name = \"--clear-history\", description = \"Clear the history of connected clients\")\n    private boolean clearConnectionHistory = false;\n\n    @Option(title = \"list_connections_with_client_options\", name = \"--client-options\", description = \"Lists all connections and the client options\")\n    private boolean clientOptions = false;\n\n    @Option(title = \"verbose\", name = \"--verbose\", description = \"Lists all connections with additional details (client options, authenticator-specific metadata and more)\")\n    private boolean verbose = false;\n\n    @Override\n    public void execute(NodeProbe probe)\n    {\n        PrintStream out = probe.output().out;\n        if (clearConnectionHistory)\n        {\n            out.println(\"Clearing connection history\");\n            probe.clearConnectionHistory();\n            return;\n        }\n\n        if (connectionsByProtocolVersion)\n        {\n            SimpleDateFormat sdf = new SimpleDateFormat(\"MMM dd, yyyy HH:mm:ss\");\n\n            out.println(\"Clients by protocol version\");\n            out.println();\n\n            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"clientsByProtocolVersion\");\n\n            if (!clients.isEmpty())\n            {\n                TableBuilder table = new TableBuilder();\n                table.add(\"Protocol-Version\", \"IP-Address\", \"Last-Seen\");\n\n                for (Map<String, String> client : clients)\n                {\n                    table.add(client.get(ClientStat.PROTOCOL_VERSION),\n                              client.get(ClientStat.INET_ADDRESS),\n                              sdf.format(new Date(Long.valueOf(client.get(ClientStat.LAST_SEEN_TIME)))));\n                }\n\n                table.printTo(out);\n                out.println();\n            }\n\n            return;\n        }\n\n        // Note: for compatbility with existing implementation if someone passes --all (listConnections),\n        // --client-options, and --metadata all three will be printed.\n        List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");\n        if (!clients.isEmpty() && (listConnections || clientOptions || verbose))\n        {\n            ImmutableList.Builder<String> tableHeaderBuilder = ImmutableList.<String>builder()\n                                                                            .add(\"Address\", \"SSL\", \"Cipher\", \"Protocol\", \"Version\",\n                                                                                 \"User\", \"Keyspace\", \"Requests\", \"Driver-Name\",\n                                                                                 \"Driver-Version\");\n            ImmutableList.Builder<String> tableFieldsBuilder = ImmutableList.<String>builder()\n                                                                            .add(ConnectedClient.ADDRESS, ConnectedClient.SSL,\n                                                                                 ConnectedClient.CIPHER, ConnectedClient.PROTOCOL,\n                                                                                 ConnectedClient.VERSION, ConnectedClient.USER,\n                                                                                 ConnectedClient.KEYSPACE, ConnectedClient.REQUESTS,\n                                                                                 ConnectedClient.DRIVER_NAME, ConnectedClient.DRIVER_VERSION);\n            if (clientOptions || verbose)\n            {\n                tableHeaderBuilder.add(\"Client-Options\");\n                tableFieldsBuilder.add(ConnectedClient.CLIENT_OPTIONS);\n            }\n\n            if (verbose)\n            {\n                tableHeaderBuilder.add(\"Auth-Mode\", \"Auth-Metadata\");\n                tableFieldsBuilder.add(ConnectedClient.AUTHENTICATION_MODE, ConnectedClient.AUTHENTICATION_METADATA);\n            }\n\n            List<String> tableHeader = tableHeaderBuilder.build();\n            List<String> tableFields = tableFieldsBuilder.build();\n            printTable(out, tableHeader, tableFields, clients);\n        }\n\n        Map<String, Integer> connectionsByUser = (Map<String, Integer>) probe.getClientMetric(\"connectedNativeClientsByUser\");\n        int total = connectionsByUser.values().stream().reduce(0, Integer::sum);\n        out.println(\"Total connected clients: \" + total);\n        out.println();\n        TableBuilder table = new TableBuilder();\n        table.add(\"User\", \"Connections\");\n        for (Entry<String, Integer> entry : connectionsByUser.entrySet())\n        {\n            table.add(entry.getKey(), entry.getValue().toString());\n        }\n        table.printTo(out);\n    }\n\n    /**\n     * Convenience function to print a table with the given header and the resolved fields for each connection.\n     *\n     * @param out         print stream to print to.\n     * @param headers     headers for the table\n     * @param tableFields the fields from {@link ConnectedClient} to retrieve from each client connection.\n     * @param clients     the clients to print, each client being a row inthe table.\n     */\n    private void printTable(PrintStream out, List<String> headers, List<String> tableFields, List<Map<String, String>> clients)\n    {\n        TableBuilder table = new TableBuilder();\n        table.add(headers);\n        for (Map<String, String> conn : clients)\n        {\n            List<String> connectionFieldValues = tableFields.stream()\n                    .map(conn::get)\n                    .collect(Collectors.toList());\n            table.add(connectionFieldValues);\n        }\n        table.printTo(out);\n        out.println();\n    }\n}\n","lineNo":121}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.Verb;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.sequences.SingleNodeSequences;\n\nimport static org.junit.Assert.assertTrue;\n\npublic class RemoveNodeTest extends TestBaseImpl\n{\n    @Test\n    public void testAbort() throws Exception\n    {\n        try (Cluster cluster = init(Cluster.build(3)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(3, \"dc0\", \"rack0\"))\n                                           .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                                   .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            cluster.filters().inbound().verbs(Verb.INITIATE_DATA_MOVEMENTS_REQ.id).drop();\n            String nodeId = cluster.get(3).callOnInstance(() -> ClusterMetadata.current().myNodeId().toUUID().toString());\n            cluster.get(3).shutdown().get();\n            Thread t = new Thread(() -> cluster.get(1).nodetoolResult(\"removenode\", nodeId, \"--force\"));\n            t.start();\n            cluster.get(1).logs().watchFor(\"Committed StartLeave\");\n            String stdout = cluster.get(1).nodetoolResult(\"removenode\", \"status\").getStdout();\n            assertTrue(String.format(\"\\\"%s\\\" does not contain MID_LEAVE\", stdout), stdout.contains(\"MID_LEAVE\"));\n\n            cluster.get(1).nodetoolResult(\"removenode\", \"abort\", nodeId).asserts().success();\n            cluster.get(2).logs().watchFor(\"Enacted CancelInProgressSequence\");\n\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(NodeId.fromString(nodeId)) == NodeState.JOINED);\n            });\n\n        }\n    }\n\n    @Test\n    public void removeNodeWithNoStreamingRequired() throws Exception\n    {\n        // 2 node cluster, keyspaces have RF=2. If we removenode(node2), then no outbound streams are created\n        // as all the data is already fully replicated to node1. In this case, ensure that RemoveNodeStreams\n        // doesn't hang indefinitely.\n        try (Cluster cluster = init(Cluster.build(2)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(2, \"dc0\", \"rack0\"))\n                                           .withConfig(config -> config.set(\"default_keyspace_rf\", \"2\")\n                                                                       .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            int toRemove = cluster.get(2).callOnInstance(() -> ClusterMetadata.current().myNodeId().id());\n            cluster.get(2).shutdown(false).get();\n\n            cluster.get(1).runOnInstance(() -> {\n                NodeId nodeId = new NodeId(toRemove);\n                InetAddressAndPort endpoint = ClusterMetadata.current().directory.endpoint(nodeId);\n                FailureDetector.instance.forceConviction(endpoint);\n                SingleNodeSequences.removeNode(nodeId, true);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(new NodeId(toRemove)) == NodeState.LEFT);\n            });\n        }\n\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.api.IInvokableInstance;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.Epoch;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.sequences.SingleNodeSequences;\nimport org.apache.cassandra.tcm.transformations.PrepareLeave;\n\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.pauseBeforeCommit;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.pauseBeforeEnacting;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.unpauseCommits;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.unpauseEnactment;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.waitForCMSToQuiesce;\nimport static org.junit.Assert.assertTrue;\n\npublic class RemoveNodeTest extends TestBaseImpl\n{\n    @Test\n    public void testAbort() throws Exception\n    {\n        try (Cluster cluster = init(Cluster.build(3)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(3, \"dc0\", \"rack0\"))\n                                           .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                                   .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            IInvokableInstance cmsInstance = cluster.get(1);\n            IInvokableInstance leavingInstance = cluster.get(3);\n            IInvokableInstance otherInstance = cluster.get(2);\n            String nodeId = leavingInstance.callOnInstance(() -> ClusterMetadata.current().myNodeId().toUUID().toString());\n            leavingInstance.shutdown().get();\n\n            // Have the CMS node pause before the mid-leave step is committed, then initiate the removal. Make a note\n            // of the _next_ epoch (i.e. when the mid-leave will be enacted), so we can pause before it is enacted\n            Callable<Epoch> pending = pauseBeforeCommit(cmsInstance, (e) -> e instanceof PrepareLeave.MidLeave);\n            Thread t = new Thread(() -> cluster.get(1).nodetoolResult(\"removenode\", nodeId, \"--force\"));\n            t.start();\n            Epoch pauseBeforeEnacting = pending.call().nextEpoch();\n\n            // Check the status of the removal\n            String stdout = cluster.get(1).nodetoolResult(\"removenode\", \"status\").getStdout();\n            assertTrue(String.format(\"\\\"%s\\\" does not contain correct node id\", stdout), stdout.contains(\"Removing node \" + nodeId));\n            assertTrue(String.format(\"\\\"%s\\\" does not contain MID_LEAVE\", stdout), stdout.contains(\"step: MID_LEAVE\"));\n\n            // Allow the CMS to proceed with committing the mid-leave step, but make the other node pause before\n            // enacting it so we can abort the removal before the final step (FINISH_LEAVE) is committed\n            Callable<?> beforeEnacted = pauseBeforeEnacting(otherInstance, pauseBeforeEnacting);\n            unpauseCommits(cmsInstance);\n            beforeEnacted.call();\n\n            // Now abort the removal. This should succeed in committing a cancellation of the removal sequence before\n            // it can be completed, as non-CMS instance is still paused.\n            cmsInstance.nodetoolResult(\"removenode\", \"abort\", nodeId).asserts().success();\n\n            // Resume processing on the non-CMS instance. It will enact the MID_LEAVE step followed by the cancellation\n            // of the removal process.\n            unpauseEnactment(otherInstance);\n            otherInstance.logs().watchFor(\"Enacted CancelInProgressSequence\");\n\n            // Finally, validate that cluster metadata is in the expected state - i.e. the \"leaving\" node is still joined\n            waitForCMSToQuiesce(cluster, cmsInstance, leavingInstance.config().num());\n            for (IInvokableInstance instance : new IInvokableInstance[] {cmsInstance, otherInstance})\n            {\n                instance.runOnInstance(() -> {\n                    ClusterMetadata metadata = ClusterMetadata.current();\n                    assertTrue(metadata.inProgressSequences.isEmpty());\n                    assertTrue(metadata.directory.peerState(NodeId.fromString(nodeId)) == NodeState.JOINED);\n                });\n            }\n        }\n    }\n\n    @Test\n    public void removeNodeWithNoStreamingRequired() throws Exception\n    {\n        // 2 node cluster, keyspaces have RF=2. If we removenode(node2), then no outbound streams are created\n        // as all the data is already fully replicated to node1. In this case, ensure that RemoveNodeStreams\n        // doesn't hang indefinitely.\n        try (Cluster cluster = init(Cluster.build(2)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(2, \"dc0\", \"rack0\"))\n                                           .withConfig(config -> config.set(\"default_keyspace_rf\", \"2\")\n                                                                       .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            int toRemove = cluster.get(2).callOnInstance(() -> ClusterMetadata.current().myNodeId().id());\n            cluster.get(2).shutdown(false).get();\n\n            cluster.get(1).runOnInstance(() -> {\n                NodeId nodeId = new NodeId(toRemove);\n                InetAddressAndPort endpoint = ClusterMetadata.current().directory.endpoint(nodeId);\n                FailureDetector.instance.forceConviction(endpoint);\n                SingleNodeSequences.removeNode(nodeId, true);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(new NodeId(toRemove)) == NodeState.LEFT);\n            });\n        }\n\n    }\n}\n","lineNo":57}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.Verb;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.sequences.SingleNodeSequences;\n\nimport static org.junit.Assert.assertTrue;\n\npublic class RemoveNodeTest extends TestBaseImpl\n{\n    @Test\n    public void testAbort() throws Exception\n    {\n        try (Cluster cluster = init(Cluster.build(3)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(3, \"dc0\", \"rack0\"))\n                                           .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                                   .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            cluster.filters().inbound().verbs(Verb.INITIATE_DATA_MOVEMENTS_REQ.id).drop();\n            String nodeId = cluster.get(3).callOnInstance(() -> ClusterMetadata.current().myNodeId().toUUID().toString());\n            cluster.get(3).shutdown().get();\n            Thread t = new Thread(() -> cluster.get(1).nodetoolResult(\"removenode\", nodeId, \"--force\"));\n            t.start();\n            cluster.get(1).logs().watchFor(\"Committed StartLeave\");\n            String stdout = cluster.get(1).nodetoolResult(\"removenode\", \"status\").getStdout();\n            assertTrue(String.format(\"\\\"%s\\\" does not contain MID_LEAVE\", stdout), stdout.contains(\"MID_LEAVE\"));\n\n            cluster.get(1).nodetoolResult(\"removenode\", \"abort\", nodeId).asserts().success();\n            cluster.get(2).logs().watchFor(\"Enacted CancelInProgressSequence\");\n\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(NodeId.fromString(nodeId)) == NodeState.JOINED);\n            });\n\n        }\n    }\n\n    @Test\n    public void removeNodeWithNoStreamingRequired() throws Exception\n    {\n        // 2 node cluster, keyspaces have RF=2. If we removenode(node2), then no outbound streams are created\n        // as all the data is already fully replicated to node1. In this case, ensure that RemoveNodeStreams\n        // doesn't hang indefinitely.\n        try (Cluster cluster = init(Cluster.build(2)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(2, \"dc0\", \"rack0\"))\n                                           .withConfig(config -> config.set(\"default_keyspace_rf\", \"2\")\n                                                                       .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            int toRemove = cluster.get(2).callOnInstance(() -> ClusterMetadata.current().myNodeId().id());\n            cluster.get(2).shutdown(false).get();\n\n            cluster.get(1).runOnInstance(() -> {\n                NodeId nodeId = new NodeId(toRemove);\n                InetAddressAndPort endpoint = ClusterMetadata.current().directory.endpoint(nodeId);\n                FailureDetector.instance.forceConviction(endpoint);\n                SingleNodeSequences.removeNode(nodeId, true);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(new NodeId(toRemove)) == NodeState.LEFT);\n            });\n        }\n\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.api.IInvokableInstance;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.Epoch;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.sequences.SingleNodeSequences;\nimport org.apache.cassandra.tcm.transformations.PrepareLeave;\n\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.pauseBeforeCommit;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.pauseBeforeEnacting;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.unpauseCommits;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.unpauseEnactment;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.waitForCMSToQuiesce;\nimport static org.junit.Assert.assertTrue;\n\npublic class RemoveNodeTest extends TestBaseImpl\n{\n    @Test\n    public void testAbort() throws Exception\n    {\n        try (Cluster cluster = init(Cluster.build(3)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(3, \"dc0\", \"rack0\"))\n                                           .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                                   .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            IInvokableInstance cmsInstance = cluster.get(1);\n            IInvokableInstance leavingInstance = cluster.get(3);\n            IInvokableInstance otherInstance = cluster.get(2);\n            String nodeId = leavingInstance.callOnInstance(() -> ClusterMetadata.current().myNodeId().toUUID().toString());\n            leavingInstance.shutdown().get();\n\n            // Have the CMS node pause before the mid-leave step is committed, then initiate the removal. Make a note\n            // of the _next_ epoch (i.e. when the mid-leave will be enacted), so we can pause before it is enacted\n            Callable<Epoch> pending = pauseBeforeCommit(cmsInstance, (e) -> e instanceof PrepareLeave.MidLeave);\n            Thread t = new Thread(() -> cluster.get(1).nodetoolResult(\"removenode\", nodeId, \"--force\"));\n            t.start();\n            Epoch pauseBeforeEnacting = pending.call().nextEpoch();\n\n            // Check the status of the removal\n            String stdout = cluster.get(1).nodetoolResult(\"removenode\", \"status\").getStdout();\n            assertTrue(String.format(\"\\\"%s\\\" does not contain correct node id\", stdout), stdout.contains(\"Removing node \" + nodeId));\n            assertTrue(String.format(\"\\\"%s\\\" does not contain MID_LEAVE\", stdout), stdout.contains(\"step: MID_LEAVE\"));\n\n            // Allow the CMS to proceed with committing the mid-leave step, but make the other node pause before\n            // enacting it so we can abort the removal before the final step (FINISH_LEAVE) is committed\n            Callable<?> beforeEnacted = pauseBeforeEnacting(otherInstance, pauseBeforeEnacting);\n            unpauseCommits(cmsInstance);\n            beforeEnacted.call();\n\n            // Now abort the removal. This should succeed in committing a cancellation of the removal sequence before\n            // it can be completed, as non-CMS instance is still paused.\n            cmsInstance.nodetoolResult(\"removenode\", \"abort\", nodeId).asserts().success();\n\n            // Resume processing on the non-CMS instance. It will enact the MID_LEAVE step followed by the cancellation\n            // of the removal process.\n            unpauseEnactment(otherInstance);\n            otherInstance.logs().watchFor(\"Enacted CancelInProgressSequence\");\n\n            // Finally, validate that cluster metadata is in the expected state - i.e. the \"leaving\" node is still joined\n            waitForCMSToQuiesce(cluster, cmsInstance, leavingInstance.config().num());\n            for (IInvokableInstance instance : new IInvokableInstance[] {cmsInstance, otherInstance})\n            {\n                instance.runOnInstance(() -> {\n                    ClusterMetadata metadata = ClusterMetadata.current();\n                    assertTrue(metadata.inProgressSequences.isEmpty());\n                    assertTrue(metadata.directory.peerState(NodeId.fromString(nodeId)) == NodeState.JOINED);\n                });\n            }\n        }\n    }\n\n    @Test\n    public void removeNodeWithNoStreamingRequired() throws Exception\n    {\n        // 2 node cluster, keyspaces have RF=2. If we removenode(node2), then no outbound streams are created\n        // as all the data is already fully replicated to node1. In this case, ensure that RemoveNodeStreams\n        // doesn't hang indefinitely.\n        try (Cluster cluster = init(Cluster.build(2)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(2, \"dc0\", \"rack0\"))\n                                           .withConfig(config -> config.set(\"default_keyspace_rf\", \"2\")\n                                                                       .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            int toRemove = cluster.get(2).callOnInstance(() -> ClusterMetadata.current().myNodeId().id());\n            cluster.get(2).shutdown(false).get();\n\n            cluster.get(1).runOnInstance(() -> {\n                NodeId nodeId = new NodeId(toRemove);\n                InetAddressAndPort endpoint = ClusterMetadata.current().directory.endpoint(nodeId);\n                FailureDetector.instance.forceConviction(endpoint);\n                SingleNodeSequences.removeNode(nodeId, true);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(new NodeId(toRemove)) == NodeState.LEFT);\n            });\n        }\n\n    }\n}\n","lineNo":58}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.net.Verb;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.sequences.SingleNodeSequences;\n\nimport static org.junit.Assert.assertTrue;\n\npublic class RemoveNodeTest extends TestBaseImpl\n{\n    @Test\n    public void testAbort() throws Exception\n    {\n        try (Cluster cluster = init(Cluster.build(3)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(3, \"dc0\", \"rack0\"))\n                                           .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                                   .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            cluster.filters().inbound().verbs(Verb.INITIATE_DATA_MOVEMENTS_REQ.id).drop();\n            String nodeId = cluster.get(3).callOnInstance(() -> ClusterMetadata.current().myNodeId().toUUID().toString());\n            cluster.get(3).shutdown().get();\n            Thread t = new Thread(() -> cluster.get(1).nodetoolResult(\"removenode\", nodeId, \"--force\"));\n            t.start();\n            cluster.get(1).logs().watchFor(\"Committed StartLeave\");\n            String stdout = cluster.get(1).nodetoolResult(\"removenode\", \"status\").getStdout();\n            assertTrue(String.format(\"\\\"%s\\\" does not contain MID_LEAVE\", stdout), stdout.contains(\"MID_LEAVE\"));\n\n            cluster.get(1).nodetoolResult(\"removenode\", \"abort\", nodeId).asserts().success();\n            cluster.get(2).logs().watchFor(\"Enacted CancelInProgressSequence\");\n\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(NodeId.fromString(nodeId)) == NodeState.JOINED);\n            });\n\n        }\n    }\n\n    @Test\n    public void removeNodeWithNoStreamingRequired() throws Exception\n    {\n        // 2 node cluster, keyspaces have RF=2. If we removenode(node2), then no outbound streams are created\n        // as all the data is already fully replicated to node1. In this case, ensure that RemoveNodeStreams\n        // doesn't hang indefinitely.\n        try (Cluster cluster = init(Cluster.build(2)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(2, \"dc0\", \"rack0\"))\n                                           .withConfig(config -> config.set(\"default_keyspace_rf\", \"2\")\n                                                                       .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            int toRemove = cluster.get(2).callOnInstance(() -> ClusterMetadata.current().myNodeId().id());\n            cluster.get(2).shutdown(false).get();\n\n            cluster.get(1).runOnInstance(() -> {\n                NodeId nodeId = new NodeId(toRemove);\n                InetAddressAndPort endpoint = ClusterMetadata.current().directory.endpoint(nodeId);\n                FailureDetector.instance.forceConviction(endpoint);\n                SingleNodeSequences.removeNode(nodeId, true);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(new NodeId(toRemove)) == NodeState.LEFT);\n            });\n        }\n\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.distributed.test;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.distributed.Cluster;\nimport org.apache.cassandra.distributed.api.Feature;\nimport org.apache.cassandra.distributed.api.IInvokableInstance;\nimport org.apache.cassandra.distributed.shared.NetworkTopology;\nimport org.apache.cassandra.gms.FailureDetector;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.Epoch;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeState;\nimport org.apache.cassandra.tcm.sequences.SingleNodeSequences;\nimport org.apache.cassandra.tcm.transformations.PrepareLeave;\n\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.pauseBeforeCommit;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.pauseBeforeEnacting;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.unpauseCommits;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.unpauseEnactment;\nimport static org.apache.cassandra.distributed.shared.ClusterUtils.waitForCMSToQuiesce;\nimport static org.junit.Assert.assertTrue;\n\npublic class RemoveNodeTest extends TestBaseImpl\n{\n    @Test\n    public void testAbort() throws Exception\n    {\n        try (Cluster cluster = init(Cluster.build(3)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(3, \"dc0\", \"rack0\"))\n                                           .withConfig(conf -> conf.set(\"hinted_handoff_enabled\", \"false\")\n                                                                   .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            IInvokableInstance cmsInstance = cluster.get(1);\n            IInvokableInstance leavingInstance = cluster.get(3);\n            IInvokableInstance otherInstance = cluster.get(2);\n            String nodeId = leavingInstance.callOnInstance(() -> ClusterMetadata.current().myNodeId().toUUID().toString());\n            leavingInstance.shutdown().get();\n\n            // Have the CMS node pause before the mid-leave step is committed, then initiate the removal. Make a note\n            // of the _next_ epoch (i.e. when the mid-leave will be enacted), so we can pause before it is enacted\n            Callable<Epoch> pending = pauseBeforeCommit(cmsInstance, (e) -> e instanceof PrepareLeave.MidLeave);\n            Thread t = new Thread(() -> cluster.get(1).nodetoolResult(\"removenode\", nodeId, \"--force\"));\n            t.start();\n            Epoch pauseBeforeEnacting = pending.call().nextEpoch();\n\n            // Check the status of the removal\n            String stdout = cluster.get(1).nodetoolResult(\"removenode\", \"status\").getStdout();\n            assertTrue(String.format(\"\\\"%s\\\" does not contain correct node id\", stdout), stdout.contains(\"Removing node \" + nodeId));\n            assertTrue(String.format(\"\\\"%s\\\" does not contain MID_LEAVE\", stdout), stdout.contains(\"step: MID_LEAVE\"));\n\n            // Allow the CMS to proceed with committing the mid-leave step, but make the other node pause before\n            // enacting it so we can abort the removal before the final step (FINISH_LEAVE) is committed\n            Callable<?> beforeEnacted = pauseBeforeEnacting(otherInstance, pauseBeforeEnacting);\n            unpauseCommits(cmsInstance);\n            beforeEnacted.call();\n\n            // Now abort the removal. This should succeed in committing a cancellation of the removal sequence before\n            // it can be completed, as non-CMS instance is still paused.\n            cmsInstance.nodetoolResult(\"removenode\", \"abort\", nodeId).asserts().success();\n\n            // Resume processing on the non-CMS instance. It will enact the MID_LEAVE step followed by the cancellation\n            // of the removal process.\n            unpauseEnactment(otherInstance);\n            otherInstance.logs().watchFor(\"Enacted CancelInProgressSequence\");\n\n            // Finally, validate that cluster metadata is in the expected state - i.e. the \"leaving\" node is still joined\n            waitForCMSToQuiesce(cluster, cmsInstance, leavingInstance.config().num());\n            for (IInvokableInstance instance : new IInvokableInstance[] {cmsInstance, otherInstance})\n            {\n                instance.runOnInstance(() -> {\n                    ClusterMetadata metadata = ClusterMetadata.current();\n                    assertTrue(metadata.inProgressSequences.isEmpty());\n                    assertTrue(metadata.directory.peerState(NodeId.fromString(nodeId)) == NodeState.JOINED);\n                });\n            }\n        }\n    }\n\n    @Test\n    public void removeNodeWithNoStreamingRequired() throws Exception\n    {\n        // 2 node cluster, keyspaces have RF=2. If we removenode(node2), then no outbound streams are created\n        // as all the data is already fully replicated to node1. In this case, ensure that RemoveNodeStreams\n        // doesn't hang indefinitely.\n        try (Cluster cluster = init(Cluster.build(2)\n                                           .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(2, \"dc0\", \"rack0\"))\n                                           .withConfig(config -> config.set(\"default_keyspace_rf\", \"2\")\n                                                                       .with(Feature.NETWORK, Feature.GOSSIP))\n                                           .start()))\n        {\n            int toRemove = cluster.get(2).callOnInstance(() -> ClusterMetadata.current().myNodeId().id());\n            cluster.get(2).shutdown(false).get();\n\n            cluster.get(1).runOnInstance(() -> {\n                NodeId nodeId = new NodeId(toRemove);\n                InetAddressAndPort endpoint = ClusterMetadata.current().directory.endpoint(nodeId);\n                FailureDetector.instance.forceConviction(endpoint);\n                SingleNodeSequences.removeNode(nodeId, true);\n            });\n\n            cluster.get(1).runOnInstance(() -> {\n                ClusterMetadata metadata = ClusterMetadata.current();\n                assertTrue(metadata.inProgressSequences.isEmpty());\n                assertTrue(metadata.directory.peerState(new NodeId(toRemove)) == NodeState.LEFT);\n            });\n        }\n\n    }\n}\n","lineNo":56}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.Sets;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.concurrent.ScheduledExecutors;\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.io.util.FileInputStreamPlus;\nimport org.apache.cassandra.io.util.FileOutputStreamPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.metrics.TCMMetrics;\nimport org.apache.cassandra.net.IVerbHandler;\nimport org.apache.cassandra.schema.DistributedSchema;\nimport org.apache.cassandra.schema.ReplicationParams;\nimport org.apache.cassandra.tcm.log.Entry;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogState;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.log.Replication;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeVersion;\nimport org.apache.cassandra.tcm.migration.Election;\nimport org.apache.cassandra.tcm.migration.GossipProcessor;\nimport org.apache.cassandra.tcm.ownership.PlacementProvider;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.sequences.InProgressSequences;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.serialization.VerboseMetadataSerializer;\nimport org.apache.cassandra.tcm.serialization.Version;\nimport org.apache.cassandra.tcm.transformations.ForceSnapshot;\nimport org.apache.cassandra.tcm.transformations.SealPeriod;\nimport org.apache.cassandra.tcm.transformations.cms.PrepareCMSReconfiguration;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.apache.cassandra.utils.JVMStabilityInspector;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.concurrent.AsyncPromise;\nimport org.apache.cassandra.utils.concurrent.Future;\nimport org.apache.cassandra.utils.concurrent.ImmediateFuture;\n\nimport static java.util.concurrent.TimeUnit.NANOSECONDS;\nimport static java.util.stream.Collectors.toSet;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.GOSSIP;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.LOCAL;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.REMOTE;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.RESET;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.emptyWithSchemaFromSystemTables;\nimport static org.apache.cassandra.utils.Clock.Global.nanoTime;\nimport static org.apache.cassandra.utils.Collectors3.toImmutableSet;\n\npublic class ClusterMetadataService\n{\n    private static final Logger logger = LoggerFactory.getLogger(ClusterMetadataService.class);\n\n    private static ClusterMetadataService instance;\n    private static Throwable trace;\n\n    public static void setInstance(ClusterMetadataService newInstance)\n    {\n        if (instance != null)\n            throw new IllegalStateException(String.format(\"Cluster metadata is already initialized to %s.\", instance),\n                                            trace);\n        instance = newInstance;\n        trace = new RuntimeException(\"Previously initialized trace\");\n    }\n\n    @VisibleForTesting\n    public static ClusterMetadataService unsetInstance()\n    {\n        ClusterMetadataService tmp = instance();\n        instance = null;\n        return tmp;\n    }\n\n    public static ClusterMetadataService instance()\n    {\n        return instance;\n    }\n\n    private final PlacementProvider placementProvider;\n    private final Processor processor;\n    private final LocalLog log;\n    private final MetadataSnapshots snapshots;\n\n    private final IVerbHandler<Replication> replicationHandler;\n    private final IVerbHandler<LogState> logNotifyHandler;\n    private final IVerbHandler<FetchCMSLog> fetchLogHandler;\n    private final IVerbHandler<Commit> commitRequestHandler;\n    private final CurrentEpochRequestHandler currentEpochHandler;\n\n    private final PeerLogFetcher peerLogFetcher;\n\n    private final AtomicBoolean commitsPaused = new AtomicBoolean();\n\n    public static State state()\n    {\n        return state(ClusterMetadata.current());\n    }\n\n    public static State state(ClusterMetadata metadata)\n    {\n        if (CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.isPresent())\n            return RESET;\n\n        if (metadata.epoch.isBefore(Epoch.EMPTY))\n            return GOSSIP;\n\n        // The node is a full member of the CMS if it has started participating in reads for distributed metadata table (which\n        // implies it is a write replica as well). In other words, it's a fully joined member of the replica set responsible for\n        // the distributed metadata table.\n        if (ClusterMetadata.current().isCMSMember(FBUtilities.getBroadcastAddressAndPort()))\n            return LOCAL;\n        return REMOTE;\n    }\n\n    ClusterMetadataService(PlacementProvider placementProvider,\n                           Function<Processor, Processor> wrapProcessor,\n                           Supplier<State> cmsStateSupplier,\n                           LocalLog.LogSpec logSpec)\n    {\n        this.placementProvider = placementProvider;\n        this.snapshots = new MetadataSnapshots.SystemKeyspaceMetadataSnapshots();\n\n        Processor localProcessor;\n        LogStorage logStorage = LogStorage.SystemKeyspace;\n        if (CassandraRelevantProperties.TCM_USE_ATOMIC_LONG_PROCESSOR.getBoolean())\n        {\n            log = LocalLog.sync(logSpec);\n            localProcessor = wrapProcessor.apply(new AtomicLongBackedProcessor(log, logSpec.isReset()));\n            fetchLogHandler = new FetchCMSLog.Handler((e, ignored) -> logStorage.getLogState(e));\n        }\n        else\n        {\n            log = LocalLog.async(logSpec);\n            localProcessor = wrapProcessor.apply(new PaxosBackedProcessor(log));\n            fetchLogHandler = new FetchCMSLog.Handler();\n        }\n\n        Commit.Replicator replicator = CassandraRelevantProperties.TCM_USE_NO_OP_REPLICATOR.getBoolean()\n                                       ? Commit.Replicator.NO_OP\n                                       : new Commit.DefaultReplicator(() -> log.metadata().directory);\n\n        RemoteProcessor remoteProcessor = new RemoteProcessor(log, Discovery.instance::discoveredNodes);\n        GossipProcessor gossipProcessor = new GossipProcessor();\n        currentEpochHandler = new CurrentEpochRequestHandler();\n\n        commitRequestHandler = new Commit.Handler(localProcessor, replicator, cmsStateSupplier);\n        processor = new SwitchableProcessor(localProcessor,\n                                            remoteProcessor,\n                                            gossipProcessor,\n                                            replicator,\n                                            cmsStateSupplier);\n\n\n        replicationHandler = new Replication.ReplicationHandler(log);\n        logNotifyHandler = new Replication.LogNotifyHandler(log);\n        peerLogFetcher = new PeerLogFetcher(log);\n    }\n\n    @VisibleForTesting\n    // todo: convert this to a factory method with an obvious name that this is just for testing\n    public ClusterMetadataService(PlacementProvider placementProvider,\n                                  MetadataSnapshots snapshots,\n                                  LocalLog log,\n                                  Processor processor,\n                                  Commit.Replicator replicator,\n                                  boolean isMemberOfOwnershipGroup)\n    {\n        this.placementProvider = placementProvider;\n        this.log = log;\n        this.processor = new SwitchableProcessor(processor, null, null, replicator, () -> State.LOCAL);\n        this.snapshots = snapshots;\n\n        replicationHandler = new Replication.ReplicationHandler(log);\n        logNotifyHandler = new Replication.LogNotifyHandler(log);\n        currentEpochHandler = new CurrentEpochRequestHandler();\n\n        fetchLogHandler = isMemberOfOwnershipGroup ? new FetchCMSLog.Handler() : null;\n        commitRequestHandler = isMemberOfOwnershipGroup ? new Commit.Handler(processor, replicator, () -> LOCAL) : null;\n\n        peerLogFetcher = new PeerLogFetcher(log);\n    }\n\n    private ClusterMetadataService(PlacementProvider placementProvider,\n                                   MetadataSnapshots snapshots,\n                                   LocalLog log,\n                                   Processor processor,\n                                   Replication.ReplicationHandler replicationHandler,\n                                   Replication.LogNotifyHandler logNotifyHandler,\n                                   CurrentEpochRequestHandler currentEpochHandler,\n                                   FetchCMSLog.Handler fetchLogHandler,\n                                   Commit.Handler commitRequestHandler,\n                                   PeerLogFetcher peerLogFetcher)\n    {\n        this.placementProvider = placementProvider;\n        this.snapshots = snapshots;\n        this.log = log;\n        this.processor = processor;\n        this.replicationHandler = replicationHandler;\n        this.logNotifyHandler = logNotifyHandler;\n        this.currentEpochHandler = currentEpochHandler;\n        this.fetchLogHandler = fetchLogHandler;\n        this.commitRequestHandler = commitRequestHandler;\n        this.peerLogFetcher = peerLogFetcher;\n    }\n\n    @SuppressWarnings(\"resource\")\n    public static void initializeForTools(boolean loadSSTables)\n    {\n        if (instance != null)\n            return;\n        ClusterMetadata emptyFromSystemTables = emptyWithSchemaFromSystemTables(Collections.singleton(\"DC1\"));\n        emptyFromSystemTables.schema.initializeKeyspaceInstances(DistributedSchema.empty(), loadSSTables);\n        emptyFromSystemTables = emptyFromSystemTables.forceEpoch(Epoch.EMPTY);\n        LocalLog.LogSpec logSpec = new LocalLog.LogSpec().withInitialState(emptyFromSystemTables)\n                                                         .withStorage(new AtomicLongBackedProcessor.InMemoryStorage());\n        LocalLog log = LocalLog.sync(logSpec);\n        log.ready();\n        ClusterMetadataService cms = new ClusterMetadataService(new UniformRangePlacement(),\n                                                                MetadataSnapshots.NO_OP,\n                                                                log,\n                                                                new AtomicLongBackedProcessor(log),\n                                                                new Replication.ReplicationHandler(log),\n                                                                new Replication.LogNotifyHandler(log),\n                                                                new CurrentEpochRequestHandler(),\n                                                                null,\n                                                                null,\n                                                                new PeerLogFetcher(log));\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        ClusterMetadataService.setInstance(cms);\n    }\n\n    @SuppressWarnings(\"resource\")\n    public static void initializeForClients()\n    {\n        if (instance != null)\n            return;\n\n        ClusterMetadataService.setInstance(StubClusterMetadataService.forClientTools());\n    }\n\n    public static void initializeForClients(DistributedSchema initialSchema)\n    {\n        if (instance != null)\n            return;\n\n        ClusterMetadataService.setInstance(StubClusterMetadataService.forClientTools(initialSchema));\n    }\n\n    public boolean isCurrentMember(InetAddressAndPort peer)\n    {\n        return ClusterMetadata.current().isCMSMember(peer);\n    }\n\n    public void upgradeFromGossip(List<String> ignoredEndpoints)\n    {\n        Set<InetAddressAndPort> ignored = ignoredEndpoints.stream().map(InetAddressAndPort::getByNameUnchecked).collect(toSet());\n        if (ignored.contains(FBUtilities.getBroadcastAddressAndPort()))\n        {\n            String msg = String.format(\"Can't ignore local host %s when doing CMS migration\", FBUtilities.getBroadcastAddressAndPort());\n            logger.error(msg);\n            throw new IllegalStateException(msg);\n        }\n\n        ClusterMetadata metadata = metadata();\n        Set<InetAddressAndPort> existingMembers = metadata.fullCMSMembers();\n\n        if (!metadata.directory.allAddresses().containsAll(ignored))\n        {\n            Set<InetAddressAndPort> allAddresses = Sets.newHashSet(metadata.directory.allAddresses());\n            String msg = String.format(\"Ignored host(s) %s don't exist in the cluster\", Sets.difference(ignored, allAddresses));\n            logger.error(msg);\n            throw new IllegalStateException(msg);\n        }\n\n        for (Map.Entry<NodeId, NodeVersion> entry : metadata.directory.versions.entrySet())\n        {\n            NodeVersion version = entry.getValue();\n            InetAddressAndPort ep = metadata.directory.getNodeAddresses(entry.getKey()).broadcastAddress;\n            if (ignored.contains(ep))\n            {\n                // todo; what do we do if an endpoint has a mismatching gossip-clustermetadata?\n                //       - we could add the node to --ignore and force this CM to it?\n                //       - require operator to bounce/manually fix the CM on that node\n                //       for now just requiring that any ignored host is also down\n//                if (FailureDetector.instance.isAlive(ep))\n//                    throw new IllegalStateException(\"Can't ignore \" + ep + \" during CMS migration - it is not down\");\n                logger.info(\"Endpoint {} running {} is ignored\", ep, version);\n                continue;\n            }\n\n            if (!version.isUpgraded())\n            {\n                String msg = String.format(\"All nodes are not yet upgraded - %s is running %s\", metadata.directory.endpoint(entry.getKey()), version);\n                logger.error(msg);\n                throw new IllegalStateException(msg);\n            }\n        }\n\n        if (existingMembers.isEmpty())\n        {\n            logger.info(\"First CMS node\");\n            Set<InetAddressAndPort> candidates = metadata\n                                                 .directory\n                                                 .allAddresses()\n                                                 .stream()\n                                                 .filter(ep -> !FBUtilities.getBroadcastAddressAndPort().equals(ep) &&\n                                                               !ignored.contains(ep))\n                                                 .collect(toImmutableSet());\n\n            Election.instance.nominateSelf(candidates, ignored, metadata::equals, metadata);\n            ClusterMetadataService.instance().sealPeriod();\n        }\n        else\n        {\n            throw new IllegalStateException(\"Can't upgrade from gossip since CMS is already initialized\");\n        }\n    }\n\n    // This method is to be used _only_ for interactive purposes (i.e. nodetool), since it assumes no retries are going to be attempted on reject.\n    public void reconfigureCMS(ReplicationParams replicationParams)\n    {\n        Transformation transformation = new PrepareCMSReconfiguration.Complex(replicationParams);\n\n        ClusterMetadataService.instance()\n                              .commit(transformation);\n\n        InProgressSequences.finishInProgressSequences(ReconfigureCMS.SequenceKey.instance);\n    }\n\n    public boolean applyFromGossip(ClusterMetadata expected, ClusterMetadata updated)\n    {\n        logger.debug(\"Applying from gossip, current={} new={}\", expected, updated);\n        if (!expected.epoch.isBefore(Epoch.EMPTY))\n            throw new IllegalStateException(\"Can't apply a ClusterMetadata from gossip with epoch \" + expected.epoch);\n        if (state() != GOSSIP)\n            throw new IllegalStateException(\"Can't apply a ClusterMetadata from gossip when CMSState is not GOSSIP: \" + state());\n\n        return log.unsafeSetCommittedFromGossip(expected, updated);\n    }\n\n    public void setFromGossip(ClusterMetadata fromGossip)\n    {\n        logger.debug(\"Setting from gossip, new={}\", fromGossip);\n        if (state() != GOSSIP)\n            throw new IllegalStateException(\"Can't apply a ClusterMetadata from gossip when CMSState is not GOSSIP: \" + state());\n        log.unsafeSetCommittedFromGossip(fromGossip);\n    }\n\n    public void forceSnapshot(ClusterMetadata snapshot)\n    {\n        commit(new ForceSnapshot(snapshot));\n    }\n\n    public void revertToEpoch(Epoch epoch)\n    {\n        logger.warn(\"Reverting to epoch {}\", epoch);\n        ClusterMetadata metadata = ClusterMetadata.current();\n        ClusterMetadata toApply = transformSnapshot(LogState.getForRecovery(epoch))\n                                  .forceEpoch(metadata.epoch.nextEpoch())\n                                  .forcePeriod(metadata.nextPeriod());\n        forceSnapshot(toApply);\n    }\n\n    /**\n     * dumps the cluster metadata at the given epoch, returns path to the generated file\n     *\n     * if the given Epoch is EMPTY, we dump the current metadata\n     *\n     * @param epoch dump clustermetadata at this epoch\n     * @param transformToEpoch transform the dumped metadata to this epoch\n     * @param version serialisation version\n     */\n    public String dumpClusterMetadata(Epoch epoch, Epoch transformToEpoch, Version version) throws IOException\n    {\n        ClusterMetadata toDump = epoch.isAfter(Epoch.EMPTY)\n                                 ? transformSnapshot(LogState.getForRecovery(epoch))\n                                 : ClusterMetadata.current();\n        toDump = toDump.forceEpoch(transformToEpoch);\n        Path p = Files.createTempFile(\"clustermetadata\", \"dump\");\n        try (FileOutputStreamPlus out = new FileOutputStreamPlus(p))\n        {\n            VerboseMetadataSerializer.serialize(ClusterMetadata.serializer, toDump, out, version);\n        }\n        logger.info(\"Dumped cluster metadata to {}\", p.toString());\n        return p.toString();\n    }\n\n    public void loadClusterMetadata(String file) throws IOException\n    {\n        logger.warn(\"Loading cluster metadata from {}\", file);\n        ClusterMetadata metadata = ClusterMetadata.current();\n        ClusterMetadata toApply = deserializeClusterMetadata(file)\n                                  .forceEpoch(metadata.epoch.nextEpoch())\n                                  .forcePeriod(metadata.nextPeriod());\n        forceSnapshot(toApply);\n    }\n\n    public static ClusterMetadata deserializeClusterMetadata(String file) throws IOException\n    {\n        try (FileInputStreamPlus fisp = new FileInputStreamPlus(file))\n        {\n            return VerboseMetadataSerializer.deserialize(ClusterMetadata.serializer, fisp);\n        }\n    }\n\n    private ClusterMetadata transformSnapshot(LogState state)\n    {\n        ClusterMetadata toApply = state.baseState;\n        for (Entry entry : state.transformations.entries())\n        {\n            Transformation.Result res = entry.transform.execute(toApply);\n            assert res.isSuccess();\n            toApply = res.success().metadata;\n        }\n        return toApply;\n    }\n\n    public final Supplier<Entry.Id> entryIdGen = new Entry.DefaultEntryIdGen();\n\n    public interface CommitSuccessHandler<T>\n    {\n        T accept(ClusterMetadata latest);\n    }\n\n    public interface CommitFailureHandler<T>\n    {\n        T accept(ExceptionCode code, String message);\n    }\n\n    public ClusterMetadata commit(Transformation transform)\n    {\n        return commit(transform,\n                      metadata -> metadata,\n                      (code, message) -> {\n                          throw new IllegalStateException(String.format(\"Can not commit transformation: \\\"%s\\\"(%s).\",\n                                                                        code, message));\n                      });\n    }\n\n    /**\n     * Attempt to commit the transformation (with retries).\n     *\n     * Since we can not rely on reliability of the network or even the fact that the committing node will stay alive\n     * for the duration of commit, we have to allow for subsequent discovery of the transformation effects, which can\n     * be made visible either by replaying the log, or receiving the metadata snapshot.\n     *\n     * In other words, there is no reliable way to find out whether _this particular_ transformation has been executed\n     * while we are allowing replay from snapshot, since even failure response from the CMS does not guarantee\n     * Paxos re-proposal, which would place the transformation into the log during proposal _by some other_ CMS node.\n     *\n     * Protocol does foresee the concept of EntryId that would allow discovery of the committed transformations\n     * without changes to binary protocol, but this change was left out from the initial implementation of TCM.\n     *\n     * @param onFailure handler checks if rejection has resulted from a retry of the same trasformation.\n     */\n    public <T1> T1 commit(Transformation transform, CommitSuccessHandler<T1> onSuccess, CommitFailureHandler<T1> onFailure)\n    {\n        if (commitsPaused.get())\n            throw new IllegalStateException(\"Commits are paused, not trying to commit \" + transform);\n\n        long startTime = nanoTime();\n        // Replay everything in-flight before attempting a commit\n        // We grab highest consecutive epoch here, since we want both local and remote processors to benefit from\n        // discover-own-commits via entry id in case of lost messages (in remote case) and Paxos re-proposals (in local case)\n        Epoch highestConsecutive = log.waitForHighestConsecutive().epoch;\n\n        Commit.Result result = processor.commit(entryIdGen.get(), transform, highestConsecutive);\n\n        try\n        {\n            if (result.isSuccess())\n            {\n                TCMMetrics.instance.commitSuccessLatency.update(nanoTime() - startTime, NANOSECONDS);\n                return onSuccess.accept(awaitAtLeast(result.success().epoch));\n            }\n            else\n            {\n                TCMMetrics.instance.recordCommitFailureLatency(nanoTime() - startTime, NANOSECONDS, result.failure().rejected);\n                return onFailure.accept(result.failure().code, result.failure().message);\n            }\n        }\n        catch (TimeoutException t)\n        {\n            throw new IllegalStateException(String.format(\"Timed out while waiting for the follower to enact the epoch %s\", result.success().epoch), t);\n        }\n        catch (InterruptedException e)\n        {\n            throw new IllegalStateException(\"Couldn't commit the transformation. Is the node shutting down?\", e);\n        }\n    }\n\n    /**\n     * Accessors\n     */\n\n    public static IVerbHandler<Replication> replicationHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.replicationHandler;\n    }\n\n    public static IVerbHandler<LogState> logNotifyHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.logNotifyHandler;\n    }\n\n    public static IVerbHandler<FetchCMSLog> fetchLogRequestHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.fetchLogHandler;\n    }\n\n    public static IVerbHandler<Commit> commitRequestHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.commitRequestHandler;\n    }\n\n    public static CurrentEpochRequestHandler currentEpochRequestHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.currentEpochHandler;\n    }\n\n    public PlacementProvider placementProvider()\n    {\n        return this.placementProvider;\n    }\n\n    @VisibleForTesting\n    public Processor processor()\n    {\n        return processor;\n    }\n\n    @VisibleForTesting\n    public LocalLog log()\n    {\n        return log;\n    }\n\n    public ClusterMetadata metadata()\n    {\n        return log.metadata();\n    }\n\n    /**\n     * Fetches log entries from directly from CMS, at least to the specified epoch.\n     *\n     * This operation is blocking and also waits for all retrieved log entries to be\n     * enacted, so on return all transformations to ClusterMetadata will be visible.\n     * @return metadata with all currently committed entries enacted.\n     */\n    public ClusterMetadata fetchLogFromCMS(Epoch awaitAtLeast)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        if (awaitAtLeast.isBefore(Epoch.FIRST))\n            return metadata;\n\n        Epoch ourEpoch = metadata.epoch;\n\n        if (ourEpoch.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n\n        Retry.Deadline deadline = Retry.Deadline.after(DatabaseDescriptor.getCmsAwaitTimeout().to(TimeUnit.NANOSECONDS),\n                                                       new Retry.Jitter(TCMMetrics.instance.fetchLogRetries));\n        // responses for ALL withhout knowing we have pending\n        metadata = processor.fetchLogAndWait(awaitAtLeast, deadline);\n        if (metadata.epoch.isBefore(awaitAtLeast))\n        {\n            throw new IllegalStateException(String.format(\"Could not catch up to epoch %s even after fetching log from CMS. Highest seen after fetching is %s.\",\n                                                          awaitAtLeast, ourEpoch));\n        }\n        return metadata;\n    }\n\n    /**\n     * Attempts to asynchronously retrieve log entries from a non-CMS peer.\n     * Fetches and applies the log state representing the delta between the current local epoch and the one requested.\n     * This is used when a message from a peer contains an epoch higher than the current local epoch. As the sender of\n     * the message must have seen and enacted the given epoch, they must (under normal circumstances) be able to supply\n     * any entries needed to catch up this node.\n     * When the returned future completes, the metadata it provides is the current published metadata at the\n     * moment of completion. In the expected case, this will have had any fetched transformations up to the requested\n     * epoch applied. If the fetch was unsuccessful (e.g. because the peer was unavailable) it will still be whatever\n     * the currently published metadata, but which entries have been enacted cannot be guaranteed.\n     * @param from peer to request log entries from\n     * @param awaitAtLeast the upper epoch required. It's expected that the peer is able to supply log entries up to at\n     *                     least this epoch.\n     * @return A future which will supply the current ClusterMetadata at the time of completion\n     */\n    public Future<ClusterMetadata> fetchLogFromPeerAsync(InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        ClusterMetadata current = ClusterMetadata.current();\n        if (FBUtilities.getBroadcastAddressAndPort().equals(from) ||\n            current.epoch.isEqualOrAfter(awaitAtLeast) ||\n            awaitAtLeast.isBefore(Epoch.FIRST))\n            return ImmediateFuture.success(current);\n\n        return peerLogFetcher.asyncFetchLog(from, awaitAtLeast);\n    }\n\n    /**\n     *\n     * IMPORTANT: this call can return _without_ catching us up, so should only be used privately.\n     *\n     * Attempts to synchronously retrieve log entries from a non-CMS peer.\n     * Fetches the log state representing the delta between the current local epoch and the one supplied.\n     * This is to be used when a message from a peer contains an epoch higher than the current local epoch. As\n     * sender of the message must have seen and enacted the given epoch, they must (under normal circumstances)\n     * be able to supply any entries needed to catch up this node.\n     * The metadata returned is the current published metadata at that time. In the expected case, this will have had\n     * any fetched transformations up to the requested epoch applied. If the fetch was unsuccessful (e.g. because the\n     * peer was unavailable) it will still be whatever the currently published metadata, but which entries have been\n     * enacted cannot be guaranteed.\n     * @param from peer to request log entries from\n     * @param awaitAtLeast the upper epoch required. It's expected that the peer is able to supply log entries up to at\n     *                     least this epoch.\n     * @return The current ClusterMetadata at the time of completion\n     */\n    private ClusterMetadata fetchLogFromPeer(ClusterMetadata metadata, InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        if (awaitAtLeast.isBefore(Epoch.FIRST) || FBUtilities.getBroadcastAddressAndPort().equals(from))\n            return ClusterMetadata.current();\n        Epoch before = metadata.epoch;\n        if (before.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n        return peerLogFetcher.fetchLogEntriesAndWait(from, awaitAtLeast);\n    }\n\n    public Future<ClusterMetadata> fetchLogFromPeerOrCMSAsync(ClusterMetadata metadata, InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        AsyncPromise<ClusterMetadata> future = new AsyncPromise<>();\n        ScheduledExecutors.optionalTasks.submit(() -> {\n            try\n            {\n                future.setSuccess(ClusterMetadataService.instance().fetchLogFromPeerOrCMS(metadata, from, awaitAtLeast));\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                logger.warn(String.format(\"Learned about epoch %s from %s, but could not fetch log.\", awaitAtLeast, from), t);\n                future.setFailure(t);\n            }\n        });\n        return future;\n    }\n\n    /**\n     * Combines {@link #fetchLogFromPeer} with {@link #fetchLogFromCMS} to synchronously fetch and apply log entries\n     * up to the requested epoch. The supplied peer will be contacted first and if after doing so, the current local\n     * metadata is not caught up to at least the required epoch, a further request is made to the CMS.\n     * The returned ClusterMetadata is guaranteed to have been published, though it may have also been superceded by\n     * further updates.\n     * If the requested epoch is not reached even after fetching from the CMS, an IllegalStateException is thrown.\n     * @param metadata a starting point for the fetch. If the requested epoch is <= the epoch of this metadata, the\n     *                 call is a no-op. It's expected that this is usually the current cluster metadata at the time of\n     *                 calling.\n     * @param from Initial peer to contact. Usually this is the sender of a message containing the requested epoch,\n     *             which means it can be assumed that this peer (if available) can supply any missing log entries.\n     * @param awaitAtLeast The requested epoch.\n     * @return A published ClusterMetadata with all entries up to (at least) the requested epoch enacted.\n     * @throws IllegalStateException if the requested epoch could not be reached, even after falling back to CMS catchup\n     */\n    public ClusterMetadata fetchLogFromPeerOrCMS(ClusterMetadata metadata, InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        if (awaitAtLeast.isBefore(Epoch.FIRST) || FBUtilities.getBroadcastAddressAndPort().equals(from))\n            return metadata;\n\n        Epoch before = metadata.epoch;\n        if (before.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n\n        metadata = fetchLogFromPeer(metadata, from, awaitAtLeast);\n        if (metadata.epoch.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n\n        metadata = fetchLogFromCMS(awaitAtLeast);\n        if (metadata.epoch.isBefore(awaitAtLeast))\n            throw new IllegalStateException(\"Still behind after fetching log from CMS\");\n        logger.debug(\"Fetched log from CMS - caught up from epoch {} to epoch {}\", before, metadata.epoch);\n        return metadata;\n    }\n\n    public ClusterMetadata awaitAtLeast(Epoch epoch) throws InterruptedException, TimeoutException\n    {\n        return log.awaitAtLeast(epoch);\n    }\n\n    public MetadataSnapshots snapshotManager()\n    {\n        return snapshots;\n    }\n\n    public ClusterMetadata sealPeriod()\n    {\n        return ClusterMetadataService.instance.commit(SealPeriod.instance,\n                                                      (ClusterMetadata metadata) -> metadata,\n                                                      (code, reason) -> {\n                                                          // If the transformation got rejected, someone else has beat us to seal this period\n                                                          return ClusterMetadata.current();\n                                                      });\n    }\n\n    public void initRecentlySealedPeriodsIndex()\n    {\n        Sealed.initIndexFromSystemTables();\n    }\n\n    public boolean isMigrating()\n    {\n        return Election.instance.isMigrating();\n    }\n\n    public void migrated()\n    {\n        Election.instance.migrated();\n    }\n    public void pauseCommits()\n    {\n        commitsPaused.set(true);\n    }\n\n    public void resumeCommits()\n    {\n        commitsPaused.set(false);\n    }\n\n    public boolean commitsPaused()\n    {\n        return commitsPaused.get();\n    }\n    /**\n     * Switchable implementation that allow us to go between local and remote implementation whenever we need it.\n     * When the node becomes a member of CMS, it switches back to being a regular member of a cluster, and all\n     * the CMS handlers get disabled.\n     */\n    @VisibleForTesting\n    public static class SwitchableProcessor implements Processor\n    {\n        private final Processor local;\n        private final RemoteProcessor remote;\n        private final GossipProcessor gossip;\n        private final Supplier<State> cmsStateSupplier;\n        private final Commit.Replicator replicator;\n\n        SwitchableProcessor(Processor local,\n                            RemoteProcessor remote,\n                            GossipProcessor gossip,\n                            Commit.Replicator replicator,\n                            Supplier<State> cmsStateSupplier)\n        {\n            this.local = local;\n            this.remote = remote;\n            this.gossip = gossip;\n            this.replicator = replicator;\n            this.cmsStateSupplier = cmsStateSupplier;\n        }\n\n        @VisibleForTesting\n        public Processor delegate()\n        {\n            return delegateInternal().right;\n        }\n\n        private Pair<State, Processor> delegateInternal()\n        {\n            State state = cmsStateSupplier.get();\n            switch (state)\n            {\n                case LOCAL:\n                case RESET:\n                    return Pair.create(state, local);\n                case REMOTE:\n                    return Pair.create(state, remote);\n                case GOSSIP:\n                    return Pair.create(state, gossip);\n            }\n            throw new IllegalStateException(\"Bad CMS state: \" + state);\n        }\n\n        @Override\n        public Commit.Result commit(Entry.Id entryId, Transformation transform, Epoch lastKnown, Retry.Deadline retryPolicy)\n        {\n            Pair<State, Processor> delegate = delegateInternal();\n            Commit.Result result = delegate.right.commit(entryId, transform, lastKnown, retryPolicy);\n            if (delegate.left == LOCAL || delegate.left == RESET)\n                replicator.send(result, null);\n            return result;\n        }\n\n        @Override\n        public ClusterMetadata fetchLogAndWait(Epoch waitFor, Retry.Deadline retryPolicy)\n        {\n            return delegate().fetchLogAndWait(waitFor, retryPolicy);\n        }\n\n        public String toString()\n        {\n            return \"SwitchableProcessor{\" +\n                   delegate() + '}';\n        }\n    }\n\n    public enum State\n    {\n        LOCAL, REMOTE, GOSSIP, RESET\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.Sets;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.concurrent.ScheduledExecutors;\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.exceptions.ExceptionCode;\nimport org.apache.cassandra.exceptions.StartupException;\nimport org.apache.cassandra.io.util.FileInputStreamPlus;\nimport org.apache.cassandra.io.util.FileOutputStreamPlus;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.metrics.TCMMetrics;\nimport org.apache.cassandra.net.IVerbHandler;\nimport org.apache.cassandra.schema.DistributedSchema;\nimport org.apache.cassandra.schema.ReplicationParams;\nimport org.apache.cassandra.tcm.listeners.SchemaListener;\nimport org.apache.cassandra.tcm.log.Entry;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogState;\nimport org.apache.cassandra.tcm.log.Replication;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.membership.NodeVersion;\nimport org.apache.cassandra.tcm.migration.Election;\nimport org.apache.cassandra.tcm.migration.GossipProcessor;\nimport org.apache.cassandra.tcm.ownership.PlacementProvider;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.sequences.InProgressSequences;\nimport org.apache.cassandra.tcm.sequences.ReconfigureCMS;\nimport org.apache.cassandra.tcm.serialization.VerboseMetadataSerializer;\nimport org.apache.cassandra.tcm.serialization.Version;\nimport org.apache.cassandra.tcm.transformations.ForceSnapshot;\nimport org.apache.cassandra.tcm.transformations.SealPeriod;\nimport org.apache.cassandra.tcm.transformations.cms.PrepareCMSReconfiguration;\nimport org.apache.cassandra.utils.FBUtilities;\nimport org.apache.cassandra.utils.JVMStabilityInspector;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.concurrent.AsyncPromise;\nimport org.apache.cassandra.utils.concurrent.Future;\nimport org.apache.cassandra.utils.concurrent.ImmediateFuture;\n\nimport static java.util.concurrent.TimeUnit.NANOSECONDS;\nimport static java.util.stream.Collectors.toSet;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.GOSSIP;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.LOCAL;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.REMOTE;\nimport static org.apache.cassandra.tcm.ClusterMetadataService.State.RESET;\nimport static org.apache.cassandra.tcm.compatibility.GossipHelper.emptyWithSchemaFromSystemTables;\nimport static org.apache.cassandra.utils.Clock.Global.nanoTime;\nimport static org.apache.cassandra.utils.Collectors3.toImmutableSet;\n\npublic class ClusterMetadataService\n{\n    private static final Logger logger = LoggerFactory.getLogger(ClusterMetadataService.class);\n\n    private static ClusterMetadataService instance;\n    private static Throwable trace;\n\n    public static void setInstance(ClusterMetadataService newInstance)\n    {\n        if (instance != null)\n            throw new IllegalStateException(String.format(\"Cluster metadata is already initialized to %s.\", instance),\n                                            trace);\n        instance = newInstance;\n        trace = new RuntimeException(\"Previously initialized trace\");\n    }\n\n    @VisibleForTesting\n    public static ClusterMetadataService unsetInstance()\n    {\n        ClusterMetadataService tmp = instance();\n        instance = null;\n        return tmp;\n    }\n\n    public static ClusterMetadataService instance()\n    {\n        return instance;\n    }\n\n    private final PlacementProvider placementProvider;\n    private final Processor processor;\n    private final LocalLog log;\n    private final MetadataSnapshots snapshots;\n\n    private final IVerbHandler<Replication> replicationHandler;\n    private final IVerbHandler<LogState> logNotifyHandler;\n    private final IVerbHandler<FetchCMSLog> fetchLogHandler;\n    private final IVerbHandler<Commit> commitRequestHandler;\n    private final CurrentEpochRequestHandler currentEpochHandler;\n\n    private final PeerLogFetcher peerLogFetcher;\n\n    private final AtomicBoolean commitsPaused = new AtomicBoolean();\n\n    public static State state()\n    {\n        return state(ClusterMetadata.current());\n    }\n\n    public static State state(ClusterMetadata metadata)\n    {\n        if (CassandraRelevantProperties.TCM_UNSAFE_BOOT_WITH_CLUSTERMETADATA.isPresent())\n            return RESET;\n\n        if (metadata.epoch.isBefore(Epoch.EMPTY))\n            return GOSSIP;\n\n        // The node is a full member of the CMS if it has started participating in reads for distributed metadata table (which\n        // implies it is a write replica as well). In other words, it's a fully joined member of the replica set responsible for\n        // the distributed metadata table.\n        if (ClusterMetadata.current().isCMSMember(FBUtilities.getBroadcastAddressAndPort()))\n            return LOCAL;\n        return REMOTE;\n    }\n\n    ClusterMetadataService(PlacementProvider placementProvider,\n                           Function<Processor, Processor> wrapProcessor,\n                           Supplier<State> cmsStateSupplier,\n                           LocalLog.LogSpec logSpec) throws StartupException\n    {\n        this.placementProvider = placementProvider;\n        this.snapshots = new MetadataSnapshots.SystemKeyspaceMetadataSnapshots();\n\n        Processor localProcessor;\n        if (CassandraRelevantProperties.TCM_USE_ATOMIC_LONG_PROCESSOR.getBoolean())\n        {\n            log = logSpec.sync().createLog();\n            localProcessor = wrapProcessor.apply(new AtomicLongBackedProcessor(log, logSpec.isReset()));\n            fetchLogHandler = new FetchCMSLog.Handler((e, ignored) -> logSpec.storage().getLogState(e));\n        }\n        else\n        {\n            log = logSpec.async().createLog();\n            localProcessor = wrapProcessor.apply(new PaxosBackedProcessor(log));\n            fetchLogHandler = new FetchCMSLog.Handler();\n        }\n\n        Commit.Replicator replicator = CassandraRelevantProperties.TCM_USE_NO_OP_REPLICATOR.getBoolean()\n                                       ? Commit.Replicator.NO_OP\n                                       : new Commit.DefaultReplicator(() -> log.metadata().directory);\n\n        RemoteProcessor remoteProcessor = new RemoteProcessor(log, Discovery.instance::discoveredNodes);\n        GossipProcessor gossipProcessor = new GossipProcessor();\n        currentEpochHandler = new CurrentEpochRequestHandler();\n\n        commitRequestHandler = new Commit.Handler(localProcessor, replicator, cmsStateSupplier);\n        processor = new SwitchableProcessor(localProcessor,\n                                            remoteProcessor,\n                                            gossipProcessor,\n                                            replicator,\n                                            cmsStateSupplier);\n\n\n        replicationHandler = new Replication.ReplicationHandler(log);\n        logNotifyHandler = new Replication.LogNotifyHandler(log);\n        peerLogFetcher = new PeerLogFetcher(log);\n    }\n\n    @VisibleForTesting\n    // todo: convert this to a factory method with an obvious name that this is just for testing\n    public ClusterMetadataService(PlacementProvider placementProvider,\n                                  MetadataSnapshots snapshots,\n                                  LocalLog log,\n                                  Processor processor,\n                                  Commit.Replicator replicator,\n                                  boolean isMemberOfOwnershipGroup)\n    {\n        this.placementProvider = placementProvider;\n        this.log = log;\n        this.processor = new SwitchableProcessor(processor, null, null, replicator, () -> State.LOCAL);\n        this.snapshots = snapshots;\n\n        replicationHandler = new Replication.ReplicationHandler(log);\n        logNotifyHandler = new Replication.LogNotifyHandler(log);\n        currentEpochHandler = new CurrentEpochRequestHandler();\n\n        fetchLogHandler = isMemberOfOwnershipGroup ? new FetchCMSLog.Handler() : null;\n        commitRequestHandler = isMemberOfOwnershipGroup ? new Commit.Handler(processor, replicator, () -> LOCAL) : null;\n\n        peerLogFetcher = new PeerLogFetcher(log);\n    }\n\n    private ClusterMetadataService(PlacementProvider placementProvider,\n                                   MetadataSnapshots snapshots,\n                                   LocalLog log,\n                                   Processor processor,\n                                   Replication.ReplicationHandler replicationHandler,\n                                   Replication.LogNotifyHandler logNotifyHandler,\n                                   CurrentEpochRequestHandler currentEpochHandler,\n                                   FetchCMSLog.Handler fetchLogHandler,\n                                   Commit.Handler commitRequestHandler,\n                                   PeerLogFetcher peerLogFetcher)\n    {\n        this.placementProvider = placementProvider;\n        this.snapshots = snapshots;\n        this.log = log;\n        this.processor = processor;\n        this.replicationHandler = replicationHandler;\n        this.logNotifyHandler = logNotifyHandler;\n        this.currentEpochHandler = currentEpochHandler;\n        this.fetchLogHandler = fetchLogHandler;\n        this.commitRequestHandler = commitRequestHandler;\n        this.peerLogFetcher = peerLogFetcher;\n    }\n\n    @SuppressWarnings(\"resource\")\n    public static void initializeForTools(boolean loadSSTables)\n    {\n        if (instance != null)\n            return;\n        ClusterMetadata emptyFromSystemTables = emptyWithSchemaFromSystemTables(Collections.singleton(\"DC1\"))\n                                                .forceEpoch(Epoch.EMPTY);\n\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withInitialState(emptyFromSystemTables)\n                                           .loadSSTables(loadSSTables)\n                                           .withDefaultListeners(false)\n                                           .withListener(new SchemaListener(loadSSTables) {\n                                               @Override\n                                               public void notifyPostCommit(ClusterMetadata prev, ClusterMetadata next, boolean fromSnapshot)\n                                               {\n                                                   // we do not need a post-commit hook for tools\n                                               }\n                                           })\n                                           .sync()\n                                           .withStorage(new AtomicLongBackedProcessor.InMemoryStorage());\n        LocalLog log = logSpec.createLog();\n        ClusterMetadataService cms = new ClusterMetadataService(new UniformRangePlacement(),\n                                                                MetadataSnapshots.NO_OP,\n                                                                log,\n                                                                new AtomicLongBackedProcessor(log),\n                                                                new Replication.ReplicationHandler(log),\n                                                                new Replication.LogNotifyHandler(log),\n                                                                new CurrentEpochRequestHandler(),\n                                                                null,\n                                                                null,\n                                                                new PeerLogFetcher(log));\n\n        log.readyUnchecked();\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        ClusterMetadataService.setInstance(cms);\n    }\n\n    @SuppressWarnings(\"resource\")\n    public static void initializeForClients()\n    {\n        if (instance != null)\n            return;\n\n        ClusterMetadataService.setInstance(StubClusterMetadataService.forClientTools());\n    }\n\n    public static void initializeForClients(DistributedSchema initialSchema)\n    {\n        if (instance != null)\n            return;\n\n        ClusterMetadataService.setInstance(StubClusterMetadataService.forClientTools(initialSchema));\n    }\n\n    public boolean isCurrentMember(InetAddressAndPort peer)\n    {\n        return ClusterMetadata.current().isCMSMember(peer);\n    }\n\n    public void upgradeFromGossip(List<String> ignoredEndpoints)\n    {\n        Set<InetAddressAndPort> ignored = ignoredEndpoints.stream().map(InetAddressAndPort::getByNameUnchecked).collect(toSet());\n        if (ignored.contains(FBUtilities.getBroadcastAddressAndPort()))\n        {\n            String msg = String.format(\"Can't ignore local host %s when doing CMS migration\", FBUtilities.getBroadcastAddressAndPort());\n            logger.error(msg);\n            throw new IllegalStateException(msg);\n        }\n\n        ClusterMetadata metadata = metadata();\n        Set<InetAddressAndPort> existingMembers = metadata.fullCMSMembers();\n\n        if (!metadata.directory.allAddresses().containsAll(ignored))\n        {\n            Set<InetAddressAndPort> allAddresses = Sets.newHashSet(metadata.directory.allAddresses());\n            String msg = String.format(\"Ignored host(s) %s don't exist in the cluster\", Sets.difference(ignored, allAddresses));\n            logger.error(msg);\n            throw new IllegalStateException(msg);\n        }\n\n        for (Map.Entry<NodeId, NodeVersion> entry : metadata.directory.versions.entrySet())\n        {\n            NodeVersion version = entry.getValue();\n            InetAddressAndPort ep = metadata.directory.getNodeAddresses(entry.getKey()).broadcastAddress;\n            if (ignored.contains(ep))\n            {\n                // todo; what do we do if an endpoint has a mismatching gossip-clustermetadata?\n                //       - we could add the node to --ignore and force this CM to it?\n                //       - require operator to bounce/manually fix the CM on that node\n                //       for now just requiring that any ignored host is also down\n//                if (FailureDetector.instance.isAlive(ep))\n//                    throw new IllegalStateException(\"Can't ignore \" + ep + \" during CMS migration - it is not down\");\n                logger.info(\"Endpoint {} running {} is ignored\", ep, version);\n                continue;\n            }\n\n            if (!version.isUpgraded())\n            {\n                String msg = String.format(\"All nodes are not yet upgraded - %s is running %s\", metadata.directory.endpoint(entry.getKey()), version);\n                logger.error(msg);\n                throw new IllegalStateException(msg);\n            }\n        }\n\n        if (existingMembers.isEmpty())\n        {\n            logger.info(\"First CMS node\");\n            Set<InetAddressAndPort> candidates = metadata\n                                                 .directory\n                                                 .allAddresses()\n                                                 .stream()\n                                                 .filter(ep -> !FBUtilities.getBroadcastAddressAndPort().equals(ep) &&\n                                                               !ignored.contains(ep))\n                                                 .collect(toImmutableSet());\n\n            Election.instance.nominateSelf(candidates, ignored, metadata::equals, metadata);\n            ClusterMetadataService.instance().sealPeriod();\n        }\n        else\n        {\n            throw new IllegalStateException(\"Can't upgrade from gossip since CMS is already initialized\");\n        }\n    }\n\n    // This method is to be used _only_ for interactive purposes (i.e. nodetool), since it assumes no retries are going to be attempted on reject.\n    public void reconfigureCMS(ReplicationParams replicationParams)\n    {\n        Transformation transformation = new PrepareCMSReconfiguration.Complex(replicationParams);\n\n        ClusterMetadataService.instance()\n                              .commit(transformation);\n\n        InProgressSequences.finishInProgressSequences(ReconfigureCMS.SequenceKey.instance);\n    }\n\n    public boolean applyFromGossip(ClusterMetadata expected, ClusterMetadata updated)\n    {\n        logger.debug(\"Applying from gossip, current={} new={}\", expected, updated);\n        if (!expected.epoch.isBefore(Epoch.EMPTY))\n            throw new IllegalStateException(\"Can't apply a ClusterMetadata from gossip with epoch \" + expected.epoch);\n        if (state() != GOSSIP)\n            throw new IllegalStateException(\"Can't apply a ClusterMetadata from gossip when CMSState is not GOSSIP: \" + state());\n\n        return log.unsafeSetCommittedFromGossip(expected, updated);\n    }\n\n    public void setFromGossip(ClusterMetadata fromGossip)\n    {\n        logger.debug(\"Setting from gossip, new={}\", fromGossip);\n        if (state() != GOSSIP)\n            throw new IllegalStateException(\"Can't apply a ClusterMetadata from gossip when CMSState is not GOSSIP: \" + state());\n        log.unsafeSetCommittedFromGossip(fromGossip);\n    }\n\n    public void forceSnapshot(ClusterMetadata snapshot)\n    {\n        commit(new ForceSnapshot(snapshot));\n    }\n\n    public void revertToEpoch(Epoch epoch)\n    {\n        logger.warn(\"Reverting to epoch {}\", epoch);\n        ClusterMetadata metadata = ClusterMetadata.current();\n        ClusterMetadata toApply = transformSnapshot(LogState.getForRecovery(epoch))\n                                  .forceEpoch(metadata.epoch.nextEpoch())\n                                  .forcePeriod(metadata.nextPeriod());\n        forceSnapshot(toApply);\n    }\n\n    /**\n     * dumps the cluster metadata at the given epoch, returns path to the generated file\n     *\n     * if the given Epoch is EMPTY, we dump the current metadata\n     *\n     * @param epoch dump clustermetadata at this epoch\n     * @param transformToEpoch transform the dumped metadata to this epoch\n     * @param version serialisation version\n     */\n    public String dumpClusterMetadata(Epoch epoch, Epoch transformToEpoch, Version version) throws IOException\n    {\n        ClusterMetadata toDump = epoch.isAfter(Epoch.EMPTY)\n                                 ? transformSnapshot(LogState.getForRecovery(epoch))\n                                 : ClusterMetadata.current();\n        toDump = toDump.forceEpoch(transformToEpoch);\n        Path p = Files.createTempFile(\"clustermetadata\", \"dump\");\n        try (FileOutputStreamPlus out = new FileOutputStreamPlus(p))\n        {\n            VerboseMetadataSerializer.serialize(ClusterMetadata.serializer, toDump, out, version);\n        }\n        logger.info(\"Dumped cluster metadata to {}\", p.toString());\n        return p.toString();\n    }\n\n    public void loadClusterMetadata(String file) throws IOException\n    {\n        logger.warn(\"Loading cluster metadata from {}\", file);\n        ClusterMetadata metadata = ClusterMetadata.current();\n        ClusterMetadata toApply = deserializeClusterMetadata(file)\n                                  .forceEpoch(metadata.epoch.nextEpoch())\n                                  .forcePeriod(metadata.nextPeriod());\n        forceSnapshot(toApply);\n    }\n\n    public static ClusterMetadata deserializeClusterMetadata(String file) throws IOException\n    {\n        try (FileInputStreamPlus fisp = new FileInputStreamPlus(file))\n        {\n            return VerboseMetadataSerializer.deserialize(ClusterMetadata.serializer, fisp);\n        }\n    }\n\n    private ClusterMetadata transformSnapshot(LogState state)\n    {\n        ClusterMetadata toApply = state.baseState;\n        for (Entry entry : state.transformations.entries())\n        {\n            Transformation.Result res = entry.transform.execute(toApply);\n            assert res.isSuccess();\n            toApply = res.success().metadata;\n        }\n        return toApply;\n    }\n\n    public final Supplier<Entry.Id> entryIdGen = new Entry.DefaultEntryIdGen();\n\n    public interface CommitSuccessHandler<T>\n    {\n        T accept(ClusterMetadata latest);\n    }\n\n    public interface CommitFailureHandler<T>\n    {\n        T accept(ExceptionCode code, String message);\n    }\n\n    public ClusterMetadata commit(Transformation transform)\n    {\n        return commit(transform,\n                      metadata -> metadata,\n                      (code, message) -> {\n                          throw new IllegalStateException(String.format(\"Can not commit transformation: \\\"%s\\\"(%s).\",\n                                                                        code, message));\n                      });\n    }\n\n    /**\n     * Attempt to commit the transformation (with retries).\n     *\n     * Since we can not rely on reliability of the network or even the fact that the committing node will stay alive\n     * for the duration of commit, we have to allow for subsequent discovery of the transformation effects, which can\n     * be made visible either by replaying the log, or receiving the metadata snapshot.\n     *\n     * In other words, there is no reliable way to find out whether _this particular_ transformation has been executed\n     * while we are allowing replay from snapshot, since even failure response from the CMS does not guarantee\n     * Paxos re-proposal, which would place the transformation into the log during proposal _by some other_ CMS node.\n     *\n     * Protocol does foresee the concept of EntryId that would allow discovery of the committed transformations\n     * without changes to binary protocol, but this change was left out from the initial implementation of TCM.\n     *\n     * @param onFailure handler checks if rejection has resulted from a retry of the same trasformation.\n     */\n    public <T1> T1 commit(Transformation transform, CommitSuccessHandler<T1> onSuccess, CommitFailureHandler<T1> onFailure)\n    {\n        if (commitsPaused.get())\n            throw new IllegalStateException(\"Commits are paused, not trying to commit \" + transform);\n\n        long startTime = nanoTime();\n        // Replay everything in-flight before attempting a commit\n        // We grab highest consecutive epoch here, since we want both local and remote processors to benefit from\n        // discover-own-commits via entry id in case of lost messages (in remote case) and Paxos re-proposals (in local case)\n        Epoch highestConsecutive = log.waitForHighestConsecutive().epoch;\n\n        Commit.Result result = processor.commit(entryIdGen.get(), transform, highestConsecutive);\n\n        try\n        {\n            if (result.isSuccess())\n            {\n                TCMMetrics.instance.commitSuccessLatency.update(nanoTime() - startTime, NANOSECONDS);\n                return onSuccess.accept(awaitAtLeast(result.success().epoch));\n            }\n            else\n            {\n                TCMMetrics.instance.recordCommitFailureLatency(nanoTime() - startTime, NANOSECONDS, result.failure().rejected);\n                return onFailure.accept(result.failure().code, result.failure().message);\n            }\n        }\n        catch (TimeoutException t)\n        {\n            throw new IllegalStateException(String.format(\"Timed out while waiting for the follower to enact the epoch %s\", result.success().epoch), t);\n        }\n        catch (InterruptedException e)\n        {\n            throw new IllegalStateException(\"Couldn't commit the transformation. Is the node shutting down?\", e);\n        }\n    }\n\n    /**\n     * Accessors\n     */\n\n    public static IVerbHandler<Replication> replicationHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.replicationHandler;\n    }\n\n    public static IVerbHandler<LogState> logNotifyHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.logNotifyHandler;\n    }\n\n    public static IVerbHandler<FetchCMSLog> fetchLogRequestHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.fetchLogHandler;\n    }\n\n    public static IVerbHandler<Commit> commitRequestHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.commitRequestHandler;\n    }\n\n    public static CurrentEpochRequestHandler currentEpochRequestHandler()\n    {\n        // Make it possible to get Verb without throwing NPE during simulation\n        ClusterMetadataService instance = ClusterMetadataService.instance();\n        if (instance == null)\n            return null;\n        return instance.currentEpochHandler;\n    }\n\n    public PlacementProvider placementProvider()\n    {\n        return this.placementProvider;\n    }\n\n    @VisibleForTesting\n    public Processor processor()\n    {\n        return processor;\n    }\n\n    @VisibleForTesting\n    public LocalLog log()\n    {\n        return log;\n    }\n\n    public ClusterMetadata metadata()\n    {\n        return log.metadata();\n    }\n\n    /**\n     * Fetches log entries from directly from CMS, at least to the specified epoch.\n     *\n     * This operation is blocking and also waits for all retrieved log entries to be\n     * enacted, so on return all transformations to ClusterMetadata will be visible.\n     * @return metadata with all currently committed entries enacted.\n     */\n    public ClusterMetadata fetchLogFromCMS(Epoch awaitAtLeast)\n    {\n        ClusterMetadata metadata = ClusterMetadata.current();\n        if (awaitAtLeast.isBefore(Epoch.FIRST))\n            return metadata;\n\n        Epoch ourEpoch = metadata.epoch;\n\n        if (ourEpoch.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n\n        Retry.Deadline deadline = Retry.Deadline.after(DatabaseDescriptor.getCmsAwaitTimeout().to(TimeUnit.NANOSECONDS),\n                                                       new Retry.Jitter(TCMMetrics.instance.fetchLogRetries));\n        // responses for ALL withhout knowing we have pending\n        metadata = processor.fetchLogAndWait(awaitAtLeast, deadline);\n        if (metadata.epoch.isBefore(awaitAtLeast))\n        {\n            throw new IllegalStateException(String.format(\"Could not catch up to epoch %s even after fetching log from CMS. Highest seen after fetching is %s.\",\n                                                          awaitAtLeast, ourEpoch));\n        }\n        return metadata;\n    }\n\n    /**\n     * Attempts to asynchronously retrieve log entries from a non-CMS peer.\n     * Fetches and applies the log state representing the delta between the current local epoch and the one requested.\n     * This is used when a message from a peer contains an epoch higher than the current local epoch. As the sender of\n     * the message must have seen and enacted the given epoch, they must (under normal circumstances) be able to supply\n     * any entries needed to catch up this node.\n     * When the returned future completes, the metadata it provides is the current published metadata at the\n     * moment of completion. In the expected case, this will have had any fetched transformations up to the requested\n     * epoch applied. If the fetch was unsuccessful (e.g. because the peer was unavailable) it will still be whatever\n     * the currently published metadata, but which entries have been enacted cannot be guaranteed.\n     * @param from peer to request log entries from\n     * @param awaitAtLeast the upper epoch required. It's expected that the peer is able to supply log entries up to at\n     *                     least this epoch.\n     * @return A future which will supply the current ClusterMetadata at the time of completion\n     */\n    public Future<ClusterMetadata> fetchLogFromPeerAsync(InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        ClusterMetadata current = ClusterMetadata.current();\n        if (FBUtilities.getBroadcastAddressAndPort().equals(from) ||\n            current.epoch.isEqualOrAfter(awaitAtLeast) ||\n            awaitAtLeast.isBefore(Epoch.FIRST))\n            return ImmediateFuture.success(current);\n\n        return peerLogFetcher.asyncFetchLog(from, awaitAtLeast);\n    }\n\n    /**\n     *\n     * IMPORTANT: this call can return _without_ catching us up, so should only be used privately.\n     *\n     * Attempts to synchronously retrieve log entries from a non-CMS peer.\n     * Fetches the log state representing the delta between the current local epoch and the one supplied.\n     * This is to be used when a message from a peer contains an epoch higher than the current local epoch. As\n     * sender of the message must have seen and enacted the given epoch, they must (under normal circumstances)\n     * be able to supply any entries needed to catch up this node.\n     * The metadata returned is the current published metadata at that time. In the expected case, this will have had\n     * any fetched transformations up to the requested epoch applied. If the fetch was unsuccessful (e.g. because the\n     * peer was unavailable) it will still be whatever the currently published metadata, but which entries have been\n     * enacted cannot be guaranteed.\n     * @param from peer to request log entries from\n     * @param awaitAtLeast the upper epoch required. It's expected that the peer is able to supply log entries up to at\n     *                     least this epoch.\n     * @return The current ClusterMetadata at the time of completion\n     */\n    private ClusterMetadata fetchLogFromPeer(ClusterMetadata metadata, InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        if (awaitAtLeast.isBefore(Epoch.FIRST) || FBUtilities.getBroadcastAddressAndPort().equals(from))\n            return ClusterMetadata.current();\n        Epoch before = metadata.epoch;\n        if (before.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n        return peerLogFetcher.fetchLogEntriesAndWait(from, awaitAtLeast);\n    }\n\n    public Future<ClusterMetadata> fetchLogFromPeerOrCMSAsync(ClusterMetadata metadata, InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        AsyncPromise<ClusterMetadata> future = new AsyncPromise<>();\n        ScheduledExecutors.optionalTasks.submit(() -> {\n            try\n            {\n                future.setSuccess(ClusterMetadataService.instance().fetchLogFromPeerOrCMS(metadata, from, awaitAtLeast));\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                logger.warn(String.format(\"Learned about epoch %s from %s, but could not fetch log.\", awaitAtLeast, from), t);\n                future.setFailure(t);\n            }\n        });\n        return future;\n    }\n\n    /**\n     * Combines {@link #fetchLogFromPeer} with {@link #fetchLogFromCMS} to synchronously fetch and apply log entries\n     * up to the requested epoch. The supplied peer will be contacted first and if after doing so, the current local\n     * metadata is not caught up to at least the required epoch, a further request is made to the CMS.\n     * The returned ClusterMetadata is guaranteed to have been published, though it may have also been superceded by\n     * further updates.\n     * If the requested epoch is not reached even after fetching from the CMS, an IllegalStateException is thrown.\n     * @param metadata a starting point for the fetch. If the requested epoch is <= the epoch of this metadata, the\n     *                 call is a no-op. It's expected that this is usually the current cluster metadata at the time of\n     *                 calling.\n     * @param from Initial peer to contact. Usually this is the sender of a message containing the requested epoch,\n     *             which means it can be assumed that this peer (if available) can supply any missing log entries.\n     * @param awaitAtLeast The requested epoch.\n     * @return A published ClusterMetadata with all entries up to (at least) the requested epoch enacted.\n     * @throws IllegalStateException if the requested epoch could not be reached, even after falling back to CMS catchup\n     */\n    public ClusterMetadata fetchLogFromPeerOrCMS(ClusterMetadata metadata, InetAddressAndPort from, Epoch awaitAtLeast)\n    {\n        if (awaitAtLeast.isBefore(Epoch.FIRST) || FBUtilities.getBroadcastAddressAndPort().equals(from))\n            return metadata;\n\n        Epoch before = metadata.epoch;\n        if (before.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n\n        metadata = fetchLogFromPeer(metadata, from, awaitAtLeast);\n        if (metadata.epoch.isEqualOrAfter(awaitAtLeast))\n            return metadata;\n\n        metadata = fetchLogFromCMS(awaitAtLeast);\n        if (metadata.epoch.isBefore(awaitAtLeast))\n            throw new IllegalStateException(\"Still behind after fetching log from CMS\");\n        logger.debug(\"Fetched log from CMS - caught up from epoch {} to epoch {}\", before, metadata.epoch);\n        return metadata;\n    }\n\n    public ClusterMetadata awaitAtLeast(Epoch epoch) throws InterruptedException, TimeoutException\n    {\n        return log.awaitAtLeast(epoch);\n    }\n\n    public MetadataSnapshots snapshotManager()\n    {\n        return snapshots;\n    }\n\n    public ClusterMetadata sealPeriod()\n    {\n        return ClusterMetadataService.instance.commit(SealPeriod.instance,\n                                                      (ClusterMetadata metadata) -> metadata,\n                                                      (code, reason) -> {\n                                                          // If the transformation got rejected, someone else has beat us to seal this period\n                                                          return ClusterMetadata.current();\n                                                      });\n    }\n\n    public void initRecentlySealedPeriodsIndex()\n    {\n        Sealed.initIndexFromSystemTables();\n    }\n\n    public boolean isMigrating()\n    {\n        return Election.instance.isMigrating();\n    }\n\n    public void migrated()\n    {\n        Election.instance.migrated();\n    }\n    public void pauseCommits()\n    {\n        commitsPaused.set(true);\n    }\n\n    public void resumeCommits()\n    {\n        commitsPaused.set(false);\n    }\n\n    public boolean commitsPaused()\n    {\n        return commitsPaused.get();\n    }\n    /**\n     * Switchable implementation that allow us to go between local and remote implementation whenever we need it.\n     * When the node becomes a member of CMS, it switches back to being a regular member of a cluster, and all\n     * the CMS handlers get disabled.\n     */\n    @VisibleForTesting\n    public static class SwitchableProcessor implements Processor\n    {\n        private final Processor local;\n        private final RemoteProcessor remote;\n        private final GossipProcessor gossip;\n        private final Supplier<State> cmsStateSupplier;\n        private final Commit.Replicator replicator;\n\n        SwitchableProcessor(Processor local,\n                            RemoteProcessor remote,\n                            GossipProcessor gossip,\n                            Commit.Replicator replicator,\n                            Supplier<State> cmsStateSupplier)\n        {\n            this.local = local;\n            this.remote = remote;\n            this.gossip = gossip;\n            this.replicator = replicator;\n            this.cmsStateSupplier = cmsStateSupplier;\n        }\n\n        @VisibleForTesting\n        public Processor delegate()\n        {\n            return delegateInternal().right;\n        }\n\n        private Pair<State, Processor> delegateInternal()\n        {\n            State state = cmsStateSupplier.get();\n            switch (state)\n            {\n                case LOCAL:\n                case RESET:\n                    return Pair.create(state, local);\n                case REMOTE:\n                    return Pair.create(state, remote);\n                case GOSSIP:\n                    return Pair.create(state, gossip);\n            }\n            throw new IllegalStateException(\"Bad CMS state: \" + state);\n        }\n\n        @Override\n        public Commit.Result commit(Entry.Id entryId, Transformation transform, Epoch lastKnown, Retry.Deadline retryPolicy)\n        {\n            Pair<State, Processor> delegate = delegateInternal();\n            Commit.Result result = delegate.right.commit(entryId, transform, lastKnown, retryPolicy);\n            if (delegate.left == LOCAL || delegate.left == RESET)\n                replicator.send(result, null);\n            return result;\n        }\n\n        @Override\n        public ClusterMetadata fetchLogAndWait(Epoch waitFor, Retry.Deadline retryPolicy)\n        {\n            return delegate().fetchLogAndWait(waitFor, retryPolicy);\n        }\n\n        public String toString()\n        {\n            return \"SwitchableProcessor{\" +\n                   delegate() + '}';\n        }\n    }\n\n    public enum State\n    {\n        LOCAL, REMOTE, GOSSIP, RESET\n    }\n}\n","lineNo":249}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n * <p/>\n * http://www.apache.org/licenses/LICENSE-2.0\n * <p/>\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra;\n\nimport java.io.IOException;\nimport java.net.UnknownHostException;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport java.util.function.Function;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.audit.AuditLogManager;\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.io.sstable.format.big.BigTableReader;\nimport org.apache.cassandra.io.sstable.indexsummary.IndexSummarySupport;\nimport org.apache.cassandra.cql3.QueryProcessor;\nimport org.apache.cassandra.dht.IPartitioner;\nimport org.apache.cassandra.io.util.File;\nimport org.apache.cassandra.locator.AbstractEndpointSnitch;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.schema.DistributedSchema;\nimport org.apache.cassandra.security.ThreadAwareSecurityManager;\nimport org.apache.cassandra.service.EmbeddedCassandraService;\nimport org.apache.cassandra.tcm.AtomicLongBackedProcessor;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.ClusterMetadataService;\nimport org.apache.cassandra.tcm.Commit;\nimport org.apache.cassandra.tcm.Epoch;\nimport org.apache.cassandra.tcm.MetadataSnapshots;\nimport org.apache.cassandra.tcm.Processor;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.ownership.PlacementProvider;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.transformations.ForceSnapshot;\nimport org.apache.cassandra.tcm.transformations.Register;\nimport org.apache.cassandra.tcm.transformations.UnsafeJoin;\nimport org.apache.cassandra.tcm.transformations.cms.Initialize;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION;\n\n/**\n * Utility methodes used by SchemaLoader and CQLTester to manage the server and its state.\n *\n */\npublic final class ServerTestUtils\n{\n    private static final Logger logger = LoggerFactory.getLogger(ServerTestUtils.class);\n\n    private static final Set<InetAddressAndPort> remoteAddrs = new HashSet<>();\n\n    public static final String DATA_CENTER = \"datacenter1\";\n    public static final String DATA_CENTER_REMOTE = \"datacenter2\";\n    public static final String RACK1 = \"rack1\";\n\n    private static boolean isServerPrepared = false;\n\n    /**\n     * Call DatabaseDescriptor.daemonInitialization ensuring that the snitch used returns fixed values for the tests\n     */\n    public static void daemonInitialization()\n    {\n        DatabaseDescriptor.daemonInitialization();\n        initSnitch();\n    }\n\n    public static void initSnitch()\n    {\n        // Register an EndpointSnitch which returns fixed values for test.\n        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()\n        {\n            @Override\n            public String getRack(InetAddressAndPort endpoint)\n            {\n                return RACK1;\n            }\n\n            @Override\n            public String getDatacenter(InetAddressAndPort endpoint)\n            {\n                if (remoteAddrs.contains(endpoint))\n                    return DATA_CENTER_REMOTE;\n\n                return DATA_CENTER;\n            }\n\n            @Override\n            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2)\n            {\n                return 0;\n            }\n        });\n    }\n\n    public static NodeId registerLocal()\n    {\n        return registerLocal(Collections.singleton(DatabaseDescriptor.getPartitioner().getRandomToken()));\n    }\n\n    public static NodeId registerLocal(Set<Token> tokens)\n    {\n        NodeId nodeId = Register.maybeRegister();\n        ClusterMetadataService.instance().commit(new UnsafeJoin(nodeId,\n                                                                tokens,\n                                                                ClusterMetadataService.instance().placementProvider()));\n        SystemKeyspace.setLocalHostId(nodeId.toUUID());\n        return nodeId;\n    }\n\n    public static void prepareServer()\n    {\n        prepareServerNoRegister();\n        registerLocal();\n        markCMS();\n    }\n\n    public static void prepareServerNoRegister()\n    {\n        daemonInitialization();\n\n        if (isServerPrepared)\n            return;\n\n        DatabaseDescriptor.setTransientReplicationEnabledUnsafe(true);\n\n        // Cleanup first\n        try\n        {\n            cleanupAndLeaveDirs();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Failed to cleanup and recreate directories.\");\n            throw new RuntimeException(e);\n        }\n\n        try\n        {\n            remoteAddrs.add(InetAddressAndPort.getByName(\"127.0.0.4\"));\n        }\n        catch (UnknownHostException e)\n        {\n            logger.error(\"Failed to lookup host\");\n            throw new RuntimeException(e);\n        }\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                logger.error(\"Fatal exception in thread \" + t, e);\n            }\n        });\n\n        ThreadAwareSecurityManager.install();\n\n        CassandraRelevantProperties.GOSSIPER_SKIP_WAITING_TO_SETTLE.setInt(0);\n        initCMS();\n        SystemKeyspace.persistLocalMetadata();\n        AuditLogManager.instance.initialize();\n\n        isServerPrepared = true;\n    }\n\n\n    /**\n     * Cleanup the directories used by the server, creating them if they do not exist.\n     */\n    public static void cleanupAndLeaveDirs() throws IOException\n    {\n        CommitLog.instance.stopUnsafe(true);\n        mkdirs(); // Creates the directories if they does not exists\n        cleanup(); // Ensure that the directories are all empty\n        CommitLog.instance.restartUnsafe();\n    }\n\n    /**\n     * Cleanup the storage related directories: commitLog, cdc, hint, caches and data directories\n     */\n    public static void cleanup()\n    {\n        // clean up commitlog\n        cleanupDirectory(DatabaseDescriptor.getCommitLogLocation());\n\n        String cdcDir = DatabaseDescriptor.getCDCLogLocation();\n        if (cdcDir != null)\n            cleanupDirectory(cdcDir);\n        cleanupDirectory(DatabaseDescriptor.getHintsDirectory());\n        cleanupSavedCaches();\n\n        // clean up data directory which are stored as data directory/keyspace/data files\n        for (String dirName : DatabaseDescriptor.getAllDataFileLocations())\n        {\n            cleanupDirectory(dirName);\n        }\n    }\n\n    private static void cleanupDirectory(File directory)\n    {\n        if (directory.exists())\n        {\n            Arrays.stream(directory.tryList()).forEach(File::deleteRecursive);\n        }\n    }\n\n    private static void cleanupDirectory(String dirName)\n    {\n        if (dirName != null)\n            cleanupDirectory(new File(dirName));\n    }\n\n    /**\n     * Creates all the storage related directories\n     */\n    public static void mkdirs()\n    {\n        DatabaseDescriptor.createAllDirectories();\n    }\n\n    public static void cleanupSavedCaches()\n    {\n        cleanupDirectory(DatabaseDescriptor.getSavedCachesLocation());\n    }\n\n    public static EmbeddedCassandraService startEmbeddedCassandraService() throws IOException\n    {\n        DatabaseDescriptor.daemonInitialization();\n        mkdirs();\n        cleanup();\n        EmbeddedCassandraService service = new EmbeddedCassandraService();\n        service.start();\n        return service;\n    }\n\n    public static void initCMS()\n    {\n        // Effectively disable automatic snapshots using AtomicLongBackedProcessopr and LocaLLog.Sync interacts\n        // badly with submitting SealPeriod transformations from the log listener. In this configuration, SealPeriod\n        // commits performed on NonPeriodicTasks threads end up actually performing the transformations as well as\n        // calling the pre and post commit listeners, which is not threadsafe. In a non-test setup the processing of\n        // log entries is always done by the dedicated log follower thread.\n        DatabaseDescriptor.setMetadataSnapshotFrequency(Integer.MAX_VALUE);\n\n        Function<LocalLog, Processor> processorFactory = AtomicLongBackedProcessor::new;\n        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();\n        boolean addListeners = true;\n        ClusterMetadata initial = new ClusterMetadata(partitioner);\n        LocalLog.LogSpec logSpec = new LocalLog.LogSpec().withInitialState(initial)\n                                                         .withDefaultListeners(addListeners);\n        LocalLog log = LocalLog.async(logSpec);\n        ResettableClusterMetadataService service = new ResettableClusterMetadataService(new UniformRangePlacement(),\n                                                                                        MetadataSnapshots.NO_OP,\n                                                                                        log,\n                                                                                        processorFactory.apply(log),\n                                                                                        Commit.Replicator.NO_OP,\n                                                                                        true);\n\n        ClusterMetadataService.setInstance(service);\n        initial.schema.initializeKeyspaceInstances(DistributedSchema.empty());\n        if (!Keyspace.isInitialized())\n            Keyspace.setInitialized();\n        log.ready();\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        service.commit(new Initialize(ClusterMetadata.current()));\n        QueryProcessor.registerStatementInvalidatingListener();\n        service.mark();\n    }\n\n    public static void recreateCMS()\n    {\n        assert ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION.getBoolean() : \"Need to set \" + ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION + \" to true for resetCMS to work\";\n        // unfortunately, for now this is sometimes necessary because of the initialisation ordering with regard to\n        // IPartitioner. For example, if a test has a requirement to use a different partitioner to the one in yaml:\n        // SchemaLoader.prepareServer\n        // |-- SchemaLoader.prepareServerNoRegister\n        // |   |-- ServerTestUtils.daemonInitialization();        # sets DD.partitioner according to yaml (i.e. BOP)\n        // |   |-- ServerTestUtils.prepareServer();               # includes inititial CMS using DD partitioner\n        // |-- StorageService.instance.setPartitionerUnsafe(M3P)  # test wants to use LongToken\n        // |-- ServerTestUtils.recreateCMS                        # recreates the CMS using the updated partitioner\n        ClusterMetadata initial = new ClusterMetadata(DatabaseDescriptor.getPartitioner());\n        initial.schema.initializeKeyspaceInstances(DistributedSchema.empty());\n        LocalLog.LogSpec logSpec = new LocalLog.LogSpec().withInitialState(initial)\n                                                         .withStorage(LogStorage.SystemKeyspace)\n                                                         .withDefaultListeners();\n        LocalLog log = LocalLog.async(logSpec);\n        log.ready();\n        ResettableClusterMetadataService cms = new ResettableClusterMetadataService(new UniformRangePlacement(),\n                                                                                    MetadataSnapshots.NO_OP,\n                                                                                    log,\n                                                                                    new AtomicLongBackedProcessor(log),\n                                                                                    Commit.Replicator.NO_OP,\n                                                                                    true);\n        ClusterMetadataService.unsetInstance();\n        ClusterMetadataService.setInstance(cms);\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        cms.mark();\n    }\n\n    public static void markCMS()\n    {\n        ClusterMetadataService cms = ClusterMetadataService.instance();\n        assert cms instanceof ResettableClusterMetadataService : \"CMS instance is not resettable\";\n        ((ResettableClusterMetadataService)cms).mark();\n    }\n\n    public static void resetCMS()\n    {\n        ClusterMetadataService cms = ClusterMetadataService.instance();\n        assert cms instanceof ResettableClusterMetadataService : \"CMS instance is not resettable\";\n        ((ResettableClusterMetadataService)cms).reset();\n    }\n\n    public static class ResettableClusterMetadataService extends ClusterMetadataService\n    {\n\n        private ClusterMetadata mark;\n\n        public ResettableClusterMetadataService(PlacementProvider placementProvider,\n                                                MetadataSnapshots snapshots,\n                                                LocalLog log,\n                                                Processor processor,\n                                                Commit.Replicator replicator,\n                                                boolean isMemberOfOwnershipGroup)\n        {\n            super(placementProvider, snapshots, log, processor, replicator, isMemberOfOwnershipGroup);\n            mark = log.metadata();\n        }\n\n        public void mark()\n        {\n            mark = log().metadata();\n        }\n\n        public Epoch reset()\n        {\n            Epoch nextEpoch = ClusterMetadata.current().epoch.nextEpoch();\n            ClusterMetadata newBaseState = mark.forceEpoch(nextEpoch);\n            return ClusterMetadataService.instance().commit(new ForceSnapshot(newBaseState)).epoch;\n        }\n    }\n\n    private ServerTestUtils()\n    {\n    }\n\n    public static List<BigTableReader> getLiveBigTableReaders(ColumnFamilyStore cfs)\n    {\n        return cfs.getLiveSSTables()\n                  .stream()\n                  .filter(BigTableReader.class::isInstance)\n                  .map(BigTableReader.class::cast)\n                  .collect(Collectors.toList());\n    }\n\n    public static <R extends SSTableReader & IndexSummarySupport<R>> List<R> getLiveIndexSummarySupportingReaders(ColumnFamilyStore cfs)\n    {\n        return cfs.getLiveSSTables()\n                  .stream()\n                  .filter(IndexSummarySupport.class::isInstance)\n                  .map(r -> (R) r)\n                  .collect(Collectors.toList());\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n * <p/>\n * http://www.apache.org/licenses/LICENSE-2.0\n * <p/>\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra;\n\nimport java.io.IOException;\nimport java.net.UnknownHostException;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.audit.AuditLogManager;\nimport org.apache.cassandra.config.CassandraRelevantProperties;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.QueryProcessor;\nimport org.apache.cassandra.db.ColumnFamilyStore;\nimport org.apache.cassandra.db.Keyspace;\nimport org.apache.cassandra.db.SystemKeyspace;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.IPartitioner;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.io.sstable.format.big.BigTableReader;\nimport org.apache.cassandra.io.sstable.indexsummary.IndexSummarySupport;\nimport org.apache.cassandra.io.util.File;\nimport org.apache.cassandra.locator.AbstractEndpointSnitch;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.locator.Replica;\nimport org.apache.cassandra.security.ThreadAwareSecurityManager;\nimport org.apache.cassandra.service.EmbeddedCassandraService;\nimport org.apache.cassandra.tcm.AtomicLongBackedProcessor;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.ClusterMetadataService;\nimport org.apache.cassandra.tcm.Commit;\nimport org.apache.cassandra.tcm.Epoch;\nimport org.apache.cassandra.tcm.MetadataSnapshots;\nimport org.apache.cassandra.tcm.Processor;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.log.SystemKeyspaceStorage;\nimport org.apache.cassandra.tcm.membership.NodeId;\nimport org.apache.cassandra.tcm.ownership.PlacementProvider;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.transformations.ForceSnapshot;\nimport org.apache.cassandra.tcm.transformations.Register;\nimport org.apache.cassandra.tcm.transformations.UnsafeJoin;\nimport org.apache.cassandra.tcm.transformations.cms.Initialize;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.apache.cassandra.config.CassandraRelevantProperties.ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION;\n\n/**\n * Utility methodes used by SchemaLoader and CQLTester to manage the server and its state.\n *\n */\npublic final class ServerTestUtils\n{\n    private static final Logger logger = LoggerFactory.getLogger(ServerTestUtils.class);\n\n    private static final Set<InetAddressAndPort> remoteAddrs = new HashSet<>();\n\n    public static final String DATA_CENTER = \"datacenter1\";\n    public static final String DATA_CENTER_REMOTE = \"datacenter2\";\n    public static final String RACK1 = \"rack1\";\n\n    private static boolean isServerPrepared = false;\n\n    /**\n     * Call DatabaseDescriptor.daemonInitialization ensuring that the snitch used returns fixed values for the tests\n     */\n    public static void daemonInitialization()\n    {\n        DatabaseDescriptor.daemonInitialization();\n        initSnitch();\n    }\n\n    public static void initSnitch()\n    {\n        // Register an EndpointSnitch which returns fixed values for test.\n        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()\n        {\n            @Override\n            public String getRack(InetAddressAndPort endpoint)\n            {\n                return RACK1;\n            }\n\n            @Override\n            public String getDatacenter(InetAddressAndPort endpoint)\n            {\n                if (remoteAddrs.contains(endpoint))\n                    return DATA_CENTER_REMOTE;\n\n                return DATA_CENTER;\n            }\n\n            @Override\n            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2)\n            {\n                return 0;\n            }\n        });\n    }\n\n    public static NodeId registerLocal()\n    {\n        return registerLocal(Collections.singleton(DatabaseDescriptor.getPartitioner().getRandomToken()));\n    }\n\n    public static NodeId registerLocal(Set<Token> tokens)\n    {\n        NodeId nodeId = Register.maybeRegister();\n        ClusterMetadataService.instance().commit(new UnsafeJoin(nodeId,\n                                                                tokens,\n                                                                ClusterMetadataService.instance().placementProvider()));\n        SystemKeyspace.setLocalHostId(nodeId.toUUID());\n        return nodeId;\n    }\n\n    public static void prepareServer()\n    {\n        prepareServerNoRegister();\n        registerLocal();\n        markCMS();\n    }\n\n    public static void prepareServerNoRegister()\n    {\n        daemonInitialization();\n\n        if (isServerPrepared)\n            return;\n\n        DatabaseDescriptor.setTransientReplicationEnabledUnsafe(true);\n\n        // Cleanup first\n        try\n        {\n            cleanupAndLeaveDirs();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Failed to cleanup and recreate directories.\");\n            throw new RuntimeException(e);\n        }\n\n        try\n        {\n            remoteAddrs.add(InetAddressAndPort.getByName(\"127.0.0.4\"));\n        }\n        catch (UnknownHostException e)\n        {\n            logger.error(\"Failed to lookup host\");\n            throw new RuntimeException(e);\n        }\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                logger.error(\"Fatal exception in thread \" + t, e);\n            }\n        });\n\n        ThreadAwareSecurityManager.install();\n\n        CassandraRelevantProperties.GOSSIPER_SKIP_WAITING_TO_SETTLE.setInt(0);\n        initCMS();\n        SystemKeyspace.persistLocalMetadata();\n        AuditLogManager.instance.initialize();\n\n        isServerPrepared = true;\n    }\n\n\n    /**\n     * Cleanup the directories used by the server, creating them if they do not exist.\n     */\n    public static void cleanupAndLeaveDirs() throws IOException\n    {\n        CommitLog.instance.stopUnsafe(true);\n        mkdirs(); // Creates the directories if they does not exists\n        cleanup(); // Ensure that the directories are all empty\n        CommitLog.instance.restartUnsafe();\n    }\n\n    /**\n     * Cleanup the storage related directories: commitLog, cdc, hint, caches and data directories\n     */\n    public static void cleanup()\n    {\n        // clean up commitlog\n        cleanupDirectory(DatabaseDescriptor.getCommitLogLocation());\n\n        String cdcDir = DatabaseDescriptor.getCDCLogLocation();\n        if (cdcDir != null)\n            cleanupDirectory(cdcDir);\n        cleanupDirectory(DatabaseDescriptor.getHintsDirectory());\n        cleanupSavedCaches();\n\n        // clean up data directory which are stored as data directory/keyspace/data files\n        for (String dirName : DatabaseDescriptor.getAllDataFileLocations())\n        {\n            cleanupDirectory(dirName);\n        }\n    }\n\n    private static void cleanupDirectory(File directory)\n    {\n        if (directory.exists())\n        {\n            Arrays.stream(directory.tryList()).forEach(File::deleteRecursive);\n        }\n    }\n\n    private static void cleanupDirectory(String dirName)\n    {\n        if (dirName != null)\n            cleanupDirectory(new File(dirName));\n    }\n\n    /**\n     * Creates all the storage related directories\n     */\n    public static void mkdirs()\n    {\n        DatabaseDescriptor.createAllDirectories();\n    }\n\n    public static void cleanupSavedCaches()\n    {\n        cleanupDirectory(DatabaseDescriptor.getSavedCachesLocation());\n    }\n\n    public static EmbeddedCassandraService startEmbeddedCassandraService() throws IOException\n    {\n        DatabaseDescriptor.daemonInitialization();\n        mkdirs();\n        cleanup();\n        EmbeddedCassandraService service = new EmbeddedCassandraService();\n        service.start();\n        return service;\n    }\n\n    public static void initCMS()\n    {\n        // Effectively disable automatic snapshots using AtomicLongBackedProcessor and LocaLLog.Sync interacts\n        // badly with submitting SealPeriod transformations from the log listener. In this configuration, SealPeriod\n        // commits performed on NonPeriodicTasks threads end up actually performing the transformations as well as\n        // calling the pre and post commit listeners, which is not threadsafe. In a non-test setup the processing of\n        // log entries is always done by the dedicated log follower thread.\n        DatabaseDescriptor.setMetadataSnapshotFrequency(Integer.MAX_VALUE);\n\n        Function<LocalLog, Processor> processorFactory = AtomicLongBackedProcessor::new;\n        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();\n        boolean addListeners = true;\n        ClusterMetadata initial = new ClusterMetadata(partitioner);\n        if (!Keyspace.isInitialized())\n            Keyspace.setInitialized();\n\n        LocalLog log = LocalLog.logSpec()\n                               .withInitialState(initial)\n                               .withDefaultListeners(addListeners)\n                               .createLog();\n\n        ResettableClusterMetadataService service = new ResettableClusterMetadataService(new UniformRangePlacement(),\n                                                                                        MetadataSnapshots.NO_OP,\n                                                                                        log,\n                                                                                        processorFactory.apply(log),\n                                                                                        Commit.Replicator.NO_OP,\n                                                                                        true);\n\n        ClusterMetadataService.setInstance(service);\n        log.readyUnchecked();\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        service.commit(new Initialize(ClusterMetadata.current()));\n        QueryProcessor.registerStatementInvalidatingListener();\n        service.mark();\n    }\n\n    public static void recreateCMS()\n    {\n        assert ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION.getBoolean() : \"Need to set \" + ORG_APACHE_CASSANDRA_DISABLE_MBEAN_REGISTRATION + \" to true for resetCMS to work\";\n        // unfortunately, for now this is sometimes necessary because of the initialisation ordering with regard to\n        // IPartitioner. For example, if a test has a requirement to use a different partitioner to the one in yaml:\n        // SchemaLoader.prepareServer\n        // |-- SchemaLoader.prepareServerNoRegister\n        // |   |-- ServerTestUtils.daemonInitialization();        # sets DD.partitioner according to yaml (i.e. BOP)\n        // |   |-- ServerTestUtils.prepareServer();               # includes inititial CMS using DD partitioner\n        // |-- StorageService.instance.setPartitionerUnsafe(M3P)  # test wants to use LongToken\n        // |-- ServerTestUtils.recreateCMS                        # recreates the CMS using the updated partitioner\n        ClusterMetadata initial = new ClusterMetadata(DatabaseDescriptor.getPartitioner());\n        LogStorage storage = LogStorage.SystemKeyspace;\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .withInitialState(initial)\n                                           .withStorage(storage)\n                                           .withDefaultListeners();\n        LocalLog log = logSpec.createLog();\n\n        ResettableClusterMetadataService cms = new ResettableClusterMetadataService(new UniformRangePlacement(),\n                                                                                    MetadataSnapshots.NO_OP,\n                                                                                    log,\n                                                                                    new AtomicLongBackedProcessor(log),\n                                                                                    Commit.Replicator.NO_OP,\n                                                                                    true);\n        ClusterMetadataService.unsetInstance();\n        ClusterMetadataService.setInstance(cms);\n        ((SystemKeyspaceStorage)LogStorage.SystemKeyspace).truncate();\n        log.readyUnchecked();\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n        cms.mark();\n    }\n\n    public static void markCMS()\n    {\n        ClusterMetadataService cms = ClusterMetadataService.instance();\n        assert cms instanceof ResettableClusterMetadataService : \"CMS instance is not resettable\";\n        ((ResettableClusterMetadataService)cms).mark();\n    }\n\n    public static void resetCMS()\n    {\n        ClusterMetadataService cms = ClusterMetadataService.instance();\n        assert cms instanceof ResettableClusterMetadataService : \"CMS instance is not resettable\";\n        ((ResettableClusterMetadataService)cms).reset();\n    }\n\n    public static class ResettableClusterMetadataService extends ClusterMetadataService\n    {\n\n        private ClusterMetadata mark;\n\n        public ResettableClusterMetadataService(PlacementProvider placementProvider,\n                                                MetadataSnapshots snapshots,\n                                                LocalLog log,\n                                                Processor processor,\n                                                Commit.Replicator replicator,\n                                                boolean isMemberOfOwnershipGroup)\n        {\n            super(placementProvider, snapshots, log, processor, replicator, isMemberOfOwnershipGroup);\n            mark = log.metadata();\n        }\n\n        public void mark()\n        {\n            mark = log().metadata();\n        }\n\n        public Epoch reset()\n        {\n            Epoch nextEpoch = ClusterMetadata.current().epoch.nextEpoch();\n            ClusterMetadata newBaseState = mark.forceEpoch(nextEpoch);\n            return ClusterMetadataService.instance().commit(new ForceSnapshot(newBaseState)).epoch;\n        }\n    }\n\n    private ServerTestUtils()\n    {\n    }\n\n    public static List<BigTableReader> getLiveBigTableReaders(ColumnFamilyStore cfs)\n    {\n        return cfs.getLiveSSTables()\n                  .stream()\n                  .filter(BigTableReader.class::isInstance)\n                  .map(BigTableReader.class::cast)\n                  .collect(Collectors.toList());\n    }\n\n    public static <R extends SSTableReader & IndexSummarySupport<R>> List<R> getLiveIndexSummarySupportingReaders(ColumnFamilyStore cfs)\n    {\n        return cfs.getLiveSSTables()\n                  .stream()\n                  .filter(IndexSummarySupport.class::isInstance)\n                  .map(r -> (R) r)\n                  .collect(Collectors.toList());\n    }\n}\n","lineNo":315}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport org.apache.cassandra.ServerTestUtils;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.schema.DistributedSchema;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.tcm.extensions.ExtensionValue;\nimport org.apache.cassandra.tcm.listeners.MetadataSnapshotListener;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.transformations.CustomTransformation;\nimport org.apache.cassandra.tcm.transformations.SealPeriod;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.junit.Assert.assertEquals;\n\npublic class LogStateTest\n{\n    @Before\n    public void setup() throws IOException\n    {\n        DatabaseDescriptor.daemonInitialization();\n        StorageService.instance.setPartitionerUnsafe(Murmur3Partitioner.instance);\n        ServerTestUtils.cleanupAndLeaveDirs();\n        CommitLog.instance.start();\n        LogStorage logStorage = LogStorage.SystemKeyspace;\n        MetadataSnapshots snapshots = new MetadataSnapshots.SystemKeyspaceMetadataSnapshots();\n        ClusterMetadata initial = new ClusterMetadata(DatabaseDescriptor.getPartitioner());\n        LocalLog.LogSpec logSpec = new LocalLog.LogSpec().withInitialState(initial)\n                                                         .withStorage(logStorage)\n                                                         .withLogListener(new MetadataSnapshotListener());\n        LocalLog log = LocalLog.sync(logSpec);\n        ClusterMetadataService cms = new ClusterMetadataService(new UniformRangePlacement(),\n                                                                snapshots,\n                                                                log,\n                                                                new AtomicLongBackedProcessor(log),\n                                                                Commit.Replicator.NO_OP,\n                                                                false);\n        ClusterMetadataService.unsetInstance();\n        ClusterMetadataService.setInstance(cms);\n        initial.schema.initializeKeyspaceInstances(DistributedSchema.empty());\n        log.ready();\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n    }\n\n    @Test\n    public void testRevertEpoch()\n    {\n        ClusterMetadataService.instance().sealPeriod();\n        List<Epoch> customEpochs = new ArrayList<>(40);\n        for (int i=0; i < 10; i++)\n        {\n            for (int j = 0; j < 4; j++)\n            {\n                ClusterMetadataService.instance()\n                                      .commit(new CustomTransformation(CustomTransformation.PokeInt.NAME,\n                                                                       new CustomTransformation.PokeInt((int) ClusterMetadata.current().epoch.getEpoch())));\n                customEpochs.add(ClusterMetadata.current().epoch);\n            }\n            ClusterMetadataService.instance().commit(SealPeriod.instance);\n        }\n\n        for (Epoch epoch : customEpochs)\n        {\n            ClusterMetadataService.instance().revertToEpoch(epoch);\n            ExtensionValue<?> val = ClusterMetadata.current().extensions.get(CustomTransformation.PokeInt.METADATA_KEY);\n            if (val == null)\n                assertEquals(Epoch.create(2), epoch); // not yet any ints poked at epoch = 2\n            else\n                // -1 since we poke the previous int to the extension:\n                assertEquals((int)(epoch.getEpoch() - 1),  ClusterMetadata.current().extensions.get(CustomTransformation.PokeInt.METADATA_KEY).getValue());\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.tcm;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport org.apache.cassandra.ServerTestUtils;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.db.commitlog.CommitLog;\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.tcm.extensions.ExtensionValue;\nimport org.apache.cassandra.tcm.listeners.MetadataSnapshotListener;\nimport org.apache.cassandra.tcm.listeners.SchemaListener;\nimport org.apache.cassandra.tcm.log.LocalLog;\nimport org.apache.cassandra.tcm.log.LogStorage;\nimport org.apache.cassandra.tcm.ownership.UniformRangePlacement;\nimport org.apache.cassandra.tcm.transformations.CustomTransformation;\nimport org.apache.cassandra.tcm.transformations.SealPeriod;\nimport org.apache.cassandra.utils.FBUtilities;\n\nimport static org.junit.Assert.assertEquals;\n\npublic class LogStateTest\n{\n    @Before\n    public void setup() throws IOException\n    {\n        DatabaseDescriptor.daemonInitialization();\n        StorageService.instance.setPartitionerUnsafe(Murmur3Partitioner.instance);\n        ServerTestUtils.cleanupAndLeaveDirs();\n        CommitLog.instance.start();\n        LogStorage logStorage = LogStorage.SystemKeyspace;\n        MetadataSnapshots snapshots = new MetadataSnapshots.SystemKeyspaceMetadataSnapshots();\n        ClusterMetadata initial = new ClusterMetadata(DatabaseDescriptor.getPartitioner());\n        LocalLog.LogSpec logSpec = LocalLog.logSpec()\n                                           .sync()\n                                           .withInitialState(initial)\n                                           .withStorage(logStorage)\n                                           .withLogListener(new MetadataSnapshotListener())\n                                           .withListener(new SchemaListener(true));\n        LocalLog log = logSpec.createLog();\n        ClusterMetadataService cms = new ClusterMetadataService(new UniformRangePlacement(),\n                                                                snapshots,\n                                                                log,\n                                                                new AtomicLongBackedProcessor(log),\n                                                                Commit.Replicator.NO_OP,\n                                                                false);\n        ClusterMetadataService.unsetInstance();\n        ClusterMetadataService.setInstance(cms);\n        log.readyUnchecked();\n        log.bootstrap(FBUtilities.getBroadcastAddressAndPort());\n    }\n\n    @Test\n    public void testRevertEpoch()\n    {\n        ClusterMetadataService.instance().sealPeriod();\n        List<Epoch> customEpochs = new ArrayList<>(40);\n        for (int i=0; i < 10; i++)\n        {\n            for (int j = 0; j < 4; j++)\n            {\n                ClusterMetadataService.instance()\n                                      .commit(new CustomTransformation(CustomTransformation.PokeInt.NAME,\n                                                                       new CustomTransformation.PokeInt((int) ClusterMetadata.current().epoch.getEpoch())));\n                customEpochs.add(ClusterMetadata.current().epoch);\n            }\n            ClusterMetadataService.instance().commit(SealPeriod.instance);\n        }\n\n        for (Epoch epoch : customEpochs)\n        {\n            ClusterMetadataService.instance().revertToEpoch(epoch);\n            ExtensionValue<?> val = ClusterMetadata.current().extensions.get(CustomTransformation.PokeInt.METADATA_KEY);\n            if (val == null)\n                assertEquals(Epoch.create(2), epoch); // not yet any ints poked at epoch = 2\n            else\n                // -1 since we poke the previous int to the extension:\n                assertEquals((int)(epoch.getEpoch() - 1),  ClusterMetadata.current().extensions.get(CustomTransformation.PokeInt.METADATA_KEY).getValue());\n        }\n    }\n}\n","lineNo":57}
{"Smelly Sample":"/*\n* Licensed to the Apache Software Foundation (ASF) under one\n* or more contributor license agreements.  See the NOTICE file\n* distributed with this work for additional information\n* regarding copyright ownership.  The ASF licenses this file\n* to you under the Apache License, Version 2.0 (the\n* \"License\"); you may not use this file except in compliance\n* with the License.  You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing,\n* software distributed under the License is distributed on an\n* \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n* KIND, either express or implied.  See the License for the\n* specific language governing permissions and limitations\n* under the License.\n*/\npackage org.apache.cassandra.db;\n\nimport java.io.IOException;\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.nio.ByteBuffer;\nimport java.util.AbstractMap;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.collect.Sets;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.config.ColumnDefinition;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.db.compaction.CompactionManager;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.dht.ByteOrderedPartitioner.BytesToken;\nimport org.apache.cassandra.dht.Range;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.locator.AbstractNetworkTopologySnitch;\nimport org.apache.cassandra.locator.TokenMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class CleanupTest\n{\n    public static final int LOOPS = 200;\n    public static final String KEYSPACE1 = \"CleanupTest1\";\n    public static final String CF_INDEXED1 = \"Indexed1\";\n    public static final String CF_STANDARD1 = \"Standard1\";\n\n    public static final String KEYSPACE2 = \"CleanupTestMultiDc\";\n    public static final String CF_INDEXED2 = \"Indexed2\";\n    public static final String CF_STANDARD2 = \"Standard2\";\n\n    public static final String KEYSPACE3 = \"CleanupSkipSSTables\";\n    public static final String CF_STANDARD3 = \"Standard3\";\n\n    public static final ByteBuffer COLUMN = ByteBufferUtil.bytes(\"birthdate\");\n    public static final ByteBuffer VALUE = ByteBuffer.allocate(8);\n    static\n    {\n        VALUE.putLong(20101229);\n        VALUE.flip();\n    }\n\n    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1),\n                                    SchemaLoader.compositeIndexCFMD(KEYSPACE1, CF_INDEXED1, true));\n\n\n        DatabaseDescriptor.setEndpointSnitch(new AbstractNetworkTopologySnitch()\n        {\n            @Override\n            public String getRack(InetAddress endpoint)\n            {\n                return \"RC1\";\n            }\n\n            @Override\n            public String getDatacenter(InetAddress endpoint)\n            {\n                return \"DC1\";\n            }\n        });\n\n        SchemaLoader.createKeyspace(KEYSPACE2,\n                                    KeyspaceParams.nts(\"DC1\", 1),\n                                    SchemaLoader.standardCFMD(KEYSPACE2, CF_STANDARD2),\n                                    SchemaLoader.compositeIndexCFMD(KEYSPACE2, CF_INDEXED2, true));\n        SchemaLoader.createKeyspace(KEYSPACE3,\n                                    KeyspaceParams.nts(\"DC1\", 1),\n                                    SchemaLoader.standardCFMD(KEYSPACE3, CF_STANDARD3));\n    }\n\n    @Test\n    public void testCleanup() throws ExecutionException, InterruptedException\n    {\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"val\", LOOPS);\n\n        // record max timestamps of the sstables pre-cleanup\n        List<Long> expectedMaxTimestamps = getMaxTimestampList(cfs);\n\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n        // with one token in the ring, owned by the local node, cleanup should be a no-op\n        CompactionManager.instance.performCleanup(cfs, 2);\n\n        // ensure max timestamp of the sstables are retained post-cleanup\n        assert expectedMaxTimestamps.equals(getMaxTimestampList(cfs));\n\n        // check data is still there\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n    }\n\n    @Test\n    public void testCleanupWithIndexes() throws IOException, ExecutionException, InterruptedException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_INDEXED1);\n\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"birthdate\", LOOPS);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n\n        ColumnDefinition cdef = cfs.metadata.getColumnDefinition(COLUMN);\n        String indexName = \"birthdate_key_index\";\n        long start = System.nanoTime();\n        while (!cfs.getBuiltIndexes().contains(indexName) && System.nanoTime() - start < TimeUnit.SECONDS.toNanos(10))\n            Thread.sleep(10);\n\n        RowFilter cf = RowFilter.create();\n        cf.add(cdef, Operator.EQ, VALUE);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).filterOn(\"birthdate\", Operator.EQ, VALUE).build()).size());\n\n        // we don't allow cleanup when the local host has no range to avoid wipping up all data when a node has not join the ring.\n        // So to make sure cleanup erase everything here, we give the localhost the tiniest possible range.\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        byte[] tk1 = new byte[1], tk2 = new byte[1];\n        tk1[0] = 2;\n        tk2[0] = 1;\n        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName(\"127.0.0.1\"));\n        tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName(\"127.0.0.2\"));\n\n        CompactionManager.instance.performCleanup(cfs, 2);\n\n        // row data should be gone\n        assertEquals(0, Util.getAll(Util.cmd(cfs).build()).size());\n\n        // not only should it be gone but there should be no data on disk, not even tombstones\n        assert cfs.getLiveSSTables().isEmpty();\n\n        // 2ary indexes should result in no results, too (although tombstones won't be gone until compacted)\n        assertEquals(0, Util.getAll(Util.cmd(cfs).filterOn(\"birthdate\", Operator.EQ, VALUE).build()).size());\n    }\n\n    @Test\n    public void testCleanupWithNewToken() throws ExecutionException, InterruptedException, UnknownHostException\n    {\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"val\", LOOPS);\n\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n\n        byte[] tk1 = new byte[1], tk2 = new byte[1];\n        tk1[0] = 2;\n        tk2[0] = 1;\n        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName(\"127.0.0.1\"));\n        tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName(\"127.0.0.2\"));\n        CompactionManager.instance.performCleanup(cfs, 2);\n\n        assertEquals(0, Util.getAll(Util.cmd(cfs).build()).size());\n    }\n\n    @Test\n    public void testCleanupWithNoTokenRange() throws Exception\n    {\n\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        tmd.clearUnsafe();\n        tmd.updateHostId(UUID.randomUUID(), InetAddress.getByName(\"127.0.0.1\"));\n        byte[] tk1 = {2};\n        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName(\"127.0.0.1\"));\n\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE2);\n        keyspace.setMetadata(KeyspaceMetadata.create(KEYSPACE2, KeyspaceParams.nts(\"DC1\", 1)));\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD2);\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"val\", LOOPS);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n\n        // remove replication on DC1\n        keyspace.setMetadata(KeyspaceMetadata.create(KEYSPACE2, KeyspaceParams.nts(\"DC1\", 0)));\n\n        // clear token range for localhost on DC1\n\n        CompactionManager.instance.performCleanup(cfs, 2);\n        assertEquals(0, Util.getAll(Util.cmd(cfs).build()).size());\n        assertTrue(cfs.getLiveSSTables().isEmpty());\n    }\n\n    @Test\n    public void testCleanupSkippingSSTables() throws UnknownHostException, ExecutionException, InterruptedException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE3);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD3);\n        cfs.disableAutoCompaction();\n        for (byte i = 0; i < 100; i++)\n        {\n            new RowUpdateBuilder(cfs.metadata, System.currentTimeMillis(), ByteBuffer.wrap(new byte[] {i}))\n                .clustering(COLUMN)\n                .add(\"val\", VALUE)\n                .build()\n                .applyUnsafe();\n            cfs.forceBlockingFlush();\n        }\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        tmd.clearUnsafe();\n        tmd.updateHostId(UUID.randomUUID(), InetAddress.getByName(\"127.0.0.1\"));\n        tmd.updateNormalToken(token(new byte[] {50}), InetAddress.getByName(\"127.0.0.1\"));\n        Set<SSTableReader> beforeFirstCleanup = Sets.newHashSet(cfs.getLiveSSTables());\n        // single token - 127.0.0.1 owns everything, cleanup should be noop\n        cfs.forceCleanup(2);\n        assertEquals(beforeFirstCleanup, cfs.getLiveSSTables());\n        tmd.updateNormalToken(token(new byte[] {120}), InetAddress.getByName(\"127.0.0.2\"));\n        cfs.forceCleanup(2);\n        for (SSTableReader sstable : cfs.getLiveSSTables())\n        {\n            assertEquals(sstable.first, sstable.last); // single-token sstables\n            assertTrue(sstable.first.getToken().compareTo(token(new byte[]{50})) <= 0);\n            // with single-token sstables they should all either be skipped or dropped:\n            assertTrue(beforeFirstCleanup.contains(sstable));\n        }\n    }\n\n\n    @Test\n    public void testNeedsCleanup() throws Exception\n    {\n        // setup\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n        fillCF(cfs, \"val\", LOOPS);\n\n        // prepare SSTable and some useful tokens\n        SSTableReader ssTable = cfs.getLiveSSTables().iterator().next();\n        final Token ssTableMin = ssTable.first.getToken();\n        final Token ssTableMax = ssTable.last.getToken();\n\n        final Token min = token((byte) 0);\n        final Token before1 = token((byte) 2);\n        final Token before2 = token((byte) 5);\n        final Token before3 = token((byte) 10);\n        final Token before4 = token((byte) 47);\n        final Token insideSsTable1 = token((byte) 50);\n        final Token insideSsTable2 = token((byte) 55);\n        final Token max = token((byte) 127, (byte) 127, (byte) 127, (byte) 127);\n\n        // test sanity check\n        assert (min.compareTo(ssTableMin) < 0);\n        assert (before1.compareTo(ssTableMin) < 0);\n        assert (before2.compareTo(ssTableMin) < 0);\n        assert (before3.compareTo(ssTableMin) < 0);\n        assert (before4.compareTo(ssTableMin) < 0);\n        assert (ssTableMin.compareTo(insideSsTable1) < 0);\n        assert (insideSsTable1.compareTo(ssTableMax) < 0);\n        assert (ssTableMin.compareTo(insideSsTable2) < 0);\n        assert (insideSsTable2.compareTo(ssTableMax) < 0);\n        assert (ssTableMax.compareTo(max) < 0);\n\n        // test cases\n        // key: needs cleanup?\n        // value: owned ranges\n        List<Map.Entry<Boolean, List<Range<Token>>>> testCases = new LinkedList<Map.Entry<Boolean, List<Range<Token>>>>()\n        {\n            {\n                add(entry(false, Arrays.asList(range(min, max)))); // SSTable owned as a whole\n                add(entry(true, Arrays.asList(range(min, insideSsTable1)))); // SSTable owned only partially\n                add(entry(true, Arrays.asList(range(insideSsTable1, max)))); // SSTable owned only partially\n                add(entry(true, Arrays.asList(range(min, ssTableMin)))); // SSTable not owned at all\n                add(entry(true, Arrays.asList(range(ssTableMax, max)))); // only last token of SSTable is owned\n                add(entry(true, Arrays.asList(range(min, insideSsTable1), range(insideSsTable2, max)))); // SSTable partially owned by two ranges\n                add(entry(true, Arrays.asList(range(ssTableMin, ssTableMax)))); // first token of SSTable is not owned\n                add(entry(false, Arrays.asList(range(before4, max)))); // first token of SSTable is not owned\n                add(entry(false, Arrays.asList(range(min, before1), range(before2, before3), range(before4, max)))); // SSTable owned by the last range\n                add(entry(true, Collections.EMPTY_LIST)); // empty token range means discard entire sstable\n            }\n        };\n\n        // check all test cases\n        for (Map.Entry<Boolean, List<Range<Token>>> testCase : testCases)\n        {\n            assertEquals(testCase.getKey(), CompactionManager.needsCleanup(ssTable, testCase.getValue()));\n        }\n    }\n    private static BytesToken token(byte ... value)\n    {\n        return new BytesToken(value);\n    }\n    private static <K, V> Map.Entry<K, V> entry(K k, V v)\n    {\n       return new AbstractMap.SimpleEntry<K, V>(k, v);\n    }\n    private static Range<Token> range(Token from, Token to)\n    {\n        return new Range<>(from, to);\n    }\n\n    protected void fillCF(ColumnFamilyStore cfs, String colName, int rowsPerSSTable)\n    {\n        CompactionManager.instance.disableAutoCompaction();\n\n        for (int i = 0; i < rowsPerSSTable; i++)\n        {\n            String key = String.valueOf(i);\n            // create a row and update the birthdate value, test that the index query fetches the new version\n            new RowUpdateBuilder(cfs.metadata, System.currentTimeMillis(), ByteBufferUtil.bytes(key))\n                    .clustering(COLUMN)\n                    .add(colName, VALUE)\n                    .build()\n                    .applyUnsafe();\n        }\n\n        cfs.forceBlockingFlush();\n    }\n\n    protected List<Long> getMaxTimestampList(ColumnFamilyStore cfs)\n    {\n        List<Long> list = new LinkedList<Long>();\n        for (SSTableReader sstable : cfs.getLiveSSTables())\n            list.add(sstable.getMaxTimestamp());\n        return list;\n    }\n}\n","Method after Refactoring":"/*\n* Licensed to the Apache Software Foundation (ASF) under one\n* or more contributor license agreements.  See the NOTICE file\n* distributed with this work for additional information\n* regarding copyright ownership.  The ASF licenses this file\n* to you under the Apache License, Version 2.0 (the\n* \"License\"); you may not use this file except in compliance\n* with the License.  You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing,\n* software distributed under the License is distributed on an\n* \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n* KIND, either express or implied.  See the License for the\n* specific language governing permissions and limitations\n* under the License.\n*/\npackage org.apache.cassandra.db;\n\nimport java.io.IOException;\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.nio.ByteBuffer;\nimport java.util.AbstractMap;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.collect.Sets;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.SchemaLoader;\nimport org.apache.cassandra.Util;\nimport org.apache.cassandra.config.ColumnDefinition;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.locator.AbstractReplicationStrategy;\nimport org.apache.cassandra.locator.IEndpointSnitch;\nimport org.apache.cassandra.locator.PendingRangeMaps;\nimport org.apache.cassandra.locator.PropertyFileSnitch;\nimport org.apache.cassandra.locator.SimpleStrategy;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.db.compaction.CompactionManager;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.dht.ByteOrderedPartitioner.BytesToken;\nimport org.apache.cassandra.dht.Range;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.dht.Token;\nimport org.apache.cassandra.locator.AbstractNetworkTopologySnitch;\nimport org.apache.cassandra.locator.TokenMetadata;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.service.StorageService;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class CleanupTest\n{\n    public static final int LOOPS = 200;\n    public static final String KEYSPACE1 = \"CleanupTest1\";\n    public static final String CF_INDEXED1 = \"Indexed1\";\n    public static final String CF_STANDARD1 = \"Standard1\";\n\n    public static final String KEYSPACE2 = \"CleanupTestMultiDc\";\n    public static final String CF_INDEXED2 = \"Indexed2\";\n    public static final String CF_STANDARD2 = \"Standard2\";\n\n    public static final String KEYSPACE3 = \"CleanupSkipSSTables\";\n    public static final String CF_STANDARD3 = \"Standard3\";\n\n    public static final ByteBuffer COLUMN = ByteBufferUtil.bytes(\"birthdate\");\n    public static final ByteBuffer VALUE = ByteBuffer.allocate(8);\n    static\n    {\n        VALUE.putLong(20101229);\n        VALUE.flip();\n    }\n\n    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1),\n                                    SchemaLoader.compositeIndexCFMD(KEYSPACE1, CF_INDEXED1, true));\n\n\n        DatabaseDescriptor.setEndpointSnitch(new AbstractNetworkTopologySnitch()\n        {\n            @Override\n            public String getRack(InetAddress endpoint)\n            {\n                return \"RC1\";\n            }\n\n            @Override\n            public String getDatacenter(InetAddress endpoint)\n            {\n                return \"DC1\";\n            }\n        });\n\n        SchemaLoader.createKeyspace(KEYSPACE2,\n                                    KeyspaceParams.nts(\"DC1\", 1),\n                                    SchemaLoader.standardCFMD(KEYSPACE2, CF_STANDARD2),\n                                    SchemaLoader.compositeIndexCFMD(KEYSPACE2, CF_INDEXED2, true));\n        SchemaLoader.createKeyspace(KEYSPACE3,\n                                    KeyspaceParams.nts(\"DC1\", 1),\n                                    SchemaLoader.standardCFMD(KEYSPACE3, CF_STANDARD3));\n    }\n\n    @Test\n    public void testCleanup() throws ExecutionException, InterruptedException, UnknownHostException\n    {\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        tmd.clearUnsafe();\n        tmd.updateNormalToken(token(new byte[]{ 50 }), InetAddress.getByName(\"127.0.0.1\"));\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"val\", LOOPS);\n\n        // record max timestamps of the sstables pre-cleanup\n        List<Long> expectedMaxTimestamps = getMaxTimestampList(cfs);\n\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n        // with one token in the ring, owned by the local node, cleanup should be a no-op\n        CompactionManager.instance.performCleanup(cfs, 2);\n\n        // ensure max timestamp of the sstables are retained post-cleanup\n        assert expectedMaxTimestamps.equals(getMaxTimestampList(cfs));\n\n        // check data is still there\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n    }\n\n    @Test\n    public void testCleanupWithIndexes() throws IOException, ExecutionException, InterruptedException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_INDEXED1);\n\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"birthdate\", LOOPS);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n\n        ColumnDefinition cdef = cfs.metadata.getColumnDefinition(COLUMN);\n        String indexName = \"birthdate_key_index\";\n        long start = System.nanoTime();\n        while (!cfs.getBuiltIndexes().contains(indexName) && System.nanoTime() - start < TimeUnit.SECONDS.toNanos(10))\n            Thread.sleep(10);\n\n        RowFilter cf = RowFilter.create();\n        cf.add(cdef, Operator.EQ, VALUE);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).filterOn(\"birthdate\", Operator.EQ, VALUE).build()).size());\n\n        // we don't allow cleanup when the local host has no range to avoid wipping up all data when a node has not join the ring.\n        // So to make sure cleanup erase everything here, we give the localhost the tiniest possible range.\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        byte[] tk1 = new byte[1], tk2 = new byte[1];\n        tk1[0] = 2;\n        tk2[0] = 1;\n        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName(\"127.0.0.1\"));\n        tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName(\"127.0.0.2\"));\n\n        CompactionManager.instance.performCleanup(cfs, 2);\n\n        // row data should be gone\n        assertEquals(0, Util.getAll(Util.cmd(cfs).build()).size());\n\n        // not only should it be gone but there should be no data on disk, not even tombstones\n        assert cfs.getLiveSSTables().isEmpty();\n\n        // 2ary indexes should result in no results, too (although tombstones won't be gone until compacted)\n        assertEquals(0, Util.getAll(Util.cmd(cfs).filterOn(\"birthdate\", Operator.EQ, VALUE).build()).size());\n    }\n\n    @Test\n    public void testCleanupWithNewToken() throws ExecutionException, InterruptedException, UnknownHostException\n    {\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"val\", LOOPS);\n\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n\n        byte[] tk1 = new byte[1], tk2 = new byte[1];\n        tk1[0] = 2;\n        tk2[0] = 1;\n        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName(\"127.0.0.1\"));\n        tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName(\"127.0.0.2\"));\n        CompactionManager.instance.performCleanup(cfs, 2);\n\n        assertEquals(0, Util.getAll(Util.cmd(cfs).build()).size());\n    }\n\n    @Test\n    public void testCleanupWithNoTokenRange() throws Exception\n    {\n\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        tmd.clearUnsafe();\n        tmd.updateHostId(UUID.randomUUID(), InetAddress.getByName(\"127.0.0.1\"));\n        byte[] tk1 = {2};\n        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName(\"127.0.0.1\"));\n\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE2);\n        keyspace.setMetadata(KeyspaceMetadata.create(KEYSPACE2, KeyspaceParams.nts(\"DC1\", 1)));\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD2);\n\n        // insert data and verify we get it back w/ range query\n        fillCF(cfs, \"val\", LOOPS);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n\n        // remove replication on DC1\n        keyspace.setMetadata(KeyspaceMetadata.create(KEYSPACE2, KeyspaceParams.nts(\"DC1\", 0)));\n\n        // clear token range for localhost on DC1\n\n        CompactionManager.instance.performCleanup(cfs, 2);\n        assertEquals(0, Util.getAll(Util.cmd(cfs).build()).size());\n        assertTrue(cfs.getLiveSSTables().isEmpty());\n    }\n\n    @Test\n    public void testCleanupSkippingSSTables() throws UnknownHostException, ExecutionException, InterruptedException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE3);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD3);\n        cfs.disableAutoCompaction();\n        for (byte i = 0; i < 100; i++)\n        {\n            new RowUpdateBuilder(cfs.metadata, System.currentTimeMillis(), ByteBuffer.wrap(new byte[] {i}))\n                .clustering(COLUMN)\n                .add(\"val\", VALUE)\n                .build()\n                .applyUnsafe();\n            cfs.forceBlockingFlush();\n        }\n        TokenMetadata tmd = StorageService.instance.getTokenMetadata();\n        tmd.clearUnsafe();\n        tmd.updateHostId(UUID.randomUUID(), InetAddress.getByName(\"127.0.0.1\"));\n        tmd.updateNormalToken(token(new byte[] {50}), InetAddress.getByName(\"127.0.0.1\"));\n        Set<SSTableReader> beforeFirstCleanup = Sets.newHashSet(cfs.getLiveSSTables());\n        // single token - 127.0.0.1 owns everything, cleanup should be noop\n        cfs.forceCleanup(2);\n        assertEquals(beforeFirstCleanup, cfs.getLiveSSTables());\n        tmd.updateNormalToken(token(new byte[] {120}), InetAddress.getByName(\"127.0.0.2\"));\n        cfs.forceCleanup(2);\n        for (SSTableReader sstable : cfs.getLiveSSTables())\n        {\n            assertEquals(sstable.first, sstable.last); // single-token sstables\n            assertTrue(sstable.first.getToken().compareTo(token(new byte[]{50})) <= 0);\n            // with single-token sstables they should all either be skipped or dropped:\n            assertTrue(beforeFirstCleanup.contains(sstable));\n        }\n    }\n\n\n    @Test\n    public void testNeedsCleanup() throws Exception\n    {\n        // setup\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n        fillCF(cfs, \"val\", LOOPS);\n\n        // prepare SSTable and some useful tokens\n        SSTableReader ssTable = cfs.getLiveSSTables().iterator().next();\n        final Token ssTableMin = ssTable.first.getToken();\n        final Token ssTableMax = ssTable.last.getToken();\n\n        final Token min = token((byte) 0);\n        final Token before1 = token((byte) 2);\n        final Token before2 = token((byte) 5);\n        final Token before3 = token((byte) 10);\n        final Token before4 = token((byte) 47);\n        final Token insideSsTable1 = token((byte) 50);\n        final Token insideSsTable2 = token((byte) 55);\n        final Token max = token((byte) 127, (byte) 127, (byte) 127, (byte) 127);\n\n        // test sanity check\n        assert (min.compareTo(ssTableMin) < 0);\n        assert (before1.compareTo(ssTableMin) < 0);\n        assert (before2.compareTo(ssTableMin) < 0);\n        assert (before3.compareTo(ssTableMin) < 0);\n        assert (before4.compareTo(ssTableMin) < 0);\n        assert (ssTableMin.compareTo(insideSsTable1) < 0);\n        assert (insideSsTable1.compareTo(ssTableMax) < 0);\n        assert (ssTableMin.compareTo(insideSsTable2) < 0);\n        assert (insideSsTable2.compareTo(ssTableMax) < 0);\n        assert (ssTableMax.compareTo(max) < 0);\n\n        // test cases\n        // key: needs cleanup?\n        // value: owned ranges\n        List<Map.Entry<Boolean, List<Range<Token>>>> testCases = new LinkedList<Map.Entry<Boolean, List<Range<Token>>>>()\n        {\n            {\n                add(entry(false, Arrays.asList(range(min, max)))); // SSTable owned as a whole\n                add(entry(true, Arrays.asList(range(min, insideSsTable1)))); // SSTable owned only partially\n                add(entry(true, Arrays.asList(range(insideSsTable1, max)))); // SSTable owned only partially\n                add(entry(true, Arrays.asList(range(min, ssTableMin)))); // SSTable not owned at all\n                add(entry(true, Arrays.asList(range(ssTableMax, max)))); // only last token of SSTable is owned\n                add(entry(true, Arrays.asList(range(min, insideSsTable1), range(insideSsTable2, max)))); // SSTable partially owned by two ranges\n                add(entry(true, Arrays.asList(range(ssTableMin, ssTableMax)))); // first token of SSTable is not owned\n                add(entry(false, Arrays.asList(range(before4, max)))); // first token of SSTable is not owned\n                add(entry(false, Arrays.asList(range(min, before1), range(before2, before3), range(before4, max)))); // SSTable owned by the last range\n                add(entry(true, Collections.EMPTY_LIST)); // empty token range means discard entire sstable\n            }\n        };\n\n        // check all test cases\n        for (Map.Entry<Boolean, List<Range<Token>>> testCase : testCases)\n        {\n            assertEquals(testCase.getKey(), CompactionManager.needsCleanup(ssTable, testCase.getValue()));\n        }\n    }\n\n    @Test\n    public void testCleanupIsAbortedWhenNodeHasPendingRanges() throws ExecutionException, InterruptedException, UnknownHostException\n    {\n        // given\n        StorageService.instance.getTokenMetadata().clearUnsafe();\n\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF_STANDARD1);\n\n        fillCF(cfs, \"val\", LOOPS);\n        assertEquals(LOOPS, Util.getAll(Util.cmd(cfs).build()).size());\n\n        Range<Token> range = range(new BytesToken(new byte[]{0}), new BytesToken(new byte[]{1}));\n        givenPendingRange(cfs, range);\n\n        // when\n        CompactionManager.AllSSTableOpStatus status = CompactionManager.instance.performCleanup(cfs, 2);\n\n        // then\n        assertEquals(\"cleanup should be aborted\", CompactionManager.AllSSTableOpStatus.ABORTED, status);\n    }\n\n    private void givenPendingRange(ColumnFamilyStore cfs, Range<Token> range) throws UnknownHostException\n    {\n        StorageService.instance.getTokenMetadata().calculatePendingRanges(createStrategy(cfs.keyspace.getName()), cfs.keyspace.getName());\n        PendingRangeMaps ranges = StorageService.instance.getTokenMetadata().getPendingRanges(cfs.keyspace.getName());\n        ranges.addPendingRange(range, InetAddress.getByName(\"127.0.0.1\"));\n    }\n\n    private AbstractReplicationStrategy createStrategy(String keyspace)\n    {\n        IEndpointSnitch snitch = new PropertyFileSnitch();\n        DatabaseDescriptor.setEndpointSnitch(snitch);\n        return new SimpleStrategy(keyspace, new TokenMetadata(), DatabaseDescriptor.getEndpointSnitch(), Collections.emptyMap());\n    }\n\n    private static BytesToken token(byte ... value)\n    {\n        return new BytesToken(value);\n    }\n    private static <K, V> Map.Entry<K, V> entry(K k, V v)\n    {\n       return new AbstractMap.SimpleEntry<K, V>(k, v);\n    }\n    private static Range<Token> range(Token from, Token to)\n    {\n        return new Range<>(from, to);\n    }\n\n    protected void fillCF(ColumnFamilyStore cfs, String colName, int rowsPerSSTable)\n    {\n        CompactionManager.instance.disableAutoCompaction();\n\n        for (int i = 0; i < rowsPerSSTable; i++)\n        {\n            String key = String.valueOf(i);\n            // create a row and update the birthdate value, test that the index query fetches the new version\n            new RowUpdateBuilder(cfs.metadata, System.currentTimeMillis(), ByteBufferUtil.bytes(key))\n                    .clustering(COLUMN)\n                    .add(colName, VALUE)\n                    .build()\n                    .applyUnsafe();\n        }\n\n        cfs.forceBlockingFlush();\n    }\n\n    protected List<Long> getMaxTimestampList(ColumnFamilyStore cfs)\n    {\n        List<Long> list = new LinkedList<Long>();\n        for (SSTableReader sstable : cfs.getLiveSSTables())\n            list.add(sstable.getMaxTimestamp());\n        return list;\n    }\n}\n","lineNo":126}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.transport.messages;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport io.netty.buffer.ByteBuf;\n\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.service.ClientState;\nimport org.apache.cassandra.service.QueryState;\nimport org.apache.cassandra.transport.*;\nimport org.apache.cassandra.utils.CassandraVersion;\n\n/**\n * The initial message of the protocol.\n * Sets up a number of connection options.\n */\npublic class StartupMessage extends Message.Request\n{\n    public static final String CQL_VERSION = \"CQL_VERSION\";\n    public static final String COMPRESSION = \"COMPRESSION\";\n    public static final String PROTOCOL_VERSIONS = \"PROTOCOL_VERSIONS\";\n    public static final String DRIVER_NAME = \"DRIVER_NAME\";\n    public static final String DRIVER_VERSION = \"DRIVER_VERSION\";\n    public static final String THROW_ON_OVERLOAD = \"THROW_ON_OVERLOAD\";\n\n    public static final Message.Codec<StartupMessage> codec = new Message.Codec<StartupMessage>()\n    {\n        public StartupMessage decode(ByteBuf body, ProtocolVersion version)\n        {\n            return new StartupMessage(upperCaseKeys(CBUtil.readStringMap(body)));\n        }\n\n        public void encode(StartupMessage msg, ByteBuf dest, ProtocolVersion version)\n        {\n            CBUtil.writeStringMap(msg.options, dest);\n        }\n\n        public int encodedSize(StartupMessage msg, ProtocolVersion version)\n        {\n            return CBUtil.sizeOfStringMap(msg.options);\n        }\n    };\n\n    public final Map<String, String> options;\n\n    public StartupMessage(Map<String, String> options)\n    {\n        super(Message.Type.STARTUP);\n        this.options = options;\n    }\n\n    @Override\n    protected Message.Response execute(QueryState state, long queryStartNanoTime, boolean traceRequest)\n    {\n        String cqlVersion = options.get(CQL_VERSION);\n        if (cqlVersion == null)\n            throw new ProtocolException(\"Missing value CQL_VERSION in STARTUP message\");\n\n        try\n        {\n            if (new CassandraVersion(cqlVersion).compareTo(new CassandraVersion(\"2.99.0\")) < 0)\n                throw new ProtocolException(String.format(\"CQL version %s is not supported by the binary protocol (supported version are >= 3.0.0)\", cqlVersion));\n        }\n        catch (IllegalArgumentException e)\n        {\n            throw new ProtocolException(e.getMessage());\n        }\n\n        if (options.containsKey(COMPRESSION))\n        {\n            String compression = options.get(COMPRESSION).toLowerCase();\n            if (compression.equals(\"snappy\"))\n            {\n                if (Compressor.SnappyCompressor.instance == null)\n                    throw new ProtocolException(\"This instance does not support Snappy compression\");\n\n                if (getSource().header.version.isGreaterOrEqualTo(ProtocolVersion.V5))\n                    throw new ProtocolException(\"Snappy compression is not supported in protocol V5\");\n\n                connection.setCompressor(Compressor.SnappyCompressor.instance);\n            }\n            else if (compression.equals(\"lz4\"))\n            {\n                connection.setCompressor(Compressor.LZ4Compressor.instance);\n            }\n            else\n            {\n                throw new ProtocolException(String.format(\"Unknown compression algorithm: %s\", compression));\n            }\n        }\n\n        connection.setThrowOnOverload(\"1\".equals(options.get(THROW_ON_OVERLOAD)));\n\n        ClientState clientState = state.getClientState();\n        clientState.setClientOptions(options);\n        String driverName = options.get(DRIVER_NAME);\n        if (null != driverName)\n        {\n            clientState.setDriverName(driverName);\n            clientState.setDriverVersion(options.get(DRIVER_VERSION));\n        }\n\n        if (DatabaseDescriptor.getAuthenticator().requireAuthentication())\n            return new AuthenticateMessage(DatabaseDescriptor.getAuthenticator().getClass().getName());\n        else\n            return new ReadyMessage();\n    }\n\n    private static Map<String, String> upperCaseKeys(Map<String, String> options)\n    {\n        Map<String, String> newMap = new HashMap<String, String>(options.size());\n        for (Map.Entry<String, String> entry : options.entrySet())\n            newMap.put(entry.getKey().toUpperCase(), entry.getValue());\n        return newMap;\n    }\n\n    @Override\n    public String toString()\n    {\n        return \"STARTUP \" + options;\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.transport.messages;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport io.netty.buffer.ByteBuf;\n\nimport org.apache.cassandra.auth.IAuthenticator;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.service.ClientState;\nimport org.apache.cassandra.service.QueryState;\nimport org.apache.cassandra.transport.*;\nimport org.apache.cassandra.utils.CassandraVersion;\n\n/**\n * The initial message of the protocol.\n * Sets up a number of connection options.\n */\npublic class StartupMessage extends Message.Request\n{\n    public static final String CQL_VERSION = \"CQL_VERSION\";\n    public static final String COMPRESSION = \"COMPRESSION\";\n    public static final String PROTOCOL_VERSIONS = \"PROTOCOL_VERSIONS\";\n    public static final String DRIVER_NAME = \"DRIVER_NAME\";\n    public static final String DRIVER_VERSION = \"DRIVER_VERSION\";\n    public static final String THROW_ON_OVERLOAD = \"THROW_ON_OVERLOAD\";\n\n    public static final Message.Codec<StartupMessage> codec = new Message.Codec<StartupMessage>()\n    {\n        public StartupMessage decode(ByteBuf body, ProtocolVersion version)\n        {\n            return new StartupMessage(upperCaseKeys(CBUtil.readStringMap(body)));\n        }\n\n        public void encode(StartupMessage msg, ByteBuf dest, ProtocolVersion version)\n        {\n            CBUtil.writeStringMap(msg.options, dest);\n        }\n\n        public int encodedSize(StartupMessage msg, ProtocolVersion version)\n        {\n            return CBUtil.sizeOfStringMap(msg.options);\n        }\n    };\n\n    private static final byte[] EMPTY_CLIENT_RESPONSE = new byte[0];\n\n    public final Map<String, String> options;\n\n    public StartupMessage(Map<String, String> options)\n    {\n        super(Message.Type.STARTUP);\n        this.options = options;\n    }\n\n    @Override\n    protected Message.Response execute(QueryState state, long queryStartNanoTime, boolean traceRequest)\n    {\n        String cqlVersion = options.get(CQL_VERSION);\n        if (cqlVersion == null)\n            throw new ProtocolException(\"Missing value CQL_VERSION in STARTUP message\");\n\n        try\n        {\n            if (new CassandraVersion(cqlVersion).compareTo(new CassandraVersion(\"2.99.0\")) < 0)\n                throw new ProtocolException(String.format(\"CQL version %s is not supported by the binary protocol (supported version are >= 3.0.0)\", cqlVersion));\n        }\n        catch (IllegalArgumentException e)\n        {\n            throw new ProtocolException(e.getMessage());\n        }\n\n        if (options.containsKey(COMPRESSION))\n        {\n            String compression = options.get(COMPRESSION).toLowerCase();\n            if (compression.equals(\"snappy\"))\n            {\n                if (Compressor.SnappyCompressor.instance == null)\n                    throw new ProtocolException(\"This instance does not support Snappy compression\");\n\n                if (getSource().header.version.isGreaterOrEqualTo(ProtocolVersion.V5))\n                    throw new ProtocolException(\"Snappy compression is not supported in protocol V5\");\n\n                connection.setCompressor(Compressor.SnappyCompressor.instance);\n            }\n            else if (compression.equals(\"lz4\"))\n            {\n                connection.setCompressor(Compressor.LZ4Compressor.instance);\n            }\n            else\n            {\n                throw new ProtocolException(String.format(\"Unknown compression algorithm: %s\", compression));\n            }\n        }\n\n        connection.setThrowOnOverload(\"1\".equals(options.get(THROW_ON_OVERLOAD)));\n\n        ClientState clientState = state.getClientState();\n        clientState.setClientOptions(options);\n        String driverName = options.get(DRIVER_NAME);\n        if (null != driverName)\n        {\n            clientState.setDriverName(driverName);\n            clientState.setDriverVersion(options.get(DRIVER_VERSION));\n        }\n\n        IAuthenticator authenticator = DatabaseDescriptor.getAuthenticator();\n        if (authenticator.requireAuthentication())\n        {\n            // If the authenticator supports early authentication, attempt to authenticate.\n            if (authenticator.supportsEarlyAuthentication())\n            {\n                IAuthenticator.SaslNegotiator negotiator = ((ServerConnection) connection).getSaslNegotiator(state);\n                // If the negotiator determines that sending an authenticate message is not necessary, attempt to authenticate here,\n                // otherwise, send an Authenticate message to begin the traditional authentication flow.\n                if (!negotiator.shouldSendAuthenticateMessage())\n                {\n                    // Attempt to authenticate the user.\n                    return AuthUtil.handleLogin(connection, state, EMPTY_CLIENT_RESPONSE, (negotiationComplete, challenge) ->\n                    {\n                        if (negotiationComplete)\n                        {\n                            // Authentication was successful, proceed.\n                            return new ReadyMessage();\n                        } else\n                        {\n                            // It's expected that any negotiator that requires a challenge will likely not support early\n                            // authentication, in this case we can just go through the traditional auth flow.\n                            return new AuthenticateMessage(DatabaseDescriptor.getAuthenticator().getClass().getName());\n                        }\n                    });\n                }\n            }\n            return new AuthenticateMessage(DatabaseDescriptor.getAuthenticator().getClass().getName());\n        }\n        else\n            return new ReadyMessage();\n    }\n\n    private static Map<String, String> upperCaseKeys(Map<String, String> options)\n    {\n        Map<String, String> newMap = new HashMap<String, String>(options.size());\n        for (Map.Entry<String, String> entry : options.entrySet())\n            newMap.put(entry.getKey().toUpperCase(), entry.getValue());\n        return newMap;\n    }\n\n    @Override\n    public String toString()\n    {\n        return \"STARTUP \" + options;\n    }\n}\n","lineNo":124}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(),\n                                                       terms.getMaxSSTableRowId(),\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       terms.getMinTerm(),\n                                                       terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n\n        completeIndexFlush(rowMapping.size(), startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowMapping.size(),\n                                                       0,\n                                                       rowMapping.maxSSTableRowId,\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       ByteBufferUtil.bytes(0),\n                                                       ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(), terms.getMaxSSTableRowId(),\n                                                       minKey, maxKey, \n                                                       terms.getMinTerm(), terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        int rowCount = indexTermType.columnMetadata().isStatic() ? rowMapping.staticRowCount : rowMapping.rowCount;\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n        long maxSSTableRowId = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticSSTableRowId : rowMapping.maxSSTableRowId;\n\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n        completeIndexFlush(rowCount, startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowCount,\n                                                       0, maxSSTableRowId,\n                                                       minKey, maxKey, \n                                                       ByteBufferUtil.bytes(0), ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","lineNo":158}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(),\n                                                       terms.getMaxSSTableRowId(),\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       terms.getMinTerm(),\n                                                       terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n\n        completeIndexFlush(rowMapping.size(), startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowMapping.size(),\n                                                       0,\n                                                       rowMapping.maxSSTableRowId,\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       ByteBufferUtil.bytes(0),\n                                                       ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(), terms.getMaxSSTableRowId(),\n                                                       minKey, maxKey, \n                                                       terms.getMinTerm(), terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        int rowCount = indexTermType.columnMetadata().isStatic() ? rowMapping.staticRowCount : rowMapping.rowCount;\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n        long maxSSTableRowId = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticSSTableRowId : rowMapping.maxSSTableRowId;\n\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n        completeIndexFlush(rowCount, startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowCount,\n                                                       0, maxSSTableRowId,\n                                                       minKey, maxKey, \n                                                       ByteBufferUtil.bytes(0), ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","lineNo":159}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(),\n                                                       terms.getMaxSSTableRowId(),\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       terms.getMinTerm(),\n                                                       terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n\n        completeIndexFlush(rowMapping.size(), startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowMapping.size(),\n                                                       0,\n                                                       rowMapping.maxSSTableRowId,\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       ByteBufferUtil.bytes(0),\n                                                       ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(), terms.getMaxSSTableRowId(),\n                                                       minKey, maxKey, \n                                                       terms.getMinTerm(), terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        int rowCount = indexTermType.columnMetadata().isStatic() ? rowMapping.staticRowCount : rowMapping.rowCount;\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n        long maxSSTableRowId = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticSSTableRowId : rowMapping.maxSSTableRowId;\n\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n        completeIndexFlush(rowCount, startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowCount,\n                                                       0, maxSSTableRowId,\n                                                       minKey, maxKey, \n                                                       ByteBufferUtil.bytes(0), ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","lineNo":180}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(),\n                                                       terms.getMaxSSTableRowId(),\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       terms.getMinTerm(),\n                                                       terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n\n        completeIndexFlush(rowMapping.size(), startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowMapping.size(),\n                                                       0,\n                                                       rowMapping.maxSSTableRowId,\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       ByteBufferUtil.bytes(0),\n                                                       ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(), terms.getMaxSSTableRowId(),\n                                                       minKey, maxKey, \n                                                       terms.getMinTerm(), terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        int rowCount = indexTermType.columnMetadata().isStatic() ? rowMapping.staticRowCount : rowMapping.rowCount;\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n        long maxSSTableRowId = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticSSTableRowId : rowMapping.maxSSTableRowId;\n\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n        completeIndexFlush(rowCount, startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowCount,\n                                                       0, maxSSTableRowId,\n                                                       minKey, maxKey, \n                                                       ByteBufferUtil.bytes(0), ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","lineNo":181}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(),\n                                                       terms.getMaxSSTableRowId(),\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       terms.getMinTerm(),\n                                                       terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n\n        completeIndexFlush(rowMapping.size(), startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowMapping.size(),\n                                                       0,\n                                                       rowMapping.maxSSTableRowId,\n                                                       rowMapping.minKey,\n                                                       rowMapping.maxKey,\n                                                       ByteBufferUtil.bytes(0),\n                                                       ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.index.sai.disk.v1;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.base.Stopwatch;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.hppc.LongArrayList;\nimport org.apache.cassandra.db.rows.Row;\nimport org.apache.cassandra.index.sai.disk.PerColumnIndexWriter;\nimport org.apache.cassandra.index.sai.disk.RowMapping;\nimport org.apache.cassandra.index.sai.disk.format.IndexComponent;\nimport org.apache.cassandra.index.sai.disk.format.IndexDescriptor;\nimport org.apache.cassandra.index.sai.disk.v1.bbtree.NumericIndexWriter;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata;\nimport org.apache.cassandra.index.sai.disk.v1.segment.SegmentWriter;\nimport org.apache.cassandra.index.sai.disk.v1.trie.LiteralIndexWriter;\nimport org.apache.cassandra.index.sai.memory.MemtableIndex;\nimport org.apache.cassandra.index.sai.memory.MemtableTermsIterator;\nimport org.apache.cassandra.index.sai.metrics.IndexMetrics;\nimport org.apache.cassandra.index.sai.utils.IndexIdentifier;\nimport org.apache.cassandra.index.sai.utils.IndexTermType;\nimport org.apache.cassandra.index.sai.utils.PrimaryKey;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.Pair;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\n\n/**\n * Column index writer that flushes indexed data directly from the corresponding Memtable index, without buffering index\n * data in memory.\n */\npublic class MemtableIndexWriter implements PerColumnIndexWriter\n{\n    private static final Logger logger = LoggerFactory.getLogger(MemtableIndexWriter.class);\n\n    private final IndexDescriptor indexDescriptor;\n    private final IndexTermType indexTermType;\n    private final IndexIdentifier indexIdentifier;\n    private final IndexMetrics indexMetrics;\n    private final MemtableIndex memtable;\n    private final RowMapping rowMapping;\n\n    public MemtableIndexWriter(MemtableIndex memtable,\n                               IndexDescriptor indexDescriptor,\n                               IndexTermType indexTermType,\n                               IndexIdentifier indexIdentifier,\n                               IndexMetrics indexMetrics,\n                               RowMapping rowMapping)\n    {\n        assert rowMapping != null && rowMapping != RowMapping.DUMMY : \"Row mapping must exist during FLUSH.\";\n\n        this.indexDescriptor = indexDescriptor;\n        this.indexTermType = indexTermType;\n        this.indexIdentifier = indexIdentifier;\n        this.indexMetrics = indexMetrics;\n        this.memtable = memtable;\n        this.rowMapping = rowMapping;\n    }\n\n    @Override\n    public void addRow(PrimaryKey key, Row row, long sstableRowId)\n    {\n        // Memtable indexes are flushed directly to disk with the aid of a mapping between primary\n        // keys and row IDs in the flushing SSTable. This writer, therefore, does nothing in\n        // response to the flushing of individual rows.\n    }\n\n    @Override\n    public void abort(Throwable cause)\n    {\n        logger.warn(indexIdentifier.logMessage(\"Aborting index memtable flush for {}...\"), indexDescriptor.sstableDescriptor, cause);\n        indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n    }\n\n    @Override\n    public void complete(Stopwatch stopwatch) throws IOException\n    {\n        assert rowMapping.isComplete() : \"Cannot complete the memtable index writer because the row mapping is not complete\";\n\n        long start = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        try\n        {\n            if (!rowMapping.hasRows() || memtable == null || memtable.isEmpty())\n            {\n                logger.debug(indexIdentifier.logMessage(\"No indexed rows to flush from SSTable {}.\"), indexDescriptor.sstableDescriptor);\n                // Write a completion marker even though we haven't written anything to the index,\n                // so we won't try to build the index again for the SSTable\n                ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, true);\n\n                return;\n            }\n\n            if (indexTermType.isVector())\n            {\n                flushVectorIndex(start, stopwatch);\n            }\n            else\n            {\n                final Iterator<Pair<ByteComparable, LongArrayList>> iterator = rowMapping.merge(memtable);\n\n                try (MemtableTermsIterator terms = new MemtableTermsIterator(memtable.getMinTerm(), memtable.getMaxTerm(), iterator))\n                {\n                    long cellCount = flush(terms);\n\n                    completeIndexFlush(cellCount, start, stopwatch);\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            logger.error(indexIdentifier.logMessage(\"Error while flushing index {}\"), t.getMessage(), t);\n            indexMetrics.memtableIndexFlushErrors.inc();\n\n            throw t;\n        }\n    }\n\n    private long flush(MemtableTermsIterator terms) throws IOException\n    {\n        SegmentWriter writer = indexTermType.isLiteral() ? new LiteralIndexWriter(indexDescriptor, indexIdentifier)\n                                                         : new NumericIndexWriter(indexDescriptor,\n                                                                                  indexIdentifier,\n                                                                                  indexTermType.fixedSizeOf());\n\n        SegmentMetadata.ComponentMetadataMap indexMetas = writer.writeCompleteSegment(terms);\n        long numRows = writer.getNumberOfRows();\n\n        // If no rows were written we need to delete any created column index components\n        // so that the index is correctly identified as being empty (only having a completion marker)\n        if (numRows == 0)\n        {\n            indexDescriptor.deleteColumnIndex(indexTermType, indexIdentifier);\n            return 0;\n        }\n\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n\n        // During index memtable flush, the data is sorted based on terms.\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       numRows,\n                                                       terms.getMinSSTableRowId(), terms.getMaxSSTableRowId(),\n                                                       minKey, maxKey, \n                                                       terms.getMinTerm(), terms.getMaxTerm(),\n                                                       indexMetas);\n\n        try (MetadataWriter metadataWriter = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(metadataWriter, Collections.singletonList(metadata));\n        }\n\n        return numRows;\n    }\n\n    private void flushVectorIndex(long startTime, Stopwatch stopwatch) throws IOException\n    {\n        int rowCount = indexTermType.columnMetadata().isStatic() ? rowMapping.staticRowCount : rowMapping.rowCount;\n        PrimaryKey minKey = indexTermType.columnMetadata().isStatic() ? rowMapping.minStaticKey : rowMapping.minKey;\n        PrimaryKey maxKey = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticKey : rowMapping.maxKey;\n        long maxSSTableRowId = indexTermType.columnMetadata().isStatic() ? rowMapping.maxStaticSSTableRowId : rowMapping.maxSSTableRowId;\n\n        SegmentMetadata.ComponentMetadataMap metadataMap = memtable.writeDirect(indexDescriptor, indexIdentifier, rowMapping::get);\n        completeIndexFlush(rowCount, startTime, stopwatch);\n\n        SegmentMetadata metadata = new SegmentMetadata(0,\n                                                       rowCount,\n                                                       0, maxSSTableRowId,\n                                                       minKey, maxKey, \n                                                       ByteBufferUtil.bytes(0), ByteBufferUtil.bytes(0),\n                                                       metadataMap);\n\n        try (MetadataWriter writer = new MetadataWriter(indexDescriptor.openPerIndexOutput(IndexComponent.META, indexIdentifier)))\n        {\n            SegmentMetadata.write(writer, Collections.singletonList(metadata));\n        }\n    }\n\n    private void completeIndexFlush(long cellCount, long startTime, Stopwatch stopwatch) throws IOException\n    {\n        // create a completion marker indicating that the index is complete and not-empty\n        ColumnCompletionMarkerUtil.create(indexDescriptor, indexIdentifier, false);\n\n        indexMetrics.memtableIndexFlushCount.inc();\n\n        long elapsedTime = stopwatch.elapsed(TimeUnit.MILLISECONDS);\n\n        logger.debug(indexIdentifier.logMessage(\"Completed flushing {} memtable index cells to SSTable {}. Duration: {} ms. Total elapsed: {} ms\"),\n                     cellCount,\n                     indexDescriptor.sstableDescriptor,\n                     elapsedTime - startTime,\n                     elapsedTime);\n\n        indexMetrics.memtableFlushCellsPerSecond.update((long) (cellCount * 1000.0 / Math.max(1, elapsedTime - startTime)));\n    }\n}\n","lineNo":182}
{"Smelly Sample":"/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n  * regarding copyright ownership.  The ASF licenses this file\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n  *\n  *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n\npackage org.apache.cassandra.harry.sut;\n\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.util.*;\nimport java.util.function.Function;\n\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.distributed.api.TokenSupplier;\nimport org.apache.cassandra.harry.gen.rng.PCGFastPure;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.membership.Location;\nimport org.apache.cassandra.tcm.membership.NodeId;\n\npublic class TokenPlacementModel\n{\n    public abstract static class ReplicationFactor\n    {\n        private final int nodesTotal;\n\n        public ReplicationFactor(int total)\n        {\n            this.nodesTotal = total;\n        }\n\n        public int total()\n        {\n            return nodesTotal;\n        }\n\n        public abstract int dcs();\n\n        public abstract KeyspaceParams asKeyspaceParams();\n\n        public abstract Map<String, Integer> asMap();\n\n        public ReplicatedRanges replicate(List<Node> nodes)\n        {\n            return replicate(toRanges(nodes), nodes);\n        }\n        public abstract ReplicatedRanges replicate(Range[] ranges, List<Node> nodes);\n    }\n\n    public static class ReplicatedRanges\n    {\n        public final Range[] ranges;\n        public final NavigableMap<Range, List<Node>> placementsForRange;\n\n        public ReplicatedRanges(Range[] ranges, NavigableMap<Range, List<Node>> placementsForRange)\n        {\n            this.ranges = ranges;\n            this.placementsForRange = placementsForRange;\n        }\n\n        public List<Node> replicasFor(long token)\n        {\n            int idx = indexedBinarySearch(ranges, range -> {\n                // exclusive start, so token at the start belongs to a lower range\n                if (token <= range.start)\n                    return 1;\n                // ie token > start && token <= end\n                if (token <= range.end ||range.end == Long.MIN_VALUE)\n                    return 0;\n\n                return -1;\n            });\n            assert idx >= 0 : String.format(\"Somehow ranges %s do not contain token %d\", Arrays.toString(ranges), token);\n            return placementsForRange.get(ranges[idx]);\n        }\n\n        public NavigableMap<Range, List<Node>> asMap()\n        {\n            return placementsForRange;\n        }\n\n        private static <T> int indexedBinarySearch(T[] arr, CompareTo<T> comparator)\n        {\n            int low = 0;\n            int high = arr.length - 1;\n\n            while (low <= high)\n            {\n                int mid = (low + high) >>> 1;\n                T midEl = arr[mid];\n                int cmp = comparator.compareTo(midEl);\n\n                if (cmp < 0)\n                    low = mid + 1;\n                else if (cmp > 0)\n                    high = mid - 1;\n                else\n                    return mid;\n            }\n            return -(low + 1); // key not found\n        }\n    }\n\n    public interface CompareTo<V>\n    {\n        int compareTo(V v);\n    }\n\n    public static void addIfUnique(List<Node> nodes, Set<Integer> names, Node node)\n    {\n        if (names.contains(node.idx()))\n            return;\n        nodes.add(node);\n        names.add(node.idx());\n    }\n\n    /**\n     * Finds a primary replica\n     */\n    public static int primaryReplica(List<Node> nodes, Range range)\n    {\n        for (int i = 0; i < nodes.size(); i++)\n        {\n            if (range.end != Long.MIN_VALUE && nodes.get(i).token() >= range.end)\n                return i;\n        }\n        return -1;\n    }\n\n\n    /**\n     * Generates token ranges from the list of nodes\n     */\n    public static Range[] toRanges(List<Node> nodes)\n    {\n        List<Long> tokens = new ArrayList<>();\n        for (Node node : nodes)\n            tokens.add(node.token());\n        tokens.add(Long.MIN_VALUE);\n        tokens.sort(Long::compareTo);\n\n        Range[] ranges = new Range[nodes.size() + 1];\n        long prev = tokens.get(0);\n        int cnt = 0;\n        for (int i = 1; i < tokens.size(); i++)\n        {\n            long current = tokens.get(i);\n            ranges[cnt++] = new Range(prev, current);\n            prev = current;\n        }\n        ranges[ranges.length - 1] = new Range(prev, Long.MIN_VALUE);\n        return ranges;\n\n    }\n\n    public static List<Node> peerStateToNodes(Object[][] resultset)\n    {\n        List<Node> nodes = new ArrayList<>();\n        for (Object[] row : resultset)\n        {\n            InetAddress address = (InetAddress) row[0];\n            Set<String> tokens = (Set<String>) row[1];\n            String dc = (String) row[2];\n            String rack = (String) row[3];\n            for (String token : tokens)\n            {\n                nodes.add(new Node(0, 0, 0, 0,\n                                   constantLookup(address.toString(),\n                                                  Long.parseLong(token),\n                                                  dc,\n                                                  rack)));\n            }\n        }\n        return nodes;\n    }\n\n    public static class NtsReplicationFactor extends ReplicationFactor\n    {\n        private final Lookup lookup = new DefaultLookup();\n        private KeyspaceParams keyspaceParams;\n        private final Map<String, Integer> map;\n\n        public NtsReplicationFactor(int... nodesPerDc)\n        {\n            super(total(nodesPerDc));\n            this.map = toMap(nodesPerDc, lookup);\n        }\n\n        public NtsReplicationFactor(int dcs, int nodesPerDc)\n        {\n            super(dcs * nodesPerDc);\n            int[] counts = new int[dcs];\n            Arrays.fill(counts, nodesPerDc);\n            this.map = toMap(counts, lookup);\n        }\n\n        public NtsReplicationFactor(Map<String, Integer> m)\n        {\n            super(m.values().stream().reduce(0, Integer::sum));\n            this.map = m;\n        }\n\n        private static int total(int... num)\n        {\n            int tmp = 0;\n            for (int i : num)\n                tmp += i;\n            return tmp;\n        }\n\n        public int dcs()\n        {\n            return map.size();\n        }\n\n        public KeyspaceParams asKeyspaceParams()\n        {\n            if (this.keyspaceParams == null)\n                this.keyspaceParams = toKeyspaceParams();\n            return this.keyspaceParams;\n        }\n\n        public Map<String, Integer> asMap()\n        {\n            return this.map;\n        }\n\n        public ReplicatedRanges replicate(Range[] ranges, List<Node> nodes)\n        {\n            return replicate(ranges, nodes, asMap());\n        }\n\n        private static <T extends Comparable<T>> void assertStrictlySorted(Collection<T> coll)\n        {\n            if (coll.size() <= 1) return;\n\n            Iterator<T> iter = coll.iterator();\n            T prev = iter.next();\n            while (iter.hasNext())\n            {\n                T next = iter.next();\n                assert next.compareTo(prev) > 0 : String.format(\"Collection does not seem to be sorted. %s and %s are in wrong order\", prev, next);\n                prev = next;\n            }\n        }\n\n        public static ReplicatedRanges replicate(Range[] ranges, List<Node> nodes, Map<String, Integer> rfs)\n        {\n            assertStrictlySorted(nodes);\n            Map<String, DatacenterNodes> template = new HashMap<>();\n\n            Map<String, List<Node>> nodesByDC = nodesByDC(nodes);\n            Map<String, Set<String>> racksByDC = racksByDC(nodes);\n\n            for (Map.Entry<String, Integer> entry : rfs.entrySet())\n            {\n                String dc = entry.getKey();\n                int rf = entry.getValue();\n                List<Node> nodesInThisDC = nodesByDC.get(dc);\n                Set<String> racksInThisDC = racksByDC.get(dc);\n                int nodeCount = nodesInThisDC == null ? 0 : nodesInThisDC.size();\n                int rackCount = racksInThisDC == null ? 0 : racksInThisDC.size();\n                if (rf <= 0 || nodeCount == 0)\n                    continue;\n\n                template.put(dc, new DatacenterNodes(rf, rackCount, nodeCount));\n            }\n\n            NavigableMap<Range, Map<String, List<Node>>> replication = new TreeMap<>();\n\n            for (Range range : ranges)\n            {\n                final int idx = primaryReplica(nodes, range);\n                int cnt = 0;\n                if (idx >= 0)\n                {\n                    int dcsToFill = template.size();\n\n                    Map<String, DatacenterNodes> nodesInDCs = new HashMap<>();\n                    for (Map.Entry<String, DatacenterNodes> e : template.entrySet())\n                        nodesInDCs.put(e.getKey(), e.getValue().copy());\n\n                    while (dcsToFill > 0 && cnt < nodes.size())\n                    {\n                        Node node = nodes.get((idx + cnt) % nodes.size());\n                        DatacenterNodes dcNodes = nodesInDCs.get(node.dc());\n                        if (dcNodes != null && dcNodes.addAndCheckIfDone(node, new Location(node.dc(), node.rack())))\n                            dcsToFill--;\n\n                        cnt++;\n                    }\n\n                    replication.put(range, mapValues(nodesInDCs, v -> v.nodes));\n                }\n                else\n                {\n                    // if the range end is larger than the highest assigned token, then treat it\n                    // as part of the wraparound and replicate it to the same nodes as the first\n                    // range. This is most likely caused by a decommission removing the node with\n                    // the largest token.\n                    replication.put(range, replication.get(ranges[0]));\n                }\n            }\n\n            return combine(replication);\n        }\n\n        /**\n         * Replicate ranges to rf nodes.\n         */\n        private static ReplicatedRanges combine(NavigableMap<Range, Map<String, List<Node>>> orig)\n        {\n\n            Range[] ranges = new Range[orig.size()];\n            int idx = 0;\n            NavigableMap<Range, List<Node>> flattened = new TreeMap<>();\n            for (Map.Entry<Range, Map<String, List<Node>>> e : orig.entrySet())\n            {\n                List<Node> placementsForRange = new ArrayList<>();\n                for (List<Node> v : e.getValue().values())\n                    placementsForRange.addAll(v);\n                ranges[idx++] = e.getKey();\n                flattened.put(e.getKey(), placementsForRange);\n            }\n            return new ReplicatedRanges(ranges, flattened);\n        }\n\n        private KeyspaceParams toKeyspaceParams()\n        {\n            Object[] args = new Object[map.size() * 2];\n            int i = 0;\n            for (Map.Entry<String, Integer> e : map.entrySet())\n            {\n                args[i * 2] = e.getKey();\n                args[i * 2 + 1] = e.getValue();\n                i++;\n            }\n\n            return KeyspaceParams.nts(args);\n        }\n\n        private Map<String, Integer> toMap(int[] nodesPerDc, Lookup lookup)\n        {\n            Map<String, Integer> map = new TreeMap<>();\n            for (int i = 0; i < nodesPerDc.length; i++)\n            {\n                map.put(lookup.dc(i + 1), nodesPerDc[i]);\n            }\n            return map;\n        }\n\n        public String toString()\n        {\n            return \"NtsReplicationFactor{\" +\n                   \"map=\" + asMap() +\n                   '}';\n        }\n    }\n\n    private static final class DatacenterNodes\n    {\n        private final List<Node> nodes = new ArrayList<>();\n        private final Set<Location> racks = new HashSet<>();\n\n        /** Number of replicas left to fill from this DC. */\n        int rfLeft;\n        int acceptableRackRepeats;\n\n        public DatacenterNodes copy()\n        {\n            return new DatacenterNodes(rfLeft, acceptableRackRepeats);\n        }\n\n        DatacenterNodes(int rf,\n                        int rackCount,\n                        int nodeCount)\n        {\n            this.rfLeft = Math.min(rf, nodeCount);\n            acceptableRackRepeats = rf - rackCount;\n        }\n\n        // for copying\n        DatacenterNodes(int rfLeft, int acceptableRackRepeats)\n        {\n            this.rfLeft = rfLeft;\n            this.acceptableRackRepeats = acceptableRackRepeats;\n        }\n\n        boolean addAndCheckIfDone(Node node, Location location)\n        {\n            if (done())\n                return false;\n\n            if (nodes.contains(node))\n                // Cannot repeat a node.\n                return false;\n\n            if (racks.add(location))\n            {\n                // New rack.\n                --rfLeft;\n                nodes.add(node);\n                return done();\n            }\n            if (acceptableRackRepeats <= 0)\n                // There must be rfLeft distinct racks left, do not add any more rack repeats.\n                return false;\n\n            nodes.add(node);\n\n            // Added a node that is from an already met rack to match RF when there aren't enough racks.\n            --acceptableRackRepeats;\n            --rfLeft;\n            return done();\n        }\n\n        boolean done()\n        {\n            assert rfLeft >= 0;\n            return rfLeft == 0;\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"DatacenterNodes{\" +\n                   \"nodes=\" + nodes +\n                   \", racks=\" + racks +\n                   \", rfLeft=\" + rfLeft +\n                   \", acceptableRackRepeats=\" + acceptableRackRepeats +\n                   '}';\n        }\n    }\n\n    private static <K extends Comparable<K>, T1, T2> Map<K, T2> mapValues(Map<K, T1> allDCs, Function<T1, T2> map)\n    {\n        NavigableMap<K, T2> res = new TreeMap<>();\n        for (Map.Entry<K, T1> e : allDCs.entrySet())\n        {\n            res.put(e.getKey(), map.apply(e.getValue()));\n        }\n        return res;\n    }\n\n    public static Map<String, List<Node>> nodesByDC(List<Node> nodes)\n    {\n        Map<String, List<Node>> nodesByDC = new HashMap<>();\n        for (Node node : nodes)\n            nodesByDC.computeIfAbsent(node.dc(), (k) -> new ArrayList<>()).add(node);\n\n        return nodesByDC;\n    }\n\n    public static Map<String, Integer> dcLayout(List<Node> nodes)\n    {\n        Map<String, List<Node>> nodesByDC = nodesByDC(nodes);\n        Map<String, Integer> layout = new HashMap<>();\n        for (Map.Entry<String, List<Node>> e : nodesByDC.entrySet())\n            layout.put(e.getKey(), e.getValue().size());\n\n        return layout;\n    }\n\n    public static Map<String, Set<String>> racksByDC(List<Node> nodes)\n    {\n        Map<String, Set<String>> racksByDC = new HashMap<>();\n        for (Node node : nodes)\n            racksByDC.computeIfAbsent(node.dc(), (k) -> new HashSet<>()).add(node.rack());\n\n        return racksByDC;\n    }\n\n\n    public static class SimpleReplicationFactor extends ReplicationFactor\n    {\n        private final Lookup lookup = new DefaultLookup();\n        public SimpleReplicationFactor(int total)\n        {\n            super(total);\n        }\n\n        public int dcs()\n        {\n            return 1;\n        }\n\n        public KeyspaceParams asKeyspaceParams()\n        {\n            return KeyspaceParams.simple(total());\n        }\n\n        public Map<String, Integer> asMap()\n        {\n            return Collections.singletonMap(lookup.dc(1), total());\n        }\n\n        public ReplicatedRanges replicate(Range[] ranges, List<Node> nodes)\n        {\n            return replicate(ranges, nodes, total());\n        }\n\n        public static ReplicatedRanges replicate(Range[] ranges, List<Node> nodes, int rf)\n        {\n            NavigableMap<Range, List<Node>> replication = new TreeMap<>();\n            for (Range range : ranges)\n            {\n                Set<Integer> names = new HashSet<>();\n                List<Node> replicas = new ArrayList<>();\n                int idx = primaryReplica(nodes, range);\n                if (idx >= 0)\n                {\n                    for (int i = idx; i < nodes.size() && replicas.size() < rf; i++)\n                        addIfUnique(replicas, names, nodes.get(i));\n\n                    for (int i = 0; replicas.size() < rf && i < idx; i++)\n                        addIfUnique(replicas, names, nodes.get(i));\n                    if (range.start == Long.MIN_VALUE)\n                        replication.put(ranges[ranges.length - 1], replicas);\n                    replication.put(range, replicas);\n                }\n                else\n                {\n                    // if the range end is larger than the highest assigned token, then treat it\n                    // as part of the wraparound and replicate it to the same nodes as the first\n                    // range. This is most likely caused by a decommission removing the node with\n                    // the largest token.\n                    replication.put(range, replication.get(ranges[0]));\n                }\n            }\n\n            return new ReplicatedRanges(ranges, Collections.unmodifiableNavigableMap(replication));\n        }\n\n        public String toString()\n        {\n            return \"SimpleReplicationFactor{\" +\n                   \"rf=\" + total() +\n                   '}';\n        }\n    }\n\n    /**\n     * A Range is responsible for the tokens between (start, end].\n     */\n    public static class Range implements Comparable<Range>\n    {\n        public final long start;\n        public final long end;\n\n        public Range(long start, long end)\n        {\n            assert end > start || end == Long.MIN_VALUE : String.format(\"Start (%d) should be smaller than end (%d)\", start, end);\n            this.start = start;\n            this.end = end;\n        }\n\n        public boolean contains(long min, long max)\n        {\n            assert max > min;\n            return min > start && (max <= end || end == Long.MIN_VALUE);\n        }\n\n        public boolean contains(long token)\n        {\n            return token > start && (token <= end || end == Long.MIN_VALUE);\n        }\n\n        public int compareTo(Range o)\n        {\n            int res = Long.compare(start, o.start);\n            if (res == 0)\n                return Long.compare(end, o.end);\n            return res;\n        }\n\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Range range = (Range) o;\n            return start == range.start && end == range.end;\n        }\n\n        public int hashCode()\n        {\n            return Objects.hash(start, end);\n        }\n\n        public String toString()\n        {\n            return \"(\" +\n                   \"\" + (start == Long.MIN_VALUE ? \"MIN\" : start) +\n                   \", \" + (end == Long.MIN_VALUE ? \"MIN\" : end) +\n                   ']';\n        }\n    }\n\n    public interface Lookup\n    {\n        String id(int nodeIdx);\n        String dc(int dcIdx);\n        String rack(int rackIdx);\n        long token(int tokenIdx);\n        Lookup forceToken(int tokenIdx, long token);\n        void reset();\n\n        default NodeId nodeId(int nodeIdx)\n        {\n            return ClusterMetadata.current().directory.peerId(addr(nodeIdx));\n        }\n\n        default InetAddressAndPort addr(int idx)\n        {\n            try\n            {\n                return InetAddressAndPort.getByName(id(idx));\n            }\n            catch (UnknownHostException e)\n            {\n                throw new RuntimeException(e);\n            }\n        }\n    }\n\n    public static Lookup constantLookup(String id, long token, String dc, String rack)\n    {\n        return new Lookup()\n        {\n            public String id(int nodeIdx)\n            {\n                return id;\n            }\n\n            public String dc(int dcIdx)\n            {\n                return dc;\n            }\n\n            public String rack(int rackIdx)\n            {\n                return rack;\n            }\n\n            @Override\n            public org.apache.cassandra.tcm.membership.NodeId nodeId(int nodeIdx)\n            {\n                return null;\n            }\n\n            public long token(int tokenIdx)\n            {\n                return token;\n            }\n\n            public Lookup forceToken(int tokenIdx, long token)\n            {\n                throw new UnsupportedOperationException();\n            }\n\n            @Override\n            public InetAddressAndPort addr(int idx)\n            {\n                return null;\n            }\n\n            public void reset()\n            {\n                throw new UnsupportedOperationException();\n            }\n        };\n    }\n\n    public static class DefaultLookup implements Lookup\n    {\n        protected final Map<Integer, Long> overrides = new HashMap<>(2);\n\n        public String id(int nodeIdx)\n        {\n            return String.format(\"127.0.%d.%d\", nodeIdx / 256, nodeIdx % 256);\n        }\n\n        public long token(int tokenIdx)\n        {\n            Long override = overrides.get(tokenIdx);\n            if (override != null)\n                return override;\n            return PCGFastPure.next(tokenIdx, 1L);\n        }\n\n        public Lookup forceToken(int tokenIdx, long token)\n        {\n            DefaultLookup newLookup = new DefaultLookup();\n            newLookup.overrides.putAll(overrides);\n            newLookup.overrides.put(tokenIdx, token);\n            return newLookup;\n        }\n\n        public void reset()\n        {\n            overrides.clear();\n        }\n\n        public String dc(int dcIdx)\n        {\n            return String.format(\"datacenter%d\", dcIdx);\n        }\n\n        public String rack(int rackIdx)\n        {\n            return String.format(\"rack%d\", rackIdx);\n        }\n    }\n\n    public static class HumanReadableTokensLookup extends DefaultLookup {\n        @Override\n        public long token(int tokenIdx)\n        {\n            Long override = overrides.get(tokenIdx);\n            if (override != null)\n                return override;\n            return tokenIdx * 100L;\n        }\n\n        public Lookup forceToken(int tokenIdx, long token)\n        {\n            DefaultLookup lookup = new HumanReadableTokensLookup();\n            lookup.overrides.putAll(overrides);\n            lookup.overrides.put(tokenIdx, token);\n            return lookup;\n        }\n    }\n\n    public static NodeFactory nodeFactory()\n    {\n        return new NodeFactory(new DefaultLookup());\n    }\n\n    public static NodeFactory nodeFactoryHumanReadable()\n    {\n        return new NodeFactory(new HumanReadableTokensLookup());\n    }\n\n    public static class NodeFactory implements TokenSupplier\n    {\n        private final Lookup lookup;\n\n        public NodeFactory(Lookup lookup)\n        {\n            this.lookup = lookup;\n        }\n\n        public Node make(int idx, int dc, int rack)\n        {\n            return new Node(idx, idx, dc, rack, lookup);\n        }\n\n        public Lookup lookup()\n        {\n            return lookup;\n        }\n\n        public Collection<String> tokens(int i)\n        {\n            return Collections.singletonList(Long.toString(lookup.token(i)));\n        }\n    }\n\n    public static class Node implements Comparable<Node>\n    {\n        private final int tokenIdx;\n        private final int nodeIdx;\n        private final int dcIdx;\n        private final int rackIdx;\n        private final Lookup lookup;\n\n        public Node(int tokenIdx, int idx, int dcIdx, int rackIdx, Lookup lookup)\n        {\n            this.tokenIdx = tokenIdx;\n            this.nodeIdx = idx;\n            this.dcIdx = dcIdx;\n            this.rackIdx = rackIdx;\n            this.lookup = lookup;\n        }\n\n        public String id()\n        {\n            return lookup.id(nodeIdx);\n        }\n\n        public int idx()\n        {\n            return nodeIdx;\n        }\n\n        public int dcIdx()\n        {\n            return dcIdx;\n        }\n\n        public int rackIdx()\n        {\n            return rackIdx;\n        }\n\n        public String dc()\n        {\n            return lookup.dc(dcIdx);\n        }\n\n        public String rack()\n        {\n            return lookup.rack(rackIdx);\n        }\n\n        public long token()\n        {\n            return lookup.token(tokenIdx);\n        }\n\n        public int tokenIdx()\n        {\n            return tokenIdx;\n        }\n\n        public Node withNewToken()\n        {\n            return new Node(tokenIdx + 100_000, nodeIdx, dcIdx, rackIdx, lookup);\n        }\n\n        public Node withToken(int tokenIdx)\n        {\n            return new Node(tokenIdx, nodeIdx, dcIdx, rackIdx, lookup);\n        }\n\n        public Node overrideToken(long override)\n        {\n            return new Node(tokenIdx, nodeIdx, dcIdx, rackIdx, lookup.forceToken(tokenIdx, override));\n        }\n        public Murmur3Partitioner.LongToken longToken()\n        {\n            return new Murmur3Partitioner.LongToken(token());\n        }\n\n        public NodeId nodeId()\n        {\n            return lookup.nodeId(idx());\n        }\n\n        public InetAddressAndPort addr()\n        {\n            return lookup.addr(idx());\n        }\n\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || !Node.class.isAssignableFrom(o.getClass())) return false;\n            Node node = (Node) o;\n            return Objects.equals(nodeIdx, node.nodeIdx);\n        }\n\n        public int hashCode()\n        {\n            return Objects.hash(nodeIdx);\n        }\n\n        public int compareTo(Node o)\n        {\n            return Long.compare(token(), o.token());\n        }\n\n        public String toString()\n        {\n            return String.format(\"%s-%s@%d\", dc(), id(), token());\n        }\n    }\n}\n","Method after Refactoring":"/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n  * regarding copyright ownership.  The ASF licenses this file\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n  *\n  *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n\npackage org.apache.cassandra.harry.sut;\n\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.util.*;\nimport java.util.function.Function;\n\nimport org.apache.cassandra.dht.Murmur3Partitioner;\nimport org.apache.cassandra.distributed.api.TokenSupplier;\nimport org.apache.cassandra.harry.gen.rng.PCGFastPure;\nimport org.apache.cassandra.locator.InetAddressAndPort;\nimport org.apache.cassandra.schema.KeyspaceParams;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.tcm.membership.Location;\nimport org.apache.cassandra.tcm.membership.NodeId;\n\npublic class TokenPlacementModel\n{\n    public abstract static class ReplicationFactor\n    {\n        private final int nodesTotal;\n\n        public ReplicationFactor(int total)\n        {\n            this.nodesTotal = total;\n        }\n\n        public int total()\n        {\n            return nodesTotal;\n        }\n\n        public abstract int dcs();\n\n        public abstract KeyspaceParams asKeyspaceParams();\n\n        public abstract Map<String, Integer> asMap();\n\n        public ReplicatedRanges replicate(List<Node> nodes)\n        {\n            return replicate(toRanges(nodes), nodes);\n        }\n\n        public abstract ReplicatedRanges replicate(Range[] ranges, List<Node> nodes);\n    }\n\n    public static class ReplicatedRanges\n    {\n        public final Range[] ranges;\n        public final NavigableMap<Range, List<Node>> placementsForRange;\n\n        public ReplicatedRanges(Range[] ranges, NavigableMap<Range, List<Node>> placementsForRange)\n        {\n            this.ranges = ranges;\n            this.placementsForRange = placementsForRange;\n        }\n\n        public List<Node> replicasFor(long token)\n        {\n            int idx = indexedBinarySearch(ranges, range -> {\n                // exclusive start, so token at the start belongs to a lower range\n                if (token <= range.start)\n                    return 1;\n                // ie token > start && token <= end\n                if (token <= range.end || range.end == Long.MIN_VALUE)\n                    return 0;\n\n                return -1;\n            });\n            assert idx >= 0 : String.format(\"Somehow ranges %s do not contain token %d\", Arrays.toString(ranges), token);\n            return placementsForRange.get(ranges[idx]);\n        }\n\n        public NavigableMap<Range, List<Node>> asMap()\n        {\n            return placementsForRange;\n        }\n\n        private static <T> int indexedBinarySearch(T[] arr, CompareTo<T> comparator)\n        {\n            int low = 0;\n            int high = arr.length - 1;\n\n            while (low <= high)\n            {\n                int mid = (low + high) >>> 1;\n                T midEl = arr[mid];\n                int cmp = comparator.compareTo(midEl);\n\n                if (cmp < 0)\n                    low = mid + 1;\n                else if (cmp > 0)\n                    high = mid - 1;\n                else\n                    return mid;\n            }\n            return -(low + 1); // key not found\n        }\n    }\n\n    public interface CompareTo<V>\n    {\n        int compareTo(V v);\n    }\n\n    public static void addIfUnique(List<Node> nodes, Set<Integer> names, Node node)\n    {\n        if (names.contains(node.idx()))\n            return;\n        nodes.add(node);\n        names.add(node.idx());\n    }\n\n    /**\n     * Finds a primary replica\n     */\n    public static int primaryReplica(List<Node> nodes, Range range)\n    {\n        for (int i = 0; i < nodes.size(); i++)\n        {\n            long token = nodes.get(i).token();\n            if (token == Long.MIN_VALUE)\n            {\n                if (range.end == token)\n                    return i;\n            }\n            else if (range.end != Long.MIN_VALUE && token >= range.end)\n                return i;\n        }\n        return -1;\n    }\n\n\n    /**\n     * Generates token ranges from the list of nodes\n     */\n    public static Range[] toRanges(List<Node> nodes)\n    {\n        boolean hasMinToken = nodes.get(0).token() == Long.MIN_VALUE;\n        List<Long> tokens = new ArrayList<>();\n        for (Node node : nodes)\n            tokens.add(node.token());\n        if (!hasMinToken)\n            tokens.add(Long.MIN_VALUE);\n        tokens.sort(Long::compareTo);\n\n        Range[] ranges = new Range[nodes.size() + (hasMinToken ? 0 : 1)];\n        long prev = tokens.get(0);\n        int cnt = 0;\n        for (int i = 1; i < tokens.size(); i++)\n        {\n            long current = tokens.get(i);\n            ranges[cnt++] = new Range(prev, current);\n            prev = current;\n        }\n        ranges[ranges.length - 1] = new Range(prev, Long.MIN_VALUE);\n        return ranges;\n\n    }\n\n    public static List<Node> peerStateToNodes(Object[][] resultset)\n    {\n        List<Node> nodes = new ArrayList<>();\n        for (Object[] row : resultset)\n        {\n            InetAddress address = (InetAddress) row[0];\n            Set<String> tokens = (Set<String>) row[1];\n            String dc = (String) row[2];\n            String rack = (String) row[3];\n            for (String token : tokens)\n            {\n                nodes.add(new Node(0, 0, 0, 0,\n                                   constantLookup(address.toString(),\n                                                  Long.parseLong(token),\n                                                  dc,\n                                                  rack)));\n            }\n        }\n        return nodes;\n    }\n\n    public static class NtsReplicationFactor extends ReplicationFactor\n    {\n        private final Lookup lookup = new DefaultLookup();\n        private KeyspaceParams keyspaceParams;\n        private final Map<String, Integer> map;\n\n        public NtsReplicationFactor(int... nodesPerDc)\n        {\n            super(total(nodesPerDc));\n            this.map = toMap(nodesPerDc, lookup);\n        }\n\n        public NtsReplicationFactor(int dcs, int nodesPerDc)\n        {\n            super(dcs * nodesPerDc);\n            int[] counts = new int[dcs];\n            Arrays.fill(counts, nodesPerDc);\n            this.map = toMap(counts, lookup);\n        }\n\n        public NtsReplicationFactor(Map<String, Integer> m)\n        {\n            super(m.values().stream().reduce(0, Integer::sum));\n            this.map = m;\n        }\n\n        private static int total(int... num)\n        {\n            int tmp = 0;\n            for (int i : num)\n                tmp += i;\n            return tmp;\n        }\n\n        public int dcs()\n        {\n            return map.size();\n        }\n\n        public KeyspaceParams asKeyspaceParams()\n        {\n            if (this.keyspaceParams == null)\n                this.keyspaceParams = toKeyspaceParams();\n            return this.keyspaceParams;\n        }\n\n        public Map<String, Integer> asMap()\n        {\n            return this.map;\n        }\n\n        public ReplicatedRanges replicate(Range[] ranges, List<Node> nodes)\n        {\n            return replicate(ranges, nodes, asMap());\n        }\n\n        public static ReplicatedRanges replicate(Range[] ranges, List<Node> nodes, Map<String, Integer> rfs)\n        {\n            assertStrictlySorted(nodes);\n            boolean minTokenOwned = nodes.stream().anyMatch(n -> n.token() == Long.MIN_VALUE);\n            Map<String, DatacenterNodes> template = new HashMap<>();\n\n            Map<String, List<Node>> nodesByDC = nodesByDC(nodes);\n            Map<String, Set<String>> racksByDC = racksByDC(nodes);\n\n            for (Map.Entry<String, Integer> entry : rfs.entrySet())\n            {\n                String dc = entry.getKey();\n                int rf = entry.getValue();\n                List<Node> nodesInThisDC = nodesByDC.get(dc);\n                Set<String> racksInThisDC = racksByDC.get(dc);\n                int nodeCount = nodesInThisDC == null ? 0 : nodesInThisDC.size();\n                int rackCount = racksInThisDC == null ? 0 : racksInThisDC.size();\n                if (rf <= 0 || nodeCount == 0)\n                    continue;\n\n                template.put(dc, new DatacenterNodes(rf, rackCount, nodeCount));\n            }\n\n            NavigableMap<Range, Map<String, List<Node>>> replication = new TreeMap<>();\n            Range skipped = null;\n            for (Range range : ranges)\n            {\n                final int idx = primaryReplica(nodes, range);\n                int cnt = 0;\n                if (idx >= 0)\n                {\n                    int dcsToFill = template.size();\n\n                    Map<String, DatacenterNodes> nodesInDCs = new HashMap<>();\n                    for (Map.Entry<String, DatacenterNodes> e : template.entrySet())\n                        nodesInDCs.put(e.getKey(), e.getValue().copy());\n\n                    while (dcsToFill > 0 && cnt < nodes.size())\n                    {\n                        Node node = nodes.get((idx + cnt) % nodes.size());\n                        DatacenterNodes dcNodes = nodesInDCs.get(node.dc());\n                        if (dcNodes != null && dcNodes.addAndCheckIfDone(node, new Location(node.dc(), node.rack())))\n                            dcsToFill--;\n\n                        cnt++;\n                    }\n\n                    replication.put(range, mapValues(nodesInDCs, v -> v.nodes));\n                }\n                else\n                {\n                    if (minTokenOwned)\n                        skipped = range;\n                    else\n                        // if the range end is larger than the highest assigned token, then treat it\n                        // as part of the wraparound and replicate it to the same nodes as the first\n                        // range. This is most likely caused by a decommission removing the node with\n                        // the largest token.\n                        replication.put(range, replication.get(ranges[0]));\n                }\n            }\n\n            // Since we allow owning MIN_TOKEN, when it is owned, we have to replicate the range explicitly.\n            if (skipped != null)\n                replication.put(skipped, replication.get(ranges[ranges.length - 1]));\n\n            return combine(replication);\n        }\n\n        /**\n         * Replicate ranges to rf nodes.\n         */\n        private static ReplicatedRanges combine(NavigableMap<Range, Map<String, List<Node>>> orig)\n        {\n\n            Range[] ranges = new Range[orig.size()];\n            int idx = 0;\n            NavigableMap<Range, List<Node>> flattened = new TreeMap<>();\n            for (Map.Entry<Range, Map<String, List<Node>>> e : orig.entrySet())\n            {\n                List<Node> placementsForRange = new ArrayList<>();\n                for (List<Node> v : e.getValue().values())\n                    placementsForRange.addAll(v);\n                ranges[idx++] = e.getKey();\n                flattened.put(e.getKey(), placementsForRange);\n            }\n            return new ReplicatedRanges(ranges, flattened);\n        }\n\n        private KeyspaceParams toKeyspaceParams()\n        {\n            Object[] args = new Object[map.size() * 2];\n            int i = 0;\n            for (Map.Entry<String, Integer> e : map.entrySet())\n            {\n                args[i * 2] = e.getKey();\n                args[i * 2 + 1] = e.getValue();\n                i++;\n            }\n\n            return KeyspaceParams.nts(args);\n        }\n\n        private Map<String, Integer> toMap(int[] nodesPerDc, Lookup lookup)\n        {\n            Map<String, Integer> map = new TreeMap<>();\n            for (int i = 0; i < nodesPerDc.length; i++)\n            {\n                map.put(lookup.dc(i + 1), nodesPerDc[i]);\n            }\n            return map;\n        }\n\n        public String toString()\n        {\n            return \"NtsReplicationFactor{\" +\n                   \"map=\" + asMap() +\n                   '}';\n        }\n    }\n\n    private static final class DatacenterNodes\n    {\n        private final List<Node> nodes = new ArrayList<>();\n        private final Set<Location> racks = new HashSet<>();\n\n        /** Number of replicas left to fill from this DC. */\n        int rfLeft;\n        int acceptableRackRepeats;\n\n        public DatacenterNodes copy()\n        {\n            return new DatacenterNodes(rfLeft, acceptableRackRepeats);\n        }\n\n        DatacenterNodes(int rf,\n                        int rackCount,\n                        int nodeCount)\n        {\n            this.rfLeft = Math.min(rf, nodeCount);\n            acceptableRackRepeats = rf - rackCount;\n        }\n\n        // for copying\n        DatacenterNodes(int rfLeft, int acceptableRackRepeats)\n        {\n            this.rfLeft = rfLeft;\n            this.acceptableRackRepeats = acceptableRackRepeats;\n        }\n\n        boolean addAndCheckIfDone(Node node, Location location)\n        {\n            if (done())\n                return false;\n\n            if (nodes.contains(node))\n                // Cannot repeat a node.\n                return false;\n\n            if (racks.add(location))\n            {\n                // New rack.\n                --rfLeft;\n                nodes.add(node);\n                return done();\n            }\n            if (acceptableRackRepeats <= 0)\n                // There must be rfLeft distinct racks left, do not add any more rack repeats.\n                return false;\n\n            nodes.add(node);\n\n            // Added a node that is from an already met rack to match RF when there aren't enough racks.\n            --acceptableRackRepeats;\n            --rfLeft;\n            return done();\n        }\n\n        boolean done()\n        {\n            assert rfLeft >= 0;\n            return rfLeft == 0;\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"DatacenterNodes{\" +\n                   \"nodes=\" + nodes +\n                   \", racks=\" + racks +\n                   \", rfLeft=\" + rfLeft +\n                   \", acceptableRackRepeats=\" + acceptableRackRepeats +\n                   '}';\n        }\n    }\n\n    private static <T extends Comparable<T>> void assertStrictlySorted(Collection<T> coll)\n    {\n        if (coll.size() <= 1) return;\n\n        Iterator<T> iter = coll.iterator();\n        T prev = iter.next();\n        while (iter.hasNext())\n        {\n            T next = iter.next();\n            assert next.compareTo(prev) > 0 : String.format(\"Collection does not seem to be sorted. %s and %s are in wrong order\", prev, next);\n            prev = next;\n        }\n    }\n\n    private static <K extends Comparable<K>, T1, T2> Map<K, T2> mapValues(Map<K, T1> allDCs, Function<T1, T2> map)\n    {\n        NavigableMap<K, T2> res = new TreeMap<>();\n        for (Map.Entry<K, T1> e : allDCs.entrySet())\n        {\n            res.put(e.getKey(), map.apply(e.getValue()));\n        }\n        return res;\n    }\n\n    public static Map<String, List<Node>> nodesByDC(List<Node> nodes)\n    {\n        Map<String, List<Node>> nodesByDC = new HashMap<>();\n        for (Node node : nodes)\n            nodesByDC.computeIfAbsent(node.dc(), (k) -> new ArrayList<>()).add(node);\n\n        return nodesByDC;\n    }\n\n    public static Map<String, Integer> dcLayout(List<Node> nodes)\n    {\n        Map<String, List<Node>> nodesByDC = nodesByDC(nodes);\n        Map<String, Integer> layout = new HashMap<>();\n        for (Map.Entry<String, List<Node>> e : nodesByDC.entrySet())\n            layout.put(e.getKey(), e.getValue().size());\n\n        return layout;\n    }\n\n    public static Map<String, Set<String>> racksByDC(List<Node> nodes)\n    {\n        Map<String, Set<String>> racksByDC = new HashMap<>();\n        for (Node node : nodes)\n            racksByDC.computeIfAbsent(node.dc(), (k) -> new HashSet<>()).add(node.rack());\n\n        return racksByDC;\n    }\n\n\n    public static class SimpleReplicationFactor extends ReplicationFactor\n    {\n        private final Lookup lookup = new DefaultLookup();\n        public SimpleReplicationFactor(int total)\n        {\n            super(total);\n        }\n\n        public int dcs()\n        {\n            return 1;\n        }\n\n        public KeyspaceParams asKeyspaceParams()\n        {\n            return KeyspaceParams.simple(total());\n        }\n\n        public Map<String, Integer> asMap()\n        {\n            return Collections.singletonMap(lookup.dc(1), total());\n        }\n\n        public ReplicatedRanges replicate(Range[] ranges, List<Node> nodes)\n        {\n            return replicate(ranges, nodes, total());\n        }\n\n        public static ReplicatedRanges replicate(Range[] ranges, List<Node> nodes, int rf)\n        {\n            assertStrictlySorted(nodes);\n            NavigableMap<Range, List<Node>> replication = new TreeMap<>();\n            boolean minTokenOwned = nodes.stream().anyMatch(n -> n.token() == Long.MIN_VALUE);\n            Range skipped = null;\n            for (Range range : ranges)\n            {\n                Set<Integer> names = new HashSet<>();\n                List<Node> replicas = new ArrayList<>();\n                int idx = primaryReplica(nodes, range);\n                if (idx >= 0)\n                {\n                    for (int i = 0; i < nodes.size() && replicas.size() < rf; i++)\n                        addIfUnique(replicas, names, nodes.get((idx + i) % nodes.size()));\n                    if (!minTokenOwned && range.start == Long.MIN_VALUE)\n                        replication.put(ranges[ranges.length - 1], replicas);\n                    replication.put(range, replicas);\n                }\n                else\n                {\n                    if (minTokenOwned)\n                        skipped = range;\n                    else\n                        // if the range end is larger than the highest assigned token, then treat it\n                        // as part of the wraparound and replicate it to the same nodes as the first\n                        // range. This is most likely caused by a decommission removing the node with\n                        // the largest token.\n                        replication.put(range, replication.get(ranges[0]));\n                }\n            }\n\n            // Since we allow owning MIN_TOKEN, when it is owned, we have to replicate the range explicitly.\n            if (skipped != null)\n                replication.put(skipped, replication.get(ranges[ranges.length - 1]));\n\n            return new ReplicatedRanges(ranges, Collections.unmodifiableNavigableMap(replication));\n        }\n\n        public String toString()\n        {\n            return \"SimpleReplicationFactor{\" +\n                   \"rf=\" + total() +\n                   '}';\n        }\n    }\n\n    /**\n     * A Range is responsible for the tokens between (start, end].\n     */\n    public static class Range implements Comparable<Range>\n    {\n        public final long start;\n        public final long end;\n\n        public Range(long start, long end)\n        {\n            assert end > start || end == Long.MIN_VALUE : String.format(\"Start (%d) should be smaller than end (%d)\", start, end);\n            this.start = start;\n            this.end = end;\n        }\n\n        public boolean contains(long min, long max)\n        {\n            assert max > min;\n            return min > start && (max <= end || end == Long.MIN_VALUE);\n        }\n\n        public boolean contains(long token)\n        {\n            return token > start && (token <= end || end == Long.MIN_VALUE);\n        }\n\n        public int compareTo(Range o)\n        {\n            int res = Long.compare(start, o.start);\n            if (res == 0)\n                return Long.compare(end, o.end);\n            return res;\n        }\n\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Range range = (Range) o;\n            return start == range.start && end == range.end;\n        }\n\n        public int hashCode()\n        {\n            return Objects.hash(start, end);\n        }\n\n        public String toString()\n        {\n            return \"(\" +\n                   \"\" + (start == Long.MIN_VALUE ? \"MIN\" : start) +\n                   \", \" + (end == Long.MIN_VALUE ? \"MIN\" : end) +\n                   ']';\n        }\n    }\n\n    public interface Lookup\n    {\n        String id(int nodeIdx);\n        String dc(int dcIdx);\n        String rack(int rackIdx);\n        long token(int tokenIdx);\n        Lookup forceToken(int tokenIdx, long token);\n        void reset();\n\n        default NodeId nodeId(int nodeIdx)\n        {\n            return ClusterMetadata.current().directory.peerId(addr(nodeIdx));\n        }\n\n        default InetAddressAndPort addr(int idx)\n        {\n            try\n            {\n                return InetAddressAndPort.getByName(id(idx));\n            }\n            catch (UnknownHostException e)\n            {\n                throw new RuntimeException(e);\n            }\n        }\n    }\n\n    public static Lookup constantLookup(String id, long token, String dc, String rack)\n    {\n        return new Lookup()\n        {\n            public String id(int nodeIdx)\n            {\n                return id;\n            }\n\n            public String dc(int dcIdx)\n            {\n                return dc;\n            }\n\n            public String rack(int rackIdx)\n            {\n                return rack;\n            }\n\n            @Override\n            public org.apache.cassandra.tcm.membership.NodeId nodeId(int nodeIdx)\n            {\n                return null;\n            }\n\n            public long token(int tokenIdx)\n            {\n                return token;\n            }\n\n            public Lookup forceToken(int tokenIdx, long token)\n            {\n                throw new UnsupportedOperationException();\n            }\n\n            @Override\n            public InetAddressAndPort addr(int idx)\n            {\n                return null;\n            }\n\n            public void reset()\n            {\n                throw new UnsupportedOperationException();\n            }\n        };\n    }\n\n    public static class DefaultLookup implements Lookup\n    {\n        protected final Map<Integer, Long> tokenOverrides = new HashMap<>(2);\n\n        public DefaultLookup()\n        {\n            // A crafty way to introduce a MIN token\n            tokenOverrides.put(10, Long.MIN_VALUE);\n        }\n\n        public String id(int nodeIdx)\n        {\n            return String.format(\"127.0.%d.%d\", nodeIdx / 256, nodeIdx % 256);\n        }\n\n        public long token(int tokenIdx)\n        {\n            Long override = tokenOverrides.get(tokenIdx);\n            if (override != null)\n                return override;\n            long token = PCGFastPure.next(tokenIdx, 1L);\n            for (Long value : tokenOverrides.values())\n            {\n                if (token == value)\n                    throw new IllegalStateException(String.format(\"Generated token %d is already used in an override\", token));\n            }\n            return token;\n        }\n\n        public Lookup forceToken(int tokenIdx, long token)\n        {\n            DefaultLookup newLookup = new DefaultLookup();\n            newLookup.tokenOverrides.putAll(tokenOverrides);\n            newLookup.tokenOverrides.put(tokenIdx, token);\n            return newLookup;\n        }\n\n        public void reset()\n        {\n            tokenOverrides.clear();\n        }\n\n        public String dc(int dcIdx)\n        {\n            return String.format(\"datacenter%d\", dcIdx);\n        }\n\n        public String rack(int rackIdx)\n        {\n            return String.format(\"rack%d\", rackIdx);\n        }\n    }\n\n    public static class HumanReadableTokensLookup extends DefaultLookup {\n        @Override\n        public long token(int tokenIdx)\n        {\n            Long override = tokenOverrides.get(tokenIdx);\n            if (override != null)\n                return override;\n            return tokenIdx * 100L;\n        }\n\n        public Lookup forceToken(int tokenIdx, long token)\n        {\n            DefaultLookup lookup = new HumanReadableTokensLookup();\n            lookup.tokenOverrides.putAll(tokenOverrides);\n            lookup.tokenOverrides.put(tokenIdx, token);\n            return lookup;\n        }\n    }\n\n    public static NodeFactory nodeFactory()\n    {\n        return new NodeFactory(new DefaultLookup());\n    }\n\n    public static NodeFactory nodeFactoryHumanReadable()\n    {\n        return new NodeFactory(new HumanReadableTokensLookup());\n    }\n\n    public static class NodeFactory implements TokenSupplier\n    {\n        private final Lookup lookup;\n\n        public NodeFactory(Lookup lookup)\n        {\n            this.lookup = lookup;\n        }\n\n        public Node make(int idx, int dc, int rack)\n        {\n            return new Node(idx, idx, dc, rack, lookup);\n        }\n\n        public Lookup lookup()\n        {\n            return lookup;\n        }\n\n        public Collection<String> tokens(int i)\n        {\n            return Collections.singletonList(Long.toString(lookup.token(i)));\n        }\n    }\n\n    public static class Node implements Comparable<Node>\n    {\n        private final int tokenIdx;\n        private final int nodeIdx;\n        private final int dcIdx;\n        private final int rackIdx;\n        private final Lookup lookup;\n\n        public Node(int tokenIdx, int idx, int dcIdx, int rackIdx, Lookup lookup)\n        {\n            this.tokenIdx = tokenIdx;\n            this.nodeIdx = idx;\n            this.dcIdx = dcIdx;\n            this.rackIdx = rackIdx;\n            this.lookup = lookup;\n        }\n\n        public String id()\n        {\n            return lookup.id(nodeIdx);\n        }\n\n        public int idx()\n        {\n            return nodeIdx;\n        }\n\n        public int dcIdx()\n        {\n            return dcIdx;\n        }\n\n        public int rackIdx()\n        {\n            return rackIdx;\n        }\n\n        public String dc()\n        {\n            return lookup.dc(dcIdx);\n        }\n\n        public String rack()\n        {\n            return lookup.rack(rackIdx);\n        }\n\n        public long token()\n        {\n            return lookup.token(tokenIdx);\n        }\n\n        public int tokenIdx()\n        {\n            return tokenIdx;\n        }\n\n        public Node withNewToken()\n        {\n            return new Node(tokenIdx + 100_000, nodeIdx, dcIdx, rackIdx, lookup);\n        }\n\n        public Node withToken(int tokenIdx)\n        {\n            return new Node(tokenIdx, nodeIdx, dcIdx, rackIdx, lookup);\n        }\n\n        public Node overrideToken(long override)\n        {\n            return new Node(tokenIdx, nodeIdx, dcIdx, rackIdx, lookup.forceToken(tokenIdx, override));\n        }\n        public Murmur3Partitioner.LongToken longToken()\n        {\n            return new Murmur3Partitioner.LongToken(token());\n        }\n\n        public NodeId nodeId()\n        {\n            return lookup.nodeId(idx());\n        }\n\n        public InetAddressAndPort addr()\n        {\n            return lookup.addr(idx());\n        }\n\n        public boolean equals(Object o)\n        {\n            if (this == o) return true;\n            if (o == null || !Node.class.isAssignableFrom(o.getClass())) return false;\n            Node node = (Node) o;\n            return Objects.equals(nodeIdx, node.nodeIdx);\n        }\n\n        public int hashCode()\n        {\n            return Objects.hash(nodeIdx);\n        }\n\n        public int compareTo(Node o)\n        {\n            return Long.compare(token(), o.token());\n        }\n\n        public String toString()\n        {\n            return String.format(\"%s-%s@%d\", dc(), id(), token());\n        }\n    }\n}\n","lineNo":139}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.EnumMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\n\nimport org.apache.commons.lang3.builder.ToStringBuilder;\nimport org.apache.commons.lang3.builder.ToStringStyle;\n\nimport org.apache.cassandra.cql3.AbstractMarker;\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.cql3.QueryOptions;\nimport org.apache.cassandra.cql3.Term;\nimport org.apache.cassandra.cql3.Terms;\nimport org.apache.cassandra.cql3.Tuples;\nimport org.apache.cassandra.cql3.Term.Terminal;\nimport org.apache.cassandra.cql3.functions.Function;\nimport org.apache.cassandra.cql3.statements.Bound;\nimport org.apache.cassandra.db.MultiCBuilder;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.index.Index;\nimport org.apache.cassandra.index.IndexRegistry;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.serializers.ListSerializer;\n\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkNotNull;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;\n\npublic abstract class MultiColumnRestriction implements SingleRestriction\n{\n    /**\n     * The columns to which the restriction apply.\n     */\n    protected final List<ColumnMetadata> columnDefs;\n\n    public MultiColumnRestriction(List<ColumnMetadata> columnDefs)\n    {\n        this.columnDefs = columnDefs;\n    }\n\n    @Override\n    public boolean isMultiColumn()\n    {\n        return true;\n    }\n\n    @Override\n    public ColumnMetadata getFirstColumn()\n    {\n        return columnDefs.get(0);\n    }\n\n    @Override\n    public ColumnMetadata getLastColumn()\n    {\n        return columnDefs.get(columnDefs.size() - 1);\n    }\n\n    @Override\n    public List<ColumnMetadata> getColumnDefs()\n    {\n        return columnDefs;\n    }\n\n    @Override\n    public final SingleRestriction mergeWith(SingleRestriction otherRestriction)\n    {\n        // We want to allow query like: (b,c) > (?, ?) AND b < ?\n        if (!otherRestriction.isMultiColumn()\n                && ((SingleColumnRestriction) otherRestriction).canBeConvertedToMultiColumnRestriction())\n        {\n            return doMergeWith(((SingleColumnRestriction) otherRestriction).toMultiColumnRestriction());\n        }\n\n        return doMergeWith(otherRestriction);\n    }\n\n    protected abstract SingleRestriction doMergeWith(SingleRestriction otherRestriction);\n\n    /**\n     * Returns the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     *\n     * @param otherRestriction the other restrictions\n     * @return the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     */\n    protected final String getColumnsInCommons(Restriction otherRestriction)\n    {\n        Set<ColumnMetadata> commons = new HashSet<>(getColumnDefs());\n        commons.retainAll(otherRestriction.getColumnDefs());\n        StringBuilder builder = new StringBuilder();\n        for (ColumnMetadata columnMetadata : commons)\n        {\n            if (builder.length() != 0)\n                builder.append(\" ,\");\n            builder.append(columnMetadata.name);\n        }\n        return builder.toString();\n    }\n\n    @Override\n    public final boolean hasSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n           if (isSupportedBy(index))\n               return true;\n\n        return false;\n    }\n\n    @Override\n    public final Index findSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public Index findSupportingIndexFromQueryPlan(Index.QueryPlan indexQueryPlan)\n    {\n        for (Index index : indexQueryPlan.getIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public boolean needsFiltering(Index.Group indexGroup)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (!isSupportedBy(indexGroup, column))\n                return true;\n        }\n        return false;\n    }\n\n    private boolean isSupportedBy(Index.Group indexGroup, ColumnMetadata column)\n    {\n        for (Index index : indexGroup.getIndexes())\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    /**\n     * Check if this type of restriction is supported for by the specified index.\n     * @param index the secondary index\n     *\n     * @return <code>true<\/code> this type of restriction is supported by the specified index,\n     * <code>false<\/code> otherwise.\n     */\n    private boolean isSupportedBy(Index index)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    protected abstract boolean isSupportedBy(Index index, ColumnMetadata def);\n\n    public static class EQRestriction extends MultiColumnRestriction\n    {\n        protected final Term value;\n\n        public EQRestriction(List<ColumnMetadata> columnDefs, Term value)\n        {\n            super(columnDefs);\n            this.value = value;\n        }\n\n        @Override\n        public boolean isEQ()\n        {\n            return true;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            value.addFunctionsTo(functions);\n        }\n\n        @Override\n        public String toString()\n        {\n            return String.format(\"EQ(%s)\", value);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes an Equal\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.EQ);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            Tuples.Value t = ((Tuples.Value) value.bind(options));\n            List<ByteBuffer> values = t.getElements();\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                builder.addElementToAll(values.get(i));\n                checkFalse(builder.containsNull(), \"Invalid null value for column %s\", columnDefs.get(i).name);\n            }\n            return builder;\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            Tuples.Value t = ((Tuples.Value) value.bind(options));\n            List<ByteBuffer> values = t.getElements();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata columnDef = columnDefs.get(i);\n                filter.add(columnDef, Operator.EQ, values.get(i));\n            }\n        }\n    }\n\n    public abstract static class INRestriction extends MultiColumnRestriction\n    {\n        public INRestriction(List<ColumnMetadata> columnDefs)\n        {\n            super(columnDefs);\n        }\n\n        /**\n         * {@inheritDoc}\n         */\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            List<List<ByteBuffer>> splitInValues = splitValues(options);\n            builder.addAllElementsToAll(splitInValues);\n\n            if (builder.containsNull())\n                throw invalidRequest(\"Invalid null value in condition for columns: %s\", ColumnMetadata.toIdentifiers(columnDefs));\n            return builder;\n        }\n\n        @Override\n        public boolean isIN()\n        {\n            return true;\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes a IN\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IN);\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            // If the relation is of the type (c) IN ((x),(y),(z)) then it is equivalent to\n            // c IN (x, y, z) and we can perform filtering\n            if (getColumnDefs().size() == 1)\n            {\n                List<List<ByteBuffer>> splitValues = splitValues(options);\n                List<ByteBuffer> values = new ArrayList<>(splitValues.size());\n                for (List<ByteBuffer> splitValue : splitValues)\n                    values.add(splitValue.get(0));\n\n                ByteBuffer buffer = ListSerializer.pack(values, values.size());\n                filter.add(getFirstColumn(), Operator.IN, buffer);\n            }\n            else\n            {\n                throw invalidRequest(\"Multicolumn IN filters are not supported\");\n            }\n        }\n\n        protected abstract List<List<ByteBuffer>> splitValues(QueryOptions options);\n    }\n\n    /**\n     * An IN restriction that has a set of terms for in values.\n     * For example: \"SELECT ... WHERE (a, b, c) IN ((1, 2, 3), (4, 5, 6))\" or \"WHERE (a, b, c) IN (?, ?)\"\n     */\n    public static class InRestrictionWithValues extends INRestriction\n    {\n        protected final List<Term> values;\n\n        public InRestrictionWithValues(List<ColumnMetadata> columnDefs, List<Term> values)\n        {\n            super(columnDefs);\n            this.values = values;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            Terms.addFunctions(values, functions);\n        }\n\n        @Override\n        public String toString()\n        {\n            return String.format(\"IN(%s)\", values);\n        }\n\n        @Override\n        protected List<List<ByteBuffer>> splitValues(QueryOptions options)\n        {\n            List<List<ByteBuffer>> buffers = new ArrayList<>(values.size());\n            for (Term value : values)\n            {\n                Term.MultiItemTerminal term = (Term.MultiItemTerminal) value.bind(options);\n                buffers.add(term.getElements());\n            }\n            return buffers;\n        }\n    }\n\n    /**\n     * An IN restriction that uses a single marker for a set of IN values that are tuples.\n     * For example: \"SELECT ... WHERE (a, b, c) IN ?\"\n     */\n    public static class InRestrictionWithMarker extends INRestriction\n    {\n        protected final AbstractMarker marker;\n\n        public InRestrictionWithMarker(List<ColumnMetadata> columnDefs, AbstractMarker marker)\n        {\n            super(columnDefs);\n            this.marker = marker;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"IN ?\";\n        }\n\n        @Override\n        protected List<List<ByteBuffer>> splitValues(QueryOptions options)\n        {\n            Tuples.InMarker inMarker = (Tuples.InMarker) marker;\n            Tuples.InValue inValue = inMarker.bind(options);\n            checkNotNull(inValue, \"Invalid null value for IN restriction\");\n            return inValue.getSplitValues();\n        }\n    }\n\n    public static class SliceRestriction extends MultiColumnRestriction\n    {\n        private final TermSlice slice;\n\n        public SliceRestriction(List<ColumnMetadata> columnDefs, Bound bound, boolean inclusive, Term term)\n        {\n            this(columnDefs, TermSlice.newInstance(bound, inclusive, term));\n        }\n\n        SliceRestriction(List<ColumnMetadata> columnDefs, TermSlice slice)\n        {\n            super(columnDefs);\n            this.slice = slice;\n        }\n\n        @Override\n        public boolean isSlice()\n        {\n            return true;\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)\n        {\n            boolean reversed = getFirstColumn().isReversedType();\n\n            EnumMap<Bound, List<ByteBuffer>> componentBounds = new EnumMap<>(Bound.class);\n            componentBounds.put(Bound.START, componentBounds(Bound.START, options));\n            componentBounds.put(Bound.END, componentBounds(Bound.END, options));\n\n            List<List<ByteBuffer>> toAdd = new ArrayList<>();\n            List<ByteBuffer> values = new ArrayList<>();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata column = columnDefs.get(i);\n                Bound b = bound.reverseIfNeeded(column);\n\n                // For mixed order columns, we need to create additional slices when 2 columns are in reverse order\n                if (reversed != column.isReversedType())\n                {\n                    reversed = column.isReversedType();\n                    // As we are switching direction we need to add the current composite\n                    toAdd.add(values);\n\n                    // The new bound side has no value for this component.  just stop\n                    if (!hasComponent(b, i, componentBounds))\n                        continue;\n\n                    // The other side has still some components. We need to end the slice that we have just open.\n                    if (hasComponent(b.reverse(), i, componentBounds))\n                        toAdd.add(values);\n\n                    // We need to rebuild where we are in this bound side\n                    values = new ArrayList<ByteBuffer>();\n\n                    List<ByteBuffer> vals = componentBounds.get(b);\n\n                    int n = Math.min(i, vals.size());\n                    for (int j = 0; j < n; j++)\n                    {\n                        ByteBuffer v = checkNotNull(vals.get(j),\n                                                    \"Invalid null value in condition for column %s\",\n                                                    columnDefs.get(j).name);\n                        values.add(v);\n                    }\n                }\n\n                if (!hasComponent(b, i, componentBounds))\n                    continue;\n\n                ByteBuffer v = checkNotNull(componentBounds.get(b).get(i), \"Invalid null value in condition for column %s\", columnDefs.get(i).name);\n                values.add(v);\n            }\n            toAdd.add(values);\n\n            if (bound.isEnd())\n                Collections.reverse(toAdd);\n\n            return builder.addAllElementsToAll(toAdd);\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return slice.isSupportedBy(column, index);\n        }\n\n        @Override\n        public boolean hasBound(Bound bound)\n        {\n            return slice.hasBound(bound);\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            slice.addFunctionsTo(functions);\n        }\n\n        @Override\n        public boolean isInclusive(Bound bound)\n        {\n            return slice.isInclusive(bound);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            checkTrue(otherRestriction.isSlice(),\n                      \"Column \\\"%s\\\" cannot be restricted by both an equality and an inequality relation\",\n                      getColumnsInCommons(otherRestriction));\n\n            if (!getFirstColumn().equals(otherRestriction.getFirstColumn()))\n            {\n                ColumnMetadata column = getFirstColumn().position() > otherRestriction.getFirstColumn().position()\n                        ? getFirstColumn() : otherRestriction.getFirstColumn();\n\n                throw invalidRequest(\"Column \\\"%s\\\" cannot be restricted by two inequalities not starting with the same column\",\n                                     column.name);\n            }\n\n            checkFalse(hasBound(Bound.START) && otherRestriction.hasBound(Bound.START),\n                       \"More than one restriction was found for the start bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n            checkFalse(hasBound(Bound.END) && otherRestriction.hasBound(Bound.END),\n                       \"More than one restriction was found for the end bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n\n            SliceRestriction otherSlice = (SliceRestriction) otherRestriction;\n            List<ColumnMetadata> newColumnDefs = columnDefs.size() >= otherSlice.columnDefs.size() ? columnDefs : otherSlice.columnDefs;\n\n            return new SliceRestriction(newColumnDefs, slice.merge(otherSlice.slice));\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            throw invalidRequest(\"Multi-column slice restrictions cannot be used for filtering.\");\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"SLICE\" + slice;\n        }\n\n        /**\n         * Similar to bounds(), but returns one ByteBuffer per-component in the bound instead of a single\n         * ByteBuffer to represent the entire bound.\n         * @param b the bound type\n         * @param options the query options\n         * @return one ByteBuffer per-component in the bound\n         */\n        private List<ByteBuffer> componentBounds(Bound b, QueryOptions options)\n        {\n            if (!slice.hasBound(b))\n                return Collections.emptyList();\n\n            Terminal terminal = slice.bound(b).bind(options);\n\n            if (terminal instanceof Tuples.Value)\n            {\n                return ((Tuples.Value) terminal).getElements();\n            }\n\n            return Collections.singletonList(terminal.get(options.getProtocolVersion()));\n        }\n\n        private boolean hasComponent(Bound b, int index, EnumMap<Bound, List<ByteBuffer>> componentBounds)\n        {\n            return componentBounds.get(b).size() > index;\n        }\n    }\n\n    public static class NotNullRestriction extends MultiColumnRestriction\n    {\n        public NotNullRestriction(List<ColumnMetadata> columnDefs)\n        {\n            super(columnDefs);\n            assert columnDefs.size() == 1;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n        }\n\n        @Override\n        public boolean isNotNull()\n        {\n            return true;\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"IS NOT NULL\";\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by a relation if it includes an IS NOT NULL clause\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IS_NOT);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Cannot use IS NOT NULL restriction for slicing\");\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Secondary indexes do not support IS NOT NULL restrictions\");\n        }\n    }\n    \n    @Override\n    public String toString()\n    {\n        return ToStringBuilder.reflectionToString(this, ToStringStyle.SHORT_PREFIX_STYLE);\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.EnumMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\n\nimport org.apache.commons.lang3.builder.ToStringBuilder;\nimport org.apache.commons.lang3.builder.ToStringStyle;\n\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.cql3.QueryOptions;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.cql3.terms.Term.Terminal;\nimport org.apache.cassandra.cql3.functions.Function;\nimport org.apache.cassandra.cql3.statements.Bound;\nimport org.apache.cassandra.db.MultiCBuilder;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.index.Index;\nimport org.apache.cassandra.index.IndexRegistry;\nimport org.apache.cassandra.schema.ColumnMetadata;\n\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkNotNull;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;\n\npublic abstract class MultiColumnRestriction implements SingleRestriction\n{\n    /**\n     * The columns to which the restriction apply.\n     */\n    protected final List<ColumnMetadata> columnDefs;\n\n    public MultiColumnRestriction(List<ColumnMetadata> columnDefs)\n    {\n        this.columnDefs = columnDefs;\n    }\n\n    @Override\n    public boolean isMultiColumn()\n    {\n        return true;\n    }\n\n    @Override\n    public ColumnMetadata getFirstColumn()\n    {\n        return columnDefs.get(0);\n    }\n\n    @Override\n    public ColumnMetadata getLastColumn()\n    {\n        return columnDefs.get(columnDefs.size() - 1);\n    }\n\n    @Override\n    public List<ColumnMetadata> getColumnDefs()\n    {\n        return columnDefs;\n    }\n\n    @Override\n    public final SingleRestriction mergeWith(SingleRestriction otherRestriction)\n    {\n        // We want to allow query like: (b,c) > (?, ?) AND b < ?\n        if (!otherRestriction.isMultiColumn()\n                && ((SingleColumnRestriction) otherRestriction).canBeConvertedToMultiColumnRestriction())\n        {\n            return doMergeWith(((SingleColumnRestriction) otherRestriction).toMultiColumnRestriction());\n        }\n\n        return doMergeWith(otherRestriction);\n    }\n\n    protected abstract SingleRestriction doMergeWith(SingleRestriction otherRestriction);\n\n    /**\n     * Returns the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     *\n     * @param otherRestriction the other restrictions\n     * @return the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     */\n    protected final String getColumnsInCommons(Restriction otherRestriction)\n    {\n        Set<ColumnMetadata> commons = new HashSet<>(getColumnDefs());\n        commons.retainAll(otherRestriction.getColumnDefs());\n        StringBuilder builder = new StringBuilder();\n        for (ColumnMetadata columnMetadata : commons)\n        {\n            if (builder.length() != 0)\n                builder.append(\" ,\");\n            builder.append(columnMetadata.name);\n        }\n        return builder.toString();\n    }\n\n    @Override\n    public final boolean hasSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n           if (isSupportedBy(index))\n               return true;\n\n        return false;\n    }\n\n    @Override\n    public final Index findSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public Index findSupportingIndexFromQueryPlan(Index.QueryPlan indexQueryPlan)\n    {\n        for (Index index : indexQueryPlan.getIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public boolean needsFiltering(Index.Group indexGroup)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (!isSupportedBy(indexGroup, column))\n                return true;\n        }\n        return false;\n    }\n\n    private boolean isSupportedBy(Index.Group indexGroup, ColumnMetadata column)\n    {\n        for (Index index : indexGroup.getIndexes())\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    /**\n     * Check if this type of restriction is supported for by the specified index.\n     * @param index the secondary index\n     *\n     * @return <code>true<\/code> this type of restriction is supported by the specified index,\n     * <code>false<\/code> otherwise.\n     */\n    private boolean isSupportedBy(Index index)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    protected abstract boolean isSupportedBy(Index index, ColumnMetadata def);\n\n    public static class EQRestriction extends MultiColumnRestriction\n    {\n        protected final Term value;\n\n        public EQRestriction(List<ColumnMetadata> columnDefs, Term value)\n        {\n            super(columnDefs);\n            this.value = value;\n        }\n\n        @Override\n        public boolean isEQ()\n        {\n            return true;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            value.addFunctionsTo(functions);\n        }\n\n        @Override\n        public String toString()\n        {\n            return String.format(\"EQ(%s)\", value);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes an Equal\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.EQ);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            Term.Terminal t = value.bind(options);\n            List<ByteBuffer> values = t.getElements();\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                builder.addElementToAll(values.get(i));\n                checkFalse(builder.containsNull(), \"Invalid null value for column %s\", columnDefs.get(i).name);\n            }\n            return builder;\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            Term.Terminal t = value.bind(options);\n            List<ByteBuffer> values = t.getElements();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata columnDef = columnDefs.get(i);\n                filter.add(columnDef, Operator.EQ, values.get(i));\n            }\n        }\n    }\n\n    public static class INRestriction extends MultiColumnRestriction\n    {\n        private final Terms values;\n\n        public INRestriction(List<ColumnMetadata> columnDefs, Terms values)\n        {\n            super(columnDefs);\n            this.values = values;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            values.addFunctionsTo(functions);\n        }\n\n        /**\n         * {@inheritDoc}\n         */\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            List<List<ByteBuffer>> elements = values.bindAndGetElements(options);\n\n            if (elements == null)\n                throw invalidRequest(\"Invalid null value for in(%s)\", ColumnMetadata.toCQLString(columnDefs));\n            if (elements == Terms.UNSET_LIST)\n                throw invalidRequest(\"Invalid unset value for in(%s)\", ColumnMetadata.toCQLString(columnDefs));\n\n            builder.addAllElementsToAll(elements);\n\n            if (builder.containsNull())\n                throw invalidRequest(\"Invalid null value in condition for columns: %s\", ColumnMetadata.toIdentifiers(columnDefs));\n            return builder;\n        }\n\n        @Override\n        public boolean isIN()\n        {\n            return true;\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes a IN\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IN);\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            // If the relation is of the type (c) IN ((x),(y),(z)) then it is equivalent to\n            // c IN (x, y, z) and we can perform filtering\n            if (getColumnDefs().size() == 1)\n            {\n                List<List<ByteBuffer>> splitValues = values.bindAndGetElements(options);\n                List<ByteBuffer> values = new ArrayList<>(splitValues.size());\n                for (List<ByteBuffer> splitValue : splitValues)\n                    values.add(splitValue.get(0));\n\n                ListType<?> type = ListType.getInstance(getFirstColumn().type, false);\n                ByteBuffer buffer = type.pack(values);\n                filter.add(getFirstColumn(), Operator.IN, buffer);\n            }\n            else\n            {\n                throw invalidRequest(\"Multicolumn IN filters are not supported\");\n            }\n        }\n    }\n\n    public static class SliceRestriction extends MultiColumnRestriction\n    {\n        private final TermSlice slice;\n\n        public SliceRestriction(List<ColumnMetadata> columnDefs, Bound bound, boolean inclusive, Term term)\n        {\n            this(columnDefs, TermSlice.newInstance(bound, inclusive, term));\n        }\n\n        SliceRestriction(List<ColumnMetadata> columnDefs, TermSlice slice)\n        {\n            super(columnDefs);\n            this.slice = slice;\n        }\n\n        @Override\n        public boolean isSlice()\n        {\n            return true;\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)\n        {\n            boolean reversed = getFirstColumn().isReversedType();\n\n            EnumMap<Bound, List<ByteBuffer>> componentBounds = new EnumMap<>(Bound.class);\n            componentBounds.put(Bound.START, componentBounds(Bound.START, options));\n            componentBounds.put(Bound.END, componentBounds(Bound.END, options));\n\n            List<List<ByteBuffer>> toAdd = new ArrayList<>();\n            List<ByteBuffer> values = new ArrayList<>();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata column = columnDefs.get(i);\n                Bound b = bound.reverseIfNeeded(column);\n\n                // For mixed order columns, we need to create additional slices when 2 columns are in reverse order\n                if (reversed != column.isReversedType())\n                {\n                    reversed = column.isReversedType();\n                    // As we are switching direction we need to add the current composite\n                    toAdd.add(values);\n\n                    // The new bound side has no value for this component.  just stop\n                    if (!hasComponent(b, i, componentBounds))\n                        continue;\n\n                    // The other side has still some components. We need to end the slice that we have just open.\n                    if (hasComponent(b.reverse(), i, componentBounds))\n                        toAdd.add(values);\n\n                    // We need to rebuild where we are in this bound side\n                    values = new ArrayList<ByteBuffer>();\n\n                    List<ByteBuffer> vals = componentBounds.get(b);\n\n                    int n = Math.min(i, vals.size());\n                    for (int j = 0; j < n; j++)\n                    {\n                        ByteBuffer v = checkNotNull(vals.get(j),\n                                                    \"Invalid null value in condition for column %s\",\n                                                    columnDefs.get(j).name);\n                        values.add(v);\n                    }\n                }\n\n                if (!hasComponent(b, i, componentBounds))\n                    continue;\n\n                ByteBuffer v = checkNotNull(componentBounds.get(b).get(i), \"Invalid null value in condition for column %s\", columnDefs.get(i).name);\n                values.add(v);\n            }\n            toAdd.add(values);\n\n            if (bound.isEnd())\n                Collections.reverse(toAdd);\n\n            return builder.addAllElementsToAll(toAdd);\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return slice.isSupportedBy(column, index);\n        }\n\n        @Override\n        public boolean hasBound(Bound bound)\n        {\n            return slice.hasBound(bound);\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            slice.addFunctionsTo(functions);\n        }\n\n        @Override\n        public boolean isInclusive(Bound bound)\n        {\n            return slice.isInclusive(bound);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            checkTrue(otherRestriction.isSlice(),\n                      \"Column \\\"%s\\\" cannot be restricted by both an equality and an inequality relation\",\n                      getColumnsInCommons(otherRestriction));\n\n            if (!getFirstColumn().equals(otherRestriction.getFirstColumn()))\n            {\n                ColumnMetadata column = getFirstColumn().position() > otherRestriction.getFirstColumn().position()\n                        ? getFirstColumn() : otherRestriction.getFirstColumn();\n\n                throw invalidRequest(\"Column \\\"%s\\\" cannot be restricted by two inequalities not starting with the same column\",\n                                     column.name);\n            }\n\n            checkFalse(hasBound(Bound.START) && otherRestriction.hasBound(Bound.START),\n                       \"More than one restriction was found for the start bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n            checkFalse(hasBound(Bound.END) && otherRestriction.hasBound(Bound.END),\n                       \"More than one restriction was found for the end bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n\n            SliceRestriction otherSlice = (SliceRestriction) otherRestriction;\n            List<ColumnMetadata> newColumnDefs = columnDefs.size() >= otherSlice.columnDefs.size() ? columnDefs : otherSlice.columnDefs;\n\n            return new SliceRestriction(newColumnDefs, slice.merge(otherSlice.slice));\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            throw invalidRequest(\"Multi-column slice restrictions cannot be used for filtering.\");\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"SLICE\" + slice;\n        }\n\n        /**\n         * Similar to bounds(), but returns one ByteBuffer per-component in the bound instead of a single\n         * ByteBuffer to represent the entire bound.\n         * @param b the bound type\n         * @param options the query options\n         * @return one ByteBuffer per-component in the bound\n         */\n        private List<ByteBuffer> componentBounds(Bound b, QueryOptions options)\n        {\n            if (!slice.hasBound(b))\n                return Collections.emptyList();\n\n            Terminal terminal = slice.bound(b).bind(options);\n            return terminal.getElements();\n        }\n\n        private boolean hasComponent(Bound b, int index, EnumMap<Bound, List<ByteBuffer>> componentBounds)\n        {\n            return componentBounds.get(b).size() > index;\n        }\n    }\n\n    public static class NotNullRestriction extends MultiColumnRestriction\n    {\n        public NotNullRestriction(List<ColumnMetadata> columnDefs)\n        {\n            super(columnDefs);\n            assert columnDefs.size() == 1;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n        }\n\n        @Override\n        public boolean isNotNull()\n        {\n            return true;\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"IS NOT NULL\";\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by a relation if it includes an IS NOT NULL clause\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IS_NOT);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Cannot use IS NOT NULL restriction for slicing\");\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Secondary indexes do not support IS NOT NULL restrictions\");\n        }\n    }\n    \n    @Override\n    public String toString()\n    {\n        return ToStringBuilder.reflectionToString(this, ToStringStyle.SHORT_PREFIX_STYLE);\n    }\n}\n","lineNo":235}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.EnumMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\n\nimport org.apache.commons.lang3.builder.ToStringBuilder;\nimport org.apache.commons.lang3.builder.ToStringStyle;\n\nimport org.apache.cassandra.cql3.AbstractMarker;\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.cql3.QueryOptions;\nimport org.apache.cassandra.cql3.Term;\nimport org.apache.cassandra.cql3.Terms;\nimport org.apache.cassandra.cql3.Tuples;\nimport org.apache.cassandra.cql3.Term.Terminal;\nimport org.apache.cassandra.cql3.functions.Function;\nimport org.apache.cassandra.cql3.statements.Bound;\nimport org.apache.cassandra.db.MultiCBuilder;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.index.Index;\nimport org.apache.cassandra.index.IndexRegistry;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.serializers.ListSerializer;\n\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkNotNull;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;\n\npublic abstract class MultiColumnRestriction implements SingleRestriction\n{\n    /**\n     * The columns to which the restriction apply.\n     */\n    protected final List<ColumnMetadata> columnDefs;\n\n    public MultiColumnRestriction(List<ColumnMetadata> columnDefs)\n    {\n        this.columnDefs = columnDefs;\n    }\n\n    @Override\n    public boolean isMultiColumn()\n    {\n        return true;\n    }\n\n    @Override\n    public ColumnMetadata getFirstColumn()\n    {\n        return columnDefs.get(0);\n    }\n\n    @Override\n    public ColumnMetadata getLastColumn()\n    {\n        return columnDefs.get(columnDefs.size() - 1);\n    }\n\n    @Override\n    public List<ColumnMetadata> getColumnDefs()\n    {\n        return columnDefs;\n    }\n\n    @Override\n    public final SingleRestriction mergeWith(SingleRestriction otherRestriction)\n    {\n        // We want to allow query like: (b,c) > (?, ?) AND b < ?\n        if (!otherRestriction.isMultiColumn()\n                && ((SingleColumnRestriction) otherRestriction).canBeConvertedToMultiColumnRestriction())\n        {\n            return doMergeWith(((SingleColumnRestriction) otherRestriction).toMultiColumnRestriction());\n        }\n\n        return doMergeWith(otherRestriction);\n    }\n\n    protected abstract SingleRestriction doMergeWith(SingleRestriction otherRestriction);\n\n    /**\n     * Returns the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     *\n     * @param otherRestriction the other restrictions\n     * @return the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     */\n    protected final String getColumnsInCommons(Restriction otherRestriction)\n    {\n        Set<ColumnMetadata> commons = new HashSet<>(getColumnDefs());\n        commons.retainAll(otherRestriction.getColumnDefs());\n        StringBuilder builder = new StringBuilder();\n        for (ColumnMetadata columnMetadata : commons)\n        {\n            if (builder.length() != 0)\n                builder.append(\" ,\");\n            builder.append(columnMetadata.name);\n        }\n        return builder.toString();\n    }\n\n    @Override\n    public final boolean hasSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n           if (isSupportedBy(index))\n               return true;\n\n        return false;\n    }\n\n    @Override\n    public final Index findSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public Index findSupportingIndexFromQueryPlan(Index.QueryPlan indexQueryPlan)\n    {\n        for (Index index : indexQueryPlan.getIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public boolean needsFiltering(Index.Group indexGroup)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (!isSupportedBy(indexGroup, column))\n                return true;\n        }\n        return false;\n    }\n\n    private boolean isSupportedBy(Index.Group indexGroup, ColumnMetadata column)\n    {\n        for (Index index : indexGroup.getIndexes())\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    /**\n     * Check if this type of restriction is supported for by the specified index.\n     * @param index the secondary index\n     *\n     * @return <code>true<\/code> this type of restriction is supported by the specified index,\n     * <code>false<\/code> otherwise.\n     */\n    private boolean isSupportedBy(Index index)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    protected abstract boolean isSupportedBy(Index index, ColumnMetadata def);\n\n    public static class EQRestriction extends MultiColumnRestriction\n    {\n        protected final Term value;\n\n        public EQRestriction(List<ColumnMetadata> columnDefs, Term value)\n        {\n            super(columnDefs);\n            this.value = value;\n        }\n\n        @Override\n        public boolean isEQ()\n        {\n            return true;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            value.addFunctionsTo(functions);\n        }\n\n        @Override\n        public String toString()\n        {\n            return String.format(\"EQ(%s)\", value);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes an Equal\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.EQ);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            Tuples.Value t = ((Tuples.Value) value.bind(options));\n            List<ByteBuffer> values = t.getElements();\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                builder.addElementToAll(values.get(i));\n                checkFalse(builder.containsNull(), \"Invalid null value for column %s\", columnDefs.get(i).name);\n            }\n            return builder;\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            Tuples.Value t = ((Tuples.Value) value.bind(options));\n            List<ByteBuffer> values = t.getElements();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata columnDef = columnDefs.get(i);\n                filter.add(columnDef, Operator.EQ, values.get(i));\n            }\n        }\n    }\n\n    public abstract static class INRestriction extends MultiColumnRestriction\n    {\n        public INRestriction(List<ColumnMetadata> columnDefs)\n        {\n            super(columnDefs);\n        }\n\n        /**\n         * {@inheritDoc}\n         */\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            List<List<ByteBuffer>> splitInValues = splitValues(options);\n            builder.addAllElementsToAll(splitInValues);\n\n            if (builder.containsNull())\n                throw invalidRequest(\"Invalid null value in condition for columns: %s\", ColumnMetadata.toIdentifiers(columnDefs));\n            return builder;\n        }\n\n        @Override\n        public boolean isIN()\n        {\n            return true;\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes a IN\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IN);\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            // If the relation is of the type (c) IN ((x),(y),(z)) then it is equivalent to\n            // c IN (x, y, z) and we can perform filtering\n            if (getColumnDefs().size() == 1)\n            {\n                List<List<ByteBuffer>> splitValues = splitValues(options);\n                List<ByteBuffer> values = new ArrayList<>(splitValues.size());\n                for (List<ByteBuffer> splitValue : splitValues)\n                    values.add(splitValue.get(0));\n\n                ByteBuffer buffer = ListSerializer.pack(values, values.size());\n                filter.add(getFirstColumn(), Operator.IN, buffer);\n            }\n            else\n            {\n                throw invalidRequest(\"Multicolumn IN filters are not supported\");\n            }\n        }\n\n        protected abstract List<List<ByteBuffer>> splitValues(QueryOptions options);\n    }\n\n    /**\n     * An IN restriction that has a set of terms for in values.\n     * For example: \"SELECT ... WHERE (a, b, c) IN ((1, 2, 3), (4, 5, 6))\" or \"WHERE (a, b, c) IN (?, ?)\"\n     */\n    public static class InRestrictionWithValues extends INRestriction\n    {\n        protected final List<Term> values;\n\n        public InRestrictionWithValues(List<ColumnMetadata> columnDefs, List<Term> values)\n        {\n            super(columnDefs);\n            this.values = values;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            Terms.addFunctions(values, functions);\n        }\n\n        @Override\n        public String toString()\n        {\n            return String.format(\"IN(%s)\", values);\n        }\n\n        @Override\n        protected List<List<ByteBuffer>> splitValues(QueryOptions options)\n        {\n            List<List<ByteBuffer>> buffers = new ArrayList<>(values.size());\n            for (Term value : values)\n            {\n                Term.MultiItemTerminal term = (Term.MultiItemTerminal) value.bind(options);\n                buffers.add(term.getElements());\n            }\n            return buffers;\n        }\n    }\n\n    /**\n     * An IN restriction that uses a single marker for a set of IN values that are tuples.\n     * For example: \"SELECT ... WHERE (a, b, c) IN ?\"\n     */\n    public static class InRestrictionWithMarker extends INRestriction\n    {\n        protected final AbstractMarker marker;\n\n        public InRestrictionWithMarker(List<ColumnMetadata> columnDefs, AbstractMarker marker)\n        {\n            super(columnDefs);\n            this.marker = marker;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"IN ?\";\n        }\n\n        @Override\n        protected List<List<ByteBuffer>> splitValues(QueryOptions options)\n        {\n            Tuples.InMarker inMarker = (Tuples.InMarker) marker;\n            Tuples.InValue inValue = inMarker.bind(options);\n            checkNotNull(inValue, \"Invalid null value for IN restriction\");\n            return inValue.getSplitValues();\n        }\n    }\n\n    public static class SliceRestriction extends MultiColumnRestriction\n    {\n        private final TermSlice slice;\n\n        public SliceRestriction(List<ColumnMetadata> columnDefs, Bound bound, boolean inclusive, Term term)\n        {\n            this(columnDefs, TermSlice.newInstance(bound, inclusive, term));\n        }\n\n        SliceRestriction(List<ColumnMetadata> columnDefs, TermSlice slice)\n        {\n            super(columnDefs);\n            this.slice = slice;\n        }\n\n        @Override\n        public boolean isSlice()\n        {\n            return true;\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)\n        {\n            boolean reversed = getFirstColumn().isReversedType();\n\n            EnumMap<Bound, List<ByteBuffer>> componentBounds = new EnumMap<>(Bound.class);\n            componentBounds.put(Bound.START, componentBounds(Bound.START, options));\n            componentBounds.put(Bound.END, componentBounds(Bound.END, options));\n\n            List<List<ByteBuffer>> toAdd = new ArrayList<>();\n            List<ByteBuffer> values = new ArrayList<>();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata column = columnDefs.get(i);\n                Bound b = bound.reverseIfNeeded(column);\n\n                // For mixed order columns, we need to create additional slices when 2 columns are in reverse order\n                if (reversed != column.isReversedType())\n                {\n                    reversed = column.isReversedType();\n                    // As we are switching direction we need to add the current composite\n                    toAdd.add(values);\n\n                    // The new bound side has no value for this component.  just stop\n                    if (!hasComponent(b, i, componentBounds))\n                        continue;\n\n                    // The other side has still some components. We need to end the slice that we have just open.\n                    if (hasComponent(b.reverse(), i, componentBounds))\n                        toAdd.add(values);\n\n                    // We need to rebuild where we are in this bound side\n                    values = new ArrayList<ByteBuffer>();\n\n                    List<ByteBuffer> vals = componentBounds.get(b);\n\n                    int n = Math.min(i, vals.size());\n                    for (int j = 0; j < n; j++)\n                    {\n                        ByteBuffer v = checkNotNull(vals.get(j),\n                                                    \"Invalid null value in condition for column %s\",\n                                                    columnDefs.get(j).name);\n                        values.add(v);\n                    }\n                }\n\n                if (!hasComponent(b, i, componentBounds))\n                    continue;\n\n                ByteBuffer v = checkNotNull(componentBounds.get(b).get(i), \"Invalid null value in condition for column %s\", columnDefs.get(i).name);\n                values.add(v);\n            }\n            toAdd.add(values);\n\n            if (bound.isEnd())\n                Collections.reverse(toAdd);\n\n            return builder.addAllElementsToAll(toAdd);\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return slice.isSupportedBy(column, index);\n        }\n\n        @Override\n        public boolean hasBound(Bound bound)\n        {\n            return slice.hasBound(bound);\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            slice.addFunctionsTo(functions);\n        }\n\n        @Override\n        public boolean isInclusive(Bound bound)\n        {\n            return slice.isInclusive(bound);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            checkTrue(otherRestriction.isSlice(),\n                      \"Column \\\"%s\\\" cannot be restricted by both an equality and an inequality relation\",\n                      getColumnsInCommons(otherRestriction));\n\n            if (!getFirstColumn().equals(otherRestriction.getFirstColumn()))\n            {\n                ColumnMetadata column = getFirstColumn().position() > otherRestriction.getFirstColumn().position()\n                        ? getFirstColumn() : otherRestriction.getFirstColumn();\n\n                throw invalidRequest(\"Column \\\"%s\\\" cannot be restricted by two inequalities not starting with the same column\",\n                                     column.name);\n            }\n\n            checkFalse(hasBound(Bound.START) && otherRestriction.hasBound(Bound.START),\n                       \"More than one restriction was found for the start bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n            checkFalse(hasBound(Bound.END) && otherRestriction.hasBound(Bound.END),\n                       \"More than one restriction was found for the end bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n\n            SliceRestriction otherSlice = (SliceRestriction) otherRestriction;\n            List<ColumnMetadata> newColumnDefs = columnDefs.size() >= otherSlice.columnDefs.size() ? columnDefs : otherSlice.columnDefs;\n\n            return new SliceRestriction(newColumnDefs, slice.merge(otherSlice.slice));\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            throw invalidRequest(\"Multi-column slice restrictions cannot be used for filtering.\");\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"SLICE\" + slice;\n        }\n\n        /**\n         * Similar to bounds(), but returns one ByteBuffer per-component in the bound instead of a single\n         * ByteBuffer to represent the entire bound.\n         * @param b the bound type\n         * @param options the query options\n         * @return one ByteBuffer per-component in the bound\n         */\n        private List<ByteBuffer> componentBounds(Bound b, QueryOptions options)\n        {\n            if (!slice.hasBound(b))\n                return Collections.emptyList();\n\n            Terminal terminal = slice.bound(b).bind(options);\n\n            if (terminal instanceof Tuples.Value)\n            {\n                return ((Tuples.Value) terminal).getElements();\n            }\n\n            return Collections.singletonList(terminal.get(options.getProtocolVersion()));\n        }\n\n        private boolean hasComponent(Bound b, int index, EnumMap<Bound, List<ByteBuffer>> componentBounds)\n        {\n            return componentBounds.get(b).size() > index;\n        }\n    }\n\n    public static class NotNullRestriction extends MultiColumnRestriction\n    {\n        public NotNullRestriction(List<ColumnMetadata> columnDefs)\n        {\n            super(columnDefs);\n            assert columnDefs.size() == 1;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n        }\n\n        @Override\n        public boolean isNotNull()\n        {\n            return true;\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"IS NOT NULL\";\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by a relation if it includes an IS NOT NULL clause\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IS_NOT);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Cannot use IS NOT NULL restriction for slicing\");\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Secondary indexes do not support IS NOT NULL restrictions\");\n        }\n    }\n    \n    @Override\n    public String toString()\n    {\n        return ToStringBuilder.reflectionToString(this, ToStringStyle.SHORT_PREFIX_STYLE);\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.EnumMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\n\nimport org.apache.commons.lang3.builder.ToStringBuilder;\nimport org.apache.commons.lang3.builder.ToStringStyle;\n\nimport org.apache.cassandra.cql3.Operator;\nimport org.apache.cassandra.cql3.QueryOptions;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.cql3.terms.Term.Terminal;\nimport org.apache.cassandra.cql3.functions.Function;\nimport org.apache.cassandra.cql3.statements.Bound;\nimport org.apache.cassandra.db.MultiCBuilder;\nimport org.apache.cassandra.db.filter.RowFilter;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.index.Index;\nimport org.apache.cassandra.index.IndexRegistry;\nimport org.apache.cassandra.schema.ColumnMetadata;\n\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkNotNull;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;\nimport static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;\n\npublic abstract class MultiColumnRestriction implements SingleRestriction\n{\n    /**\n     * The columns to which the restriction apply.\n     */\n    protected final List<ColumnMetadata> columnDefs;\n\n    public MultiColumnRestriction(List<ColumnMetadata> columnDefs)\n    {\n        this.columnDefs = columnDefs;\n    }\n\n    @Override\n    public boolean isMultiColumn()\n    {\n        return true;\n    }\n\n    @Override\n    public ColumnMetadata getFirstColumn()\n    {\n        return columnDefs.get(0);\n    }\n\n    @Override\n    public ColumnMetadata getLastColumn()\n    {\n        return columnDefs.get(columnDefs.size() - 1);\n    }\n\n    @Override\n    public List<ColumnMetadata> getColumnDefs()\n    {\n        return columnDefs;\n    }\n\n    @Override\n    public final SingleRestriction mergeWith(SingleRestriction otherRestriction)\n    {\n        // We want to allow query like: (b,c) > (?, ?) AND b < ?\n        if (!otherRestriction.isMultiColumn()\n                && ((SingleColumnRestriction) otherRestriction).canBeConvertedToMultiColumnRestriction())\n        {\n            return doMergeWith(((SingleColumnRestriction) otherRestriction).toMultiColumnRestriction());\n        }\n\n        return doMergeWith(otherRestriction);\n    }\n\n    protected abstract SingleRestriction doMergeWith(SingleRestriction otherRestriction);\n\n    /**\n     * Returns the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     *\n     * @param otherRestriction the other restrictions\n     * @return the names of the columns that are specified within this <code>Restrictions<\/code> and the other one\n     * as a comma separated <code>String<\/code>.\n     */\n    protected final String getColumnsInCommons(Restriction otherRestriction)\n    {\n        Set<ColumnMetadata> commons = new HashSet<>(getColumnDefs());\n        commons.retainAll(otherRestriction.getColumnDefs());\n        StringBuilder builder = new StringBuilder();\n        for (ColumnMetadata columnMetadata : commons)\n        {\n            if (builder.length() != 0)\n                builder.append(\" ,\");\n            builder.append(columnMetadata.name);\n        }\n        return builder.toString();\n    }\n\n    @Override\n    public final boolean hasSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n           if (isSupportedBy(index))\n               return true;\n\n        return false;\n    }\n\n    @Override\n    public final Index findSupportingIndex(IndexRegistry indexRegistry)\n    {\n        for (Index index : indexRegistry.listIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public Index findSupportingIndexFromQueryPlan(Index.QueryPlan indexQueryPlan)\n    {\n        for (Index index : indexQueryPlan.getIndexes())\n            if (isSupportedBy(index))\n                return index;\n        return null;\n    }\n\n    @Override\n    public boolean needsFiltering(Index.Group indexGroup)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (!isSupportedBy(indexGroup, column))\n                return true;\n        }\n        return false;\n    }\n\n    private boolean isSupportedBy(Index.Group indexGroup, ColumnMetadata column)\n    {\n        for (Index index : indexGroup.getIndexes())\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    /**\n     * Check if this type of restriction is supported for by the specified index.\n     * @param index the secondary index\n     *\n     * @return <code>true<\/code> this type of restriction is supported by the specified index,\n     * <code>false<\/code> otherwise.\n     */\n    private boolean isSupportedBy(Index index)\n    {\n        for (ColumnMetadata column : columnDefs)\n        {\n            if (isSupportedBy(index, column))\n                return true;\n        }\n        return false;\n    }\n\n    protected abstract boolean isSupportedBy(Index index, ColumnMetadata def);\n\n    public static class EQRestriction extends MultiColumnRestriction\n    {\n        protected final Term value;\n\n        public EQRestriction(List<ColumnMetadata> columnDefs, Term value)\n        {\n            super(columnDefs);\n            this.value = value;\n        }\n\n        @Override\n        public boolean isEQ()\n        {\n            return true;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            value.addFunctionsTo(functions);\n        }\n\n        @Override\n        public String toString()\n        {\n            return String.format(\"EQ(%s)\", value);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes an Equal\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.EQ);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            Term.Terminal t = value.bind(options);\n            List<ByteBuffer> values = t.getElements();\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                builder.addElementToAll(values.get(i));\n                checkFalse(builder.containsNull(), \"Invalid null value for column %s\", columnDefs.get(i).name);\n            }\n            return builder;\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            Term.Terminal t = value.bind(options);\n            List<ByteBuffer> values = t.getElements();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata columnDef = columnDefs.get(i);\n                filter.add(columnDef, Operator.EQ, values.get(i));\n            }\n        }\n    }\n\n    public static class INRestriction extends MultiColumnRestriction\n    {\n        private final Terms values;\n\n        public INRestriction(List<ColumnMetadata> columnDefs, Terms values)\n        {\n            super(columnDefs);\n            this.values = values;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            values.addFunctionsTo(functions);\n        }\n\n        /**\n         * {@inheritDoc}\n         */\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            List<List<ByteBuffer>> elements = values.bindAndGetElements(options);\n\n            if (elements == null)\n                throw invalidRequest(\"Invalid null value for in(%s)\", ColumnMetadata.toCQLString(columnDefs));\n            if (elements == Terms.UNSET_LIST)\n                throw invalidRequest(\"Invalid unset value for in(%s)\", ColumnMetadata.toCQLString(columnDefs));\n\n            builder.addAllElementsToAll(elements);\n\n            if (builder.containsNull())\n                throw invalidRequest(\"Invalid null value in condition for columns: %s\", ColumnMetadata.toIdentifiers(columnDefs));\n            return builder;\n        }\n\n        @Override\n        public boolean isIN()\n        {\n            return true;\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by more than one relation if it includes a IN\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IN);\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            // If the relation is of the type (c) IN ((x),(y),(z)) then it is equivalent to\n            // c IN (x, y, z) and we can perform filtering\n            if (getColumnDefs().size() == 1)\n            {\n                List<List<ByteBuffer>> splitValues = values.bindAndGetElements(options);\n                List<ByteBuffer> values = new ArrayList<>(splitValues.size());\n                for (List<ByteBuffer> splitValue : splitValues)\n                    values.add(splitValue.get(0));\n\n                ListType<?> type = ListType.getInstance(getFirstColumn().type, false);\n                ByteBuffer buffer = type.pack(values);\n                filter.add(getFirstColumn(), Operator.IN, buffer);\n            }\n            else\n            {\n                throw invalidRequest(\"Multicolumn IN filters are not supported\");\n            }\n        }\n    }\n\n    public static class SliceRestriction extends MultiColumnRestriction\n    {\n        private final TermSlice slice;\n\n        public SliceRestriction(List<ColumnMetadata> columnDefs, Bound bound, boolean inclusive, Term term)\n        {\n            this(columnDefs, TermSlice.newInstance(bound, inclusive, term));\n        }\n\n        SliceRestriction(List<ColumnMetadata> columnDefs, TermSlice slice)\n        {\n            super(columnDefs);\n            this.slice = slice;\n        }\n\n        @Override\n        public boolean isSlice()\n        {\n            return true;\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)\n        {\n            boolean reversed = getFirstColumn().isReversedType();\n\n            EnumMap<Bound, List<ByteBuffer>> componentBounds = new EnumMap<>(Bound.class);\n            componentBounds.put(Bound.START, componentBounds(Bound.START, options));\n            componentBounds.put(Bound.END, componentBounds(Bound.END, options));\n\n            List<List<ByteBuffer>> toAdd = new ArrayList<>();\n            List<ByteBuffer> values = new ArrayList<>();\n\n            for (int i = 0, m = columnDefs.size(); i < m; i++)\n            {\n                ColumnMetadata column = columnDefs.get(i);\n                Bound b = bound.reverseIfNeeded(column);\n\n                // For mixed order columns, we need to create additional slices when 2 columns are in reverse order\n                if (reversed != column.isReversedType())\n                {\n                    reversed = column.isReversedType();\n                    // As we are switching direction we need to add the current composite\n                    toAdd.add(values);\n\n                    // The new bound side has no value for this component.  just stop\n                    if (!hasComponent(b, i, componentBounds))\n                        continue;\n\n                    // The other side has still some components. We need to end the slice that we have just open.\n                    if (hasComponent(b.reverse(), i, componentBounds))\n                        toAdd.add(values);\n\n                    // We need to rebuild where we are in this bound side\n                    values = new ArrayList<ByteBuffer>();\n\n                    List<ByteBuffer> vals = componentBounds.get(b);\n\n                    int n = Math.min(i, vals.size());\n                    for (int j = 0; j < n; j++)\n                    {\n                        ByteBuffer v = checkNotNull(vals.get(j),\n                                                    \"Invalid null value in condition for column %s\",\n                                                    columnDefs.get(j).name);\n                        values.add(v);\n                    }\n                }\n\n                if (!hasComponent(b, i, componentBounds))\n                    continue;\n\n                ByteBuffer v = checkNotNull(componentBounds.get(b).get(i), \"Invalid null value in condition for column %s\", columnDefs.get(i).name);\n                values.add(v);\n            }\n            toAdd.add(values);\n\n            if (bound.isEnd())\n                Collections.reverse(toAdd);\n\n            return builder.addAllElementsToAll(toAdd);\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return slice.isSupportedBy(column, index);\n        }\n\n        @Override\n        public boolean hasBound(Bound bound)\n        {\n            return slice.hasBound(bound);\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n            slice.addFunctionsTo(functions);\n        }\n\n        @Override\n        public boolean isInclusive(Bound bound)\n        {\n            return slice.isInclusive(bound);\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            checkTrue(otherRestriction.isSlice(),\n                      \"Column \\\"%s\\\" cannot be restricted by both an equality and an inequality relation\",\n                      getColumnsInCommons(otherRestriction));\n\n            if (!getFirstColumn().equals(otherRestriction.getFirstColumn()))\n            {\n                ColumnMetadata column = getFirstColumn().position() > otherRestriction.getFirstColumn().position()\n                        ? getFirstColumn() : otherRestriction.getFirstColumn();\n\n                throw invalidRequest(\"Column \\\"%s\\\" cannot be restricted by two inequalities not starting with the same column\",\n                                     column.name);\n            }\n\n            checkFalse(hasBound(Bound.START) && otherRestriction.hasBound(Bound.START),\n                       \"More than one restriction was found for the start bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n            checkFalse(hasBound(Bound.END) && otherRestriction.hasBound(Bound.END),\n                       \"More than one restriction was found for the end bound on %s\",\n                       getColumnsInCommons(otherRestriction));\n\n            SliceRestriction otherSlice = (SliceRestriction) otherRestriction;\n            List<ColumnMetadata> newColumnDefs = columnDefs.size() >= otherSlice.columnDefs.size() ? columnDefs : otherSlice.columnDefs;\n\n            return new SliceRestriction(newColumnDefs, slice.merge(otherSlice.slice));\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter,\n                                         IndexRegistry indexRegistry,\n                                         QueryOptions options)\n        {\n            throw invalidRequest(\"Multi-column slice restrictions cannot be used for filtering.\");\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"SLICE\" + slice;\n        }\n\n        /**\n         * Similar to bounds(), but returns one ByteBuffer per-component in the bound instead of a single\n         * ByteBuffer to represent the entire bound.\n         * @param b the bound type\n         * @param options the query options\n         * @return one ByteBuffer per-component in the bound\n         */\n        private List<ByteBuffer> componentBounds(Bound b, QueryOptions options)\n        {\n            if (!slice.hasBound(b))\n                return Collections.emptyList();\n\n            Terminal terminal = slice.bound(b).bind(options);\n            return terminal.getElements();\n        }\n\n        private boolean hasComponent(Bound b, int index, EnumMap<Bound, List<ByteBuffer>> componentBounds)\n        {\n            return componentBounds.get(b).size() > index;\n        }\n    }\n\n    public static class NotNullRestriction extends MultiColumnRestriction\n    {\n        public NotNullRestriction(List<ColumnMetadata> columnDefs)\n        {\n            super(columnDefs);\n            assert columnDefs.size() == 1;\n        }\n\n        @Override\n        public void addFunctionsTo(List<Function> functions)\n        {\n        }\n\n        @Override\n        public boolean isNotNull()\n        {\n            return true;\n        }\n\n        @Override\n        public String toString()\n        {\n            return \"IS NOT NULL\";\n        }\n\n        @Override\n        public SingleRestriction doMergeWith(SingleRestriction otherRestriction)\n        {\n            throw invalidRequest(\"%s cannot be restricted by a relation if it includes an IS NOT NULL clause\",\n                                 getColumnsInCommons(otherRestriction));\n        }\n\n        @Override\n        protected boolean isSupportedBy(Index index, ColumnMetadata column)\n        {\n            return index.supportsExpression(column, Operator.IS_NOT);\n        }\n\n        @Override\n        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Cannot use IS NOT NULL restriction for slicing\");\n        }\n\n        @Override\n        public final void addToRowFilter(RowFilter filter, IndexRegistry indexRegistry, QueryOptions options)\n        {\n            throw new UnsupportedOperationException(\"Secondary indexes do not support IS NOT NULL restrictions\");\n        }\n    }\n    \n    @Override\n    public String toString()\n    {\n        return ToStringBuilder.reflectionToString(this, ToStringStyle.SHORT_PREFIX_STYLE);\n    }\n}\n","lineNo":248}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.statements.schema;\n\nimport java.nio.ByteBuffer;\nimport java.util.List;\nimport java.util.Set;\n\nimport com.google.common.base.Objects;\nimport com.google.common.collect.ImmutableSet;\nimport com.google.common.collect.Lists;\n\nimport org.apache.cassandra.audit.AuditLogContext;\nimport org.apache.cassandra.audit.AuditLogEntryType;\nimport org.apache.cassandra.auth.FunctionResource;\nimport org.apache.cassandra.auth.IResource;\nimport org.apache.cassandra.auth.Permission;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.functions.FunctionName;\nimport org.apache.cassandra.cql3.functions.ScalarFunction;\nimport org.apache.cassandra.cql3.functions.UDAggregate;\nimport org.apache.cassandra.cql3.functions.UDFunction;\nimport org.apache.cassandra.cql3.functions.UserFunction;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.schema.UserFunctions.FunctionsDiff;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.Keyspaces;\nimport org.apache.cassandra.schema.Keyspaces.KeyspacesDiff;\nimport org.apache.cassandra.schema.Schema;\nimport org.apache.cassandra.serializers.MarshalException;\nimport org.apache.cassandra.service.ClientState;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.transport.Event.SchemaChange;\nimport org.apache.cassandra.transport.Event.SchemaChange.Change;\nimport org.apache.cassandra.transport.Event.SchemaChange.Target;\n\nimport static java.lang.String.format;\nimport static java.lang.String.join;\nimport static java.util.Collections.singleton;\nimport static java.util.Collections.singletonList;\nimport static java.util.stream.Collectors.toList;\n\nimport static com.google.common.collect.Iterables.concat;\nimport static com.google.common.collect.Iterables.transform;\n\npublic final class CreateAggregateStatement extends AlterSchemaStatement\n{\n    private final String aggregateName;\n    private final List<CQL3Type.Raw> rawArgumentTypes;\n    private final CQL3Type.Raw rawStateType;\n    private final FunctionName stateFunctionName;\n    private final FunctionName finalFunctionName;\n    private final Term.Raw rawInitialValue;\n    private final boolean orReplace;\n    private final boolean ifNotExists;\n\n    public CreateAggregateStatement(String keyspaceName,\n                                    String aggregateName,\n                                    List<CQL3Type.Raw> rawArgumentTypes,\n                                    CQL3Type.Raw rawStateType,\n                                    FunctionName stateFunctionName,\n                                    FunctionName finalFunctionName,\n                                    Term.Raw rawInitialValue,\n                                    boolean orReplace,\n                                    boolean ifNotExists)\n    {\n        super(keyspaceName);\n        this.aggregateName = aggregateName;\n        this.rawArgumentTypes = rawArgumentTypes;\n        this.rawStateType = rawStateType;\n        this.stateFunctionName = stateFunctionName;\n        this.finalFunctionName = finalFunctionName;\n        this.rawInitialValue = rawInitialValue;\n        this.orReplace = orReplace;\n        this.ifNotExists = ifNotExists;\n    }\n\n    @Override\n    public Keyspaces apply(ClusterMetadata metadata)\n    {\n        if (ifNotExists && orReplace)\n            throw ire(\"Cannot use both 'OR REPLACE' and 'IF NOT EXISTS' directives\");\n\n        if (!FunctionName.isNameValid(aggregateName))\n            throw ire(\"Aggregate name '%s' is invalid\", aggregateName);\n\n        rawArgumentTypes.stream()\n                        .filter(raw -> !raw.isImplicitlyFrozen() && raw.isFrozen())\n                        .findFirst()\n                        .ifPresent(t -> { throw ire(\"Argument '%s' cannot be frozen; remove frozen<> modifier from '%s'\", t, t); });\n\n        if (!rawStateType.isImplicitlyFrozen() && rawStateType.isFrozen())\n            throw ire(\"State type '%s' cannot be frozen; remove frozen<> modifier from '%s'\", rawStateType, rawStateType);\n\n        Keyspaces schema = metadata.schema.getKeyspaces();\n        KeyspaceMetadata keyspace = schema.getNullable(keyspaceName);\n        if (null == keyspace)\n            throw ire(\"Keyspace '%s' doesn't exist\", keyspaceName);\n\n        /*\n         * Resolve the state function\n         */\n\n        List<AbstractType<?>> argumentTypes =\n            rawArgumentTypes.stream()\n                            .map(t -> t.prepare(keyspaceName, keyspace.types).getType().udfType())\n                            .collect(toList());\n        AbstractType<?> stateType = rawStateType.prepare(keyspaceName, keyspace.types).getType().udfType();\n        List<AbstractType<?>> stateFunctionArguments = Lists.newArrayList(concat(singleton(stateType), argumentTypes));\n\n        UserFunction stateFunction =\n            keyspace.userFunctions\n                    .find(stateFunctionName, stateFunctionArguments)\n                    .orElseThrow(() -> ire(\"State function %s doesn't exist\", stateFunctionString()));\n\n        if (stateFunction.isAggregate())\n            throw ire(\"State function %s isn't a scalar function\", stateFunctionString());\n\n        if (!stateFunction.returnType().equals(stateType))\n        {\n            throw ire(\"State function %s return type must be the same as the first argument type - check STYPE, argument and return types\",\n                      stateFunctionString());\n        }\n\n        /*\n         * Resolve the final function and return type\n         */\n\n        UserFunction finalFunction = null;\n        AbstractType<?> returnType = stateFunction.returnType();\n\n        if (null != finalFunctionName)\n        {\n            finalFunction = keyspace.userFunctions.find(finalFunctionName, singletonList(stateType)).orElse(null);\n            if (null == finalFunction)\n                throw ire(\"Final function %s doesn't exist\", finalFunctionString());\n\n            if (finalFunction.isAggregate())\n                throw ire(\"Final function %s isn't a scalar function\", finalFunctionString());\n\n            // override return type with that of the final function\n            returnType = finalFunction.returnType();\n        }\n\n        /*\n         * Validate initial condition\n         */\n\n        ByteBuffer initialValue = null;\n        if (null != rawInitialValue)\n        {\n            initialValue = Terms.asBytes(keyspaceName, rawInitialValue.toString(), stateType);\n\n            if (null != initialValue)\n            {\n                try\n                {\n                    stateType.validate(initialValue);\n                }\n                catch (MarshalException e)\n                {\n                    throw ire(\"Invalid value for INITCOND of type %s\", stateType.asCQL3Type());\n                }\n            }\n\n            // Converts initcond to a CQL literal and parse it back to avoid another CASSANDRA-11064\n            String initialValueString = stateType.asCQL3Type().toCQLLiteral(initialValue);\n            if (!Objects.equal(initialValue, stateType.asCQL3Type().fromCQLLiteral(keyspaceName, initialValueString)))\n                throw new AssertionError(String.format(\"CQL literal '%s' (from type %s) parsed with a different value\", initialValueString, stateType.asCQL3Type()));\n\n            if (Constants.NULL_LITERAL != rawInitialValue && isNullOrEmpty(stateType, initialValue))\n                throw ire(\"INITCOND must not be empty for all types except TEXT, ASCII, BLOB\");\n        }\n\n        if (!((UDFunction) stateFunction).isCalledOnNullInput() && null == initialValue)\n        {\n            throw ire(\"Cannot create aggregate '%s' without INITCOND because state function %s does not accept 'null' arguments\",\n                      aggregateName,\n                      stateFunctionName);\n        }\n\n        /*\n         * Create or replace\n         */\n\n        UDAggregate aggregate =\n            new UDAggregate(new FunctionName(keyspaceName, aggregateName),\n                            argumentTypes,\n                            returnType,\n                            (ScalarFunction) stateFunction,\n                            (ScalarFunction) finalFunction,\n                            initialValue);\n\n        UserFunction existingAggregate = keyspace.userFunctions.find(aggregate.name(), argumentTypes).orElse(null);\n        if (null != existingAggregate)\n        {\n            if (!existingAggregate.isAggregate())\n                throw ire(\"Aggregate '%s' cannot replace a function\", aggregateName);\n\n            if (ifNotExists)\n                return schema;\n\n            if (!orReplace)\n                throw ire(\"Aggregate '%s' already exists\", aggregateName);\n\n            if (!returnType.isCompatibleWith(existingAggregate.returnType()))\n            {\n                throw ire(\"Cannot replace aggregate '%s', the new return type %s isn't compatible with the return type %s of existing function\",\n                          aggregateName,\n                          returnType.asCQL3Type(),\n                          existingAggregate.returnType().asCQL3Type());\n            }\n        }\n\n        return schema.withAddedOrUpdated(keyspace.withSwapped(keyspace.userFunctions.withAddedOrUpdated(aggregate)));\n    }\n\n    private static boolean isNullOrEmpty(AbstractType<?> type, ByteBuffer bb)\n    {\n        return bb == null ||\n               (bb.remaining() == 0 && type.isEmptyValueMeaningless());\n    }\n\n    SchemaChange schemaChangeEvent(KeyspacesDiff diff)\n    {\n        assert diff.altered.size() == 1;\n        FunctionsDiff<UDAggregate> udasDiff = diff.altered.get(0).udas;\n\n        assert udasDiff.created.size() + udasDiff.altered.size() == 1;\n        boolean created = !udasDiff.created.isEmpty();\n\n        return new SchemaChange(created ? Change.CREATED : Change.UPDATED,\n                                Target.AGGREGATE,\n                                keyspaceName,\n                                aggregateName,\n                                rawArgumentTypes.stream().map(CQL3Type.Raw::toString).collect(toList()));\n    }\n\n    public void authorize(ClientState client)\n    {\n        FunctionName name = new FunctionName(keyspaceName, aggregateName);\n\n        if (Schema.instance.findUserFunction(name, Lists.transform(rawArgumentTypes, t -> t.prepare(keyspaceName).getType())).isPresent() && orReplace)\n            client.ensurePermission(Permission.ALTER, FunctionResource.functionFromCql(keyspaceName, aggregateName, rawArgumentTypes));\n        else\n            client.ensurePermission(Permission.CREATE, FunctionResource.keyspace(keyspaceName));\n\n        FunctionResource stateFunction =\n            FunctionResource.functionFromCql(stateFunctionName, Lists.newArrayList(concat(singleton(rawStateType), rawArgumentTypes)));\n        client.ensurePermission(Permission.EXECUTE, stateFunction);\n\n        if (null != finalFunctionName)\n            client.ensurePermission(Permission.EXECUTE, FunctionResource.functionFromCql(finalFunctionName, singletonList(rawStateType)));\n    }\n\n    @Override\n    Set<IResource> createdResources(KeyspacesDiff diff)\n    {\n        assert diff.altered.size() == 1;\n        FunctionsDiff<UDAggregate> udasDiff = diff.altered.get(0).udas;\n\n        assert udasDiff.created.size() + udasDiff.altered.size() == 1;\n\n        return udasDiff.created.isEmpty()\n             ? ImmutableSet.of()\n             : ImmutableSet.of(FunctionResource.functionFromCql(keyspaceName, aggregateName, rawArgumentTypes));\n    }\n\n    @Override\n    public AuditLogContext getAuditLogContext()\n    {\n        return new AuditLogContext(AuditLogEntryType.CREATE_AGGREGATE, keyspaceName, aggregateName);\n    }\n\n    public String toString()\n    {\n        return String.format(\"%s (%s, %s)\", getClass().getSimpleName(), keyspaceName, aggregateName);\n    }\n\n    private String stateFunctionString()\n    {\n        return format(\"%s(%s)\", stateFunctionName, join(\", \", transform(concat(singleton(rawStateType), rawArgumentTypes), Object::toString)));\n    }\n\n    private String finalFunctionString()\n    {\n        return format(\"%s(%s)\", finalFunctionName, rawStateType);\n    }\n\n    public static final class Raw extends CQLStatement.Raw\n    {\n        private final FunctionName aggregateName;\n        private final List<CQL3Type.Raw> rawArgumentTypes;\n        private final CQL3Type.Raw rawStateType;\n        private final String stateFunctionName;\n        private final String finalFunctionName;\n        private final Term.Raw rawInitialValue;\n        private final boolean orReplace;\n        private final boolean ifNotExists;\n\n        public Raw(FunctionName aggregateName,\n                   List<CQL3Type.Raw> rawArgumentTypes,\n                   CQL3Type.Raw rawStateType,\n                   String stateFunctionName,\n                   String finalFunctionName,\n                   Term.Raw rawInitialValue,\n                   boolean orReplace,\n                   boolean ifNotExists)\n        {\n            this.aggregateName = aggregateName;\n            this.rawArgumentTypes = rawArgumentTypes;\n            this.rawStateType = rawStateType;\n            this.stateFunctionName = stateFunctionName;\n            this.finalFunctionName = finalFunctionName;\n            this.rawInitialValue = rawInitialValue;\n            this.orReplace = orReplace;\n            this.ifNotExists = ifNotExists;\n        }\n\n        public CreateAggregateStatement prepare(ClientState state)\n        {\n            String keyspaceName = aggregateName.hasKeyspace() ? aggregateName.keyspace : state.getKeyspace();\n\n            return new CreateAggregateStatement(keyspaceName,\n                                                aggregateName.name,\n                                                rawArgumentTypes,\n                                                rawStateType,\n                                                new FunctionName(keyspaceName, stateFunctionName),\n                                                null != finalFunctionName ? new FunctionName(keyspaceName, finalFunctionName) : null,\n                                                rawInitialValue,\n                                                orReplace,\n                                                ifNotExists);\n        }\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.statements.schema;\n\nimport java.nio.ByteBuffer;\nimport java.util.List;\nimport java.util.Set;\n\nimport com.google.common.base.Objects;\nimport com.google.common.collect.ImmutableSet;\nimport com.google.common.collect.Lists;\n\nimport org.apache.cassandra.audit.AuditLogContext;\nimport org.apache.cassandra.audit.AuditLogEntryType;\nimport org.apache.cassandra.auth.FunctionResource;\nimport org.apache.cassandra.auth.IResource;\nimport org.apache.cassandra.auth.Permission;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.functions.FunctionName;\nimport org.apache.cassandra.cql3.functions.ScalarFunction;\nimport org.apache.cassandra.cql3.functions.UDAggregate;\nimport org.apache.cassandra.cql3.functions.UDFunction;\nimport org.apache.cassandra.cql3.functions.UserFunction;\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.schema.UserFunctions.FunctionsDiff;\nimport org.apache.cassandra.schema.KeyspaceMetadata;\nimport org.apache.cassandra.schema.Keyspaces;\nimport org.apache.cassandra.schema.Keyspaces.KeyspacesDiff;\nimport org.apache.cassandra.schema.Schema;\nimport org.apache.cassandra.serializers.MarshalException;\nimport org.apache.cassandra.service.ClientState;\nimport org.apache.cassandra.tcm.ClusterMetadata;\nimport org.apache.cassandra.transport.Event.SchemaChange;\nimport org.apache.cassandra.transport.Event.SchemaChange.Change;\nimport org.apache.cassandra.transport.Event.SchemaChange.Target;\n\nimport static java.lang.String.format;\nimport static java.lang.String.join;\nimport static java.util.Collections.singleton;\nimport static java.util.Collections.singletonList;\nimport static java.util.stream.Collectors.toList;\n\nimport static com.google.common.collect.Iterables.concat;\nimport static com.google.common.collect.Iterables.transform;\n\npublic final class CreateAggregateStatement extends AlterSchemaStatement\n{\n    private final String aggregateName;\n    private final List<CQL3Type.Raw> rawArgumentTypes;\n    private final CQL3Type.Raw rawStateType;\n    private final FunctionName stateFunctionName;\n    private final FunctionName finalFunctionName;\n    private final Term.Raw rawInitialValue;\n    private final boolean orReplace;\n    private final boolean ifNotExists;\n\n    public CreateAggregateStatement(String keyspaceName,\n                                    String aggregateName,\n                                    List<CQL3Type.Raw> rawArgumentTypes,\n                                    CQL3Type.Raw rawStateType,\n                                    FunctionName stateFunctionName,\n                                    FunctionName finalFunctionName,\n                                    Term.Raw rawInitialValue,\n                                    boolean orReplace,\n                                    boolean ifNotExists)\n    {\n        super(keyspaceName);\n        this.aggregateName = aggregateName;\n        this.rawArgumentTypes = rawArgumentTypes;\n        this.rawStateType = rawStateType;\n        this.stateFunctionName = stateFunctionName;\n        this.finalFunctionName = finalFunctionName;\n        this.rawInitialValue = rawInitialValue;\n        this.orReplace = orReplace;\n        this.ifNotExists = ifNotExists;\n    }\n\n    @Override\n    public Keyspaces apply(ClusterMetadata metadata)\n    {\n        if (ifNotExists && orReplace)\n            throw ire(\"Cannot use both 'OR REPLACE' and 'IF NOT EXISTS' directives\");\n\n        if (!FunctionName.isNameValid(aggregateName))\n            throw ire(\"Aggregate name '%s' is invalid\", aggregateName);\n\n        rawArgumentTypes.stream()\n                        .filter(raw -> !raw.isImplicitlyFrozen() && raw.isFrozen())\n                        .findFirst()\n                        .ifPresent(t -> { throw ire(\"Argument '%s' cannot be frozen; remove frozen<> modifier from '%s'\", t, t); });\n\n        if (!rawStateType.isImplicitlyFrozen() && rawStateType.isFrozen())\n            throw ire(\"State type '%s' cannot be frozen; remove frozen<> modifier from '%s'\", rawStateType, rawStateType);\n\n        Keyspaces schema = metadata.schema.getKeyspaces();\n        KeyspaceMetadata keyspace = schema.getNullable(keyspaceName);\n        if (null == keyspace)\n            throw ire(\"Keyspace '%s' doesn't exist\", keyspaceName);\n\n        /*\n         * Resolve the state function\n         */\n\n        List<AbstractType<?>> argumentTypes =\n            rawArgumentTypes.stream()\n                            .map(t -> t.prepare(keyspaceName, keyspace.types).getType().udfType())\n                            .collect(toList());\n        AbstractType<?> stateType = rawStateType.prepare(keyspaceName, keyspace.types).getType().udfType();\n        List<AbstractType<?>> stateFunctionArguments = Lists.newArrayList(concat(singleton(stateType), argumentTypes));\n\n        UserFunction stateFunction =\n            keyspace.userFunctions\n                    .find(stateFunctionName, stateFunctionArguments)\n                    .orElseThrow(() -> ire(\"State function %s doesn't exist\", stateFunctionString()));\n\n        if (stateFunction.isAggregate())\n            throw ire(\"State function %s isn't a scalar function\", stateFunctionString());\n\n        if (!stateFunction.returnType().equals(stateType))\n        {\n            throw ire(\"State function %s return type must be the same as the first argument type - check STYPE, argument and return types\",\n                      stateFunctionString());\n        }\n\n        /*\n         * Resolve the final function and return type\n         */\n\n        UserFunction finalFunction = null;\n        AbstractType<?> returnType = stateFunction.returnType();\n\n        if (null != finalFunctionName)\n        {\n            finalFunction = keyspace.userFunctions.find(finalFunctionName, singletonList(stateType)).orElse(null);\n            if (null == finalFunction)\n                throw ire(\"Final function %s doesn't exist\", finalFunctionString());\n\n            if (finalFunction.isAggregate())\n                throw ire(\"Final function %s isn't a scalar function\", finalFunctionString());\n\n            // override return type with that of the final function\n            returnType = finalFunction.returnType();\n        }\n\n        /*\n         * Validate initial condition\n         */\n\n        ByteBuffer initialValue = null;\n        if (null != rawInitialValue)\n        {\n            String term = rawInitialValue.toString();\n            initialValue = Term.asBytes(keyspaceName, term, stateType);\n\n            if (null != initialValue)\n            {\n                try\n                {\n                    stateType.validate(initialValue);\n                }\n                catch (MarshalException e)\n                {\n                    throw ire(\"Invalid value for INITCOND of type %s\", stateType.asCQL3Type());\n                }\n            }\n\n            // Converts initcond to a CQL literal and parse it back to avoid another CASSANDRA-11064\n            String initialValueString = stateType.asCQL3Type().toCQLLiteral(initialValue);\n            if (!Objects.equal(initialValue, stateType.asCQL3Type().fromCQLLiteral(initialValueString)))\n                throw new AssertionError(String.format(\"CQL literal '%s' (from type %s) parsed with a different value\", initialValueString, stateType.asCQL3Type()));\n\n            if (Constants.NULL_LITERAL != rawInitialValue && isNullOrEmpty(stateType, initialValue))\n                throw ire(\"INITCOND must not be empty for all types except TEXT, ASCII, BLOB\");\n        }\n\n        if (!((UDFunction) stateFunction).isCalledOnNullInput() && null == initialValue)\n        {\n            throw ire(\"Cannot create aggregate '%s' without INITCOND because state function %s does not accept 'null' arguments\",\n                      aggregateName,\n                      stateFunctionName);\n        }\n\n        /*\n         * Create or replace\n         */\n\n        UDAggregate aggregate =\n            new UDAggregate(new FunctionName(keyspaceName, aggregateName),\n                            argumentTypes,\n                            returnType,\n                            (ScalarFunction) stateFunction,\n                            (ScalarFunction) finalFunction,\n                            initialValue);\n\n        UserFunction existingAggregate = keyspace.userFunctions.find(aggregate.name(), argumentTypes).orElse(null);\n        if (null != existingAggregate)\n        {\n            if (!existingAggregate.isAggregate())\n                throw ire(\"Aggregate '%s' cannot replace a function\", aggregateName);\n\n            if (ifNotExists)\n                return schema;\n\n            if (!orReplace)\n                throw ire(\"Aggregate '%s' already exists\", aggregateName);\n\n            if (!returnType.isCompatibleWith(existingAggregate.returnType()))\n            {\n                throw ire(\"Cannot replace aggregate '%s', the new return type %s isn't compatible with the return type %s of existing function\",\n                          aggregateName,\n                          returnType.asCQL3Type(),\n                          existingAggregate.returnType().asCQL3Type());\n            }\n        }\n\n        return schema.withAddedOrUpdated(keyspace.withSwapped(keyspace.userFunctions.withAddedOrUpdated(aggregate)));\n    }\n\n    private static boolean isNullOrEmpty(AbstractType<?> type, ByteBuffer bb)\n    {\n        return bb == null ||\n               (bb.remaining() == 0 && type.isEmptyValueMeaningless());\n    }\n\n    SchemaChange schemaChangeEvent(KeyspacesDiff diff)\n    {\n        assert diff.altered.size() == 1;\n        FunctionsDiff<UDAggregate> udasDiff = diff.altered.get(0).udas;\n\n        assert udasDiff.created.size() + udasDiff.altered.size() == 1;\n        boolean created = !udasDiff.created.isEmpty();\n\n        return new SchemaChange(created ? Change.CREATED : Change.UPDATED,\n                                Target.AGGREGATE,\n                                keyspaceName,\n                                aggregateName,\n                                rawArgumentTypes.stream().map(CQL3Type.Raw::toString).collect(toList()));\n    }\n\n    public void authorize(ClientState client)\n    {\n        FunctionName name = new FunctionName(keyspaceName, aggregateName);\n\n        if (Schema.instance.findUserFunction(name, Lists.transform(rawArgumentTypes, t -> t.prepare(keyspaceName).getType())).isPresent() && orReplace)\n            client.ensurePermission(Permission.ALTER, FunctionResource.functionFromCql(keyspaceName, aggregateName, rawArgumentTypes));\n        else\n            client.ensurePermission(Permission.CREATE, FunctionResource.keyspace(keyspaceName));\n\n        FunctionResource stateFunction =\n            FunctionResource.functionFromCql(stateFunctionName, Lists.newArrayList(concat(singleton(rawStateType), rawArgumentTypes)));\n        client.ensurePermission(Permission.EXECUTE, stateFunction);\n\n        if (null != finalFunctionName)\n            client.ensurePermission(Permission.EXECUTE, FunctionResource.functionFromCql(finalFunctionName, singletonList(rawStateType)));\n    }\n\n    @Override\n    Set<IResource> createdResources(KeyspacesDiff diff)\n    {\n        assert diff.altered.size() == 1;\n        FunctionsDiff<UDAggregate> udasDiff = diff.altered.get(0).udas;\n\n        assert udasDiff.created.size() + udasDiff.altered.size() == 1;\n\n        return udasDiff.created.isEmpty()\n             ? ImmutableSet.of()\n             : ImmutableSet.of(FunctionResource.functionFromCql(keyspaceName, aggregateName, rawArgumentTypes));\n    }\n\n    @Override\n    public AuditLogContext getAuditLogContext()\n    {\n        return new AuditLogContext(AuditLogEntryType.CREATE_AGGREGATE, keyspaceName, aggregateName);\n    }\n\n    public String toString()\n    {\n        return String.format(\"%s (%s, %s)\", getClass().getSimpleName(), keyspaceName, aggregateName);\n    }\n\n    private String stateFunctionString()\n    {\n        return format(\"%s(%s)\", stateFunctionName, join(\", \", transform(concat(singleton(rawStateType), rawArgumentTypes), Object::toString)));\n    }\n\n    private String finalFunctionString()\n    {\n        return format(\"%s(%s)\", finalFunctionName, rawStateType);\n    }\n\n    public static final class Raw extends CQLStatement.Raw\n    {\n        private final FunctionName aggregateName;\n        private final List<CQL3Type.Raw> rawArgumentTypes;\n        private final CQL3Type.Raw rawStateType;\n        private final String stateFunctionName;\n        private final String finalFunctionName;\n        private final Term.Raw rawInitialValue;\n        private final boolean orReplace;\n        private final boolean ifNotExists;\n\n        public Raw(FunctionName aggregateName,\n                   List<CQL3Type.Raw> rawArgumentTypes,\n                   CQL3Type.Raw rawStateType,\n                   String stateFunctionName,\n                   String finalFunctionName,\n                   Term.Raw rawInitialValue,\n                   boolean orReplace,\n                   boolean ifNotExists)\n        {\n            this.aggregateName = aggregateName;\n            this.rawArgumentTypes = rawArgumentTypes;\n            this.rawStateType = rawStateType;\n            this.stateFunctionName = stateFunctionName;\n            this.finalFunctionName = finalFunctionName;\n            this.rawInitialValue = rawInitialValue;\n            this.orReplace = orReplace;\n            this.ifNotExists = ifNotExists;\n        }\n\n        public CreateAggregateStatement prepare(ClientState state)\n        {\n            String keyspaceName = aggregateName.hasKeyspace() ? aggregateName.keyspace : state.getKeyspace();\n\n            return new CreateAggregateStatement(keyspaceName,\n                                                aggregateName.name,\n                                                rawArgumentTypes,\n                                                rawStateType,\n                                                new FunctionName(keyspaceName, stateFunctionName),\n                                                null != finalFunctionName ? new FunctionName(keyspaceName, finalFunctionName) : null,\n                                                rawInitialValue,\n                                                orReplace,\n                                                ifNotExists);\n        }\n    }\n}\n","lineNo":169}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.db.marshal;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.function.Consumer;\n\nimport org.apache.cassandra.cql3.Maps;\nimport org.apache.cassandra.cql3.Term;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.exceptions.SyntaxException;\nimport org.apache.cassandra.serializers.CollectionSerializer;\nimport org.apache.cassandra.serializers.MapSerializer;\nimport org.apache.cassandra.serializers.MarshalException;\nimport org.apache.cassandra.transport.ProtocolVersion;\nimport org.apache.cassandra.utils.JsonUtils;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable.Version;\nimport org.apache.cassandra.utils.bytecomparable.ByteSource;\nimport org.apache.cassandra.utils.bytecomparable.ByteSourceInverse;\nimport org.apache.cassandra.utils.Pair;\n\npublic class MapType<K, V> extends CollectionType<Map<K, V>>\n{\n    // interning instances\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> instances = new ConcurrentHashMap<>();\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> frozenInstances = new ConcurrentHashMap<>();\n\n    private final AbstractType<K> keys;\n    private final AbstractType<V> values;\n    private final MapSerializer<K, V> serializer;\n    private final boolean isMultiCell;\n\n    public static MapType<?, ?> getInstance(TypeParser parser) throws ConfigurationException, SyntaxException\n    {\n        List<AbstractType<?>> l = parser.getTypeParameters();\n        if (l.size() != 2)\n            throw new ConfigurationException(\"MapType takes exactly 2 type parameters\");\n\n        return getInstance(l.get(0).freeze(), l.get(1).freeze(), true);\n    }\n\n    public static <K, V> MapType<K, V> getInstance(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> internMap = isMultiCell ? instances : frozenInstances;\n        Pair<AbstractType<?>, AbstractType<?>> p = Pair.create(keys, values);\n        MapType<K, V> t = internMap.get(p);\n        return null == t\n             ? internMap.computeIfAbsent(p, k -> new MapType<>(k.left, k.right, isMultiCell))\n             : t;\n    }\n\n    private MapType(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        super(ComparisonType.CUSTOM, Kind.MAP);\n        this.keys = keys;\n        this.values = values;\n        this.serializer = MapSerializer.getInstance(keys.getSerializer(),\n                                                    values.getSerializer(),\n                                                    keys.comparatorSet);\n        this.isMultiCell = isMultiCell;\n    }\n\n    @Override\n    public <T> boolean referencesUserType(T name, ValueAccessor<T> accessor)\n    {\n        return keys.referencesUserType(name, accessor) || values.referencesUserType(name, accessor);\n    }\n\n    @Override\n    public MapType<?,?> withUpdatedUserType(UserType udt)\n    {\n        if (!referencesUserType(udt.name))\n            return this;\n\n        (isMultiCell ? instances : frozenInstances).remove(Pair.create(keys, values));\n\n        return getInstance(keys.withUpdatedUserType(udt), values.withUpdatedUserType(udt), isMultiCell);\n    }\n\n    @Override\n    public AbstractType<?> expandUserTypes()\n    {\n        return getInstance(keys.expandUserTypes(), values.expandUserTypes(), isMultiCell);\n    }\n\n    @Override\n    public boolean referencesDuration()\n    {\n        // Maps cannot be created with duration as keys\n        return getValuesType().referencesDuration();\n    }\n\n    public AbstractType<K> getKeysType()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> getValuesType()\n    {\n        return values;\n    }\n\n    public AbstractType<K> nameComparator()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> valueComparator()\n    {\n        return values;\n    }\n\n    @Override\n    public boolean isMultiCell()\n    {\n        return isMultiCell;\n    }\n\n    @Override\n    public List<AbstractType<?>> subTypes()\n    {\n        return Arrays.asList(keys, values);\n    }\n\n    @Override\n    public AbstractType<?> freeze()\n    {\n        // freeze key/value to match org.apache.cassandra.cql3.CQL3Type.Raw.RawCollection.freeze\n        return isMultiCell ? getInstance(this.keys.freeze(), this.values.freeze(), false) : this;\n    }\n\n    @Override\n    public AbstractType<?> unfreeze()\n    {\n        return isMultiCell ? this : getInstance(this.keys, this.values, true);\n    }\n\n    @Override\n    public AbstractType<?> freezeNestedMulticellTypes()\n    {\n        if (!isMultiCell())\n            return this;\n\n        AbstractType<?> keyType = (keys.isFreezable() && keys.isMultiCell())\n                                ? keys.freeze()\n                                : keys.freezeNestedMulticellTypes();\n\n        AbstractType<?> valueType = (values.isFreezable() && values.isMultiCell())\n                                  ? values.freeze()\n                                  : values.freezeNestedMulticellTypes();\n\n        return getInstance(keyType, valueType, isMultiCell);\n    }\n\n    @Override\n    public boolean isCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isCompatibleWith(tprev.values);\n    }\n\n    @Override\n    public boolean isValueCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isValueCompatibleWith(tprev.values);\n    }\n\n    public <RL, TR> int compareCustom(RL left, ValueAccessor<RL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        return compareMaps(keys, values, left, accessorL, right, accessorR);\n    }\n\n    public static <TL, TR> int compareMaps(AbstractType<?> keysComparator, AbstractType<?> valuesComparator, TL left, ValueAccessor<TL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        if (accessorL.isEmpty(left) || accessorR.isEmpty(right))\n            return Boolean.compare(accessorR.isEmpty(right), accessorL.isEmpty(left));\n\n\n        int sizeL = CollectionSerializer.readCollectionSize(left, accessorL);\n        int sizeR = CollectionSerializer.readCollectionSize(right, accessorR);\n\n        int offsetL = CollectionSerializer.sizeOfCollectionSize();\n        int offsetR = CollectionSerializer.sizeOfCollectionSize();\n\n        for (int i = 0; i < Math.min(sizeL, sizeR); i++)\n        {\n            TL k1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(k1, accessorL);\n            TR k2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(k2, accessorR);\n            int cmp = keysComparator.compare(k1, accessorL, k2, accessorR);\n            if (cmp != 0)\n                return cmp;\n\n            TL v1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(v1, accessorL);\n            TR v2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(v2, accessorR);\n            cmp = valuesComparator.compare(v1, accessorL, v2, accessorR);\n            if (cmp != 0)\n                return cmp;\n        }\n\n        return Integer.compare(sizeL, sizeR);\n    }\n\n    @Override\n    public <T> ByteSource asComparableBytes(ValueAccessor<T> accessor, T data, Version version)\n    {\n        return asComparableBytesMap(getKeysType(), getValuesType(), accessor, data, version);\n    }\n\n    @Override\n    public <T> T fromComparableBytes(ValueAccessor<T> accessor, ByteSource.Peekable comparableBytes, Version version)\n    {\n        return fromComparableBytesMap(accessor, comparableBytes, version, getKeysType(), getValuesType());\n    }\n\n    static <V> ByteSource asComparableBytesMap(AbstractType<?> keysComparator,\n                                               AbstractType<?> valuesComparator,\n                                               ValueAccessor<V> accessor,\n                                               V data,\n                                               Version version)\n    {\n        if (accessor.isEmpty(data))\n            return null;\n\n        int offset = 0;\n        int size = CollectionSerializer.readCollectionSize(data, accessor);\n        offset += CollectionSerializer.sizeOfCollectionSize();\n        ByteSource[] srcs = new ByteSource[size * 2];\n        for (int i = 0; i < size; ++i)\n        {\n            V k = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(k, accessor);\n            srcs[i * 2 + 0] = keysComparator.asComparableBytes(accessor, k, version);\n            V v = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(v, accessor);\n            srcs[i * 2 + 1] = valuesComparator.asComparableBytes(accessor, v, version);\n        }\n        return ByteSource.withTerminatorMaybeLegacy(version, 0x00, srcs);\n    }\n\n    static <V> V fromComparableBytesMap(ValueAccessor<V> accessor,\n                                        ByteSource.Peekable comparableBytes,\n                                        Version version,\n                                        AbstractType<?> keysComparator,\n                                        AbstractType<?> valuesComparator)\n    {\n        if (comparableBytes == null)\n            return accessor.empty();\n        assert version != ByteComparable.Version.LEGACY; // legacy translation is not reversible\n\n        List<V> buffers = new ArrayList<>();\n        int separator = comparableBytes.next();\n        while (separator != ByteSource.TERMINATOR)\n        {\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : keysComparator.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : valuesComparator.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n        }\n        return CollectionSerializer.pack(buffers, accessor,buffers.size() / 2);\n    }\n\n    @Override\n    public MapSerializer<K, V> getSerializer()\n    {\n        return serializer;\n    }\n\n    @Override\n    protected int collectionSize(List<ByteBuffer> values)\n    {\n        return values.size() / 2;\n    }\n\n    public String toString(boolean ignoreFreezing)\n    {\n        boolean includeFrozenType = !ignoreFreezing && !isMultiCell();\n\n        StringBuilder sb = new StringBuilder();\n        if (includeFrozenType)\n            sb.append(FrozenType.class.getName()).append(\"(\");\n        sb.append(getClass().getName()).append(TypeParser.stringifyTypeParameters(Arrays.asList(keys, values), ignoreFreezing || !isMultiCell));\n        if (includeFrozenType)\n            sb.append(\")\");\n        return sb.toString();\n    }\n\n    public List<ByteBuffer> serializedValues(Iterator<Cell<?>> cells)\n    {\n        assert isMultiCell;\n        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();\n        while (cells.hasNext())\n        {\n            Cell<?> c = cells.next();\n            bbs.add(c.path().get(0));\n            bbs.add(c.buffer());\n        }\n        return bbs;\n    }\n\n    @Override\n    public Term fromJSONObject(Object parsed) throws MarshalException\n    {\n        if (parsed instanceof String)\n            parsed = JsonUtils.decodeJson((String) parsed);\n\n        if (!(parsed instanceof Map))\n            throw new MarshalException(String.format(\n                    \"Expected a map, but got a %s: %s\", parsed.getClass().getSimpleName(), parsed));\n\n        Map<?, ?> map = (Map<?, ?>) parsed;\n        Map<Term, Term> terms = new HashMap<>(map.size());\n        for (Map.Entry<?, ?> entry : map.entrySet())\n        {\n            if (entry.getKey() == null)\n                throw new MarshalException(\"Invalid null key in map\");\n\n            if (entry.getValue() == null)\n                throw new MarshalException(\"Invalid null value in map\");\n\n            terms.put(keys.fromJSONObject(entry.getKey()), values.fromJSONObject(entry.getValue()));\n        }\n        return new Maps.DelayedValue(keys, terms);\n    }\n\n    @Override\n    public String toJSONString(ByteBuffer buffer, ProtocolVersion protocolVersion)\n    {\n        ByteBuffer value = buffer.duplicate();\n        StringBuilder sb = new StringBuilder(\"{\");\n        int size = CollectionSerializer.readCollectionSize(value, ByteBufferAccessor.instance);\n        int offset = CollectionSerializer.sizeOfCollectionSize();\n        for (int i = 0; i < size; i++)\n        {\n            if (i > 0)\n                sb.append(\", \");\n\n            // map keys must be JSON strings, so convert non-string keys to strings\n            ByteBuffer kv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(kv, ByteBufferAccessor.instance);\n            String key = keys.toJSONString(kv, protocolVersion);\n            if (key.startsWith(\"\\\"\"))\n                sb.append(key);\n            else\n                sb.append('\"').append(JsonUtils.quoteAsJsonString(key)).append('\"');\n\n            sb.append(\": \");\n            ByteBuffer vv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(vv, ByteBufferAccessor.instance);\n            sb.append(values.toJSONString(vv, protocolVersion));\n        }\n        return sb.append(\"}\").toString();\n    }\n\n    @Override\n    public void forEach(ByteBuffer input, Consumer<ByteBuffer> action)\n    {\n        throw new UnsupportedOperationException();\n    }\n\n    @Override\n    public ByteBuffer getMaskedValue()\n    {\n        return decompose(Collections.emptyMap());\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.db.marshal;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.function.Consumer;\n\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.exceptions.SyntaxException;\nimport org.apache.cassandra.serializers.CollectionSerializer;\nimport org.apache.cassandra.serializers.MapSerializer;\nimport org.apache.cassandra.serializers.MarshalException;\nimport org.apache.cassandra.transport.ProtocolVersion;\nimport org.apache.cassandra.utils.JsonUtils;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable.Version;\nimport org.apache.cassandra.utils.bytecomparable.ByteSource;\nimport org.apache.cassandra.utils.bytecomparable.ByteSourceInverse;\nimport org.apache.cassandra.utils.Pair;\n\npublic class MapType<K, V> extends CollectionType<Map<K, V>>\n{\n    // interning instances\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> instances = new ConcurrentHashMap<>();\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> frozenInstances = new ConcurrentHashMap<>();\n\n    private final AbstractType<K> keys;\n    private final AbstractType<V> values;\n    private final MapSerializer<K, V> serializer;\n    private final boolean isMultiCell;\n\n    public static MapType<?, ?> getInstance(TypeParser parser) throws ConfigurationException, SyntaxException\n    {\n        List<AbstractType<?>> l = parser.getTypeParameters();\n        if (l.size() != 2)\n            throw new ConfigurationException(\"MapType takes exactly 2 type parameters\");\n\n        return getInstance(l.get(0).freeze(), l.get(1).freeze(), true);\n    }\n\n    public static <K, V> MapType<K, V> getInstance(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> internMap = isMultiCell ? instances : frozenInstances;\n        Pair<AbstractType<?>, AbstractType<?>> p = Pair.create(keys, values);\n        MapType<K, V> t = internMap.get(p);\n        return null == t\n             ? internMap.computeIfAbsent(p, k -> new MapType<>(k.left, k.right, isMultiCell))\n             : t;\n    }\n\n    private MapType(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        super(ComparisonType.CUSTOM, Kind.MAP);\n        this.keys = keys;\n        this.values = values;\n        this.serializer = MapSerializer.getInstance(keys.getSerializer(),\n                                                    values.getSerializer(),\n                                                    keys.comparatorSet);\n        this.isMultiCell = isMultiCell;\n    }\n\n    @Override\n    public <T> boolean referencesUserType(T name, ValueAccessor<T> accessor)\n    {\n        return keys.referencesUserType(name, accessor) || values.referencesUserType(name, accessor);\n    }\n\n    @Override\n    public MapType<?,?> withUpdatedUserType(UserType udt)\n    {\n        if (!referencesUserType(udt.name))\n            return this;\n\n        (isMultiCell ? instances : frozenInstances).remove(Pair.create(keys, values));\n\n        return getInstance(keys.withUpdatedUserType(udt), values.withUpdatedUserType(udt), isMultiCell);\n    }\n\n    @Override\n    public AbstractType<?> expandUserTypes()\n    {\n        return getInstance(keys.expandUserTypes(), values.expandUserTypes(), isMultiCell);\n    }\n\n    @Override\n    public boolean referencesDuration()\n    {\n        // Maps cannot be created with duration as keys\n        return getValuesType().referencesDuration();\n    }\n\n    public AbstractType<K> getKeysType()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> getValuesType()\n    {\n        return values;\n    }\n\n    public AbstractType<K> nameComparator()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> valueComparator()\n    {\n        return values;\n    }\n\n    @Override\n    public boolean isMultiCell()\n    {\n        return isMultiCell;\n    }\n\n    @Override\n    public List<AbstractType<?>> subTypes()\n    {\n        return Arrays.asList(keys, values);\n    }\n\n    @Override\n    public AbstractType<?> freeze()\n    {\n        // freeze key/value to match org.apache.cassandra.cql3.CQL3Type.Raw.RawCollection.freeze\n        return isMultiCell ? getInstance(this.keys.freeze(), this.values.freeze(), false) : this;\n    }\n\n    @Override\n    public AbstractType<?> unfreeze()\n    {\n        return isMultiCell ? this : getInstance(this.keys, this.values, true);\n    }\n\n    @Override\n    public AbstractType<?> freezeNestedMulticellTypes()\n    {\n        if (!isMultiCell())\n            return this;\n\n        AbstractType<?> keyType = (keys.isFreezable() && keys.isMultiCell())\n                                ? keys.freeze()\n                                : keys.freezeNestedMulticellTypes();\n\n        AbstractType<?> valueType = (values.isFreezable() && values.isMultiCell())\n                                  ? values.freeze()\n                                  : values.freezeNestedMulticellTypes();\n\n        return getInstance(keyType, valueType, isMultiCell);\n    }\n\n    @Override\n    public boolean isCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isCompatibleWith(tprev.values);\n    }\n\n    @Override\n    public boolean isValueCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isValueCompatibleWith(tprev.values);\n    }\n\n    public <RL, TR> int compareCustom(RL left, ValueAccessor<RL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        return compareMaps(keys, values, left, accessorL, right, accessorR);\n    }\n\n    public static <TL, TR> int compareMaps(AbstractType<?> keysComparator, AbstractType<?> valuesComparator, TL left, ValueAccessor<TL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        if (accessorL.isEmpty(left) || accessorR.isEmpty(right))\n            return Boolean.compare(accessorR.isEmpty(right), accessorL.isEmpty(left));\n\n\n        int sizeL = CollectionSerializer.readCollectionSize(left, accessorL);\n        int sizeR = CollectionSerializer.readCollectionSize(right, accessorR);\n\n        int offsetL = CollectionSerializer.sizeOfCollectionSize();\n        int offsetR = CollectionSerializer.sizeOfCollectionSize();\n\n        for (int i = 0; i < Math.min(sizeL, sizeR); i++)\n        {\n            TL k1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(k1, accessorL);\n            TR k2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(k2, accessorR);\n            int cmp = keysComparator.compare(k1, accessorL, k2, accessorR);\n            if (cmp != 0)\n                return cmp;\n\n            TL v1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(v1, accessorL);\n            TR v2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(v2, accessorR);\n            cmp = valuesComparator.compare(v1, accessorL, v2, accessorR);\n            if (cmp != 0)\n                return cmp;\n        }\n\n        return Integer.compare(sizeL, sizeR);\n    }\n\n    @Override\n    public <T> ByteSource asComparableBytes(ValueAccessor<T> accessor, T data, Version version)\n    {\n        if (accessor.isEmpty(data))\n            return null;\n\n        int offset = 0;\n        int size = CollectionSerializer.readCollectionSize(data, accessor);\n        offset += CollectionSerializer.sizeOfCollectionSize();\n        ByteSource[] srcs = new ByteSource[size * 2];\n        for (int i = 0; i < size; ++i)\n        {\n            T k = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(k, accessor);\n            srcs[i * 2 + 0] = keys.asComparableBytes(accessor, k, version);\n            T v = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(v, accessor);\n            srcs[i * 2 + 1] = values.asComparableBytes(accessor, v, version);\n        }\n        return ByteSource.withTerminatorMaybeLegacy(version, 0x00, srcs);\n    }\n\n    @Override\n    public <T> T fromComparableBytes(ValueAccessor<T> accessor, ByteSource.Peekable comparableBytes, Version version)\n    {\n        if (comparableBytes == null)\n            return accessor.empty();\n        assert version != Version.LEGACY; // legacy translation is not reversible\n\n        List<T> buffers = new ArrayList<>();\n        int separator = comparableBytes.next();\n        while (separator != ByteSource.TERMINATOR)\n        {\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : keys.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : values.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n        }\n        return getSerializer().pack(buffers, accessor);\n    }\n\n    @Override\n    public MapSerializer<K, V> getSerializer()\n    {\n        return serializer;\n    }\n\n    public String toString(boolean ignoreFreezing)\n    {\n        boolean includeFrozenType = !ignoreFreezing && !isMultiCell();\n\n        StringBuilder sb = new StringBuilder();\n        if (includeFrozenType)\n            sb.append(FrozenType.class.getName()).append('(');\n        sb.append(getClass().getName()).append(TypeParser.stringifyTypeParameters(Arrays.asList(keys, values), ignoreFreezing || !isMultiCell));\n        if (includeFrozenType)\n            sb.append(')');\n        return sb.toString();\n    }\n\n    public List<ByteBuffer> serializedValues(Iterator<Cell<?>> cells)\n    {\n        assert isMultiCell;\n        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();\n        while (cells.hasNext())\n        {\n            Cell<?> c = cells.next();\n            bbs.add(c.path().get(0));\n            bbs.add(c.buffer());\n        }\n        return bbs;\n    }\n\n    @Override\n    public Term fromJSONObject(Object parsed) throws MarshalException\n    {\n        if (parsed instanceof String)\n            parsed = JsonUtils.decodeJson((String) parsed);\n\n        if (!(parsed instanceof Map))\n            throw new MarshalException(String.format(\n                    \"Expected a map, but got a %s: %s\", parsed.getClass().getSimpleName(), parsed));\n\n        Map<?, ?> map = (Map<?, ?>) parsed;\n        List<Term> terms = new ArrayList<>(map.size() << 1);\n        for (Map.Entry<?, ?> entry : map.entrySet())\n        {\n            if (entry.getKey() == null)\n                throw new MarshalException(\"Invalid null key in map\");\n\n            if (entry.getValue() == null)\n                throw new MarshalException(\"Invalid null value in map\");\n\n            terms.add(keys.fromJSONObject(entry.getKey()));\n            terms.add(values.fromJSONObject(entry.getValue()));\n        }\n        return new MultiElements.DelayedValue(this, terms);\n    }\n\n    @Override\n    public String toJSONString(ByteBuffer buffer, ProtocolVersion protocolVersion)\n    {\n        ByteBuffer value = buffer.duplicate();\n        StringBuilder sb = new StringBuilder(\"{\");\n        int size = CollectionSerializer.readCollectionSize(value, ByteBufferAccessor.instance);\n        int offset = CollectionSerializer.sizeOfCollectionSize();\n        for (int i = 0; i < size; i++)\n        {\n            if (i > 0)\n                sb.append(\", \");\n\n            // map keys must be JSON strings, so convert non-string keys to strings\n            ByteBuffer kv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(kv, ByteBufferAccessor.instance);\n            String key = keys.toJSONString(kv, protocolVersion);\n            if (key.startsWith(\"\\\"\"))\n                sb.append(key);\n            else\n                sb.append('\"').append(JsonUtils.quoteAsJsonString(key)).append('\"');\n\n            sb.append(\": \");\n            ByteBuffer vv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(vv, ByteBufferAccessor.instance);\n            sb.append(values.toJSONString(vv, protocolVersion));\n        }\n        return sb.append(\"}\").toString();\n    }\n\n    @Override\n    public void forEach(ByteBuffer input, Consumer<ByteBuffer> action)\n    {\n        throw new UnsupportedOperationException();\n    }\n\n    @Override\n    public ByteBuffer getMaskedValue()\n    {\n        return decompose(Collections.emptyMap());\n    }\n\n    @Override\n    public List<ByteBuffer> filterSortAndValidateElements(List<ByteBuffer> buffers)\n    {\n        // We depend on Maps to be properly sorted by their keys, so use a sorted map implementation here.\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>(getKeysType());\n        Iterator<ByteBuffer> iter = buffers.iterator();\n        while (iter.hasNext())\n        {\n            ByteBuffer keyBytes = iter.next();\n            ByteBuffer valueBytes = iter.next();\n\n            if (keyBytes == null || valueBytes == null)\n                throw new MarshalException(\"null is not supported inside collections\");\n\n            getKeysType().validate(keyBytes);\n            getValuesType().validate(valueBytes);\n\n            map.put(keyBytes, valueBytes);\n        }\n\n        List<ByteBuffer> sortedBuffers = new ArrayList<>(map.size() << 1);\n        for (Map.Entry<ByteBuffer, ByteBuffer> entry : map.entrySet())\n        {\n            sortedBuffers.add(entry.getKey());\n            sortedBuffers.add(entry.getValue());\n        }\n        return sortedBuffers;\n    }\n}\n","lineNo":242}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.db.marshal;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.function.Consumer;\n\nimport org.apache.cassandra.cql3.Maps;\nimport org.apache.cassandra.cql3.Term;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.exceptions.SyntaxException;\nimport org.apache.cassandra.serializers.CollectionSerializer;\nimport org.apache.cassandra.serializers.MapSerializer;\nimport org.apache.cassandra.serializers.MarshalException;\nimport org.apache.cassandra.transport.ProtocolVersion;\nimport org.apache.cassandra.utils.JsonUtils;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable.Version;\nimport org.apache.cassandra.utils.bytecomparable.ByteSource;\nimport org.apache.cassandra.utils.bytecomparable.ByteSourceInverse;\nimport org.apache.cassandra.utils.Pair;\n\npublic class MapType<K, V> extends CollectionType<Map<K, V>>\n{\n    // interning instances\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> instances = new ConcurrentHashMap<>();\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> frozenInstances = new ConcurrentHashMap<>();\n\n    private final AbstractType<K> keys;\n    private final AbstractType<V> values;\n    private final MapSerializer<K, V> serializer;\n    private final boolean isMultiCell;\n\n    public static MapType<?, ?> getInstance(TypeParser parser) throws ConfigurationException, SyntaxException\n    {\n        List<AbstractType<?>> l = parser.getTypeParameters();\n        if (l.size() != 2)\n            throw new ConfigurationException(\"MapType takes exactly 2 type parameters\");\n\n        return getInstance(l.get(0).freeze(), l.get(1).freeze(), true);\n    }\n\n    public static <K, V> MapType<K, V> getInstance(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> internMap = isMultiCell ? instances : frozenInstances;\n        Pair<AbstractType<?>, AbstractType<?>> p = Pair.create(keys, values);\n        MapType<K, V> t = internMap.get(p);\n        return null == t\n             ? internMap.computeIfAbsent(p, k -> new MapType<>(k.left, k.right, isMultiCell))\n             : t;\n    }\n\n    private MapType(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        super(ComparisonType.CUSTOM, Kind.MAP);\n        this.keys = keys;\n        this.values = values;\n        this.serializer = MapSerializer.getInstance(keys.getSerializer(),\n                                                    values.getSerializer(),\n                                                    keys.comparatorSet);\n        this.isMultiCell = isMultiCell;\n    }\n\n    @Override\n    public <T> boolean referencesUserType(T name, ValueAccessor<T> accessor)\n    {\n        return keys.referencesUserType(name, accessor) || values.referencesUserType(name, accessor);\n    }\n\n    @Override\n    public MapType<?,?> withUpdatedUserType(UserType udt)\n    {\n        if (!referencesUserType(udt.name))\n            return this;\n\n        (isMultiCell ? instances : frozenInstances).remove(Pair.create(keys, values));\n\n        return getInstance(keys.withUpdatedUserType(udt), values.withUpdatedUserType(udt), isMultiCell);\n    }\n\n    @Override\n    public AbstractType<?> expandUserTypes()\n    {\n        return getInstance(keys.expandUserTypes(), values.expandUserTypes(), isMultiCell);\n    }\n\n    @Override\n    public boolean referencesDuration()\n    {\n        // Maps cannot be created with duration as keys\n        return getValuesType().referencesDuration();\n    }\n\n    public AbstractType<K> getKeysType()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> getValuesType()\n    {\n        return values;\n    }\n\n    public AbstractType<K> nameComparator()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> valueComparator()\n    {\n        return values;\n    }\n\n    @Override\n    public boolean isMultiCell()\n    {\n        return isMultiCell;\n    }\n\n    @Override\n    public List<AbstractType<?>> subTypes()\n    {\n        return Arrays.asList(keys, values);\n    }\n\n    @Override\n    public AbstractType<?> freeze()\n    {\n        // freeze key/value to match org.apache.cassandra.cql3.CQL3Type.Raw.RawCollection.freeze\n        return isMultiCell ? getInstance(this.keys.freeze(), this.values.freeze(), false) : this;\n    }\n\n    @Override\n    public AbstractType<?> unfreeze()\n    {\n        return isMultiCell ? this : getInstance(this.keys, this.values, true);\n    }\n\n    @Override\n    public AbstractType<?> freezeNestedMulticellTypes()\n    {\n        if (!isMultiCell())\n            return this;\n\n        AbstractType<?> keyType = (keys.isFreezable() && keys.isMultiCell())\n                                ? keys.freeze()\n                                : keys.freezeNestedMulticellTypes();\n\n        AbstractType<?> valueType = (values.isFreezable() && values.isMultiCell())\n                                  ? values.freeze()\n                                  : values.freezeNestedMulticellTypes();\n\n        return getInstance(keyType, valueType, isMultiCell);\n    }\n\n    @Override\n    public boolean isCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isCompatibleWith(tprev.values);\n    }\n\n    @Override\n    public boolean isValueCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isValueCompatibleWith(tprev.values);\n    }\n\n    public <RL, TR> int compareCustom(RL left, ValueAccessor<RL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        return compareMaps(keys, values, left, accessorL, right, accessorR);\n    }\n\n    public static <TL, TR> int compareMaps(AbstractType<?> keysComparator, AbstractType<?> valuesComparator, TL left, ValueAccessor<TL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        if (accessorL.isEmpty(left) || accessorR.isEmpty(right))\n            return Boolean.compare(accessorR.isEmpty(right), accessorL.isEmpty(left));\n\n\n        int sizeL = CollectionSerializer.readCollectionSize(left, accessorL);\n        int sizeR = CollectionSerializer.readCollectionSize(right, accessorR);\n\n        int offsetL = CollectionSerializer.sizeOfCollectionSize();\n        int offsetR = CollectionSerializer.sizeOfCollectionSize();\n\n        for (int i = 0; i < Math.min(sizeL, sizeR); i++)\n        {\n            TL k1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(k1, accessorL);\n            TR k2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(k2, accessorR);\n            int cmp = keysComparator.compare(k1, accessorL, k2, accessorR);\n            if (cmp != 0)\n                return cmp;\n\n            TL v1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(v1, accessorL);\n            TR v2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(v2, accessorR);\n            cmp = valuesComparator.compare(v1, accessorL, v2, accessorR);\n            if (cmp != 0)\n                return cmp;\n        }\n\n        return Integer.compare(sizeL, sizeR);\n    }\n\n    @Override\n    public <T> ByteSource asComparableBytes(ValueAccessor<T> accessor, T data, Version version)\n    {\n        return asComparableBytesMap(getKeysType(), getValuesType(), accessor, data, version);\n    }\n\n    @Override\n    public <T> T fromComparableBytes(ValueAccessor<T> accessor, ByteSource.Peekable comparableBytes, Version version)\n    {\n        return fromComparableBytesMap(accessor, comparableBytes, version, getKeysType(), getValuesType());\n    }\n\n    static <V> ByteSource asComparableBytesMap(AbstractType<?> keysComparator,\n                                               AbstractType<?> valuesComparator,\n                                               ValueAccessor<V> accessor,\n                                               V data,\n                                               Version version)\n    {\n        if (accessor.isEmpty(data))\n            return null;\n\n        int offset = 0;\n        int size = CollectionSerializer.readCollectionSize(data, accessor);\n        offset += CollectionSerializer.sizeOfCollectionSize();\n        ByteSource[] srcs = new ByteSource[size * 2];\n        for (int i = 0; i < size; ++i)\n        {\n            V k = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(k, accessor);\n            srcs[i * 2 + 0] = keysComparator.asComparableBytes(accessor, k, version);\n            V v = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(v, accessor);\n            srcs[i * 2 + 1] = valuesComparator.asComparableBytes(accessor, v, version);\n        }\n        return ByteSource.withTerminatorMaybeLegacy(version, 0x00, srcs);\n    }\n\n    static <V> V fromComparableBytesMap(ValueAccessor<V> accessor,\n                                        ByteSource.Peekable comparableBytes,\n                                        Version version,\n                                        AbstractType<?> keysComparator,\n                                        AbstractType<?> valuesComparator)\n    {\n        if (comparableBytes == null)\n            return accessor.empty();\n        assert version != ByteComparable.Version.LEGACY; // legacy translation is not reversible\n\n        List<V> buffers = new ArrayList<>();\n        int separator = comparableBytes.next();\n        while (separator != ByteSource.TERMINATOR)\n        {\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : keysComparator.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : valuesComparator.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n        }\n        return CollectionSerializer.pack(buffers, accessor,buffers.size() / 2);\n    }\n\n    @Override\n    public MapSerializer<K, V> getSerializer()\n    {\n        return serializer;\n    }\n\n    @Override\n    protected int collectionSize(List<ByteBuffer> values)\n    {\n        return values.size() / 2;\n    }\n\n    public String toString(boolean ignoreFreezing)\n    {\n        boolean includeFrozenType = !ignoreFreezing && !isMultiCell();\n\n        StringBuilder sb = new StringBuilder();\n        if (includeFrozenType)\n            sb.append(FrozenType.class.getName()).append(\"(\");\n        sb.append(getClass().getName()).append(TypeParser.stringifyTypeParameters(Arrays.asList(keys, values), ignoreFreezing || !isMultiCell));\n        if (includeFrozenType)\n            sb.append(\")\");\n        return sb.toString();\n    }\n\n    public List<ByteBuffer> serializedValues(Iterator<Cell<?>> cells)\n    {\n        assert isMultiCell;\n        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();\n        while (cells.hasNext())\n        {\n            Cell<?> c = cells.next();\n            bbs.add(c.path().get(0));\n            bbs.add(c.buffer());\n        }\n        return bbs;\n    }\n\n    @Override\n    public Term fromJSONObject(Object parsed) throws MarshalException\n    {\n        if (parsed instanceof String)\n            parsed = JsonUtils.decodeJson((String) parsed);\n\n        if (!(parsed instanceof Map))\n            throw new MarshalException(String.format(\n                    \"Expected a map, but got a %s: %s\", parsed.getClass().getSimpleName(), parsed));\n\n        Map<?, ?> map = (Map<?, ?>) parsed;\n        Map<Term, Term> terms = new HashMap<>(map.size());\n        for (Map.Entry<?, ?> entry : map.entrySet())\n        {\n            if (entry.getKey() == null)\n                throw new MarshalException(\"Invalid null key in map\");\n\n            if (entry.getValue() == null)\n                throw new MarshalException(\"Invalid null value in map\");\n\n            terms.put(keys.fromJSONObject(entry.getKey()), values.fromJSONObject(entry.getValue()));\n        }\n        return new Maps.DelayedValue(keys, terms);\n    }\n\n    @Override\n    public String toJSONString(ByteBuffer buffer, ProtocolVersion protocolVersion)\n    {\n        ByteBuffer value = buffer.duplicate();\n        StringBuilder sb = new StringBuilder(\"{\");\n        int size = CollectionSerializer.readCollectionSize(value, ByteBufferAccessor.instance);\n        int offset = CollectionSerializer.sizeOfCollectionSize();\n        for (int i = 0; i < size; i++)\n        {\n            if (i > 0)\n                sb.append(\", \");\n\n            // map keys must be JSON strings, so convert non-string keys to strings\n            ByteBuffer kv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(kv, ByteBufferAccessor.instance);\n            String key = keys.toJSONString(kv, protocolVersion);\n            if (key.startsWith(\"\\\"\"))\n                sb.append(key);\n            else\n                sb.append('\"').append(JsonUtils.quoteAsJsonString(key)).append('\"');\n\n            sb.append(\": \");\n            ByteBuffer vv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(vv, ByteBufferAccessor.instance);\n            sb.append(values.toJSONString(vv, protocolVersion));\n        }\n        return sb.append(\"}\").toString();\n    }\n\n    @Override\n    public void forEach(ByteBuffer input, Consumer<ByteBuffer> action)\n    {\n        throw new UnsupportedOperationException();\n    }\n\n    @Override\n    public ByteBuffer getMaskedValue()\n    {\n        return decompose(Collections.emptyMap());\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.db.marshal;\n\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.function.Consumer;\n\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.db.rows.Cell;\nimport org.apache.cassandra.exceptions.ConfigurationException;\nimport org.apache.cassandra.exceptions.SyntaxException;\nimport org.apache.cassandra.serializers.CollectionSerializer;\nimport org.apache.cassandra.serializers.MapSerializer;\nimport org.apache.cassandra.serializers.MarshalException;\nimport org.apache.cassandra.transport.ProtocolVersion;\nimport org.apache.cassandra.utils.JsonUtils;\nimport org.apache.cassandra.utils.bytecomparable.ByteComparable.Version;\nimport org.apache.cassandra.utils.bytecomparable.ByteSource;\nimport org.apache.cassandra.utils.bytecomparable.ByteSourceInverse;\nimport org.apache.cassandra.utils.Pair;\n\npublic class MapType<K, V> extends CollectionType<Map<K, V>>\n{\n    // interning instances\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> instances = new ConcurrentHashMap<>();\n    private static final ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> frozenInstances = new ConcurrentHashMap<>();\n\n    private final AbstractType<K> keys;\n    private final AbstractType<V> values;\n    private final MapSerializer<K, V> serializer;\n    private final boolean isMultiCell;\n\n    public static MapType<?, ?> getInstance(TypeParser parser) throws ConfigurationException, SyntaxException\n    {\n        List<AbstractType<?>> l = parser.getTypeParameters();\n        if (l.size() != 2)\n            throw new ConfigurationException(\"MapType takes exactly 2 type parameters\");\n\n        return getInstance(l.get(0).freeze(), l.get(1).freeze(), true);\n    }\n\n    public static <K, V> MapType<K, V> getInstance(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        ConcurrentHashMap<Pair<AbstractType<?>, AbstractType<?>>, MapType> internMap = isMultiCell ? instances : frozenInstances;\n        Pair<AbstractType<?>, AbstractType<?>> p = Pair.create(keys, values);\n        MapType<K, V> t = internMap.get(p);\n        return null == t\n             ? internMap.computeIfAbsent(p, k -> new MapType<>(k.left, k.right, isMultiCell))\n             : t;\n    }\n\n    private MapType(AbstractType<K> keys, AbstractType<V> values, boolean isMultiCell)\n    {\n        super(ComparisonType.CUSTOM, Kind.MAP);\n        this.keys = keys;\n        this.values = values;\n        this.serializer = MapSerializer.getInstance(keys.getSerializer(),\n                                                    values.getSerializer(),\n                                                    keys.comparatorSet);\n        this.isMultiCell = isMultiCell;\n    }\n\n    @Override\n    public <T> boolean referencesUserType(T name, ValueAccessor<T> accessor)\n    {\n        return keys.referencesUserType(name, accessor) || values.referencesUserType(name, accessor);\n    }\n\n    @Override\n    public MapType<?,?> withUpdatedUserType(UserType udt)\n    {\n        if (!referencesUserType(udt.name))\n            return this;\n\n        (isMultiCell ? instances : frozenInstances).remove(Pair.create(keys, values));\n\n        return getInstance(keys.withUpdatedUserType(udt), values.withUpdatedUserType(udt), isMultiCell);\n    }\n\n    @Override\n    public AbstractType<?> expandUserTypes()\n    {\n        return getInstance(keys.expandUserTypes(), values.expandUserTypes(), isMultiCell);\n    }\n\n    @Override\n    public boolean referencesDuration()\n    {\n        // Maps cannot be created with duration as keys\n        return getValuesType().referencesDuration();\n    }\n\n    public AbstractType<K> getKeysType()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> getValuesType()\n    {\n        return values;\n    }\n\n    public AbstractType<K> nameComparator()\n    {\n        return keys;\n    }\n\n    public AbstractType<V> valueComparator()\n    {\n        return values;\n    }\n\n    @Override\n    public boolean isMultiCell()\n    {\n        return isMultiCell;\n    }\n\n    @Override\n    public List<AbstractType<?>> subTypes()\n    {\n        return Arrays.asList(keys, values);\n    }\n\n    @Override\n    public AbstractType<?> freeze()\n    {\n        // freeze key/value to match org.apache.cassandra.cql3.CQL3Type.Raw.RawCollection.freeze\n        return isMultiCell ? getInstance(this.keys.freeze(), this.values.freeze(), false) : this;\n    }\n\n    @Override\n    public AbstractType<?> unfreeze()\n    {\n        return isMultiCell ? this : getInstance(this.keys, this.values, true);\n    }\n\n    @Override\n    public AbstractType<?> freezeNestedMulticellTypes()\n    {\n        if (!isMultiCell())\n            return this;\n\n        AbstractType<?> keyType = (keys.isFreezable() && keys.isMultiCell())\n                                ? keys.freeze()\n                                : keys.freezeNestedMulticellTypes();\n\n        AbstractType<?> valueType = (values.isFreezable() && values.isMultiCell())\n                                  ? values.freeze()\n                                  : values.freezeNestedMulticellTypes();\n\n        return getInstance(keyType, valueType, isMultiCell);\n    }\n\n    @Override\n    public boolean isCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isCompatibleWith(tprev.values);\n    }\n\n    @Override\n    public boolean isValueCompatibleWithFrozen(CollectionType<?> previous)\n    {\n        assert !isMultiCell;\n        MapType<?, ?> tprev = (MapType<?, ?>) previous;\n        return keys.isCompatibleWith(tprev.keys) && values.isValueCompatibleWith(tprev.values);\n    }\n\n    public <RL, TR> int compareCustom(RL left, ValueAccessor<RL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        return compareMaps(keys, values, left, accessorL, right, accessorR);\n    }\n\n    public static <TL, TR> int compareMaps(AbstractType<?> keysComparator, AbstractType<?> valuesComparator, TL left, ValueAccessor<TL> accessorL, TR right, ValueAccessor<TR> accessorR)\n    {\n        if (accessorL.isEmpty(left) || accessorR.isEmpty(right))\n            return Boolean.compare(accessorR.isEmpty(right), accessorL.isEmpty(left));\n\n\n        int sizeL = CollectionSerializer.readCollectionSize(left, accessorL);\n        int sizeR = CollectionSerializer.readCollectionSize(right, accessorR);\n\n        int offsetL = CollectionSerializer.sizeOfCollectionSize();\n        int offsetR = CollectionSerializer.sizeOfCollectionSize();\n\n        for (int i = 0; i < Math.min(sizeL, sizeR); i++)\n        {\n            TL k1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(k1, accessorL);\n            TR k2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(k2, accessorR);\n            int cmp = keysComparator.compare(k1, accessorL, k2, accessorR);\n            if (cmp != 0)\n                return cmp;\n\n            TL v1 = CollectionSerializer.readValue(left, accessorL, offsetL);\n            offsetL += CollectionSerializer.sizeOfValue(v1, accessorL);\n            TR v2 = CollectionSerializer.readValue(right, accessorR, offsetR);\n            offsetR += CollectionSerializer.sizeOfValue(v2, accessorR);\n            cmp = valuesComparator.compare(v1, accessorL, v2, accessorR);\n            if (cmp != 0)\n                return cmp;\n        }\n\n        return Integer.compare(sizeL, sizeR);\n    }\n\n    @Override\n    public <T> ByteSource asComparableBytes(ValueAccessor<T> accessor, T data, Version version)\n    {\n        if (accessor.isEmpty(data))\n            return null;\n\n        int offset = 0;\n        int size = CollectionSerializer.readCollectionSize(data, accessor);\n        offset += CollectionSerializer.sizeOfCollectionSize();\n        ByteSource[] srcs = new ByteSource[size * 2];\n        for (int i = 0; i < size; ++i)\n        {\n            T k = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(k, accessor);\n            srcs[i * 2 + 0] = keys.asComparableBytes(accessor, k, version);\n            T v = CollectionSerializer.readValue(data, accessor, offset);\n            offset += CollectionSerializer.sizeOfValue(v, accessor);\n            srcs[i * 2 + 1] = values.asComparableBytes(accessor, v, version);\n        }\n        return ByteSource.withTerminatorMaybeLegacy(version, 0x00, srcs);\n    }\n\n    @Override\n    public <T> T fromComparableBytes(ValueAccessor<T> accessor, ByteSource.Peekable comparableBytes, Version version)\n    {\n        if (comparableBytes == null)\n            return accessor.empty();\n        assert version != Version.LEGACY; // legacy translation is not reversible\n\n        List<T> buffers = new ArrayList<>();\n        int separator = comparableBytes.next();\n        while (separator != ByteSource.TERMINATOR)\n        {\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : keys.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n            buffers.add(ByteSourceInverse.nextComponentNull(separator)\n                        ? null\n                        : values.fromComparableBytes(accessor, comparableBytes, version));\n            separator = comparableBytes.next();\n        }\n        return getSerializer().pack(buffers, accessor);\n    }\n\n    @Override\n    public MapSerializer<K, V> getSerializer()\n    {\n        return serializer;\n    }\n\n    public String toString(boolean ignoreFreezing)\n    {\n        boolean includeFrozenType = !ignoreFreezing && !isMultiCell();\n\n        StringBuilder sb = new StringBuilder();\n        if (includeFrozenType)\n            sb.append(FrozenType.class.getName()).append('(');\n        sb.append(getClass().getName()).append(TypeParser.stringifyTypeParameters(Arrays.asList(keys, values), ignoreFreezing || !isMultiCell));\n        if (includeFrozenType)\n            sb.append(')');\n        return sb.toString();\n    }\n\n    public List<ByteBuffer> serializedValues(Iterator<Cell<?>> cells)\n    {\n        assert isMultiCell;\n        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();\n        while (cells.hasNext())\n        {\n            Cell<?> c = cells.next();\n            bbs.add(c.path().get(0));\n            bbs.add(c.buffer());\n        }\n        return bbs;\n    }\n\n    @Override\n    public Term fromJSONObject(Object parsed) throws MarshalException\n    {\n        if (parsed instanceof String)\n            parsed = JsonUtils.decodeJson((String) parsed);\n\n        if (!(parsed instanceof Map))\n            throw new MarshalException(String.format(\n                    \"Expected a map, but got a %s: %s\", parsed.getClass().getSimpleName(), parsed));\n\n        Map<?, ?> map = (Map<?, ?>) parsed;\n        List<Term> terms = new ArrayList<>(map.size() << 1);\n        for (Map.Entry<?, ?> entry : map.entrySet())\n        {\n            if (entry.getKey() == null)\n                throw new MarshalException(\"Invalid null key in map\");\n\n            if (entry.getValue() == null)\n                throw new MarshalException(\"Invalid null value in map\");\n\n            terms.add(keys.fromJSONObject(entry.getKey()));\n            terms.add(values.fromJSONObject(entry.getValue()));\n        }\n        return new MultiElements.DelayedValue(this, terms);\n    }\n\n    @Override\n    public String toJSONString(ByteBuffer buffer, ProtocolVersion protocolVersion)\n    {\n        ByteBuffer value = buffer.duplicate();\n        StringBuilder sb = new StringBuilder(\"{\");\n        int size = CollectionSerializer.readCollectionSize(value, ByteBufferAccessor.instance);\n        int offset = CollectionSerializer.sizeOfCollectionSize();\n        for (int i = 0; i < size; i++)\n        {\n            if (i > 0)\n                sb.append(\", \");\n\n            // map keys must be JSON strings, so convert non-string keys to strings\n            ByteBuffer kv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(kv, ByteBufferAccessor.instance);\n            String key = keys.toJSONString(kv, protocolVersion);\n            if (key.startsWith(\"\\\"\"))\n                sb.append(key);\n            else\n                sb.append('\"').append(JsonUtils.quoteAsJsonString(key)).append('\"');\n\n            sb.append(\": \");\n            ByteBuffer vv = CollectionSerializer.readValue(value, ByteBufferAccessor.instance, offset);\n            offset += CollectionSerializer.sizeOfValue(vv, ByteBufferAccessor.instance);\n            sb.append(values.toJSONString(vv, protocolVersion));\n        }\n        return sb.append(\"}\").toString();\n    }\n\n    @Override\n    public void forEach(ByteBuffer input, Consumer<ByteBuffer> action)\n    {\n        throw new UnsupportedOperationException();\n    }\n\n    @Override\n    public ByteBuffer getMaskedValue()\n    {\n        return decompose(Collections.emptyMap());\n    }\n\n    @Override\n    public List<ByteBuffer> filterSortAndValidateElements(List<ByteBuffer> buffers)\n    {\n        // We depend on Maps to be properly sorted by their keys, so use a sorted map implementation here.\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>(getKeysType());\n        Iterator<ByteBuffer> iter = buffers.iterator();\n        while (iter.hasNext())\n        {\n            ByteBuffer keyBytes = iter.next();\n            ByteBuffer valueBytes = iter.next();\n\n            if (keyBytes == null || valueBytes == null)\n                throw new MarshalException(\"null is not supported inside collections\");\n\n            getKeysType().validate(keyBytes);\n            getValuesType().validate(valueBytes);\n\n            map.put(keyBytes, valueBytes);\n        }\n\n        List<ByteBuffer> sortedBuffers = new ArrayList<>(map.size() << 1);\n        for (Map.Entry<ByteBuffer, ByteBuffer> entry : map.entrySet())\n        {\n            sortedBuffers.add(entry.getKey());\n            sortedBuffers.add(entry.getValue());\n        }\n        return sortedBuffers;\n    }\n}\n","lineNo":244}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.conditions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.SetType;\nimport org.apache.cassandra.db.rows.*;\nimport org.apache.cassandra.exceptions.InvalidRequestException;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.TimeUUID;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport static org.apache.cassandra.cql3.Operator.*;\nimport static org.apache.cassandra.utils.ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n\npublic class ColumnConditionTest\n{\n    public static final ByteBuffer ZERO = Int32Type.instance.fromString(\"0\");\n    public static final ByteBuffer ONE = Int32Type.instance.fromString(\"1\");\n    public static final ByteBuffer TWO = Int32Type.instance.fromString(\"2\");\n\n    private static Row newRow(ColumnMetadata definition, ByteBuffer value)\n    {\n        BufferCell cell = new BufferCell(definition, 0L, Cell.NO_TTL, Cell.NO_DELETION_TIME, value, null);\n        return BTreeRow.singleCellRow(Clustering.EMPTY, cell);\n    }\n\n    private static Row newRow(ColumnMetadata definition, List<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        long now = System.currentTimeMillis();\n        if (values != null)\n        {\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                TimeUUID uuid = TimeUUID.Generator.atUnixMillis(now, i);\n                ByteBuffer key = uuid.toBytes();\n                ByteBuffer value = values.get(i);\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 value,\n                                                 CellPath.create(key));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, SortedSet<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (ByteBuffer value : values)\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                                                 CellPath.create(value));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, Map<ByteBuffer, ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet())\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 entry.getValue(),\n                                                 CellPath.create(entry.getKey()));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static boolean conditionApplies(ByteBuffer rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", Int32Type.instance);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(List<ByteBuffer> rowValue, Operator op, List<ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Lists.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(List<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(Map<ByteBuffer, ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedSet<ByteBuffer> rowValue, Operator op, SortedSet<ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Sets.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(SortedSet<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedMap<ByteBuffer, ByteBuffer> rowValue, Operator op, SortedMap<ByteBuffer, ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Maps.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    @FunctionalInterface\n    public interface CheckedFunction {\n        void apply();\n    }\n\n    private static void assertThrowsIRE(CheckedFunction runnable, String errorMessage)\n    {\n        try\n        {\n            runnable.apply();\n            fail(\"Expected InvalidRequestException was not thrown\");\n        } catch (InvalidRequestException e)\n        {\n            Assert.assertTrue(\"Expected error message to contain '\" + errorMessage + \"', but got '\" + e.getMessage() + \"'\",\n                              e.getMessage().contains(errorMessage));\n        }\n    }\n\n    @Test\n    public void testSimpleBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(ONE, EQ, ONE));\n        assertFalse(conditionApplies(TWO, EQ, ONE));\n        assertFalse(conditionApplies(ONE, EQ, TWO));\n        assertFalse(conditionApplies(ONE, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, EQ, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(ONE, EQ, null));\n        assertFalse(conditionApplies(null, EQ, ONE));\n        assertTrue(conditionApplies((ByteBuffer) null, EQ, (ByteBuffer) null));\n\n        // NEQ\n        assertFalse(conditionApplies(ONE, NEQ, ONE));\n        assertTrue(conditionApplies(TWO, NEQ, ONE));\n        assertTrue(conditionApplies(ONE, NEQ, TWO));\n        assertTrue(conditionApplies(ONE, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(ONE, NEQ, null));\n        assertTrue(conditionApplies(null, NEQ, ONE));\n        assertFalse(conditionApplies((ByteBuffer) null, NEQ, (ByteBuffer) null));\n\n        // LT\n        assertFalse(conditionApplies(ONE, LT, ONE));\n        assertFalse(conditionApplies(TWO, LT, ONE));\n        assertTrue(conditionApplies(ONE, LT, TWO));\n        assertFalse(conditionApplies(ONE, LT, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, LT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LT, null), \"Invalid comparison with null for operator \\\"<\\\"\");\n        assertFalse(conditionApplies(null, LT, ONE));\n\n        // LTE\n        assertTrue(conditionApplies(ONE, LTE, ONE));\n        assertFalse(conditionApplies(TWO, LTE, ONE));\n        assertTrue(conditionApplies(ONE, LTE, TWO));\n        assertFalse(conditionApplies(ONE, LTE, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LTE, null), \"Invalid comparison with null for operator \\\"<=\\\"\");\n        assertFalse(conditionApplies(null, LTE, ONE));\n\n        // GT\n        assertFalse(conditionApplies(ONE, GT, ONE));\n        assertTrue(conditionApplies(TWO, GT, ONE));\n        assertFalse(conditionApplies(ONE, GT, TWO));\n        assertTrue(conditionApplies(ONE, GT, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GT, null), \"Invalid comparison with null for operator \\\">\\\"\");\n        assertFalse(conditionApplies(null, GT, ONE));\n\n        // GTE\n        assertTrue(conditionApplies(ONE, GTE, ONE));\n        assertTrue(conditionApplies(TWO, GTE, ONE));\n        assertFalse(conditionApplies(ONE, GTE, TWO));\n        assertTrue(conditionApplies(ONE, GTE, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, GTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GTE, null), \"Invalid comparison with null for operator \\\">=\\\"\");\n        assertFalse(conditionApplies(null, GTE, ONE));\n    }\n\n    private static List<ByteBuffer> list(ByteBuffer... values)\n    {\n        return Arrays.asList(values);\n    }\n\n    @Test\n    // sets use the same check as lists\n    public void testListCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(list(ONE), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(), EQ, list()));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list()));\n        assertFalse(conditionApplies(list(), EQ, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(list(ONE), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(), NEQ, list()));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list()));\n        assertTrue(conditionApplies(list(), NEQ, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(list(ONE), LT, list(ONE)));\n        assertFalse(conditionApplies(list(), LT, list()));\n        assertFalse(conditionApplies(list(ONE), LT, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LT, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LT, list()));\n        assertTrue(conditionApplies(list(), LT, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(), LTE, list()));\n        assertFalse(conditionApplies(list(ONE), LTE, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LTE, list()));\n        assertTrue(conditionApplies(list(), LTE, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(), GT, list()));\n        assertTrue(conditionApplies(list(ONE), GT, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GT, list()));\n        assertFalse(conditionApplies(list(), GT, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(list(ONE), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(), GTE, list()));\n        assertTrue(conditionApplies(list(ONE), GTE, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GTE, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GTE, list()));\n        assertFalse(conditionApplies(list(), GTE, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(list(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    private static SortedSet<ByteBuffer> set(ByteBuffer... values)\n    {\n        SortedSet<ByteBuffer> results = new TreeSet<>(Int32Type.instance);\n        results.addAll(Arrays.asList(values));\n        return results;\n    }\n\n    @Test\n    public void testSetCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(set(ONE), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(), EQ, set()));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), EQ, set()));\n        assertFalse(conditionApplies(set(), EQ, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(set(ONE), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(), NEQ, set()));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set()));\n        assertTrue(conditionApplies(set(), NEQ, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(set(ONE), LT, set(ONE)));\n        assertFalse(conditionApplies(set(), LT, set()));\n        assertFalse(conditionApplies(set(ONE), LT, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LT, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LT, set()));\n        assertTrue(conditionApplies(set(), LT, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(), LTE, set()));\n        assertFalse(conditionApplies(set(ONE), LTE, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LTE, set()));\n        assertTrue(conditionApplies(set(), LTE, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE)));\n        assertFalse(conditionApplies(set(), GT, set()));\n        assertTrue(conditionApplies(set(ONE), GT, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GT, set()));\n        assertFalse(conditionApplies(set(), GT, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(set(ONE), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(), GTE, set()));\n        assertTrue(conditionApplies(set(ONE), GTE, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GTE, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GTE, set()));\n        assertFalse(conditionApplies(set(), GTE, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // CONTAINS\n        assertTrue(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(set(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    // values should be a list of key, value, key, value, ...\n    private static SortedMap<ByteBuffer, ByteBuffer> map(ByteBuffer... values)\n    {\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>();\n        for (int i = 0; i < values.length; i += 2)\n            map.put(values[i], values[i + 1]);\n\n        return map;\n    }\n\n    @Test\n    public void testMapCollectionBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), EQ, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map()));\n        assertFalse(conditionApplies(map(), EQ, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), NEQ, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map()));\n        assertTrue(conditionApplies(map(), NEQ, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), LT, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map()));\n        assertTrue(conditionApplies(map(), LT, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), LTE, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map()));\n        assertTrue(conditionApplies(map(), LTE, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), GT, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map()));\n        assertFalse(conditionApplies(map(), GT, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), GTE, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map()));\n        assertFalse(conditionApplies(map(), GTE, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ZERO));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n        //CONTAINS KEY\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ZERO));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ONE));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ONE));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ONE));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.conditions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.SetType;\nimport org.apache.cassandra.db.rows.*;\nimport org.apache.cassandra.exceptions.InvalidRequestException;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.TimeUUID;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport static org.apache.cassandra.cql3.Operator.*;\nimport static org.apache.cassandra.utils.ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n\npublic class ColumnConditionTest\n{\n    public static final ByteBuffer ZERO = Int32Type.instance.fromString(\"0\");\n    public static final ByteBuffer ONE = Int32Type.instance.fromString(\"1\");\n    public static final ByteBuffer TWO = Int32Type.instance.fromString(\"2\");\n\n    private static Row newRow(ColumnMetadata definition, ByteBuffer value)\n    {\n        BufferCell cell = new BufferCell(definition, 0L, Cell.NO_TTL, Cell.NO_DELETION_TIME, value, null);\n        return BTreeRow.singleCellRow(Clustering.EMPTY, cell);\n    }\n\n    private static Row newRow(ColumnMetadata definition, List<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        long now = System.currentTimeMillis();\n        if (values != null)\n        {\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                TimeUUID uuid = TimeUUID.Generator.atUnixMillis(now, i);\n                ByteBuffer key = uuid.toBytes();\n                ByteBuffer value = values.get(i);\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 value,\n                                                 CellPath.create(key));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, SortedSet<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (ByteBuffer value : values)\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                                                 CellPath.create(value));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, Map<ByteBuffer, ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet())\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 entry.getValue(),\n                                                 CellPath.create(entry.getKey()));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static boolean conditionApplies(ByteBuffer rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", Int32Type.instance);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(List<ByteBuffer> rowValue, Operator op, List<ByteBuffer> conditionValue)\n    {\n        ListType<Integer> type = ListType.getInstance(Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(List<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(Map<ByteBuffer, ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedSet<ByteBuffer> rowValue, Operator op, SortedSet<ByteBuffer> conditionValue)\n    {\n        SetType<Integer> type = SetType.getInstance(Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, new ArrayList<>(conditionValue))));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(SortedSet<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedMap<ByteBuffer, ByteBuffer> rowValue, Operator op, SortedMap<ByteBuffer, ByteBuffer> conditionValue)\n    {\n        MapType<Integer, Integer> type = MapType.getInstance(Int32Type.instance, Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        List<ByteBuffer> value = new ArrayList<>(conditionValue.size() * 2);\n        for (Map.Entry<ByteBuffer, ByteBuffer> entry: conditionValue.entrySet())\n        {\n            value.add(entry.getKey());\n            value.add(entry.getValue());\n        }\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, value)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    @FunctionalInterface\n    public interface CheckedFunction {\n        void apply();\n    }\n\n    private static void assertThrowsIRE(CheckedFunction runnable, String errorMessage)\n    {\n        try\n        {\n            runnable.apply();\n            fail(\"Expected InvalidRequestException was not thrown\");\n        } catch (InvalidRequestException e)\n        {\n            Assert.assertTrue(\"Expected error message to contain '\" + errorMessage + \"', but got '\" + e.getMessage() + \"'\",\n                              e.getMessage().contains(errorMessage));\n        }\n    }\n\n    @Test\n    public void testSimpleBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(ONE, EQ, ONE));\n        assertFalse(conditionApplies(TWO, EQ, ONE));\n        assertFalse(conditionApplies(ONE, EQ, TWO));\n        assertFalse(conditionApplies(ONE, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, EQ, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(ONE, EQ, null));\n        assertFalse(conditionApplies(null, EQ, ONE));\n        assertTrue(conditionApplies((ByteBuffer) null, EQ, (ByteBuffer) null));\n\n        // NEQ\n        assertFalse(conditionApplies(ONE, NEQ, ONE));\n        assertTrue(conditionApplies(TWO, NEQ, ONE));\n        assertTrue(conditionApplies(ONE, NEQ, TWO));\n        assertTrue(conditionApplies(ONE, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(ONE, NEQ, null));\n        assertTrue(conditionApplies(null, NEQ, ONE));\n        assertFalse(conditionApplies((ByteBuffer) null, NEQ, (ByteBuffer) null));\n\n        // LT\n        assertFalse(conditionApplies(ONE, LT, ONE));\n        assertFalse(conditionApplies(TWO, LT, ONE));\n        assertTrue(conditionApplies(ONE, LT, TWO));\n        assertFalse(conditionApplies(ONE, LT, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, LT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LT, null), \"Invalid comparison with null for operator \\\"<\\\"\");\n        assertFalse(conditionApplies(null, LT, ONE));\n\n        // LTE\n        assertTrue(conditionApplies(ONE, LTE, ONE));\n        assertFalse(conditionApplies(TWO, LTE, ONE));\n        assertTrue(conditionApplies(ONE, LTE, TWO));\n        assertFalse(conditionApplies(ONE, LTE, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LTE, null), \"Invalid comparison with null for operator \\\"<=\\\"\");\n        assertFalse(conditionApplies(null, LTE, ONE));\n\n        // GT\n        assertFalse(conditionApplies(ONE, GT, ONE));\n        assertTrue(conditionApplies(TWO, GT, ONE));\n        assertFalse(conditionApplies(ONE, GT, TWO));\n        assertTrue(conditionApplies(ONE, GT, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GT, null), \"Invalid comparison with null for operator \\\">\\\"\");\n        assertFalse(conditionApplies(null, GT, ONE));\n\n        // GTE\n        assertTrue(conditionApplies(ONE, GTE, ONE));\n        assertTrue(conditionApplies(TWO, GTE, ONE));\n        assertFalse(conditionApplies(ONE, GTE, TWO));\n        assertTrue(conditionApplies(ONE, GTE, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, GTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GTE, null), \"Invalid comparison with null for operator \\\">=\\\"\");\n        assertFalse(conditionApplies(null, GTE, ONE));\n    }\n\n    private static List<ByteBuffer> list(ByteBuffer... values)\n    {\n        return Arrays.asList(values);\n    }\n\n    @Test\n    // sets use the same check as lists\n    public void testListCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(list(ONE), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(), EQ, list()));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list()));\n        assertFalse(conditionApplies(list(), EQ, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(list(ONE), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(), NEQ, list()));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list()));\n        assertTrue(conditionApplies(list(), NEQ, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(list(ONE), LT, list(ONE)));\n        assertFalse(conditionApplies(list(), LT, list()));\n        assertFalse(conditionApplies(list(ONE), LT, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LT, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LT, list()));\n        assertTrue(conditionApplies(list(), LT, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(), LTE, list()));\n        assertFalse(conditionApplies(list(ONE), LTE, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LTE, list()));\n        assertTrue(conditionApplies(list(), LTE, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(), GT, list()));\n        assertTrue(conditionApplies(list(ONE), GT, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GT, list()));\n        assertFalse(conditionApplies(list(), GT, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(list(ONE), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(), GTE, list()));\n        assertTrue(conditionApplies(list(ONE), GTE, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GTE, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GTE, list()));\n        assertFalse(conditionApplies(list(), GTE, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(list(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    private static SortedSet<ByteBuffer> set(ByteBuffer... values)\n    {\n        SortedSet<ByteBuffer> results = new TreeSet<>(Int32Type.instance);\n        results.addAll(Arrays.asList(values));\n        return results;\n    }\n\n    @Test\n    public void testSetCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(set(ONE), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(), EQ, set()));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), EQ, set()));\n        assertFalse(conditionApplies(set(), EQ, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(set(ONE), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(), NEQ, set()));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set()));\n        assertTrue(conditionApplies(set(), NEQ, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(set(ONE), LT, set(ONE)));\n        assertFalse(conditionApplies(set(), LT, set()));\n        assertFalse(conditionApplies(set(ONE), LT, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LT, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LT, set()));\n        assertTrue(conditionApplies(set(), LT, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(), LTE, set()));\n        assertFalse(conditionApplies(set(ONE), LTE, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LTE, set()));\n        assertTrue(conditionApplies(set(), LTE, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE)));\n        assertFalse(conditionApplies(set(), GT, set()));\n        assertTrue(conditionApplies(set(ONE), GT, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GT, set()));\n        assertFalse(conditionApplies(set(), GT, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(set(ONE), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(), GTE, set()));\n        assertTrue(conditionApplies(set(ONE), GTE, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GTE, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GTE, set()));\n        assertFalse(conditionApplies(set(), GTE, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // CONTAINS\n        assertTrue(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(set(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    // values should be a list of key, value, key, value, ...\n    private static SortedMap<ByteBuffer, ByteBuffer> map(ByteBuffer... values)\n    {\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>();\n        for (int i = 0; i < values.length; i += 2)\n            map.put(values[i], values[i + 1]);\n\n        return map;\n    }\n\n    @Test\n    public void testMapCollectionBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), EQ, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map()));\n        assertFalse(conditionApplies(map(), EQ, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), NEQ, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map()));\n        assertTrue(conditionApplies(map(), NEQ, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), LT, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map()));\n        assertTrue(conditionApplies(map(), LT, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), LTE, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map()));\n        assertTrue(conditionApplies(map(), LTE, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), GT, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map()));\n        assertFalse(conditionApplies(map(), GT, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), GTE, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map()));\n        assertFalse(conditionApplies(map(), GTE, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ZERO));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n        //CONTAINS KEY\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ZERO));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ONE));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ONE));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ONE));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n    }\n}\n","lineNo":135}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.conditions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.SetType;\nimport org.apache.cassandra.db.rows.*;\nimport org.apache.cassandra.exceptions.InvalidRequestException;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.TimeUUID;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport static org.apache.cassandra.cql3.Operator.*;\nimport static org.apache.cassandra.utils.ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n\npublic class ColumnConditionTest\n{\n    public static final ByteBuffer ZERO = Int32Type.instance.fromString(\"0\");\n    public static final ByteBuffer ONE = Int32Type.instance.fromString(\"1\");\n    public static final ByteBuffer TWO = Int32Type.instance.fromString(\"2\");\n\n    private static Row newRow(ColumnMetadata definition, ByteBuffer value)\n    {\n        BufferCell cell = new BufferCell(definition, 0L, Cell.NO_TTL, Cell.NO_DELETION_TIME, value, null);\n        return BTreeRow.singleCellRow(Clustering.EMPTY, cell);\n    }\n\n    private static Row newRow(ColumnMetadata definition, List<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        long now = System.currentTimeMillis();\n        if (values != null)\n        {\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                TimeUUID uuid = TimeUUID.Generator.atUnixMillis(now, i);\n                ByteBuffer key = uuid.toBytes();\n                ByteBuffer value = values.get(i);\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 value,\n                                                 CellPath.create(key));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, SortedSet<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (ByteBuffer value : values)\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                                                 CellPath.create(value));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, Map<ByteBuffer, ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet())\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 entry.getValue(),\n                                                 CellPath.create(entry.getKey()));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static boolean conditionApplies(ByteBuffer rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", Int32Type.instance);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(List<ByteBuffer> rowValue, Operator op, List<ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Lists.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(List<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(Map<ByteBuffer, ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedSet<ByteBuffer> rowValue, Operator op, SortedSet<ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Sets.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(SortedSet<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedMap<ByteBuffer, ByteBuffer> rowValue, Operator op, SortedMap<ByteBuffer, ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Maps.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    @FunctionalInterface\n    public interface CheckedFunction {\n        void apply();\n    }\n\n    private static void assertThrowsIRE(CheckedFunction runnable, String errorMessage)\n    {\n        try\n        {\n            runnable.apply();\n            fail(\"Expected InvalidRequestException was not thrown\");\n        } catch (InvalidRequestException e)\n        {\n            Assert.assertTrue(\"Expected error message to contain '\" + errorMessage + \"', but got '\" + e.getMessage() + \"'\",\n                              e.getMessage().contains(errorMessage));\n        }\n    }\n\n    @Test\n    public void testSimpleBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(ONE, EQ, ONE));\n        assertFalse(conditionApplies(TWO, EQ, ONE));\n        assertFalse(conditionApplies(ONE, EQ, TWO));\n        assertFalse(conditionApplies(ONE, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, EQ, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(ONE, EQ, null));\n        assertFalse(conditionApplies(null, EQ, ONE));\n        assertTrue(conditionApplies((ByteBuffer) null, EQ, (ByteBuffer) null));\n\n        // NEQ\n        assertFalse(conditionApplies(ONE, NEQ, ONE));\n        assertTrue(conditionApplies(TWO, NEQ, ONE));\n        assertTrue(conditionApplies(ONE, NEQ, TWO));\n        assertTrue(conditionApplies(ONE, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(ONE, NEQ, null));\n        assertTrue(conditionApplies(null, NEQ, ONE));\n        assertFalse(conditionApplies((ByteBuffer) null, NEQ, (ByteBuffer) null));\n\n        // LT\n        assertFalse(conditionApplies(ONE, LT, ONE));\n        assertFalse(conditionApplies(TWO, LT, ONE));\n        assertTrue(conditionApplies(ONE, LT, TWO));\n        assertFalse(conditionApplies(ONE, LT, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, LT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LT, null), \"Invalid comparison with null for operator \\\"<\\\"\");\n        assertFalse(conditionApplies(null, LT, ONE));\n\n        // LTE\n        assertTrue(conditionApplies(ONE, LTE, ONE));\n        assertFalse(conditionApplies(TWO, LTE, ONE));\n        assertTrue(conditionApplies(ONE, LTE, TWO));\n        assertFalse(conditionApplies(ONE, LTE, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LTE, null), \"Invalid comparison with null for operator \\\"<=\\\"\");\n        assertFalse(conditionApplies(null, LTE, ONE));\n\n        // GT\n        assertFalse(conditionApplies(ONE, GT, ONE));\n        assertTrue(conditionApplies(TWO, GT, ONE));\n        assertFalse(conditionApplies(ONE, GT, TWO));\n        assertTrue(conditionApplies(ONE, GT, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GT, null), \"Invalid comparison with null for operator \\\">\\\"\");\n        assertFalse(conditionApplies(null, GT, ONE));\n\n        // GTE\n        assertTrue(conditionApplies(ONE, GTE, ONE));\n        assertTrue(conditionApplies(TWO, GTE, ONE));\n        assertFalse(conditionApplies(ONE, GTE, TWO));\n        assertTrue(conditionApplies(ONE, GTE, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, GTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GTE, null), \"Invalid comparison with null for operator \\\">=\\\"\");\n        assertFalse(conditionApplies(null, GTE, ONE));\n    }\n\n    private static List<ByteBuffer> list(ByteBuffer... values)\n    {\n        return Arrays.asList(values);\n    }\n\n    @Test\n    // sets use the same check as lists\n    public void testListCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(list(ONE), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(), EQ, list()));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list()));\n        assertFalse(conditionApplies(list(), EQ, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(list(ONE), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(), NEQ, list()));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list()));\n        assertTrue(conditionApplies(list(), NEQ, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(list(ONE), LT, list(ONE)));\n        assertFalse(conditionApplies(list(), LT, list()));\n        assertFalse(conditionApplies(list(ONE), LT, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LT, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LT, list()));\n        assertTrue(conditionApplies(list(), LT, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(), LTE, list()));\n        assertFalse(conditionApplies(list(ONE), LTE, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LTE, list()));\n        assertTrue(conditionApplies(list(), LTE, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(), GT, list()));\n        assertTrue(conditionApplies(list(ONE), GT, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GT, list()));\n        assertFalse(conditionApplies(list(), GT, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(list(ONE), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(), GTE, list()));\n        assertTrue(conditionApplies(list(ONE), GTE, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GTE, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GTE, list()));\n        assertFalse(conditionApplies(list(), GTE, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(list(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    private static SortedSet<ByteBuffer> set(ByteBuffer... values)\n    {\n        SortedSet<ByteBuffer> results = new TreeSet<>(Int32Type.instance);\n        results.addAll(Arrays.asList(values));\n        return results;\n    }\n\n    @Test\n    public void testSetCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(set(ONE), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(), EQ, set()));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), EQ, set()));\n        assertFalse(conditionApplies(set(), EQ, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(set(ONE), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(), NEQ, set()));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set()));\n        assertTrue(conditionApplies(set(), NEQ, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(set(ONE), LT, set(ONE)));\n        assertFalse(conditionApplies(set(), LT, set()));\n        assertFalse(conditionApplies(set(ONE), LT, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LT, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LT, set()));\n        assertTrue(conditionApplies(set(), LT, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(), LTE, set()));\n        assertFalse(conditionApplies(set(ONE), LTE, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LTE, set()));\n        assertTrue(conditionApplies(set(), LTE, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE)));\n        assertFalse(conditionApplies(set(), GT, set()));\n        assertTrue(conditionApplies(set(ONE), GT, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GT, set()));\n        assertFalse(conditionApplies(set(), GT, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(set(ONE), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(), GTE, set()));\n        assertTrue(conditionApplies(set(ONE), GTE, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GTE, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GTE, set()));\n        assertFalse(conditionApplies(set(), GTE, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // CONTAINS\n        assertTrue(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(set(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    // values should be a list of key, value, key, value, ...\n    private static SortedMap<ByteBuffer, ByteBuffer> map(ByteBuffer... values)\n    {\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>();\n        for (int i = 0; i < values.length; i += 2)\n            map.put(values[i], values[i + 1]);\n\n        return map;\n    }\n\n    @Test\n    public void testMapCollectionBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), EQ, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map()));\n        assertFalse(conditionApplies(map(), EQ, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), NEQ, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map()));\n        assertTrue(conditionApplies(map(), NEQ, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), LT, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map()));\n        assertTrue(conditionApplies(map(), LT, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), LTE, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map()));\n        assertTrue(conditionApplies(map(), LTE, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), GT, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map()));\n        assertFalse(conditionApplies(map(), GT, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), GTE, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map()));\n        assertFalse(conditionApplies(map(), GTE, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ZERO));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n        //CONTAINS KEY\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ZERO));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ONE));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ONE));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ONE));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.conditions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.SetType;\nimport org.apache.cassandra.db.rows.*;\nimport org.apache.cassandra.exceptions.InvalidRequestException;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.TimeUUID;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport static org.apache.cassandra.cql3.Operator.*;\nimport static org.apache.cassandra.utils.ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n\npublic class ColumnConditionTest\n{\n    public static final ByteBuffer ZERO = Int32Type.instance.fromString(\"0\");\n    public static final ByteBuffer ONE = Int32Type.instance.fromString(\"1\");\n    public static final ByteBuffer TWO = Int32Type.instance.fromString(\"2\");\n\n    private static Row newRow(ColumnMetadata definition, ByteBuffer value)\n    {\n        BufferCell cell = new BufferCell(definition, 0L, Cell.NO_TTL, Cell.NO_DELETION_TIME, value, null);\n        return BTreeRow.singleCellRow(Clustering.EMPTY, cell);\n    }\n\n    private static Row newRow(ColumnMetadata definition, List<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        long now = System.currentTimeMillis();\n        if (values != null)\n        {\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                TimeUUID uuid = TimeUUID.Generator.atUnixMillis(now, i);\n                ByteBuffer key = uuid.toBytes();\n                ByteBuffer value = values.get(i);\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 value,\n                                                 CellPath.create(key));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, SortedSet<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (ByteBuffer value : values)\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                                                 CellPath.create(value));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, Map<ByteBuffer, ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet())\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 entry.getValue(),\n                                                 CellPath.create(entry.getKey()));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static boolean conditionApplies(ByteBuffer rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", Int32Type.instance);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(List<ByteBuffer> rowValue, Operator op, List<ByteBuffer> conditionValue)\n    {\n        ListType<Integer> type = ListType.getInstance(Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(List<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(Map<ByteBuffer, ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedSet<ByteBuffer> rowValue, Operator op, SortedSet<ByteBuffer> conditionValue)\n    {\n        SetType<Integer> type = SetType.getInstance(Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, new ArrayList<>(conditionValue))));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(SortedSet<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedMap<ByteBuffer, ByteBuffer> rowValue, Operator op, SortedMap<ByteBuffer, ByteBuffer> conditionValue)\n    {\n        MapType<Integer, Integer> type = MapType.getInstance(Int32Type.instance, Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        List<ByteBuffer> value = new ArrayList<>(conditionValue.size() * 2);\n        for (Map.Entry<ByteBuffer, ByteBuffer> entry: conditionValue.entrySet())\n        {\n            value.add(entry.getKey());\n            value.add(entry.getValue());\n        }\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, value)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    @FunctionalInterface\n    public interface CheckedFunction {\n        void apply();\n    }\n\n    private static void assertThrowsIRE(CheckedFunction runnable, String errorMessage)\n    {\n        try\n        {\n            runnable.apply();\n            fail(\"Expected InvalidRequestException was not thrown\");\n        } catch (InvalidRequestException e)\n        {\n            Assert.assertTrue(\"Expected error message to contain '\" + errorMessage + \"', but got '\" + e.getMessage() + \"'\",\n                              e.getMessage().contains(errorMessage));\n        }\n    }\n\n    @Test\n    public void testSimpleBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(ONE, EQ, ONE));\n        assertFalse(conditionApplies(TWO, EQ, ONE));\n        assertFalse(conditionApplies(ONE, EQ, TWO));\n        assertFalse(conditionApplies(ONE, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, EQ, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(ONE, EQ, null));\n        assertFalse(conditionApplies(null, EQ, ONE));\n        assertTrue(conditionApplies((ByteBuffer) null, EQ, (ByteBuffer) null));\n\n        // NEQ\n        assertFalse(conditionApplies(ONE, NEQ, ONE));\n        assertTrue(conditionApplies(TWO, NEQ, ONE));\n        assertTrue(conditionApplies(ONE, NEQ, TWO));\n        assertTrue(conditionApplies(ONE, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(ONE, NEQ, null));\n        assertTrue(conditionApplies(null, NEQ, ONE));\n        assertFalse(conditionApplies((ByteBuffer) null, NEQ, (ByteBuffer) null));\n\n        // LT\n        assertFalse(conditionApplies(ONE, LT, ONE));\n        assertFalse(conditionApplies(TWO, LT, ONE));\n        assertTrue(conditionApplies(ONE, LT, TWO));\n        assertFalse(conditionApplies(ONE, LT, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, LT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LT, null), \"Invalid comparison with null for operator \\\"<\\\"\");\n        assertFalse(conditionApplies(null, LT, ONE));\n\n        // LTE\n        assertTrue(conditionApplies(ONE, LTE, ONE));\n        assertFalse(conditionApplies(TWO, LTE, ONE));\n        assertTrue(conditionApplies(ONE, LTE, TWO));\n        assertFalse(conditionApplies(ONE, LTE, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LTE, null), \"Invalid comparison with null for operator \\\"<=\\\"\");\n        assertFalse(conditionApplies(null, LTE, ONE));\n\n        // GT\n        assertFalse(conditionApplies(ONE, GT, ONE));\n        assertTrue(conditionApplies(TWO, GT, ONE));\n        assertFalse(conditionApplies(ONE, GT, TWO));\n        assertTrue(conditionApplies(ONE, GT, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GT, null), \"Invalid comparison with null for operator \\\">\\\"\");\n        assertFalse(conditionApplies(null, GT, ONE));\n\n        // GTE\n        assertTrue(conditionApplies(ONE, GTE, ONE));\n        assertTrue(conditionApplies(TWO, GTE, ONE));\n        assertFalse(conditionApplies(ONE, GTE, TWO));\n        assertTrue(conditionApplies(ONE, GTE, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, GTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GTE, null), \"Invalid comparison with null for operator \\\">=\\\"\");\n        assertFalse(conditionApplies(null, GTE, ONE));\n    }\n\n    private static List<ByteBuffer> list(ByteBuffer... values)\n    {\n        return Arrays.asList(values);\n    }\n\n    @Test\n    // sets use the same check as lists\n    public void testListCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(list(ONE), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(), EQ, list()));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list()));\n        assertFalse(conditionApplies(list(), EQ, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(list(ONE), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(), NEQ, list()));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list()));\n        assertTrue(conditionApplies(list(), NEQ, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(list(ONE), LT, list(ONE)));\n        assertFalse(conditionApplies(list(), LT, list()));\n        assertFalse(conditionApplies(list(ONE), LT, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LT, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LT, list()));\n        assertTrue(conditionApplies(list(), LT, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(), LTE, list()));\n        assertFalse(conditionApplies(list(ONE), LTE, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LTE, list()));\n        assertTrue(conditionApplies(list(), LTE, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(), GT, list()));\n        assertTrue(conditionApplies(list(ONE), GT, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GT, list()));\n        assertFalse(conditionApplies(list(), GT, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(list(ONE), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(), GTE, list()));\n        assertTrue(conditionApplies(list(ONE), GTE, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GTE, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GTE, list()));\n        assertFalse(conditionApplies(list(), GTE, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(list(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    private static SortedSet<ByteBuffer> set(ByteBuffer... values)\n    {\n        SortedSet<ByteBuffer> results = new TreeSet<>(Int32Type.instance);\n        results.addAll(Arrays.asList(values));\n        return results;\n    }\n\n    @Test\n    public void testSetCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(set(ONE), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(), EQ, set()));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), EQ, set()));\n        assertFalse(conditionApplies(set(), EQ, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(set(ONE), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(), NEQ, set()));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set()));\n        assertTrue(conditionApplies(set(), NEQ, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(set(ONE), LT, set(ONE)));\n        assertFalse(conditionApplies(set(), LT, set()));\n        assertFalse(conditionApplies(set(ONE), LT, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LT, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LT, set()));\n        assertTrue(conditionApplies(set(), LT, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(), LTE, set()));\n        assertFalse(conditionApplies(set(ONE), LTE, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LTE, set()));\n        assertTrue(conditionApplies(set(), LTE, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE)));\n        assertFalse(conditionApplies(set(), GT, set()));\n        assertTrue(conditionApplies(set(ONE), GT, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GT, set()));\n        assertFalse(conditionApplies(set(), GT, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(set(ONE), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(), GTE, set()));\n        assertTrue(conditionApplies(set(ONE), GTE, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GTE, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GTE, set()));\n        assertFalse(conditionApplies(set(), GTE, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // CONTAINS\n        assertTrue(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(set(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    // values should be a list of key, value, key, value, ...\n    private static SortedMap<ByteBuffer, ByteBuffer> map(ByteBuffer... values)\n    {\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>();\n        for (int i = 0; i < values.length; i += 2)\n            map.put(values[i], values[i + 1]);\n\n        return map;\n    }\n\n    @Test\n    public void testMapCollectionBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), EQ, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map()));\n        assertFalse(conditionApplies(map(), EQ, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), NEQ, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map()));\n        assertTrue(conditionApplies(map(), NEQ, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), LT, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map()));\n        assertTrue(conditionApplies(map(), LT, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), LTE, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map()));\n        assertTrue(conditionApplies(map(), LTE, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), GT, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map()));\n        assertFalse(conditionApplies(map(), GT, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), GTE, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map()));\n        assertFalse(conditionApplies(map(), GTE, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ZERO));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n        //CONTAINS KEY\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ZERO));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ONE));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ONE));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ONE));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n    }\n}\n","lineNo":160}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.conditions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.SetType;\nimport org.apache.cassandra.db.rows.*;\nimport org.apache.cassandra.exceptions.InvalidRequestException;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.TimeUUID;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport static org.apache.cassandra.cql3.Operator.*;\nimport static org.apache.cassandra.utils.ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n\npublic class ColumnConditionTest\n{\n    public static final ByteBuffer ZERO = Int32Type.instance.fromString(\"0\");\n    public static final ByteBuffer ONE = Int32Type.instance.fromString(\"1\");\n    public static final ByteBuffer TWO = Int32Type.instance.fromString(\"2\");\n\n    private static Row newRow(ColumnMetadata definition, ByteBuffer value)\n    {\n        BufferCell cell = new BufferCell(definition, 0L, Cell.NO_TTL, Cell.NO_DELETION_TIME, value, null);\n        return BTreeRow.singleCellRow(Clustering.EMPTY, cell);\n    }\n\n    private static Row newRow(ColumnMetadata definition, List<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        long now = System.currentTimeMillis();\n        if (values != null)\n        {\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                TimeUUID uuid = TimeUUID.Generator.atUnixMillis(now, i);\n                ByteBuffer key = uuid.toBytes();\n                ByteBuffer value = values.get(i);\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 value,\n                                                 CellPath.create(key));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, SortedSet<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (ByteBuffer value : values)\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                                                 CellPath.create(value));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, Map<ByteBuffer, ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet())\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 entry.getValue(),\n                                                 CellPath.create(entry.getKey()));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static boolean conditionApplies(ByteBuffer rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", Int32Type.instance);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(List<ByteBuffer> rowValue, Operator op, List<ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Lists.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(List<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(Map<ByteBuffer, ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedSet<ByteBuffer> rowValue, Operator op, SortedSet<ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Sets.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(SortedSet<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedMap<ByteBuffer, ByteBuffer> rowValue, Operator op, SortedMap<ByteBuffer, ByteBuffer> conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Maps.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    @FunctionalInterface\n    public interface CheckedFunction {\n        void apply();\n    }\n\n    private static void assertThrowsIRE(CheckedFunction runnable, String errorMessage)\n    {\n        try\n        {\n            runnable.apply();\n            fail(\"Expected InvalidRequestException was not thrown\");\n        } catch (InvalidRequestException e)\n        {\n            Assert.assertTrue(\"Expected error message to contain '\" + errorMessage + \"', but got '\" + e.getMessage() + \"'\",\n                              e.getMessage().contains(errorMessage));\n        }\n    }\n\n    @Test\n    public void testSimpleBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(ONE, EQ, ONE));\n        assertFalse(conditionApplies(TWO, EQ, ONE));\n        assertFalse(conditionApplies(ONE, EQ, TWO));\n        assertFalse(conditionApplies(ONE, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, EQ, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(ONE, EQ, null));\n        assertFalse(conditionApplies(null, EQ, ONE));\n        assertTrue(conditionApplies((ByteBuffer) null, EQ, (ByteBuffer) null));\n\n        // NEQ\n        assertFalse(conditionApplies(ONE, NEQ, ONE));\n        assertTrue(conditionApplies(TWO, NEQ, ONE));\n        assertTrue(conditionApplies(ONE, NEQ, TWO));\n        assertTrue(conditionApplies(ONE, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(ONE, NEQ, null));\n        assertTrue(conditionApplies(null, NEQ, ONE));\n        assertFalse(conditionApplies((ByteBuffer) null, NEQ, (ByteBuffer) null));\n\n        // LT\n        assertFalse(conditionApplies(ONE, LT, ONE));\n        assertFalse(conditionApplies(TWO, LT, ONE));\n        assertTrue(conditionApplies(ONE, LT, TWO));\n        assertFalse(conditionApplies(ONE, LT, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, LT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LT, null), \"Invalid comparison with null for operator \\\"<\\\"\");\n        assertFalse(conditionApplies(null, LT, ONE));\n\n        // LTE\n        assertTrue(conditionApplies(ONE, LTE, ONE));\n        assertFalse(conditionApplies(TWO, LTE, ONE));\n        assertTrue(conditionApplies(ONE, LTE, TWO));\n        assertFalse(conditionApplies(ONE, LTE, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LTE, null), \"Invalid comparison with null for operator \\\"<=\\\"\");\n        assertFalse(conditionApplies(null, LTE, ONE));\n\n        // GT\n        assertFalse(conditionApplies(ONE, GT, ONE));\n        assertTrue(conditionApplies(TWO, GT, ONE));\n        assertFalse(conditionApplies(ONE, GT, TWO));\n        assertTrue(conditionApplies(ONE, GT, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GT, null), \"Invalid comparison with null for operator \\\">\\\"\");\n        assertFalse(conditionApplies(null, GT, ONE));\n\n        // GTE\n        assertTrue(conditionApplies(ONE, GTE, ONE));\n        assertTrue(conditionApplies(TWO, GTE, ONE));\n        assertFalse(conditionApplies(ONE, GTE, TWO));\n        assertTrue(conditionApplies(ONE, GTE, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, GTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GTE, null), \"Invalid comparison with null for operator \\\">=\\\"\");\n        assertFalse(conditionApplies(null, GTE, ONE));\n    }\n\n    private static List<ByteBuffer> list(ByteBuffer... values)\n    {\n        return Arrays.asList(values);\n    }\n\n    @Test\n    // sets use the same check as lists\n    public void testListCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(list(ONE), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(), EQ, list()));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list()));\n        assertFalse(conditionApplies(list(), EQ, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(list(ONE), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(), NEQ, list()));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list()));\n        assertTrue(conditionApplies(list(), NEQ, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(list(ONE), LT, list(ONE)));\n        assertFalse(conditionApplies(list(), LT, list()));\n        assertFalse(conditionApplies(list(ONE), LT, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LT, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LT, list()));\n        assertTrue(conditionApplies(list(), LT, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(), LTE, list()));\n        assertFalse(conditionApplies(list(ONE), LTE, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LTE, list()));\n        assertTrue(conditionApplies(list(), LTE, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(), GT, list()));\n        assertTrue(conditionApplies(list(ONE), GT, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GT, list()));\n        assertFalse(conditionApplies(list(), GT, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(list(ONE), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(), GTE, list()));\n        assertTrue(conditionApplies(list(ONE), GTE, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GTE, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GTE, list()));\n        assertFalse(conditionApplies(list(), GTE, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(list(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    private static SortedSet<ByteBuffer> set(ByteBuffer... values)\n    {\n        SortedSet<ByteBuffer> results = new TreeSet<>(Int32Type.instance);\n        results.addAll(Arrays.asList(values));\n        return results;\n    }\n\n    @Test\n    public void testSetCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(set(ONE), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(), EQ, set()));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), EQ, set()));\n        assertFalse(conditionApplies(set(), EQ, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(set(ONE), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(), NEQ, set()));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set()));\n        assertTrue(conditionApplies(set(), NEQ, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(set(ONE), LT, set(ONE)));\n        assertFalse(conditionApplies(set(), LT, set()));\n        assertFalse(conditionApplies(set(ONE), LT, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LT, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LT, set()));\n        assertTrue(conditionApplies(set(), LT, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(), LTE, set()));\n        assertFalse(conditionApplies(set(ONE), LTE, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LTE, set()));\n        assertTrue(conditionApplies(set(), LTE, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE)));\n        assertFalse(conditionApplies(set(), GT, set()));\n        assertTrue(conditionApplies(set(ONE), GT, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GT, set()));\n        assertFalse(conditionApplies(set(), GT, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(set(ONE), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(), GTE, set()));\n        assertTrue(conditionApplies(set(ONE), GTE, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GTE, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GTE, set()));\n        assertFalse(conditionApplies(set(), GTE, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // CONTAINS\n        assertTrue(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(set(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    // values should be a list of key, value, key, value, ...\n    private static SortedMap<ByteBuffer, ByteBuffer> map(ByteBuffer... values)\n    {\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>();\n        for (int i = 0; i < values.length; i += 2)\n            map.put(values[i], values[i + 1]);\n\n        return map;\n    }\n\n    @Test\n    public void testMapCollectionBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), EQ, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map()));\n        assertFalse(conditionApplies(map(), EQ, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), NEQ, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map()));\n        assertTrue(conditionApplies(map(), NEQ, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), LT, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map()));\n        assertTrue(conditionApplies(map(), LT, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), LTE, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map()));\n        assertTrue(conditionApplies(map(), LTE, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), GT, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map()));\n        assertFalse(conditionApplies(map(), GT, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), GTE, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map()));\n        assertFalse(conditionApplies(map(), GTE, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ZERO));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n        //CONTAINS KEY\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ZERO));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ONE));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ONE));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ONE));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.conditions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ListType;\nimport org.apache.cassandra.db.marshal.MapType;\nimport org.apache.cassandra.db.marshal.SetType;\nimport org.apache.cassandra.db.rows.*;\nimport org.apache.cassandra.exceptions.InvalidRequestException;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.utils.ByteBufferUtil;\nimport org.apache.cassandra.utils.TimeUUID;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport static org.apache.cassandra.cql3.Operator.*;\nimport static org.apache.cassandra.utils.ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n\npublic class ColumnConditionTest\n{\n    public static final ByteBuffer ZERO = Int32Type.instance.fromString(\"0\");\n    public static final ByteBuffer ONE = Int32Type.instance.fromString(\"1\");\n    public static final ByteBuffer TWO = Int32Type.instance.fromString(\"2\");\n\n    private static Row newRow(ColumnMetadata definition, ByteBuffer value)\n    {\n        BufferCell cell = new BufferCell(definition, 0L, Cell.NO_TTL, Cell.NO_DELETION_TIME, value, null);\n        return BTreeRow.singleCellRow(Clustering.EMPTY, cell);\n    }\n\n    private static Row newRow(ColumnMetadata definition, List<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        long now = System.currentTimeMillis();\n        if (values != null)\n        {\n            for (int i = 0, m = values.size(); i < m; i++)\n            {\n                TimeUUID uuid = TimeUUID.Generator.atUnixMillis(now, i);\n                ByteBuffer key = uuid.toBytes();\n                ByteBuffer value = values.get(i);\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 value,\n                                                 CellPath.create(key));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, SortedSet<ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (ByteBuffer value : values)\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                                                 CellPath.create(value));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static Row newRow(ColumnMetadata definition, Map<ByteBuffer, ByteBuffer> values)\n    {\n        Row.Builder builder = BTreeRow.sortedBuilder();\n        builder.newRow(Clustering.EMPTY);\n        if (values != null)\n        {\n            for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet())\n            {\n                BufferCell cell = new BufferCell(definition,\n                                                 0L,\n                                                 Cell.NO_TTL,\n                                                 Cell.NO_DELETION_TIME,\n                                                 entry.getValue(),\n                                                 CellPath.create(entry.getKey()));\n                builder.addCell(cell);\n            }\n        }\n        return builder.build();\n    }\n\n    private static boolean conditionApplies(ByteBuffer rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", Int32Type.instance);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(List<ByteBuffer> rowValue, Operator op, List<ByteBuffer> conditionValue)\n    {\n        ListType<Integer> type = ListType.getInstance(Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(List<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", ListType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(Map<ByteBuffer, ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedSet<ByteBuffer> rowValue, Operator op, SortedSet<ByteBuffer> conditionValue)\n    {\n        SetType<Integer> type = SetType.getInstance(Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, new ArrayList<>(conditionValue))));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionContainsApplies(SortedSet<ByteBuffer> rowValue, Operator op, ByteBuffer conditionValue)\n    {\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", SetType.getInstance(Int32Type.instance, true));\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new Constants.Value(conditionValue)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    private static boolean conditionApplies(SortedMap<ByteBuffer, ByteBuffer> rowValue, Operator op, SortedMap<ByteBuffer, ByteBuffer> conditionValue)\n    {\n        MapType<Integer, Integer> type = MapType.getInstance(Int32Type.instance, Int32Type.instance, true);\n        ColumnMetadata definition = ColumnMetadata.regularColumn(\"ks\", \"cf\", \"c\", type);\n        List<ByteBuffer> value = new ArrayList<>(conditionValue.size() * 2);\n        for (Map.Entry<ByteBuffer, ByteBuffer> entry: conditionValue.entrySet())\n        {\n            value.add(entry.getKey());\n            value.add(entry.getValue());\n        }\n        ColumnCondition condition = ColumnCondition.condition(definition, op, Terms.of(new MultiElements.Value(type, value)));\n        ColumnCondition.Bound bound = condition.bind(QueryOptions.DEFAULT);\n        return bound.appliesTo(newRow(definition, rowValue));\n    }\n\n    @FunctionalInterface\n    public interface CheckedFunction {\n        void apply();\n    }\n\n    private static void assertThrowsIRE(CheckedFunction runnable, String errorMessage)\n    {\n        try\n        {\n            runnable.apply();\n            fail(\"Expected InvalidRequestException was not thrown\");\n        } catch (InvalidRequestException e)\n        {\n            Assert.assertTrue(\"Expected error message to contain '\" + errorMessage + \"', but got '\" + e.getMessage() + \"'\",\n                              e.getMessage().contains(errorMessage));\n        }\n    }\n\n    @Test\n    public void testSimpleBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(ONE, EQ, ONE));\n        assertFalse(conditionApplies(TWO, EQ, ONE));\n        assertFalse(conditionApplies(ONE, EQ, TWO));\n        assertFalse(conditionApplies(ONE, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, EQ, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, EQ, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(ONE, EQ, null));\n        assertFalse(conditionApplies(null, EQ, ONE));\n        assertTrue(conditionApplies((ByteBuffer) null, EQ, (ByteBuffer) null));\n\n        // NEQ\n        assertFalse(conditionApplies(ONE, NEQ, ONE));\n        assertTrue(conditionApplies(TWO, NEQ, ONE));\n        assertTrue(conditionApplies(ONE, NEQ, TWO));\n        assertTrue(conditionApplies(ONE, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, NEQ, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(ONE, NEQ, null));\n        assertTrue(conditionApplies(null, NEQ, ONE));\n        assertFalse(conditionApplies((ByteBuffer) null, NEQ, (ByteBuffer) null));\n\n        // LT\n        assertFalse(conditionApplies(ONE, LT, ONE));\n        assertFalse(conditionApplies(TWO, LT, ONE));\n        assertTrue(conditionApplies(ONE, LT, TWO));\n        assertFalse(conditionApplies(ONE, LT, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, LT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LT, null), \"Invalid comparison with null for operator \\\"<\\\"\");\n        assertFalse(conditionApplies(null, LT, ONE));\n\n        // LTE\n        assertTrue(conditionApplies(ONE, LTE, ONE));\n        assertFalse(conditionApplies(TWO, LTE, ONE));\n        assertTrue(conditionApplies(ONE, LTE, TWO));\n        assertFalse(conditionApplies(ONE, LTE, EMPTY_BYTE_BUFFER));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, LTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, LTE, null), \"Invalid comparison with null for operator \\\"<=\\\"\");\n        assertFalse(conditionApplies(null, LTE, ONE));\n\n        // GT\n        assertFalse(conditionApplies(ONE, GT, ONE));\n        assertTrue(conditionApplies(TWO, GT, ONE));\n        assertFalse(conditionApplies(ONE, GT, TWO));\n        assertTrue(conditionApplies(ONE, GT, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, ONE));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GT, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GT, null), \"Invalid comparison with null for operator \\\">\\\"\");\n        assertFalse(conditionApplies(null, GT, ONE));\n\n        // GTE\n        assertTrue(conditionApplies(ONE, GTE, ONE));\n        assertTrue(conditionApplies(TWO, GTE, ONE));\n        assertFalse(conditionApplies(ONE, GTE, TWO));\n        assertTrue(conditionApplies(ONE, GTE, EMPTY_BYTE_BUFFER));\n        assertFalse(conditionApplies(EMPTY_BYTE_BUFFER, GTE, ONE));\n        assertTrue(conditionApplies(EMPTY_BYTE_BUFFER, GTE, EMPTY_BYTE_BUFFER));\n        assertThrowsIRE(() -> conditionApplies(ONE, GTE, null), \"Invalid comparison with null for operator \\\">=\\\"\");\n        assertFalse(conditionApplies(null, GTE, ONE));\n    }\n\n    private static List<ByteBuffer> list(ByteBuffer... values)\n    {\n        return Arrays.asList(values);\n    }\n\n    @Test\n    // sets use the same check as lists\n    public void testListCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(list(ONE), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(), EQ, list()));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), EQ, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), EQ, list()));\n        assertFalse(conditionApplies(list(), EQ, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(list(ONE), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(), NEQ, list()));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), NEQ, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), NEQ, list()));\n        assertTrue(conditionApplies(list(), NEQ, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(list(ONE), LT, list(ONE)));\n        assertFalse(conditionApplies(list(), LT, list()));\n        assertFalse(conditionApplies(list(ONE), LT, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LT, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LT, list()));\n        assertTrue(conditionApplies(list(), LT, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(), LTE, list()));\n        assertFalse(conditionApplies(list(ONE), LTE, list(ZERO)));\n        assertTrue(conditionApplies(list(ZERO), LTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE, ONE), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE), LTE, list(ONE, ONE)));\n        assertFalse(conditionApplies(list(ONE), LTE, list()));\n        assertTrue(conditionApplies(list(), LTE, list(ONE)));\n\n        assertFalse(conditionApplies(list(ONE), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(), GT, list()));\n        assertTrue(conditionApplies(list(ONE), GT, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GT, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GT, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GT, list()));\n        assertFalse(conditionApplies(list(), GT, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ONE)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(list(ONE), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(), GTE, list()));\n        assertTrue(conditionApplies(list(ONE), GTE, list(ZERO)));\n        assertFalse(conditionApplies(list(ZERO), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ONE, ONE), GTE, list(ONE)));\n        assertFalse(conditionApplies(list(ONE), GTE, list(ONE, ONE)));\n        assertTrue(conditionApplies(list(ONE), GTE, list()));\n        assertFalse(conditionApplies(list(), GTE, list(ONE)));\n\n        assertTrue(conditionApplies(list(ONE), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ONE)));\n        assertTrue(conditionApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, list(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(list(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(list(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(list(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    private static SortedSet<ByteBuffer> set(ByteBuffer... values)\n    {\n        SortedSet<ByteBuffer> results = new TreeSet<>(Int32Type.instance);\n        results.addAll(Arrays.asList(values));\n        return results;\n    }\n\n    @Test\n    public void testSetCollectionBoundAppliesTo() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(set(ONE), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(), EQ, set()));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), EQ, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), EQ, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), EQ, set()));\n        assertFalse(conditionApplies(set(), EQ, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(set(ONE), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(), NEQ, set()));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), NEQ, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), NEQ, set()));\n        assertTrue(conditionApplies(set(), NEQ, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(set(ONE), LT, set(ONE)));\n        assertFalse(conditionApplies(set(), LT, set()));\n        assertFalse(conditionApplies(set(ONE), LT, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LT, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LT, set()));\n        assertTrue(conditionApplies(set(), LT, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(), LTE, set()));\n        assertFalse(conditionApplies(set(ONE), LTE, set(ZERO)));\n        assertTrue(conditionApplies(set(ZERO), LTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE, TWO), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE), LTE, set(ONE, TWO)));\n        assertFalse(conditionApplies(set(ONE), LTE, set()));\n        assertTrue(conditionApplies(set(), LTE, set(ONE)));\n\n        assertFalse(conditionApplies(set(ONE), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE)));\n        assertFalse(conditionApplies(set(), GT, set()));\n        assertTrue(conditionApplies(set(ONE), GT, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GT, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GT, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GT, set()));\n        assertFalse(conditionApplies(set(), GT, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ONE)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(set(ONE), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(), GTE, set()));\n        assertTrue(conditionApplies(set(ONE), GTE, set(ZERO)));\n        assertFalse(conditionApplies(set(ZERO), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ONE, TWO), GTE, set(ONE)));\n        assertFalse(conditionApplies(set(ONE), GTE, set(ONE, TWO)));\n        assertTrue(conditionApplies(set(ONE), GTE, set()));\n        assertFalse(conditionApplies(set(), GTE, set(ONE)));\n\n        assertTrue(conditionApplies(set(ONE), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ONE)));\n        assertTrue(conditionApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, set(ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // CONTAINS\n        assertTrue(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(set(ZERO, ONE), CONTAINS, TWO));\n\n        assertFalse(conditionContainsApplies(set(ZERO, ONE, TWO), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(set(ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n    }\n\n    // values should be a list of key, value, key, value, ...\n    private static SortedMap<ByteBuffer, ByteBuffer> map(ByteBuffer... values)\n    {\n        SortedMap<ByteBuffer, ByteBuffer> map = new TreeMap<>();\n        for (int i = 0; i < values.length; i += 2)\n            map.put(values[i], values[i + 1]);\n\n        return map;\n    }\n\n    @Test\n    public void testMapCollectionBoundIsSatisfiedByValue() throws InvalidRequestException\n    {\n        // EQ\n        assertTrue(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), EQ, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map()));\n        assertFalse(conditionApplies(map(), EQ, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), EQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), EQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // NEQ\n        assertFalse(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), NEQ, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map()));\n        assertTrue(conditionApplies(map(), NEQ, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), NEQ, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), NEQ, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LT\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), LT, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LT, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map()));\n        assertTrue(conditionApplies(map(), LT, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // LTE\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), LTE, map()));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ZERO, ONE)));\n        assertTrue(conditionApplies(map(ZERO, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ZERO)));\n        assertTrue(conditionApplies(map(ONE, ZERO), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE, TWO, ONE), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), LTE, map(ONE, ONE, TWO, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map()));\n        assertTrue(conditionApplies(map(), LTE, map(ONE, ONE)));\n\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), LTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), LTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GT\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(), GT, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GT, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map()));\n        assertFalse(conditionApplies(map(), GT, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GT, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GT, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        // GTE\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(), GTE, map()));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ZERO, ONE)));\n        assertFalse(conditionApplies(map(ZERO, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ZERO)));\n        assertFalse(conditionApplies(map(ONE, ZERO), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE, TWO, ONE), GTE, map(ONE, ONE)));\n        assertFalse(conditionApplies(map(ONE, ONE), GTE, map(ONE, ONE, TWO, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map()));\n        assertFalse(conditionApplies(map(), GTE, map(ONE, ONE)));\n\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertFalse(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ONE, ONE), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n        assertFalse(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ONE)));\n        assertTrue(conditionApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), GTE, map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE)));\n        assertTrue(conditionApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), GTE, map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER)));\n\n        //CONTAINS\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS, ZERO));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ONE));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ONE));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n        //CONTAINS KEY\n        assertTrue(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ZERO));\n        assertFalse(conditionContainsApplies(map(ZERO, ONE), CONTAINS_KEY, ONE));\n\n        assertFalse(conditionContainsApplies(map(ONE, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertFalse(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ONE));\n        assertTrue(conditionContainsApplies(map(ByteBufferUtil.EMPTY_BYTE_BUFFER, ONE), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertTrue(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ONE));\n        assertFalse(conditionContainsApplies(map(ONE, ByteBufferUtil.EMPTY_BYTE_BUFFER), CONTAINS_KEY, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n\n    }\n}\n","lineNo":177}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport com.google.common.collect.Iterables;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.Term.MultiItemTerminal;\nimport org.apache.cassandra.cql3.statements.Bound;\n\nimport org.apache.cassandra.db.*;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ReversedType;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static java.util.Arrays.asList;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class ClusteringColumnRestrictionsTest\n{\n    @BeforeClass\n    public static void setupDD()\n    {\n        DatabaseDescriptor.daemonInitialization();\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithNoRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n    }\n\n    /**\n     * Test 'clustering_0 = 1' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_1 = 1' with 2 clustering columns\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_0 IN (1, 2, 3)' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn()\n    {\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value2);\n        assertStartBound(get(bounds, 2), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n        assertEndBound(get(bounds, 1), true, value2);\n        assertEndBound(get(bounds, 2), true, value3);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one descending clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test 'clustering_0 = 1 AND clustering_1 IN (1, 2, 3)'\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        Restriction in = newSingleIN(tableMetadata, 1, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value1);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n        assertEndBound(get(bounds, 2), true, value1, value3);\n    }\n\n    /**\n     * Test equal and slice restrictions (e.g 'clustering_0 = 0 clustering_1 > 1')\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        Restriction eq = newSingleEq(tableMetadata, 0, value3);\n\n        Restriction slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, true, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, false, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 1, Bound.END, false, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value2);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 1, Bound.END, true, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) = (1, 2)' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        Restriction eq = newMultiEq(tableMetadata, 0, value1, value2);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) IN ((1, 2), (2, 3))' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), true, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), true, value2, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column in reverse\n     * order\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 descending clustering columns (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingAndOneAscendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0) > (1) AND (clustering_0, clustering1) < (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n        assertEndBound(get(bounds, 2), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneAscendingAndOneDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n        assertStartBound(get(bounds, 2), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n        assertEndBound(get(bounds, 2), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 ascending clustering column and 2 descending\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoAscendingAndTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // clustering_0 IN (1, 2) AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, in);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value2, value2);\n        assertStartBound(get(bounds, 3), false, value2, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n        assertEndBound(get(bounds, 2), false, value2, value2, value3, value4);\n        assertEndBound(get(bounds, 3), true, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value2, value3);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value4, value3, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value4, value3);\n        assertEndBound(get(bounds, 2), true, value4, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions with ascending, descending, ascending and descending columns\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithAscendingDescendingColumnMix()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC, Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), false, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value2);\n        assertEndBound(get(bounds, 4), true, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), true, value4, value3);\n        assertStartBound(get(bounds, 5), true, value4, value3, value2, value1);\n        assertStartBound(get(bounds, 6), false, value4, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value4);\n        assertEndBound(get(bounds, 4), false, value4, value3, value2);\n        assertEndBound(get(bounds, 5), true, value4, value3, value2);\n        assertEndBound(get(bounds, 6), true, value4);\n    }\n\n    /**\n     * Test mixing single and multi equals restrictions (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 2 AND (clustering_2, clustering_3) = (3, 4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value2);\n        multiEq = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, singleEq, singleEq2, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 = 3\n        singleEq = newSingleEq(tableMetadata, 2, value3);\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3) AND clustering_3 = 4\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        singleEq2 = newSingleEq(tableMetadata, 3, value4);\n        multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Test clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiINRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3), asList(value4, value5));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiIN);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value4, value5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3));\n        restrictions = restrictions(tableMetadata, multiIN, singleEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 5 AND (clustering_2, clustering_3) IN ((2, 3), (4, 5))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value5);\n        multiIN = newMultiIN(tableMetadata, 2, asList(value2, value3), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, singleEq, multiIN, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value5, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value5, value4, value5);\n    }\n\n    /**\n     * Test mixing single equal restrictions with multi-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3) AND (clustering_1) < (4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) => (2, 3) AND (clustering_1, clustering_2) <= (4, 5)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3);\n        multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value4, value5);\n    }\n\n    /**\n     * Test mixing multi equal restrictions with single-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqAndSingleSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 > 3\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction singleSlice = newSingleSlice(tableMetadata, 2, Bound.START, false, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, singleSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) > (3, 4)\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) IN ((3, 4), (4, 5))\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, multiEq, multiIN);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) = (3, 4)\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, multiEq, multiEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty start.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyStart(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isBottom());\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty end.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyEnd(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isTop());\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a start with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertStartBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, true, isInclusive, elements);\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a end with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertEndBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, false, isInclusive, elements);\n    }\n\n    private static void assertBound(ClusteringBound<?> bound, boolean isStart, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertEquals(\"the bound size is not the expected one:\", elements.length, bound.size());\n        assertEquals(\"the bound should be a \" + (isStart ? \"start\" : \"end\") + \" but is a \" + (bound.isStart() ? \"start\" : \"end\"), isStart, bound.isStart());\n        assertEquals(\"the bound inclusiveness is not the expected one\", isInclusive, bound.isInclusive());\n        for (int i = 0, m = elements.length; i < m; i++)\n        {\n            ByteBuffer element = elements[i];\n            assertEquals(String.format(\"the element %s of the bound is not the expected one: expected %s but was %s\",\n                                       i,\n                                       ByteBufferUtil.toInt(element),\n                                       ByteBufferUtil.toInt(bound.bufferAt(i))), bound.bufferAt(i), element);\n        }\n    }\n\n    private static TableMetadata newTableMetadata(Sort... sorts)\n    {\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (Sort sort : sorts)\n            types.add(sort == Sort.ASC ? Int32Type.instance : ReversedType.getInstance(Int32Type.instance));\n\n        TableMetadata.Builder builder =\n            TableMetadata.builder(\"keyspace\", \"test\")\n                         .addPartitionKeyColumn(\"partition_key\", Int32Type.instance);\n\n        for (int i = 0; i < sorts.length; i++)\n            builder.addClusteringColumn(\"clustering_\" + i, types.get(i));\n\n        return builder.build();\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param value the equality value\n     * @return a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleEq(TableMetadata tableMetadata, int index, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.EQRestriction(columnDef, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param values the equality value\n     * @return a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiEq(TableMetadata tableMetadata, int firstIndex, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, firstIndex + i));\n        }\n        return new MultiColumnRestriction.EQRestriction(columnMetadatas, toMultiItemTerminal(values));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the index of the first clustering column\n     * @param values the in values\n     * @return a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    @SafeVarargs\n    private static Restriction newMultiIN(TableMetadata tableMetadata, int firstIndex, List<ByteBuffer>... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        List<Term> terms = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, firstIndex + i));\n            terms.add(toMultiItemTerminal(values[i].toArray(new ByteBuffer[0])));\n        }\n        return new MultiColumnRestriction.InRestrictionWithValues(columnMetadatas, terms);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param values the in values\n     * @return a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleIN(TableMetadata tableMetadata, int index, ByteBuffer... values)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.InRestrictionWithValues(columnDef, toTerms(values));\n    }\n\n    /**\n     * Returns the clustering <code>ColumnMetadata<\/code> for the specified position.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @return the clustering <code>ColumnMetadata<\/code> for the specified position.\n     */\n    private static ColumnMetadata getClusteringColumnDefinition(TableMetadata tableMetadata, int index)\n    {\n        return tableMetadata.clusteringColumns().get(index);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param value the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleSlice(TableMetadata tableMetadata, int index, Bound bound, boolean inclusive, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.SliceRestriction(columnDef, bound, inclusive, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param values the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiSlice(TableMetadata tableMetadata, int firstIndex, Bound bound, boolean inclusive, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, i + firstIndex));\n        }\n        return new MultiColumnRestriction.SliceRestriction(columnMetadatas, bound, inclusive, toMultiItemTerminal(values));\n    }\n\n    /**\n     * Converts the specified values into a <code>MultiItemTerminal<\/code>.\n     *\n     * @param values the values to convert.\n     * @return the term corresponding to the specified values.\n     */\n    private static MultiItemTerminal toMultiItemTerminal(ByteBuffer... values)\n    {\n        return new Tuples.Value(values);\n    }\n\n    /**\n     * Converts the specified value into a term.\n     *\n     * @param value the value to convert.\n     * @return the term corresponding to the specified value.\n     */\n    private static Term toTerm(ByteBuffer value)\n    {\n        return new Constants.Value(value);\n    }\n\n    /**\n     * Converts the specified values into a <code>List<\/code> of terms.\n     *\n     * @param values the values to convert.\n     * @return a <code>List<\/code> of terms corresponding to the specified values.\n     */\n    private static List<Term> toTerms(ByteBuffer... values)\n    {\n        List<Term> terms = new ArrayList<>();\n        for (ByteBuffer value : values)\n            terms.add(toTerm(value));\n        return terms;\n    }\n\n    private static <T> T get(SortedSet<T> set, int i)\n    {\n        return Iterables.get(set, i);\n    }\n    \n    private static ClusteringColumnRestrictions restrictions(TableMetadata table, Restriction... restrictions)\n    {\n        ClusteringColumnRestrictions clusteringColumnRestrictions = new ClusteringColumnRestrictions(table, false);\n        for (Restriction restriction : restrictions)\n            clusteringColumnRestrictions = clusteringColumnRestrictions.mergeWith(restriction, null);\n        return clusteringColumnRestrictions;\n    }\n\n    private enum Sort\n    {\n        ASC,\n        DESC;\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport com.google.common.collect.Iterables;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.db.marshal.TupleType;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.statements.Bound;\n\nimport org.apache.cassandra.db.*;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ReversedType;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static java.util.Arrays.asList;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class ClusteringColumnRestrictionsTest\n{\n    @BeforeClass\n    public static void setupDD()\n    {\n        DatabaseDescriptor.daemonInitialization();\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithNoRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n    }\n\n    /**\n     * Test 'clustering_0 = 1' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_1 = 1' with 2 clustering columns\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_0 IN (1, 2, 3)' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn()\n    {\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value2);\n        assertStartBound(get(bounds, 2), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n        assertEndBound(get(bounds, 1), true, value2);\n        assertEndBound(get(bounds, 2), true, value3);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one descending clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test 'clustering_0 = 1 AND clustering_1 IN (1, 2, 3)'\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        Restriction in = newSingleIN(tableMetadata, 1, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value1);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n        assertEndBound(get(bounds, 2), true, value1, value3);\n    }\n\n    /**\n     * Test equal and slice restrictions (e.g 'clustering_0 = 0 clustering_1 > 1')\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        Restriction eq = newSingleEq(tableMetadata, 0, value3);\n\n        Restriction slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, true, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, false, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 1, Bound.END, false, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value2);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 1, Bound.END, true, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) = (1, 2)' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        Restriction eq = newMultiEq(tableMetadata, 0, value1, value2);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) IN ((1, 2), (2, 3))' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), true, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), true, value2, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column in reverse\n     * order\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 descending clustering columns (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingAndOneAscendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0) > (1) AND (clustering_0, clustering1) < (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n        assertEndBound(get(bounds, 2), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneAscendingAndOneDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n        assertStartBound(get(bounds, 2), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n        assertEndBound(get(bounds, 2), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 ascending clustering column and 2 descending\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoAscendingAndTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // clustering_0 IN (1, 2) AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, in);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value2, value2);\n        assertStartBound(get(bounds, 3), false, value2, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n        assertEndBound(get(bounds, 2), false, value2, value2, value3, value4);\n        assertEndBound(get(bounds, 3), true, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value2, value3);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value4, value3, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value4, value3);\n        assertEndBound(get(bounds, 2), true, value4, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions with ascending, descending, ascending and descending columns\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithAscendingDescendingColumnMix()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC, Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), false, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value2);\n        assertEndBound(get(bounds, 4), true, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), true, value4, value3);\n        assertStartBound(get(bounds, 5), true, value4, value3, value2, value1);\n        assertStartBound(get(bounds, 6), false, value4, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value4);\n        assertEndBound(get(bounds, 4), false, value4, value3, value2);\n        assertEndBound(get(bounds, 5), true, value4, value3, value2);\n        assertEndBound(get(bounds, 6), true, value4);\n    }\n\n    /**\n     * Test mixing single and multi equals restrictions (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 2 AND (clustering_2, clustering_3) = (3, 4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value2);\n        multiEq = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, singleEq, singleEq2, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 = 3\n        singleEq = newSingleEq(tableMetadata, 2, value3);\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3) AND clustering_3 = 4\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        singleEq2 = newSingleEq(tableMetadata, 3, value4);\n        multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Test clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiINRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3), asList(value4, value5));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiIN);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value4, value5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3));\n        restrictions = restrictions(tableMetadata, multiIN, singleEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 5 AND (clustering_2, clustering_3) IN ((2, 3), (4, 5))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value5);\n        multiIN = newMultiIN(tableMetadata, 2, asList(value2, value3), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, singleEq, multiIN, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value5, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value5, value4, value5);\n    }\n\n    /**\n     * Test mixing single equal restrictions with multi-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3) AND (clustering_1) < (4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) => (2, 3) AND (clustering_1, clustering_2) <= (4, 5)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3);\n        multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value4, value5);\n    }\n\n    /**\n     * Test mixing multi equal restrictions with single-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqAndSingleSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 > 3\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction singleSlice = newSingleSlice(tableMetadata, 2, Bound.START, false, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, singleSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) > (3, 4)\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) IN ((3, 4), (4, 5))\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, multiEq, multiIN);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) = (3, 4)\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, multiEq, multiEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty start.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyStart(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isBottom());\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty end.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyEnd(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isTop());\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a start with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertStartBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, true, isInclusive, elements);\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a end with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertEndBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, false, isInclusive, elements);\n    }\n\n    private static void assertBound(ClusteringBound<?> bound, boolean isStart, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertEquals(\"the bound size is not the expected one:\", elements.length, bound.size());\n        assertEquals(\"the bound should be a \" + (isStart ? \"start\" : \"end\") + \" but is a \" + (bound.isStart() ? \"start\" : \"end\"), isStart, bound.isStart());\n        assertEquals(\"the bound inclusiveness is not the expected one\", isInclusive, bound.isInclusive());\n        for (int i = 0, m = elements.length; i < m; i++)\n        {\n            ByteBuffer element = elements[i];\n            assertEquals(String.format(\"the element %s of the bound is not the expected one: expected %s but was %s\",\n                                       i,\n                                       ByteBufferUtil.toInt(element),\n                                       ByteBufferUtil.toInt(bound.bufferAt(i))), bound.bufferAt(i), element);\n        }\n    }\n\n    private static TableMetadata newTableMetadata(Sort... sorts)\n    {\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (Sort sort : sorts)\n            types.add(sort == Sort.ASC ? Int32Type.instance : ReversedType.getInstance(Int32Type.instance));\n\n        TableMetadata.Builder builder =\n            TableMetadata.builder(\"keyspace\", \"test\")\n                         .addPartitionKeyColumn(\"partition_key\", Int32Type.instance);\n\n        for (int i = 0; i < sorts.length; i++)\n            builder.addClusteringColumn(\"clustering_\" + i, types.get(i));\n\n        return builder.build();\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param value the equality value\n     * @return a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleEq(TableMetadata tableMetadata, int index, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.EQRestriction(columnDef, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param values the equality value\n     * @return a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiEq(TableMetadata tableMetadata, int firstIndex, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columns = new ArrayList<>();\n        List<AbstractType<?>> types = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, firstIndex + i);\n            columns.add(column);\n            types.add(column.type);\n        }\n        TupleType tupleType = new TupleType(types);\n        return new MultiColumnRestriction.EQRestriction(columns, new MultiElements.Value(tupleType, Arrays.asList(values)));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the index of the first clustering column\n     * @param values the in values\n     * @return a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    @SafeVarargs\n    private static Restriction newMultiIN(TableMetadata tableMetadata, int firstIndex, List<ByteBuffer>... values)\n    {\n        List<ColumnMetadata> columns = new ArrayList<>();\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (int i = 0; i < values[0].size(); i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, firstIndex + i);\n            columns.add(column);\n            types.add(column.type);\n        }\n\n        TupleType tupleType = new TupleType(types);\n\n        List<Term> terms = new ArrayList<>(values.length);\n        for (int i = 0; i < values.length; i++)\n        {\n            terms.add(new MultiElements.Value(tupleType, values[i]));\n        }\n        return new MultiColumnRestriction.INRestriction(columns, Terms.of(terms));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param values the in values\n     * @return a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleIN(TableMetadata tableMetadata, int index, ByteBuffer... values)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.INRestriction(columnDef, toTerms(values));\n    }\n\n    /**\n     * Returns the clustering <code>ColumnMetadata<\/code> for the specified position.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @return the clustering <code>ColumnMetadata<\/code> for the specified position.\n     */\n    private static ColumnMetadata getClusteringColumnDefinition(TableMetadata tableMetadata, int index)\n    {\n        return tableMetadata.clusteringColumns().get(index);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param value the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleSlice(TableMetadata tableMetadata, int index, Bound bound, boolean inclusive, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.SliceRestriction(columnDef, bound, inclusive, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param values the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiSlice(TableMetadata tableMetadata, int firstIndex, Bound bound, boolean inclusive, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>(values.length);\n        List<AbstractType<?>> types = new ArrayList<>(values.length);\n        for (int i = 0; i < values.length; i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, i + firstIndex);\n            columnMetadatas.add(column);\n            types.add(column.type);\n        }\n        TupleType type = new TupleType(types);\n        return new MultiColumnRestriction.SliceRestriction(columnMetadatas, bound, inclusive, new MultiElements.Value(type, Arrays.asList(values)));\n    }\n\n    /**\n     * Converts the specified value into a term.\n     *\n     * @param value the value to convert.\n     * @return the term corresponding to the specified value.\n     */\n    private static Term toTerm(ByteBuffer value)\n    {\n        return new Constants.Value(value);\n    }\n\n    /**\n     * Converts the specified values into a <code>List<\/code> of terms.\n     *\n     * @param values the values to convert.\n     * @return a <code>List<\/code> of terms corresponding to the specified values.\n     */\n    private static Terms toTerms(ByteBuffer... values)\n    {\n        List<Term> terms = new ArrayList<>();\n        for (ByteBuffer value : values)\n            terms.add(toTerm(value));\n        return Terms.of(terms);\n    }\n\n    private static <T> T get(SortedSet<T> set, int i)\n    {\n        return Iterables.get(set, i);\n    }\n    \n    private static ClusteringColumnRestrictions restrictions(TableMetadata table, Restriction... restrictions)\n    {\n        ClusteringColumnRestrictions clusteringColumnRestrictions = new ClusteringColumnRestrictions(table, false);\n        for (Restriction restriction : restrictions)\n            clusteringColumnRestrictions = clusteringColumnRestrictions.mergeWith(restriction, null);\n        return clusteringColumnRestrictions;\n    }\n\n    private enum Sort\n    {\n        ASC,\n        DESC;\n    }\n}\n","lineNo":1696}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport com.google.common.collect.Iterables;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.Term.MultiItemTerminal;\nimport org.apache.cassandra.cql3.statements.Bound;\n\nimport org.apache.cassandra.db.*;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ReversedType;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static java.util.Arrays.asList;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class ClusteringColumnRestrictionsTest\n{\n    @BeforeClass\n    public static void setupDD()\n    {\n        DatabaseDescriptor.daemonInitialization();\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithNoRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n    }\n\n    /**\n     * Test 'clustering_0 = 1' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_1 = 1' with 2 clustering columns\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_0 IN (1, 2, 3)' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn()\n    {\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value2);\n        assertStartBound(get(bounds, 2), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n        assertEndBound(get(bounds, 1), true, value2);\n        assertEndBound(get(bounds, 2), true, value3);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one descending clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test 'clustering_0 = 1 AND clustering_1 IN (1, 2, 3)'\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        Restriction in = newSingleIN(tableMetadata, 1, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value1);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n        assertEndBound(get(bounds, 2), true, value1, value3);\n    }\n\n    /**\n     * Test equal and slice restrictions (e.g 'clustering_0 = 0 clustering_1 > 1')\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        Restriction eq = newSingleEq(tableMetadata, 0, value3);\n\n        Restriction slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, true, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, false, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 1, Bound.END, false, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value2);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 1, Bound.END, true, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) = (1, 2)' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        Restriction eq = newMultiEq(tableMetadata, 0, value1, value2);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) IN ((1, 2), (2, 3))' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), true, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), true, value2, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column in reverse\n     * order\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 descending clustering columns (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingAndOneAscendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0) > (1) AND (clustering_0, clustering1) < (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n        assertEndBound(get(bounds, 2), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneAscendingAndOneDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n        assertStartBound(get(bounds, 2), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n        assertEndBound(get(bounds, 2), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 ascending clustering column and 2 descending\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoAscendingAndTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // clustering_0 IN (1, 2) AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, in);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value2, value2);\n        assertStartBound(get(bounds, 3), false, value2, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n        assertEndBound(get(bounds, 2), false, value2, value2, value3, value4);\n        assertEndBound(get(bounds, 3), true, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value2, value3);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value4, value3, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value4, value3);\n        assertEndBound(get(bounds, 2), true, value4, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions with ascending, descending, ascending and descending columns\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithAscendingDescendingColumnMix()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC, Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), false, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value2);\n        assertEndBound(get(bounds, 4), true, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), true, value4, value3);\n        assertStartBound(get(bounds, 5), true, value4, value3, value2, value1);\n        assertStartBound(get(bounds, 6), false, value4, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value4);\n        assertEndBound(get(bounds, 4), false, value4, value3, value2);\n        assertEndBound(get(bounds, 5), true, value4, value3, value2);\n        assertEndBound(get(bounds, 6), true, value4);\n    }\n\n    /**\n     * Test mixing single and multi equals restrictions (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 2 AND (clustering_2, clustering_3) = (3, 4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value2);\n        multiEq = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, singleEq, singleEq2, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 = 3\n        singleEq = newSingleEq(tableMetadata, 2, value3);\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3) AND clustering_3 = 4\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        singleEq2 = newSingleEq(tableMetadata, 3, value4);\n        multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Test clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiINRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3), asList(value4, value5));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiIN);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value4, value5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3));\n        restrictions = restrictions(tableMetadata, multiIN, singleEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 5 AND (clustering_2, clustering_3) IN ((2, 3), (4, 5))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value5);\n        multiIN = newMultiIN(tableMetadata, 2, asList(value2, value3), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, singleEq, multiIN, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value5, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value5, value4, value5);\n    }\n\n    /**\n     * Test mixing single equal restrictions with multi-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3) AND (clustering_1) < (4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) => (2, 3) AND (clustering_1, clustering_2) <= (4, 5)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3);\n        multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value4, value5);\n    }\n\n    /**\n     * Test mixing multi equal restrictions with single-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqAndSingleSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 > 3\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction singleSlice = newSingleSlice(tableMetadata, 2, Bound.START, false, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, singleSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) > (3, 4)\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) IN ((3, 4), (4, 5))\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, multiEq, multiIN);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) = (3, 4)\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, multiEq, multiEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty start.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyStart(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isBottom());\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty end.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyEnd(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isTop());\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a start with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertStartBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, true, isInclusive, elements);\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a end with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertEndBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, false, isInclusive, elements);\n    }\n\n    private static void assertBound(ClusteringBound<?> bound, boolean isStart, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertEquals(\"the bound size is not the expected one:\", elements.length, bound.size());\n        assertEquals(\"the bound should be a \" + (isStart ? \"start\" : \"end\") + \" but is a \" + (bound.isStart() ? \"start\" : \"end\"), isStart, bound.isStart());\n        assertEquals(\"the bound inclusiveness is not the expected one\", isInclusive, bound.isInclusive());\n        for (int i = 0, m = elements.length; i < m; i++)\n        {\n            ByteBuffer element = elements[i];\n            assertEquals(String.format(\"the element %s of the bound is not the expected one: expected %s but was %s\",\n                                       i,\n                                       ByteBufferUtil.toInt(element),\n                                       ByteBufferUtil.toInt(bound.bufferAt(i))), bound.bufferAt(i), element);\n        }\n    }\n\n    private static TableMetadata newTableMetadata(Sort... sorts)\n    {\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (Sort sort : sorts)\n            types.add(sort == Sort.ASC ? Int32Type.instance : ReversedType.getInstance(Int32Type.instance));\n\n        TableMetadata.Builder builder =\n            TableMetadata.builder(\"keyspace\", \"test\")\n                         .addPartitionKeyColumn(\"partition_key\", Int32Type.instance);\n\n        for (int i = 0; i < sorts.length; i++)\n            builder.addClusteringColumn(\"clustering_\" + i, types.get(i));\n\n        return builder.build();\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param value the equality value\n     * @return a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleEq(TableMetadata tableMetadata, int index, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.EQRestriction(columnDef, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param values the equality value\n     * @return a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiEq(TableMetadata tableMetadata, int firstIndex, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, firstIndex + i));\n        }\n        return new MultiColumnRestriction.EQRestriction(columnMetadatas, toMultiItemTerminal(values));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the index of the first clustering column\n     * @param values the in values\n     * @return a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    @SafeVarargs\n    private static Restriction newMultiIN(TableMetadata tableMetadata, int firstIndex, List<ByteBuffer>... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        List<Term> terms = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, firstIndex + i));\n            terms.add(toMultiItemTerminal(values[i].toArray(new ByteBuffer[0])));\n        }\n        return new MultiColumnRestriction.InRestrictionWithValues(columnMetadatas, terms);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param values the in values\n     * @return a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleIN(TableMetadata tableMetadata, int index, ByteBuffer... values)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.InRestrictionWithValues(columnDef, toTerms(values));\n    }\n\n    /**\n     * Returns the clustering <code>ColumnMetadata<\/code> for the specified position.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @return the clustering <code>ColumnMetadata<\/code> for the specified position.\n     */\n    private static ColumnMetadata getClusteringColumnDefinition(TableMetadata tableMetadata, int index)\n    {\n        return tableMetadata.clusteringColumns().get(index);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param value the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleSlice(TableMetadata tableMetadata, int index, Bound bound, boolean inclusive, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.SliceRestriction(columnDef, bound, inclusive, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param values the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiSlice(TableMetadata tableMetadata, int firstIndex, Bound bound, boolean inclusive, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, i + firstIndex));\n        }\n        return new MultiColumnRestriction.SliceRestriction(columnMetadatas, bound, inclusive, toMultiItemTerminal(values));\n    }\n\n    /**\n     * Converts the specified values into a <code>MultiItemTerminal<\/code>.\n     *\n     * @param values the values to convert.\n     * @return the term corresponding to the specified values.\n     */\n    private static MultiItemTerminal toMultiItemTerminal(ByteBuffer... values)\n    {\n        return new Tuples.Value(values);\n    }\n\n    /**\n     * Converts the specified value into a term.\n     *\n     * @param value the value to convert.\n     * @return the term corresponding to the specified value.\n     */\n    private static Term toTerm(ByteBuffer value)\n    {\n        return new Constants.Value(value);\n    }\n\n    /**\n     * Converts the specified values into a <code>List<\/code> of terms.\n     *\n     * @param values the values to convert.\n     * @return a <code>List<\/code> of terms corresponding to the specified values.\n     */\n    private static List<Term> toTerms(ByteBuffer... values)\n    {\n        List<Term> terms = new ArrayList<>();\n        for (ByteBuffer value : values)\n            terms.add(toTerm(value));\n        return terms;\n    }\n\n    private static <T> T get(SortedSet<T> set, int i)\n    {\n        return Iterables.get(set, i);\n    }\n    \n    private static ClusteringColumnRestrictions restrictions(TableMetadata table, Restriction... restrictions)\n    {\n        ClusteringColumnRestrictions clusteringColumnRestrictions = new ClusteringColumnRestrictions(table, false);\n        for (Restriction restriction : restrictions)\n            clusteringColumnRestrictions = clusteringColumnRestrictions.mergeWith(restriction, null);\n        return clusteringColumnRestrictions;\n    }\n\n    private enum Sort\n    {\n        ASC,\n        DESC;\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport com.google.common.collect.Iterables;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.db.marshal.TupleType;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.statements.Bound;\n\nimport org.apache.cassandra.db.*;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ReversedType;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static java.util.Arrays.asList;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class ClusteringColumnRestrictionsTest\n{\n    @BeforeClass\n    public static void setupDD()\n    {\n        DatabaseDescriptor.daemonInitialization();\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithNoRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n    }\n\n    /**\n     * Test 'clustering_0 = 1' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_1 = 1' with 2 clustering columns\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_0 IN (1, 2, 3)' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn()\n    {\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value2);\n        assertStartBound(get(bounds, 2), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n        assertEndBound(get(bounds, 1), true, value2);\n        assertEndBound(get(bounds, 2), true, value3);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one descending clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test 'clustering_0 = 1 AND clustering_1 IN (1, 2, 3)'\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        Restriction in = newSingleIN(tableMetadata, 1, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value1);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n        assertEndBound(get(bounds, 2), true, value1, value3);\n    }\n\n    /**\n     * Test equal and slice restrictions (e.g 'clustering_0 = 0 clustering_1 > 1')\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        Restriction eq = newSingleEq(tableMetadata, 0, value3);\n\n        Restriction slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, true, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, false, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 1, Bound.END, false, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value2);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 1, Bound.END, true, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) = (1, 2)' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        Restriction eq = newMultiEq(tableMetadata, 0, value1, value2);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) IN ((1, 2), (2, 3))' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), true, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), true, value2, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column in reverse\n     * order\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 descending clustering columns (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingAndOneAscendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0) > (1) AND (clustering_0, clustering1) < (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n        assertEndBound(get(bounds, 2), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneAscendingAndOneDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n        assertStartBound(get(bounds, 2), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n        assertEndBound(get(bounds, 2), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 ascending clustering column and 2 descending\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoAscendingAndTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // clustering_0 IN (1, 2) AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, in);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value2, value2);\n        assertStartBound(get(bounds, 3), false, value2, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n        assertEndBound(get(bounds, 2), false, value2, value2, value3, value4);\n        assertEndBound(get(bounds, 3), true, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value2, value3);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value4, value3, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value4, value3);\n        assertEndBound(get(bounds, 2), true, value4, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions with ascending, descending, ascending and descending columns\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithAscendingDescendingColumnMix()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC, Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), false, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value2);\n        assertEndBound(get(bounds, 4), true, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), true, value4, value3);\n        assertStartBound(get(bounds, 5), true, value4, value3, value2, value1);\n        assertStartBound(get(bounds, 6), false, value4, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value4);\n        assertEndBound(get(bounds, 4), false, value4, value3, value2);\n        assertEndBound(get(bounds, 5), true, value4, value3, value2);\n        assertEndBound(get(bounds, 6), true, value4);\n    }\n\n    /**\n     * Test mixing single and multi equals restrictions (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 2 AND (clustering_2, clustering_3) = (3, 4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value2);\n        multiEq = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, singleEq, singleEq2, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 = 3\n        singleEq = newSingleEq(tableMetadata, 2, value3);\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3) AND clustering_3 = 4\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        singleEq2 = newSingleEq(tableMetadata, 3, value4);\n        multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Test clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiINRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3), asList(value4, value5));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiIN);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value4, value5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3));\n        restrictions = restrictions(tableMetadata, multiIN, singleEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 5 AND (clustering_2, clustering_3) IN ((2, 3), (4, 5))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value5);\n        multiIN = newMultiIN(tableMetadata, 2, asList(value2, value3), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, singleEq, multiIN, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value5, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value5, value4, value5);\n    }\n\n    /**\n     * Test mixing single equal restrictions with multi-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3) AND (clustering_1) < (4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) => (2, 3) AND (clustering_1, clustering_2) <= (4, 5)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3);\n        multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value4, value5);\n    }\n\n    /**\n     * Test mixing multi equal restrictions with single-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqAndSingleSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 > 3\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction singleSlice = newSingleSlice(tableMetadata, 2, Bound.START, false, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, singleSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) > (3, 4)\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) IN ((3, 4), (4, 5))\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, multiEq, multiIN);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) = (3, 4)\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, multiEq, multiEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty start.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyStart(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isBottom());\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty end.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyEnd(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isTop());\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a start with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertStartBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, true, isInclusive, elements);\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a end with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertEndBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, false, isInclusive, elements);\n    }\n\n    private static void assertBound(ClusteringBound<?> bound, boolean isStart, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertEquals(\"the bound size is not the expected one:\", elements.length, bound.size());\n        assertEquals(\"the bound should be a \" + (isStart ? \"start\" : \"end\") + \" but is a \" + (bound.isStart() ? \"start\" : \"end\"), isStart, bound.isStart());\n        assertEquals(\"the bound inclusiveness is not the expected one\", isInclusive, bound.isInclusive());\n        for (int i = 0, m = elements.length; i < m; i++)\n        {\n            ByteBuffer element = elements[i];\n            assertEquals(String.format(\"the element %s of the bound is not the expected one: expected %s but was %s\",\n                                       i,\n                                       ByteBufferUtil.toInt(element),\n                                       ByteBufferUtil.toInt(bound.bufferAt(i))), bound.bufferAt(i), element);\n        }\n    }\n\n    private static TableMetadata newTableMetadata(Sort... sorts)\n    {\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (Sort sort : sorts)\n            types.add(sort == Sort.ASC ? Int32Type.instance : ReversedType.getInstance(Int32Type.instance));\n\n        TableMetadata.Builder builder =\n            TableMetadata.builder(\"keyspace\", \"test\")\n                         .addPartitionKeyColumn(\"partition_key\", Int32Type.instance);\n\n        for (int i = 0; i < sorts.length; i++)\n            builder.addClusteringColumn(\"clustering_\" + i, types.get(i));\n\n        return builder.build();\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param value the equality value\n     * @return a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleEq(TableMetadata tableMetadata, int index, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.EQRestriction(columnDef, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param values the equality value\n     * @return a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiEq(TableMetadata tableMetadata, int firstIndex, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columns = new ArrayList<>();\n        List<AbstractType<?>> types = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, firstIndex + i);\n            columns.add(column);\n            types.add(column.type);\n        }\n        TupleType tupleType = new TupleType(types);\n        return new MultiColumnRestriction.EQRestriction(columns, new MultiElements.Value(tupleType, Arrays.asList(values)));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the index of the first clustering column\n     * @param values the in values\n     * @return a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    @SafeVarargs\n    private static Restriction newMultiIN(TableMetadata tableMetadata, int firstIndex, List<ByteBuffer>... values)\n    {\n        List<ColumnMetadata> columns = new ArrayList<>();\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (int i = 0; i < values[0].size(); i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, firstIndex + i);\n            columns.add(column);\n            types.add(column.type);\n        }\n\n        TupleType tupleType = new TupleType(types);\n\n        List<Term> terms = new ArrayList<>(values.length);\n        for (int i = 0; i < values.length; i++)\n        {\n            terms.add(new MultiElements.Value(tupleType, values[i]));\n        }\n        return new MultiColumnRestriction.INRestriction(columns, Terms.of(terms));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param values the in values\n     * @return a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleIN(TableMetadata tableMetadata, int index, ByteBuffer... values)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.INRestriction(columnDef, toTerms(values));\n    }\n\n    /**\n     * Returns the clustering <code>ColumnMetadata<\/code> for the specified position.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @return the clustering <code>ColumnMetadata<\/code> for the specified position.\n     */\n    private static ColumnMetadata getClusteringColumnDefinition(TableMetadata tableMetadata, int index)\n    {\n        return tableMetadata.clusteringColumns().get(index);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param value the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleSlice(TableMetadata tableMetadata, int index, Bound bound, boolean inclusive, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.SliceRestriction(columnDef, bound, inclusive, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param values the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiSlice(TableMetadata tableMetadata, int firstIndex, Bound bound, boolean inclusive, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>(values.length);\n        List<AbstractType<?>> types = new ArrayList<>(values.length);\n        for (int i = 0; i < values.length; i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, i + firstIndex);\n            columnMetadatas.add(column);\n            types.add(column.type);\n        }\n        TupleType type = new TupleType(types);\n        return new MultiColumnRestriction.SliceRestriction(columnMetadatas, bound, inclusive, new MultiElements.Value(type, Arrays.asList(values)));\n    }\n\n    /**\n     * Converts the specified value into a term.\n     *\n     * @param value the value to convert.\n     * @return the term corresponding to the specified value.\n     */\n    private static Term toTerm(ByteBuffer value)\n    {\n        return new Constants.Value(value);\n    }\n\n    /**\n     * Converts the specified values into a <code>List<\/code> of terms.\n     *\n     * @param values the values to convert.\n     * @return a <code>List<\/code> of terms corresponding to the specified values.\n     */\n    private static Terms toTerms(ByteBuffer... values)\n    {\n        List<Term> terms = new ArrayList<>();\n        for (ByteBuffer value : values)\n            terms.add(toTerm(value));\n        return Terms.of(terms);\n    }\n\n    private static <T> T get(SortedSet<T> set, int i)\n    {\n        return Iterables.get(set, i);\n    }\n    \n    private static ClusteringColumnRestrictions restrictions(TableMetadata table, Restriction... restrictions)\n    {\n        ClusteringColumnRestrictions clusteringColumnRestrictions = new ClusteringColumnRestrictions(table, false);\n        for (Restriction restriction : restrictions)\n            clusteringColumnRestrictions = clusteringColumnRestrictions.mergeWith(restriction, null);\n        return clusteringColumnRestrictions;\n    }\n\n    private enum Sort\n    {\n        ASC,\n        DESC;\n    }\n}\n","lineNo":1720}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport com.google.common.collect.Iterables;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.Term.MultiItemTerminal;\nimport org.apache.cassandra.cql3.statements.Bound;\n\nimport org.apache.cassandra.db.*;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ReversedType;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static java.util.Arrays.asList;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class ClusteringColumnRestrictionsTest\n{\n    @BeforeClass\n    public static void setupDD()\n    {\n        DatabaseDescriptor.daemonInitialization();\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithNoRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n    }\n\n    /**\n     * Test 'clustering_0 = 1' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_1 = 1' with 2 clustering columns\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_0 IN (1, 2, 3)' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn()\n    {\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value2);\n        assertStartBound(get(bounds, 2), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n        assertEndBound(get(bounds, 1), true, value2);\n        assertEndBound(get(bounds, 2), true, value3);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one descending clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test 'clustering_0 = 1 AND clustering_1 IN (1, 2, 3)'\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        Restriction in = newSingleIN(tableMetadata, 1, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value1);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n        assertEndBound(get(bounds, 2), true, value1, value3);\n    }\n\n    /**\n     * Test equal and slice restrictions (e.g 'clustering_0 = 0 clustering_1 > 1')\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        Restriction eq = newSingleEq(tableMetadata, 0, value3);\n\n        Restriction slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, true, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, false, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 1, Bound.END, false, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value2);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 1, Bound.END, true, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) = (1, 2)' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        Restriction eq = newMultiEq(tableMetadata, 0, value1, value2);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) IN ((1, 2), (2, 3))' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), true, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), true, value2, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column in reverse\n     * order\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 descending clustering columns (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingAndOneAscendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0) > (1) AND (clustering_0, clustering1) < (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n        assertEndBound(get(bounds, 2), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneAscendingAndOneDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n        assertStartBound(get(bounds, 2), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n        assertEndBound(get(bounds, 2), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 ascending clustering column and 2 descending\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoAscendingAndTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // clustering_0 IN (1, 2) AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, in);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value2, value2);\n        assertStartBound(get(bounds, 3), false, value2, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n        assertEndBound(get(bounds, 2), false, value2, value2, value3, value4);\n        assertEndBound(get(bounds, 3), true, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value2, value3);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value4, value3, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value4, value3);\n        assertEndBound(get(bounds, 2), true, value4, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions with ascending, descending, ascending and descending columns\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithAscendingDescendingColumnMix()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC, Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), false, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value2);\n        assertEndBound(get(bounds, 4), true, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), true, value4, value3);\n        assertStartBound(get(bounds, 5), true, value4, value3, value2, value1);\n        assertStartBound(get(bounds, 6), false, value4, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value4);\n        assertEndBound(get(bounds, 4), false, value4, value3, value2);\n        assertEndBound(get(bounds, 5), true, value4, value3, value2);\n        assertEndBound(get(bounds, 6), true, value4);\n    }\n\n    /**\n     * Test mixing single and multi equals restrictions (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 2 AND (clustering_2, clustering_3) = (3, 4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value2);\n        multiEq = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, singleEq, singleEq2, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 = 3\n        singleEq = newSingleEq(tableMetadata, 2, value3);\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3) AND clustering_3 = 4\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        singleEq2 = newSingleEq(tableMetadata, 3, value4);\n        multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Test clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiINRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3), asList(value4, value5));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiIN);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value4, value5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3));\n        restrictions = restrictions(tableMetadata, multiIN, singleEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 5 AND (clustering_2, clustering_3) IN ((2, 3), (4, 5))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value5);\n        multiIN = newMultiIN(tableMetadata, 2, asList(value2, value3), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, singleEq, multiIN, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value5, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value5, value4, value5);\n    }\n\n    /**\n     * Test mixing single equal restrictions with multi-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3) AND (clustering_1) < (4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) => (2, 3) AND (clustering_1, clustering_2) <= (4, 5)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3);\n        multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value4, value5);\n    }\n\n    /**\n     * Test mixing multi equal restrictions with single-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqAndSingleSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 > 3\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction singleSlice = newSingleSlice(tableMetadata, 2, Bound.START, false, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, singleSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) > (3, 4)\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) IN ((3, 4), (4, 5))\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, multiEq, multiIN);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) = (3, 4)\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, multiEq, multiEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty start.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyStart(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isBottom());\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty end.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyEnd(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isTop());\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a start with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertStartBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, true, isInclusive, elements);\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a end with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertEndBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, false, isInclusive, elements);\n    }\n\n    private static void assertBound(ClusteringBound<?> bound, boolean isStart, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertEquals(\"the bound size is not the expected one:\", elements.length, bound.size());\n        assertEquals(\"the bound should be a \" + (isStart ? \"start\" : \"end\") + \" but is a \" + (bound.isStart() ? \"start\" : \"end\"), isStart, bound.isStart());\n        assertEquals(\"the bound inclusiveness is not the expected one\", isInclusive, bound.isInclusive());\n        for (int i = 0, m = elements.length; i < m; i++)\n        {\n            ByteBuffer element = elements[i];\n            assertEquals(String.format(\"the element %s of the bound is not the expected one: expected %s but was %s\",\n                                       i,\n                                       ByteBufferUtil.toInt(element),\n                                       ByteBufferUtil.toInt(bound.bufferAt(i))), bound.bufferAt(i), element);\n        }\n    }\n\n    private static TableMetadata newTableMetadata(Sort... sorts)\n    {\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (Sort sort : sorts)\n            types.add(sort == Sort.ASC ? Int32Type.instance : ReversedType.getInstance(Int32Type.instance));\n\n        TableMetadata.Builder builder =\n            TableMetadata.builder(\"keyspace\", \"test\")\n                         .addPartitionKeyColumn(\"partition_key\", Int32Type.instance);\n\n        for (int i = 0; i < sorts.length; i++)\n            builder.addClusteringColumn(\"clustering_\" + i, types.get(i));\n\n        return builder.build();\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param value the equality value\n     * @return a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleEq(TableMetadata tableMetadata, int index, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.EQRestriction(columnDef, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param values the equality value\n     * @return a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiEq(TableMetadata tableMetadata, int firstIndex, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, firstIndex + i));\n        }\n        return new MultiColumnRestriction.EQRestriction(columnMetadatas, toMultiItemTerminal(values));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the index of the first clustering column\n     * @param values the in values\n     * @return a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    @SafeVarargs\n    private static Restriction newMultiIN(TableMetadata tableMetadata, int firstIndex, List<ByteBuffer>... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        List<Term> terms = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, firstIndex + i));\n            terms.add(toMultiItemTerminal(values[i].toArray(new ByteBuffer[0])));\n        }\n        return new MultiColumnRestriction.InRestrictionWithValues(columnMetadatas, terms);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param values the in values\n     * @return a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleIN(TableMetadata tableMetadata, int index, ByteBuffer... values)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.InRestrictionWithValues(columnDef, toTerms(values));\n    }\n\n    /**\n     * Returns the clustering <code>ColumnMetadata<\/code> for the specified position.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @return the clustering <code>ColumnMetadata<\/code> for the specified position.\n     */\n    private static ColumnMetadata getClusteringColumnDefinition(TableMetadata tableMetadata, int index)\n    {\n        return tableMetadata.clusteringColumns().get(index);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param value the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleSlice(TableMetadata tableMetadata, int index, Bound bound, boolean inclusive, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.SliceRestriction(columnDef, bound, inclusive, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param values the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiSlice(TableMetadata tableMetadata, int firstIndex, Bound bound, boolean inclusive, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            columnMetadatas.add(getClusteringColumnDefinition(tableMetadata, i + firstIndex));\n        }\n        return new MultiColumnRestriction.SliceRestriction(columnMetadatas, bound, inclusive, toMultiItemTerminal(values));\n    }\n\n    /**\n     * Converts the specified values into a <code>MultiItemTerminal<\/code>.\n     *\n     * @param values the values to convert.\n     * @return the term corresponding to the specified values.\n     */\n    private static MultiItemTerminal toMultiItemTerminal(ByteBuffer... values)\n    {\n        return new Tuples.Value(values);\n    }\n\n    /**\n     * Converts the specified value into a term.\n     *\n     * @param value the value to convert.\n     * @return the term corresponding to the specified value.\n     */\n    private static Term toTerm(ByteBuffer value)\n    {\n        return new Constants.Value(value);\n    }\n\n    /**\n     * Converts the specified values into a <code>List<\/code> of terms.\n     *\n     * @param values the values to convert.\n     * @return a <code>List<\/code> of terms corresponding to the specified values.\n     */\n    private static List<Term> toTerms(ByteBuffer... values)\n    {\n        List<Term> terms = new ArrayList<>();\n        for (ByteBuffer value : values)\n            terms.add(toTerm(value));\n        return terms;\n    }\n\n    private static <T> T get(SortedSet<T> set, int i)\n    {\n        return Iterables.get(set, i);\n    }\n    \n    private static ClusteringColumnRestrictions restrictions(TableMetadata table, Restriction... restrictions)\n    {\n        ClusteringColumnRestrictions clusteringColumnRestrictions = new ClusteringColumnRestrictions(table, false);\n        for (Restriction restriction : restrictions)\n            clusteringColumnRestrictions = clusteringColumnRestrictions.mergeWith(restriction, null);\n        return clusteringColumnRestrictions;\n    }\n\n    private enum Sort\n    {\n        ASC,\n        DESC;\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.cassandra.cql3.restrictions;\n\nimport java.nio.ByteBuffer;\nimport java.util.*;\n\nimport com.google.common.collect.Iterables;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.cassandra.cql3.terms.Constants;\nimport org.apache.cassandra.cql3.terms.MultiElements;\nimport org.apache.cassandra.cql3.terms.Term;\nimport org.apache.cassandra.cql3.terms.Terms;\nimport org.apache.cassandra.db.marshal.TupleType;\nimport org.apache.cassandra.schema.ColumnMetadata;\nimport org.apache.cassandra.schema.TableMetadata;\nimport org.apache.cassandra.config.DatabaseDescriptor;\nimport org.apache.cassandra.cql3.*;\nimport org.apache.cassandra.cql3.statements.Bound;\n\nimport org.apache.cassandra.db.*;\nimport org.apache.cassandra.db.marshal.AbstractType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.ReversedType;\nimport org.apache.cassandra.utils.ByteBufferUtil;\n\nimport static java.util.Arrays.asList;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\npublic class ClusteringColumnRestrictionsTest\n{\n    @BeforeClass\n    public static void setupDD()\n    {\n        DatabaseDescriptor.daemonInitialization();\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithNoRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n    }\n\n    /**\n     * Test 'clustering_0 = 1' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_1 = 1' with 2 clustering columns\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneEqRestrictionsAndTwoClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer clustering_0 = ByteBufferUtil.bytes(1);\n        Restriction eq = newSingleEq(tableMetadata, 0, clustering_0);\n\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, clustering_0);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, clustering_0);\n    }\n\n    /**\n     * Test 'clustering_0 IN (1, 2, 3)' with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithOneInRestrictionsAndOneClusteringColumn()\n    {\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value2);\n        assertStartBound(get(bounds, 2), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n        assertEndBound(get(bounds, 1), true, value2);\n        assertEndBound(get(bounds, 2), true, value3);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test slice restriction (e.g 'clustering_0 > 1') with only one descending clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithSliceRestrictionsAndOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newSingleSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test 'clustering_0 = 1 AND clustering_1 IN (1, 2, 3)'\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        Restriction in = newSingleIN(tableMetadata, 1, value1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value1);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n        assertEndBound(get(bounds, 2), true, value1, value3);\n    }\n\n    /**\n     * Test equal and slice restrictions (e.g 'clustering_0 = 0 clustering_1 > 1')\n     */\n    @Test\n    public void testBoundsAsClusteringWithEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        Restriction eq = newSingleEq(tableMetadata, 0, value3);\n\n        Restriction slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, true, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.END, false, value1);\n        restrictions =  restrictions(tableMetadata, eq, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value1);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, false, value1);\n        Restriction slice2 = newSingleSlice(tableMetadata, 1, Bound.END, false, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value3, value2);\n\n        slice = newSingleSlice(tableMetadata, 1, Bound.START, true, value1);\n        slice2 = newSingleSlice(tableMetadata, 1, Bound.END, true, value2);\n        restrictions =  restrictions(tableMetadata, eq, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value3, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value3, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) = (1, 2)' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        Restriction eq = newMultiEq(tableMetadata, 0, value1, value2);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, eq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test '(clustering_0, clustering_1) IN ((1, 2), (2, 3))' with two clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiInRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        Restriction in = newMultiIN(tableMetadata, 0, asList(value1, value2), asList(value2, value3));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, in);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), true, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), true, value2, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC);\n\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0) > (1)') with only one clustering column in reverse\n     * order\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoClusteringColumn()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 descending clustering columns (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneDescendingAndOneAscendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.DESC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), false, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0) > (1) AND (clustering_0, clustering1) < (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value2);\n        assertStartBound(get(bounds, 1), false, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value2, value1);\n        assertEndBound(get(bounds, 1), false, value1);\n        assertEndBound(get(bounds, 2), true, value1);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 1 descending clustering column and 1 ascending\n     * (e.g '(clustering_0, clustering_1) > (1, 2)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithOneAscendingAndOneDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n\n        // (clustering_0, clustering1) > (1, 2)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1) <= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) < (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // (clustering_0, clustering1) > (1, 2) AND (clustering_0) < (2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n\n        // (clustering_0, clustering1) >= (1, 2) AND (clustering_0, clustering1) <= (2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n        assertStartBound(get(bounds, 2), true, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEndBound(get(bounds, 1), false, value2);\n        assertEndBound(get(bounds, 2), true, value2);\n    }\n\n    /**\n     * Test multi-column slice restrictions with 2 ascending clustering column and 2 descending\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithTwoAscendingAndTwoDescendingClusteringColumns()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.DESC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions =  restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n\n        // clustering_0 IN (1, 2) AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction in = newSingleIN(tableMetadata, 0, value1, value2);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, in);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value2, value2);\n        assertStartBound(get(bounds, 3), false, value2, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1);\n        assertEndBound(get(bounds, 2), false, value2, value2, value3, value4);\n        assertEndBound(get(bounds, 3), true, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEmptyEnd(get(bounds, 0));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value2, value3);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2);\n        assertStartBound(get(bounds, 1), false, value1, value2);\n        assertStartBound(get(bounds, 2), true, value4, value3, value2, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), false, value4, value3);\n        assertEndBound(get(bounds, 2), true, value4, value3);\n    }\n\n    /**\n     * Test multi-column slice restrictions with ascending, descending, ascending and descending columns\n     * (e.g '(clustering_0, clustering1, clustering_3, clustering4) > (1, 2, 3, 4)')\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiSliceRestrictionsWithAscendingDescendingColumnMix()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.DESC, Sort.ASC, Sort.DESC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4)\n        Restriction slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, slice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2, clustering_3) > (2, 3, 4)\n        Restriction eq = newSingleEq(tableMetadata, 0, value1);\n        slice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice, eq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(3, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n\n        // (clustering_0, clustering1) >= (1, 2)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n        assertEmptyEnd(get(bounds, 1));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEmptyEnd(get(bounds, 3));\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) <= (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, true, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) < (1, 2, 3, 4)\n        slice = newMultiSlice(tableMetadata, 0, Bound.END, false, value1, value2, value3, value4);\n        restrictions = restrictions(tableMetadata, slice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEmptyStart(get(bounds, 0));\n        assertStartBound(get(bounds, 1), true, value1, value2);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 3), false, value1, value2);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(4, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3);\n        assertEndBound(get(bounds, 2), true, value1, value2, value3);\n        assertEndBound(get(bounds, 3), true, value1);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) > (1, 2, 3, 4) AND (clustering_0, clustering_1) < (2, 3)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, false, value1, value2, value3, value4);\n        Restriction slice2 = newMultiSlice(tableMetadata, 0, Bound.END, false, value2, value3);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), false, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(5, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), false, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value2);\n        assertEndBound(get(bounds, 4), true, value2);\n\n        // (clustering_0, clustering1, clustering_2, clustering_3) >= (1, 2, 3, 4) AND (clustering_0, clustering1, clustering_2, clustering_3) <= (4, 3, 2, 1)\n        slice = newMultiSlice(tableMetadata, 0, Bound.START, true, value1, value2, value3, value4);\n        slice2 = newMultiSlice(tableMetadata, 0, Bound.END, true, value4, value3, value2, value1);\n        restrictions = restrictions(tableMetadata, slice, slice2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1);\n        assertStartBound(get(bounds, 1), true, value1, value2, value3);\n        assertStartBound(get(bounds, 2), false, value1, value2, value3);\n        assertStartBound(get(bounds, 3), false, value1);\n        assertStartBound(get(bounds, 4), true, value4, value3);\n        assertStartBound(get(bounds, 5), true, value4, value3, value2, value1);\n        assertStartBound(get(bounds, 6), false, value4, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(7, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value2);\n        assertEndBound(get(bounds, 1), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 2), true, value1, value2);\n        assertEndBound(get(bounds, 3), false, value4);\n        assertEndBound(get(bounds, 4), false, value4, value3, value2);\n        assertEndBound(get(bounds, 5), true, value4, value3, value2);\n        assertEndBound(get(bounds, 6), true, value4);\n    }\n\n    /**\n     * Test mixing single and multi equals restrictions (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiEqRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 2 AND (clustering_2, clustering_3) = (3, 4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value2);\n        multiEq = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, singleEq, singleEq2, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 = 3\n        singleEq = newSingleEq(tableMetadata, 2, value3);\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) = (2, 3) AND clustering_3 = 4\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        singleEq2 = newSingleEq(tableMetadata, 3, value4);\n        multiEq = newMultiEq(tableMetadata, 1, value2, value3);\n        restrictions = restrictions(tableMetadata, singleEq, multiEq, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Test clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndMultiINRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3), (4, 5))\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3), asList(value4, value5));\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiIN);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value4, value5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) IN ((2, 3))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiIN = newMultiIN(tableMetadata, 1, asList(value2, value3));\n        restrictions = restrictions(tableMetadata, multiIN, singleEq);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3);\n\n        // clustering_0 = 1 AND clustering_1 = 5 AND (clustering_2, clustering_3) IN ((2, 3), (4, 5))\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction singleEq2 = newSingleEq(tableMetadata, 1, value5);\n        multiIN = newMultiIN(tableMetadata, 2, asList(value2, value3), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, singleEq, multiIN, singleEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertStartBound(get(bounds, 1), true, value1, value5, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value5, value2, value3);\n        assertEndBound(get(bounds, 1), true, value1, value5, value4, value5);\n    }\n\n    /**\n     * Test mixing single equal restrictions with multi-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithSingleEqAndSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3)\n        Restriction singleEq = newSingleEq(tableMetadata, 0, value1);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, singleEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3) AND (clustering_1) < (4)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, false, value2, value3);\n        Restriction multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, false, value4);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), false, value1, value4);\n\n        // clustering_0 = 1 AND (clustering_1, clustering_2) => (2, 3) AND (clustering_1, clustering_2) <= (4, 5)\n        singleEq = newSingleEq(tableMetadata, 0, value1);\n        multiSlice = newMultiSlice(tableMetadata, 1, Bound.START, true, value2, value3);\n        multiSlice2 = newMultiSlice(tableMetadata, 1, Bound.END, true, value4, value5);\n        restrictions = restrictions(tableMetadata, multiSlice2, singleEq, multiSlice);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value4, value5);\n    }\n\n    /**\n     * Test mixing multi equal restrictions with single-column slice restrictions\n     * (e.g. clustering_0 = 1 AND (clustering_1, clustering_2) > (2, 3))\n     */\n    @Test\n    public void testBoundsAsClusteringWithMultiEqAndSingleSliceRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n\n        // (clustering_0, clustering_1) = (1, 2) AND clustering_2 > 3\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction singleSlice = newSingleSlice(tableMetadata, 2, Bound.START, false, value3);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, singleSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n    }\n\n    @Test\n    public void testBoundsAsClusteringWithSeveralMultiColumnRestrictions()\n    {\n        TableMetadata tableMetadata = newTableMetadata(Sort.ASC, Sort.ASC, Sort.ASC, Sort.ASC);\n\n        ByteBuffer value1 = ByteBufferUtil.bytes(1);\n        ByteBuffer value2 = ByteBufferUtil.bytes(2);\n        ByteBuffer value3 = ByteBufferUtil.bytes(3);\n        ByteBuffer value4 = ByteBufferUtil.bytes(4);\n        ByteBuffer value5 = ByteBufferUtil.bytes(5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) > (3, 4)\n        Restriction multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiSlice = newMultiSlice(tableMetadata, 2, Bound.START, false, value3, value4);\n        ClusteringColumnRestrictions restrictions = restrictions(tableMetadata, multiEq, multiSlice);\n\n        SortedSet<ClusteringBound<?>> bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), false, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) IN ((3, 4), (4, 5))\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiIN = newMultiIN(tableMetadata, 2, asList(value3, value4), asList(value4, value5));\n        restrictions = restrictions(tableMetadata, multiEq, multiIN);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertStartBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(2, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n        assertEndBound(get(bounds, 1), true, value1, value2, value4, value5);\n\n        // (clustering_0, clustering_1) = (1, 2) AND (clustering_2, clustering_3) = (3, 4)\n        multiEq = newMultiEq(tableMetadata, 0, value1, value2);\n        Restriction multiEq2 = newMultiEq(tableMetadata, 2, value3, value4);\n        restrictions = restrictions(tableMetadata, multiEq, multiEq2);\n\n        bounds = restrictions.boundsAsClustering(Bound.START, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertStartBound(get(bounds, 0), true, value1, value2, value3, value4);\n\n        bounds = restrictions.boundsAsClustering(Bound.END, QueryOptions.DEFAULT);\n        assertEquals(1, bounds.size());\n        assertEndBound(get(bounds, 0), true, value1, value2, value3, value4);\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty start.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyStart(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isBottom());\n    }\n\n    /**\n     * Asserts that the specified <code>Bound<\/code> is an empty end.\n     *\n     * @param bound the bound to check\n     */\n    private static void assertEmptyEnd(ClusteringBound<?> bound)\n    {\n        assertTrue(bound.isTop());\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a start with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertStartBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, true, isInclusive, elements);\n    }\n\n    /**\n     * Asserts that the specified <code>ClusteringBound<\/code> is a end with the specified elements.\n     *\n     * @param bound the bound to check\n     * @param isInclusive if the bound is expected to be inclusive\n     * @param elements the expected elements of the clustering\n     */\n    private static void assertEndBound(ClusteringBound<?> bound, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertBound(bound, false, isInclusive, elements);\n    }\n\n    private static void assertBound(ClusteringBound<?> bound, boolean isStart, boolean isInclusive, ByteBuffer... elements)\n    {\n        assertEquals(\"the bound size is not the expected one:\", elements.length, bound.size());\n        assertEquals(\"the bound should be a \" + (isStart ? \"start\" : \"end\") + \" but is a \" + (bound.isStart() ? \"start\" : \"end\"), isStart, bound.isStart());\n        assertEquals(\"the bound inclusiveness is not the expected one\", isInclusive, bound.isInclusive());\n        for (int i = 0, m = elements.length; i < m; i++)\n        {\n            ByteBuffer element = elements[i];\n            assertEquals(String.format(\"the element %s of the bound is not the expected one: expected %s but was %s\",\n                                       i,\n                                       ByteBufferUtil.toInt(element),\n                                       ByteBufferUtil.toInt(bound.bufferAt(i))), bound.bufferAt(i), element);\n        }\n    }\n\n    private static TableMetadata newTableMetadata(Sort... sorts)\n    {\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (Sort sort : sorts)\n            types.add(sort == Sort.ASC ? Int32Type.instance : ReversedType.getInstance(Int32Type.instance));\n\n        TableMetadata.Builder builder =\n            TableMetadata.builder(\"keyspace\", \"test\")\n                         .addPartitionKeyColumn(\"partition_key\", Int32Type.instance);\n\n        for (int i = 0; i < sorts.length; i++)\n            builder.addClusteringColumn(\"clustering_\" + i, types.get(i));\n\n        return builder.build();\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param value the equality value\n     * @return a new <code>SingleColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleEq(TableMetadata tableMetadata, int index, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.EQRestriction(columnDef, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param values the equality value\n     * @return a new <code>MultiColumnRestriction.EQ<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiEq(TableMetadata tableMetadata, int firstIndex, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columns = new ArrayList<>();\n        List<AbstractType<?>> types = new ArrayList<>();\n        for (int i = 0; i < values.length; i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, firstIndex + i);\n            columns.add(column);\n            types.add(column.type);\n        }\n        TupleType tupleType = new TupleType(types);\n        return new MultiColumnRestriction.EQRestriction(columns, new MultiElements.Value(tupleType, Arrays.asList(values)));\n    }\n\n    /**\n     * Creates a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the index of the first clustering column\n     * @param values the in values\n     * @return a new <code>MultiColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    @SafeVarargs\n    private static Restriction newMultiIN(TableMetadata tableMetadata, int firstIndex, List<ByteBuffer>... values)\n    {\n        List<ColumnMetadata> columns = new ArrayList<>();\n        List<AbstractType<?>> types = new ArrayList<>();\n\n        for (int i = 0; i < values[0].size(); i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, firstIndex + i);\n            columns.add(column);\n            types.add(column.type);\n        }\n\n        TupleType tupleType = new TupleType(types);\n\n        List<Term> terms = new ArrayList<>(values.length);\n        for (int i = 0; i < values.length; i++)\n        {\n            terms.add(new MultiElements.Value(tupleType, values[i]));\n        }\n        return new MultiColumnRestriction.INRestriction(columns, Terms.of(terms));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param values the in values\n     * @return a new <code>SingleColumnRestriction.IN<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleIN(TableMetadata tableMetadata, int index, ByteBuffer... values)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.INRestriction(columnDef, toTerms(values));\n    }\n\n    /**\n     * Returns the clustering <code>ColumnMetadata<\/code> for the specified position.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @return the clustering <code>ColumnMetadata<\/code> for the specified position.\n     */\n    private static ColumnMetadata getClusteringColumnDefinition(TableMetadata tableMetadata, int index)\n    {\n        return tableMetadata.clusteringColumns().get(index);\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param index the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param value the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newSingleSlice(TableMetadata tableMetadata, int index, Bound bound, boolean inclusive, ByteBuffer value)\n    {\n        ColumnMetadata columnDef = getClusteringColumnDefinition(tableMetadata, index);\n        return new SingleColumnRestriction.SliceRestriction(columnDef, bound, inclusive, toTerm(value));\n    }\n\n    /**\n     * Creates a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column.\n     *\n     * @param tableMetadata the column family meta data\n     * @param firstIndex the clustering column index\n     * @param bound the slice bound\n     * @param inclusive <code>true<\/code> if the bound is inclusive\n     * @param values the bound value\n     * @return a new <code>SingleColumnRestriction.Slice<\/code> instance for the specified clustering column\n     */\n    private static Restriction newMultiSlice(TableMetadata tableMetadata, int firstIndex, Bound bound, boolean inclusive, ByteBuffer... values)\n    {\n        List<ColumnMetadata> columnMetadatas = new ArrayList<>(values.length);\n        List<AbstractType<?>> types = new ArrayList<>(values.length);\n        for (int i = 0; i < values.length; i++)\n        {\n            ColumnMetadata column = getClusteringColumnDefinition(tableMetadata, i + firstIndex);\n            columnMetadatas.add(column);\n            types.add(column.type);\n        }\n        TupleType type = new TupleType(types);\n        return new MultiColumnRestriction.SliceRestriction(columnMetadatas, bound, inclusive, new MultiElements.Value(type, Arrays.asList(values)));\n    }\n\n    /**\n     * Converts the specified value into a term.\n     *\n     * @param value the value to convert.\n     * @return the term corresponding to the specified value.\n     */\n    private static Term toTerm(ByteBuffer value)\n    {\n        return new Constants.Value(value);\n    }\n\n    /**\n     * Converts the specified values into a <code>List<\/code> of terms.\n     *\n     * @param values the values to convert.\n     * @return a <code>List<\/code> of terms corresponding to the specified values.\n     */\n    private static Terms toTerms(ByteBuffer... values)\n    {\n        List<Term> terms = new ArrayList<>();\n        for (ByteBuffer value : values)\n            terms.add(toTerm(value));\n        return Terms.of(terms);\n    }\n\n    private static <T> T get(SortedSet<T> set, int i)\n    {\n        return Iterables.get(set, i);\n    }\n    \n    private static ClusteringColumnRestrictions restrictions(TableMetadata table, Restriction... restrictions)\n    {\n        ClusteringColumnRestrictions clusteringColumnRestrictions = new ClusteringColumnRestrictions(table, false);\n        for (Restriction restriction : restrictions)\n            clusteringColumnRestrictions = clusteringColumnRestrictions.mergeWith(restriction, null);\n        return clusteringColumnRestrictions;\n    }\n\n    private enum Sort\n    {\n        ASC,\n        DESC;\n    }\n}\n","lineNo":1793}
{"Smelly Sample":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.schema;\n\nimport java.math.BigInteger;\nimport java.nio.ByteBuffer;\nimport java.util.Arrays;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.BooleanType;\nimport org.apache.cassandra.db.marshal.CompositeType;\nimport org.apache.cassandra.db.marshal.FloatType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.IntegerType;\nimport org.apache.cassandra.db.marshal.TupleType;\nimport org.apache.cassandra.db.marshal.UTF8Type;\n\nimport static org.junit.Assert.assertEquals;\n\npublic class TableMetadataTest\n{\n    @Test\n    public void testPartitionKeyAsCQLLiteral()\n    {\n        String keyspaceName = \"keyspace\";\n        String tableName = \"table\";\n\n        // composite type\n        CompositeType type1 = CompositeType.getInstance(UTF8Type.instance, UTF8Type.instance, UTF8Type.instance);\n        TableMetadata metadata1 = TableMetadata.builder(keyspaceName, tableName)\n                                               .addPartitionKeyColumn(\"key\", type1)\n                                               .offline()\n                                               .build();\n        assertEquals(\"('test:', 'composite!', 'type)')\",\n                     metadata1.partitionKeyAsCQLLiteral(type1.decompose(\"test:\", \"composite!\", \"type)\")));\n\n        // composite type with tuple\n        CompositeType type2 = CompositeType.getInstance(new TupleType(Arrays.asList(FloatType.instance, UTF8Type.instance)),\n                                                        IntegerType.instance);\n        TableMetadata metadata2 = TableMetadata.builder(keyspaceName, tableName)\n                                               .addPartitionKeyColumn(\"key\", type2)\n                                               .offline()\n                                               .build();\n        ByteBuffer tupleValue = TupleType.buildValue(new ByteBuffer[]{ FloatType.instance.decompose(0.33f),\n                                                                       UTF8Type.instance.decompose(\"tuple test\") });\n        assertEquals(\"((0.33, 'tuple test'), 10)\",\n                     metadata2.partitionKeyAsCQLLiteral(type2.decompose(tupleValue, BigInteger.valueOf(10))));\n\n        // plain type\n        TableMetadata metadata3 = TableMetadata.builder(keyspaceName, tableName)\n                                               .offline()\n                                               .addPartitionKeyColumn(\"key\", UTF8Type.instance).build();\n        assertEquals(\"'non-composite test'\",\n                     metadata3.partitionKeyAsCQLLiteral(UTF8Type.instance.decompose(\"non-composite test\")));\n    }\n\n    @Test\n    public void testPrimaryKeyAsCQLLiteral()\n    {\n        String keyspaceName = \"keyspace\";\n        String tableName = \"table\";\n\n        TableMetadata metadata;\n\n        // one partition key column, no clustering key\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"key\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"'Test'\", metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"Test\"), Clustering.EMPTY));\n\n        // two partition key columns, no clustering key\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"k1\", UTF8Type.instance)\n                                .addPartitionKeyColumn(\"k2\", Int32Type.instance)\n                                .build();\n        assertEquals(\"('Test', -12)\",\n                     metadata.primaryKeyAsCQLLiteral(CompositeType.getInstance(UTF8Type.instance, Int32Type.instance)\n                                                                  .decompose(\"Test\", -12), Clustering.EMPTY));\n\n        // one partition key column, one clustering key column\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"key\", UTF8Type.instance)\n                                .addClusteringColumn(\"clustering\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"('k', 'Cluster')\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"),\n                                                     Clustering.make(UTF8Type.instance.decompose(\"Cluster\"))));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.EMPTY));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.STATIC_CLUSTERING));\n\n        // one partition key column, two clustering key columns\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"key\", UTF8Type.instance)\n                                .addClusteringColumn(\"c1\", UTF8Type.instance)\n                                .addClusteringColumn(\"c2\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"('k', 'c1', 'c2')\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"),\n                                                     Clustering.make(UTF8Type.instance.decompose(\"c1\"),\n                                                                     UTF8Type.instance.decompose(\"c2\"))));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.EMPTY));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.STATIC_CLUSTERING));\n\n        // two partition key columns, two clustering key columns\n        CompositeType composite = CompositeType.getInstance(Int32Type.instance, BooleanType.instance);\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"k1\", Int32Type.instance)\n                                .addPartitionKeyColumn(\"k2\", BooleanType.instance)\n                                .addClusteringColumn(\"c1\", UTF8Type.instance)\n                                .addClusteringColumn(\"c2\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"(0, true, 'Cluster_1', 'Cluster_2')\",\n                     metadata.primaryKeyAsCQLLiteral(composite.decompose(0, true),\n                                                     Clustering.make(UTF8Type.instance.decompose(\"Cluster_1\"),\n                                                                     UTF8Type.instance.decompose(\"Cluster_2\"))));\n        assertEquals(\"(1, true)\",\n                     metadata.primaryKeyAsCQLLiteral(composite.decompose(1, true), Clustering.EMPTY));\n        assertEquals(\"(2, true)\",\n                     metadata.primaryKeyAsCQLLiteral(composite.decompose(2, true), Clustering.STATIC_CLUSTERING));\n    }\n}\n","Method after Refactoring":"/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.cassandra.schema;\n\nimport java.math.BigInteger;\nimport java.nio.ByteBuffer;\nimport java.util.Arrays;\n\nimport org.junit.Test;\n\nimport org.apache.cassandra.db.Clustering;\nimport org.apache.cassandra.db.marshal.BooleanType;\nimport org.apache.cassandra.db.marshal.CompositeType;\nimport org.apache.cassandra.db.marshal.FloatType;\nimport org.apache.cassandra.db.marshal.Int32Type;\nimport org.apache.cassandra.db.marshal.IntegerType;\nimport org.apache.cassandra.db.marshal.TupleType;\nimport org.apache.cassandra.db.marshal.UTF8Type;\n\nimport static org.junit.Assert.assertEquals;\n\npublic class TableMetadataTest\n{\n    @Test\n    public void testPartitionKeyAsCQLLiteral()\n    {\n        String keyspaceName = \"keyspace\";\n        String tableName = \"table\";\n\n        // composite type\n        CompositeType type1 = CompositeType.getInstance(UTF8Type.instance, UTF8Type.instance, UTF8Type.instance);\n        TableMetadata metadata1 = TableMetadata.builder(keyspaceName, tableName)\n                                               .addPartitionKeyColumn(\"key\", type1)\n                                               .offline()\n                                               .build();\n        assertEquals(\"('test:', 'composite!', 'type)')\",\n                     metadata1.partitionKeyAsCQLLiteral(type1.decompose(\"test:\", \"composite!\", \"type)\")));\n\n        // composite type with tuple\n        TupleType tupleType = new TupleType(Arrays.asList(FloatType.instance, UTF8Type.instance));\n        CompositeType type2 = CompositeType.getInstance(tupleType,\n                                                        IntegerType.instance);\n        TableMetadata metadata2 = TableMetadata.builder(keyspaceName, tableName)\n                                               .addPartitionKeyColumn(\"key\", type2)\n                                               .offline()\n                                               .build();\n        ByteBuffer tupleValue = tupleType.pack(FloatType.instance.decompose(0.33f),\n                                               UTF8Type.instance.decompose(\"tuple test\"));\n        assertEquals(\"((0.33, 'tuple test'), 10)\",\n                     metadata2.partitionKeyAsCQLLiteral(type2.decompose(tupleValue, BigInteger.valueOf(10))));\n\n        // plain type\n        TableMetadata metadata3 = TableMetadata.builder(keyspaceName, tableName)\n                                               .offline()\n                                               .addPartitionKeyColumn(\"key\", UTF8Type.instance).build();\n        assertEquals(\"'non-composite test'\",\n                     metadata3.partitionKeyAsCQLLiteral(UTF8Type.instance.decompose(\"non-composite test\")));\n    }\n\n    @Test\n    public void testPrimaryKeyAsCQLLiteral()\n    {\n        String keyspaceName = \"keyspace\";\n        String tableName = \"table\";\n\n        TableMetadata metadata;\n\n        // one partition key column, no clustering key\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"key\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"'Test'\", metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"Test\"), Clustering.EMPTY));\n\n        // two partition key columns, no clustering key\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"k1\", UTF8Type.instance)\n                                .addPartitionKeyColumn(\"k2\", Int32Type.instance)\n                                .build();\n        assertEquals(\"('Test', -12)\",\n                     metadata.primaryKeyAsCQLLiteral(CompositeType.getInstance(UTF8Type.instance, Int32Type.instance)\n                                                                  .decompose(\"Test\", -12), Clustering.EMPTY));\n\n        // one partition key column, one clustering key column\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"key\", UTF8Type.instance)\n                                .addClusteringColumn(\"clustering\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"('k', 'Cluster')\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"),\n                                                     Clustering.make(UTF8Type.instance.decompose(\"Cluster\"))));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.EMPTY));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.STATIC_CLUSTERING));\n\n        // one partition key column, two clustering key columns\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"key\", UTF8Type.instance)\n                                .addClusteringColumn(\"c1\", UTF8Type.instance)\n                                .addClusteringColumn(\"c2\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"('k', 'c1', 'c2')\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"),\n                                                     Clustering.make(UTF8Type.instance.decompose(\"c1\"),\n                                                                     UTF8Type.instance.decompose(\"c2\"))));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.EMPTY));\n        assertEquals(\"'k'\",\n                     metadata.primaryKeyAsCQLLiteral(UTF8Type.instance.decompose(\"k\"), Clustering.STATIC_CLUSTERING));\n\n        // two partition key columns, two clustering key columns\n        CompositeType composite = CompositeType.getInstance(Int32Type.instance, BooleanType.instance);\n        metadata = TableMetadata.builder(keyspaceName, tableName)\n                                .offline()\n                                .addPartitionKeyColumn(\"k1\", Int32Type.instance)\n                                .addPartitionKeyColumn(\"k2\", BooleanType.instance)\n                                .addClusteringColumn(\"c1\", UTF8Type.instance)\n                                .addClusteringColumn(\"c2\", UTF8Type.instance)\n                                .build();\n        assertEquals(\"(0, true, 'Cluster_1', 'Cluster_2')\",\n                     metadata.primaryKeyAsCQLLiteral(composite.decompose(0, true),\n                                                     Clustering.make(UTF8Type.instance.decompose(\"Cluster_1\"),\n                                                                     UTF8Type.instance.decompose(\"Cluster_2\"))));\n        assertEquals(\"(1, true)\",\n                     metadata.primaryKeyAsCQLLiteral(composite.decompose(1, true), Clustering.EMPTY));\n        assertEquals(\"(2, true)\",\n                     metadata.primaryKeyAsCQLLiteral(composite.decompose(2, true), Clustering.STATIC_CLUSTERING));\n    }\n}\n","lineNo":56}
